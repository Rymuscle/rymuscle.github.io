{"meta":{"title":"Lant's Blog","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Service Mesh","slug":"2018-07-10-ServiceMesh","date":"2018-07-10T02:26:27.000Z","updated":"2018-07-10T03:25:32.000Z","comments":true,"path":"2018/07/10/2018-07-10-ServiceMesh/","link":"","permalink":"http://blog.renyimin.com/2018/07/10/2018-07-10-ServiceMesh/","excerpt":"","text":"","categories":[{"name":"服务","slug":"服务","permalink":"http://blog.renyimin.com/categories/服务/"}],"tags":[{"name":"服务","slug":"服务","permalink":"http://blog.renyimin.com/tags/服务/"}]},{"title":"32. RabbitMQ集群 -- 单机","slug":"rabbitmq/2018-06-26-rabbitmq-32","date":"2018-06-26T07:28:16.000Z","updated":"2018-07-18T02:27:21.000Z","comments":true,"path":"2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","link":"","permalink":"http://blog.renyimin.com/2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","excerpt":"","text":"前言当你需要在生产环境中部署RabbitMQ时, 需要注意的是, 单实例在生产环境虽然部署起来很容易, 但是当你的rabbitmq服务器遇到内存崩溃或者断电的情况时, 这款高性能的产品就要成为你的耻辱了, 将会为你造成极大的问题!因此你需要将你的RabbitMQ变成高可用的才行; 内建集群简介 RabbitMQ最优秀的功能之一就是其内建集群, 这款消息队列中间件产品本身是基于Erlang编写, Erlang语言天生具备分布式特性(通过同步Erlang集群各节点的magic cookie来实现), 因此, RabbitMQ天然支持Clustering, 这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。 RabbitMQ内建集群用来完成两个目标: 允许生产者和消费者在RabbitMQ节点崩溃的情况下继续运行;你可以失去一个RabbitMQ节点, 同时客户端可以重新连接到集群中的任何其他节点并继续生产或者消费消息, 就像什么都没有发生一样; 通过增加更多的节点来线性扩展消息吞吐量;如果RabbitMQ正疲于应对庞大的消息通信量的话, 那么线性地增加更多的节点则会增加更多性能; 了解内部元数据RabbitMQ内部会始终同步四种类型的内部元数据: 队列元数据: 队列名称和它的属性 (是否可持久化, 是否自动删除); 交换器元数据: 交换器名称、类型和属性 (可持久化等); 绑定元数据: 一张简单的表格展示了如何将消息路由到队列; vhost元数据: 为vhost内的队列、交换器和绑定提供命名空间和安全属性; 内存or磁盘节点每个Rabbitmq节点, 不管是单一节点系统或者是庞大集群的一部分, 要么是内存节点(RAM node), 要么是磁盘节点(disk node): 内存节点将所有的队列、交换器、绑定、用户、权限和vhost的元数据定义都仅存储在内存中; 而磁盘节点则将元数据存储在磁盘中; 非集群单一节点在单一节点的非集群环境中, RabbitMQ默认会将元数据都存放在内存中;但是, 会将标记为可持久化的队列和交换器(以及它们的绑定)存储到硬盘上, 存储到硬盘上可以确保队列和交换器在重启Rabbitmq节点后重新被创建; 单节点类型只允许磁盘类型的节点? 集群节点而当你引入Rabbitmq集群后, RabbitMQ需要追踪的元数据类型包括: 集群节点位置, 以及节点与已记录的其他类型的元数据的关系, 在这里, 集群对元数据的存储提供了选择: 将元数据存储到磁盘上 (集群中创建节点时的默认设置) 或者仅存储到RAM内存中 集群的类型Rabbit集群模式大概分为三种: 普通模式、镜像模式 普通模式 普通模式(也就是默认的集群模式), 对于该集群模式, 当你将多个节点组合成集群后, 需要注意的是: 不是每一个节点都有所有队列的完全拷贝 在非集群的单一节点中, 所有关于队列的信息(元数据、状态、内容)都完全存储在该节点上; 但是如果在普通集群模式下创建队列的话, 集群只会在当前节点而不是所有节点上创建完整的队列信息(元数据、状态、内容); 而其他非所有者的节点, 只知道队列的元数据和指向该队列存在的哪个节点的指针; 因此当集群中队列所有者的节点崩溃时, 该节点的队列和关联的绑定就都消失了, 并且附加在这些队列上的消费者就会无法获取其订阅的信息, 并且生产者也无法将匹配该队列绑定信息的消息发送到队列中; 接下来需要了解的一个问题是: 为什么在默认的集群模式下, RabbitMQ不将队列内容和状态复制到所有的节点上? 其实有两个原因 存储空间: 如果每个集群节点都拥有所有Queue的完全数据拷贝, 那么每个节点的存储空间会非常大, 集群的消息积压能力会非常弱(无法通过集群节点的扩容提高消息积压能力); 性能: 消息的发布者需要将消息复制到每一个集群节点, 对于持久化消息来说, 网络和磁盘的负载都会明显增加, 最终只能保持集群性能平稳(甚至更糟); 所以, 通过设置集群中的唯一节点来负责特定队列, 只有该负责节点才会因队列消息而遭受磁盘活动的影响所有其他节点需要将接受到的该队列的消息传递给该队列的所有者节点, 因此, 往RabbitMQ集群添加更多的节点意味着你将拥有更多的节点来传播队列, 这些新增节点为你带来了性能的提升; 但是有人可能会想: 是否可以让消费者重新连接到集群上, 这样不就可以重新创建队列了? 但需要注意的是: 因为一般如果我们的队列设置的是持久化的, 而在该队列的主节点挂掉之后, 重新连接到队列时, 一般也不会修改队列的持久化属性; 这就需要注意一个问题, 仅当你之前创建的队列为非持久化时, 你才可以重新创建该队列为持久化, 因为这是为了保证你之前的持久化队列节点在重新被恢复启动后, 其中的消息还会被恢复, 而如果你创建一个新的持久化队列, 如果覆盖之前的持久化队列, 那消息不就丢了!!所以如果之前是持久化队列, 而且还是以持久化的方式创建该队列, 集群就会报404 NOT FOUND错误 待尝试~~ 本机配置集群 在开始配置集群前, 首先要确保现存的Rabbitmq没有运行, 因此需要关闭节点 (本机为mac, 关闭操作如下) 123renyimindeMacBook-Pro:~ renyimin$ brew services stop rabbitmqStopping `rabbitmq`... (might take a while)==&gt; Successfully stopped `rabbitmq` (label: homebrew.mxcl.rabbitmq) 可以发现一个问题, 就是停止Rabbitmq服务之后, 貌似 RabbitMQ Management 的Web UI界面还是可以正常打开运行; 所以正确的关闭节点貌似是 rabbitmqctl stop 开始配置集群前需要注意: 通常来讲, 使用 rabbitmq-server 命令启动节点之后就大功告成了, 但是如果不用额外参数的话, 该命令会使用默认的节点名称 rabbit 和监听端口 5672;所以如果你想用该命令在一台机器上同时启动5个节点的话, 那么第2，3，4，5个节点都会因为节点名称和端口号冲突而导致启动失败; 因此, 为了在本机正常启动5个节点, 可以在每次调用 rabbitmq-server前, 通过设置环境变量 RABBITMQ_NODENAME, RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口号!在此处做实验时, 将会采用 rabbit, rabbit_1,…,4 命名节点名; 端口号为5612，5613，…5615 注意, 到目前为止, 虽然尚未谈论RabbitMQ的插件, 不过你有可能已经启用了一部分插件了; 如果确实如此的话, 你需要在启动集群节点前将插件禁用!这是因为像 RabbitMQ Management 这样的插件会监听专门的端口来提供服务(例如 Management 插件的 Web UI), 目前还没讲到如何设置插件监听不同的端口, 所以当第二个节点和之后的节点启动了它们的插件后, 就会和第一个启动节点的c插件相冲突, 然后节点就都崩溃了;可以先不禁用插件, 这样在启动多个节点时, 可以根据报错一个个关闭插件也可以; (rabbitmq-plugins disable 插件名) RabbitMQ集群的搭建 启动节点 注意: 启动的时候, 直接加上 -detached 参数的话, 可能会有些报错信息比如 error : cannot_delete_plugins_expand_dir, 这就是因为需要使用root权限才可以, 你可以使用 pa aux | grep rabbitmq 查看是否三个进程都成功启动了 注意: 启动时, 貌似不能像书上那样, RABBITMQ_NODENAME 只设置节点名, 最好设置上节点host 如下: 1234567renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit_1@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5674 RABBITMQ_NODENAME=rabbit_2@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ 然后可以查看个节点状态 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost status 现在启动了三个节点 rabbit, rabbit_1, rabbit_2, 并且每个节点都会有系统的主机名在@后; 但是每个节点仍然是独立节点, 拥有自己的元数据, 并且不知道其他节点的存在; 集群中的第一个节点rabbit,将初始元数据带入集群, 并且无需被告知加入; 而第二个和之后的节点, 将加入第一个节点rabbit, 并获取rabbit节点的元数据; 要将rabbit_1和rabbit_2节点加入rabbit, 要停止该Erlang节点上运行的rabbitmq应用程序, 并重设它们的元数据, 这样它们才可以被加入rabbit节点并且获取rabbit节点的元数据; 可以使用 rabbitmqctl 来完成这些工作 停止rabbit_1节点上的应用程序12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost stop_appStopping rabbit application on node rabbit_1@renyimindeMacBook-Pro ... - 重设rabbit_1节点的元数据和状态为清空状态 123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost resetResetting node rabbit_1@renyimindeMacBook-Pro ...``` - 这样你就准备好了一个 停止运行的并且清空了的 rabbit 应用, 现在可以准备好将其加入到集群中的第一个节点rabbit中:注意书上的 `cluster` 命令好像已经不用了, 换成了 `join_cluster` renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost join_cluster rabbit@localhost Clustering node rabbit_1@localhost with rabbit@localhost renyimindeMacBook-Pro:~ renyimin$ 12- 最后, 可以重启第二个节点的应用程序 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_app Starting node rabbit_1@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ 12- 节点rabbit_2加入集群的步骤同上, 具体操作如下: renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_app Starting node rabbit_1@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_app Stopping rabbit application on node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost reset Resetting node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost join_cluster rabbit@localhost Clustering node rabbit_2@localhost with rabbit@localhost renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_app Starting node rabbit_2@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ 124. 查看集群状态, 可以在任意一个节点通过 `` 进行查看 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_status Cluster status of node rabbit@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_status Cluster status of node rabbit_1@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit@localhost,[]}, {rabbit_1@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost cluster_status Cluster status of node rabbit_2@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit_2@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ 12345675. 注意, 上面使用比较多的 `rabbitmqctl` 命令的关键参数是 `-n`, 这会告诉rabbitmqctl命令, 你想在指定节点而非默认节点`rabbit@`上执行命令;6. 注意: 记住, Erlang节点间通过Erlang cookie的方式来允许互相通信。因为rabbitmqctl使用Erlang OPT通信机制来和Rabbit节点通信, 运行rabbitmqctl的机器和所要连接的Rabbit节点必须使用相同的Erlang cookie, 否则你会得到一个错误; 当然, 上面的集群是在本机做伪集群, Erlang cookie 自然也都是一致的!7. 将节点从集群中删除 `forget_cluster_node` renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_1@localhost Removing node rabbit_1@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_2@localhost Removing node rabbit_2@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_3@localhost Removing node rabbit_3@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ 1234567 ## [RabbitMQ集群节点类型](http://www.rabbitmq.com/clustering.html#change-type)1. 可以在将节点加入集群时, 设定节点的类型 ([参考](http://www.rabbitmq.com/clustering.html#creating-ram)) 比如 `rabbitmqctl -n rabbit_3@localhost join_cluster --ram rabbit@localhost` 2. 之前已经通过 `rabbitmqctl cluster_status` 查看了集群的状态, 里面比较重要的是 `nodes` 部分 - 下面告诉你有三个节点加入了集群, 并且三个节点都是 disc 磁盘节点! [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit_2@localhost,[]}]}] 12345 - running_nodes 部分告诉你集群中的哪些节点正在运行; 3. 现在你可以连接到这三个running_nodes中的任何一个, 并且开始创建队列, 发布消息或者执行任何其他AMQP任务; 4. 你也可以对节点类型进行修改, 如下将rabbit_2节点类型修改为内存节点 (注意: 修改节点类型, 需要先停止节点应用) renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_app Stopping rabbit application on node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost change_cluster_node_type ram Turning rabbit_2@localhost into a ram node renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_app Starting node rabbit_2@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_status Cluster status of node rabbit_1@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost]}, {ram,[rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit@localhost,[]}, {rabbit_1@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ ``` 测试","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"22. Priority Queues - queue(x-max-priority) + message(priority)","slug":"rabbitmq/2018-06-19-rabbitmq-22","date":"2018-06-19T11:21:32.000Z","updated":"2018-06-30T05:52:46.000Z","comments":true,"path":"2018/06/19/rabbitmq/2018-06-19-rabbitmq-22/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/rabbitmq/2018-06-19-rabbitmq-22/","excerpt":"","text":"Priority Queues 官网文档: https://www.rabbitmq.com/priority.html 译文 从3.5.0版本开始, RabbitMQ已经在核心中实现了 优先级队列任何队列都可以使用客户端提供的可选参数 $arguments 变成优先的 (但与其他功能不同, 此处可以使用可选参数 $arguments, 但是不支持使用 策略)该实现支持有限的优先级数量: 255, 建议1至10之间的值; 使用客户端提供的可选参数 $arguments要声明优先级队列, 请使用queue的可选参数 $arguments 的 x-max-priority 选项, 该参数应该是1到255之间的正整数, 表示队列应该支持的最大优先级; AMQP 0-9-1规范对于优先级的工作方式有些模糊, 它认为所有的队列中必须至少有两个是支持优先级的, 并且可以支持多达10个, 它没有定义 “没有优先级属性的消息该如何被处理”; 与AMQP 0-9-1规格相比, 默认情况下, RabbitMQ队列不支持优先级, 在创建优先级队列时, 您可以根据需要指定任意数量的优先级别, 不过需要注意:队列的每个优先级会有一些 内存 和 磁盘 方面的成本, 还会有额外的CPU成本, 特别是在消费时, 所以你可能不希望创建大量的优先级别; 消息的 priority 字段被定义为无符号字节, 所以实际上优先级应该在0到255之间;没有 priority 属性的消息被视为优先级为0, 优先级高于队列最大值的消息被视为以最高优先级发布; 资源使用注意事项如果需要优先级队列, 建议使用1到10之间的级别, 目前使用更多优先级将消耗更多资源(Erlang进程); 与消费者互动理解消费者在处理优先级队列时的工作方式非常重要, 默认情况下, 消费者在做任何应答之前, 可能已经被发送了大量的消息;因此, 如果一个饥饿的消费者连接一个消息稍后才会被发布到的空队列, 这样, 消息可能不会在队列中等待任何时间, 在这种情况下, 优先队列将不会有机会对它们做优先级;在大多数情况下, 你会希望在消费者手动确认模式下使用 basic.qos, 以限制可以随时发送的消息数量, 从而允许优先化消息; 其他功能的互动通常, 优先级队列具有标准RabbitMQ队列的所有功能: 它们支持持久性, 分页, 镜像等; 有几点需要注意的互动:应该过期的消息仍然只会从队列的头部过期, 这些消息将永远不会传递, 但它们将显示在队列统计信息中;设置了 max-length 的Queues, 通常情况下, 将从队列头部丢弃消息以强制执行限制, 这意味着可能会丢弃更高优先级的消息, 为低优先级的消息让路, 这可能不是你所期望的; 为什么 策略 定义是不可能的为队列定义可选参数最方便的方法是通过策略, 策略是配置队列长度限制, TTL等的推荐方式;但是, 策略不能用于配置优先级, 因为策略是动态的, 并且可以在声明队列后进行更改, 优先级队列在队列声明之后永远不会改变它们支持的优先级的数量, 因此策略不是一个安全的选项; Rabbit中的消息可以按 发送顺序, 优先级高低 被依次消费 默认不设置 队列,消息 优先级 的情况下, 如果只有一个消费者, 消息会按照其发送顺序被依次消费 如果设置了 队列,消息 优先级, 如果只有一个消费者, 消息会按照 全部按照发送顺序, 发送顺序+优先级顺序, 全部按照优先级顺序 3种顺序依次被消费;下面会有案例展示; 要想使用优先级, 需要使用 队列的 $arguments的 x-max-priority 选项 和 消息的 $arguments的 priority 选项; 分析当有多个consumer时, 是无法保证消息按发送顺序或者优先级顺序被消费的, 因为每个consumer视自身能力, 都有自己的消费速度, 而且也不一定稳定; 当queue只有一个consumer时, 如果producer先发送msg到queue, 然后再启动consumer, 此时和consumer的Qos是否设置或者设置大小无关, msg会按照优先级由高到低依次输出 (不会按照producer的发送顺序输出) 像之前译文中 与消费者互动 中提到的, 这是因为 msg 有足够的时间(消费者启动后连接到server的时间), 在Rabbit Server中按照优先级做好排序; 当queue只有一个consumer时, 如果先启动consumer, 然后producer再发送msg到queue 如果没有设置Qos, 即便consumer处理每条消息的速度很慢, 但是由于运行producer之后, 消息会在第一时间被尽可能全量发送到consumer, 所以queue中貌似没什么时间对msg按照优先级进行排序, 所以consumer拿到的msg只是按照发送顺序排列的消息;先启动consumer的情况下, 消费者是处于饥饿状态; 如果设置了Qos的prefetch_count=1, 先启动consumer:理论上, 第一个msg是按照发送顺序, 但是之后的9个msg都会按照优先级顺序;但实际测试发现, 有时候msg还会全部按照优先级顺序来, 这是个比较疑惑的问题, 后来通过观察WebUI管理界面可以发现, 在启动消费者之后, 并不一定队列可以立马识别到该consumer (因为队列的 Consumers 这列显示还是0), 如果你此时运行了producer, 那么相当于先运行了producer, 然后再启动了消费者, 所以msg将会全部按照优先级循序被消费; 所以要为了确保消费者被先启动, 最好确保队列的Consumers这列显示为1, 表示消费者已经连接上来了;并且即使你prefetch_count=1的同时设置了 sleep(5), 还是会出现上面的问题(也是由于虽然你先执行了consumer, 但其实producer先被执行的原因)! 如果设置了Qos, 但是设置的prefetch_count&gt;1(比如为2)理论上也是前两个msg按照发送顺序, 而后面8个按照优先级顺序;实际测试发现, 如果出现上面的意外, 也会导致10条消息全部按照优先级顺序被消费; 小结 先运行生产者生产消息, 然后再启动消费者, 消息会有时间按照优先级在queue中做好排序, 然后发送给消费者; (这种条件类似消费能力不足的情况, 类似于consumer消费过慢并产生很多unack, 导致queue中的msg有时间按照优先级排序) 先启动消费者, 然后运行生产者生产消息, 消息会根据消费者设置的Qos, 优先会有prefetch_count数量的消费者被直接发送到consumer, 这些msg会按照发送先后顺序被消费, 之后的剩余消费者则会按照优先级顺序被消费; 总之, 消息想要按照设置的优先级别来被消费, 在将消息发出去之前, Server必须有足够的时间对Queue中的消息按照优先级高低做好排序; 所以消息优先级貌似对应用场景比较苛刻(消费者貌似要唯一, 而且需要消费者阻塞产生unacked之后, server中的消息貌似才有时间排序), 所以这个优先级感觉挺鸡肋, 如果需要消息有顺序, 还不如就 保证消费者唯一, 然后按照发送顺序消费即可! 测试 针对上面的分析进行测试 在生产者中准备好10条优先级为0-10的消息, 然后乱序发送, 代码参考此处 消费者使用三种Qos做测试 1. 默认Qos, 不做限制 2. Qos设置为1, 3. Qos设置为2, 代码参考此处 先运行生产者, 然后启动消费者, 会发现乱序的消息会按照优先级高低依次输出 而如果先启动消费者, 之后再运行生产者, 你会发现, 由于会有prefetch_count的msg被发送给消费者, 所以这部分消息会按照发送先后顺序被消费, 而其余消息会按照优先级高低依次输出;如果没有设置Qos, 或者Qos设置的数量大于消息条数, 那么msg都会按照发送的先后顺序被消费, 不会按照优先级顺序被消费; 如果消息设置了选项 priority, 而队列没有设置 x-max-priority, 效果会怎样? 1234567renyimindeMacBook-Pro:Rabbit renyimin$ php artisan msgPriorityConsumerIn AMQPChannel.php line 188: PRECONDITION_FAILED - inequivalent arg &apos;x-max-priority&apos; for queue &apos;msgPrioityQueue&apos; in vhost &apos;/&apos;: received none but current is the value &apos;10&apos; of type &apos;signedint&apos; renyimindeMacBook-Pro:Rabbit renyimin$ 队列设置了优先级选项 x-max-priority, 如果消息不设置优先级选项 priority, 效果会怎样? 无论设置适当的Qos先启动消费者, 还是先运行生产者生产消息, msg都是按照发送顺序被消费的! 如果有多个消费者, 消息被发送给不同的消费者, 这样还会按照优先级顺序进行么? 多个消费者是无法保证Msg被消费的顺序的; 集群模式下, 发送顺序和优先级顺序如何保证? ~~ 待续","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"21. Consumer Priorities 消费者优先级","slug":"rabbitmq/2018-06-15-rabbitmq-21","date":"2018-06-15T05:04:26.000Z","updated":"2018-06-20T04:47:52.000Z","comments":true,"path":"2018/06/15/rabbitmq/2018-06-15-rabbitmq-21/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/rabbitmq/2018-06-15-rabbitmq-21/","excerpt":"","text":"Consumer Priorities简介 消费者优先级允许你确保高优先级的消费者在活跃时接收消息, 并且消息只有在高优先级消费者阻塞时才会转向低优先级的消费者; 通常, 连接到队列的活跃消费者, 是以循环方式接收来自它的消息; 当使用 “消费者优先级” 时, 如果存在多个活跃的消费者具有相同的高优先级, 消息也会被循环传送给它们; 定义活跃消费者 活跃的消费者是可以无需等待就能收到消息的消费者. 假如一个消费者无法接收消息, 它就会变成阻塞状态: 因为它的 channel 在发布 basic.qos 之后已达到 unack消息的最大数量, 或者仅仅是因为网络拥塞; 当消费者的 “优先级” 被使用时, 你可以期望你的最高优先级的消费者接收所有的消息, 直到它们被阻塞, 然后较低优先级的消费者才将开始接收一些消息; 理解RabbitMQ仍然会优先传递消息是很重要的: 如果有一个活跃着的低优先级消费者已经准备好了, Rabbitmq是不会等待高优先级的阻塞消费者变成非阻塞的(它会将消息发送给已经准备好的低优先级的活跃消费者); 使用消费者 优先级别 将 basic.consume() 方法中的 $arguments 参数的 x-priority 属性设置为整数值, 未指定值的消费者优先级为0, 更大的数字表示更高的优先级, 并且可以使用正数和负数; 注意事项: 多个消费者共同绑定同一个队列时, 可以给消费者设置优先级, 这样, 优先级高的消费者会优先拿到消息并进行处理, 除非优先级高的都处于阻塞状态(unack达到Qos设置的上限值), 否则优先级低的消费者不会拿到消息;通过运行下面示例(没有设置Qos时)可以发现, 同时启动两个消费者, 然后刷新生产者, 会发现在高优先级的消费者在默认不设置Qos时, 即不阻塞的情况下, 低优先级消费者一条消息也不会收到(没有消费结果); 接下来可以模拟高优先级的Consumer阻塞, 会发现, 高优先级的Consumer在拿到第一条消息之后, 优先级低的Consumer会立刻开始进行处理注意, 模拟消费者阻塞时, 不能只通过比如 sleep(50) 这种方式, 因为这样只是消费者的处理速度变慢, 但是消费者的预取量还是默认的(即不受限制), 所以消息还是会被发送给优先级高的消费者, 这样造成的结果就是 优先级高的消费者一直在缓慢地消费, 而优先级低的消费者一直在闲置;所以还需要给优先级高的消费者设定 Qos=1 来进行测试(设定为1是为了测试方便, 也可以设置其他值, 不过这样的话, 就会有qos所设置的数量的消息被优先推送给高优先级的consumer, 直到达到qos量之后才会阻塞), 这样才能使得优先级高的消费者在没有急时ack的情况下被阻塞, 从而让优先级低的Consumer进行消费实例运行(刷新生产者10次尝试一下)可以发现, 高优先级的消费者在拿到第一条消息后, 后面的消息都给了 低优先级的消费者, 但随着高优先级的消费者进行ack而恢复活跃状态, 又会优先拿到消息 示例可查看: 生产者 TestPriorityConsumerController.php 消费者 priorityConsumer1.php, priorityConsumer2.php 扩展https://www.rabbitmq.com/blog/2013/12/16/using-consumer-priorities-with-rabbitmq/","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"20. 消费者预取 Consumer Prefetch","slug":"rabbitmq/2018-06-13-rabbitmq-20","date":"2018-06-13T11:23:36.000Z","updated":"2018-07-18T09:30:02.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","excerpt":"","text":"Consumer Prefetch 作为限制 unack 消息数量的更自然有效的方法; AMQP 0-9-1 指定了 basic.qos 方法, 以便你在消费者进行消费时, 可以限制channel(或connection)上未确认消息的数量; 但是值得注意的是: channel 并不是理想的设定范围, 因为单个channel可能从多个队列进行消费, channel和queue需要为每个发送的消息相互协调, 以确保它们不会超出限制, 这在单台机器上会慢, 而在整个集群中使用时会非常慢; 此外, 对于许多用途, 指定适用于每个消费者的预取计数更会简单一些; 因此, RabbitMQ在 basic.qos 方法中重新定义了全局标志的含义 (在php-amqplib中basic_qos()的第三个参数a_global): 请注意, 在大多数API中, 全局标志的默认值为false; (php-amqplib的basic_qos()方法的第三个参数a_global默认也为false) 简要分析 在使用RabbitMQ时, 如果完全不配置QoS, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是采用默认方式, 将队列中的所有消息按照网络和客户端允许的速度尽快轮发到与队列绑定的consumers端; 而consumers会在本地缓存所有投递过来的messages, 这样的话, 就可能会导致 如果某个消费者的业务逻辑处理比较复杂(将会在较长时间之后才会操作完成并进行ack), 这也就导致消费慢的Consumer将会在本地堆积很多消息, 从而导致内存不足或者对其他进程造成影响 (消费者可能被撑到假死); 而其他消费能力强的Consumers, 可能已经很快地消费完成处于闲置状态, 从而造成资源浪费; 同时, 新启的消费者也无法分担已经被之前消费者缓存到其本地的消息, 所以此时即便启动更多消费者, 也无力缓解大量的unack消息积压, 让你产生疑惑; 而当你设置了Qos之后, RabbitMQ虽然也是将队列中的消息尽快轮发到Consumers中, 但是因为消费者具有的 prefetch_count 消息预取值上限, 所以RabbitMQ在轮发消息的时候, 如果发现消费者的 unack 消息达到了 prefetch_count 的值, 即使rabbitmq中有很多ready的就绪消息, 也不会给该Consumer继续投递消息了(只有消费者的 unack消息小于prefetch_count的值时, 才会继续通过轮发方式给该consumer投递ready消息), 如果此时有新的消费者加入, 它也将会拿到未投递出去的ready消息! 可以通过启动 prefetchCountConsumer1，prefetchCountConsumer2 两个消费者(prefetch_count 均为10), 然后使用下面测试中的生产者发送100条消息, 前期观察会发现队列中消息的最大 unacked 为20, 并且你会发现队列中处于ready状态的消息会每次2个的递减, 这就预示着, 每次这两个消费者只要unacked的消息书小于prefetch_count(10), Rabbitmq才会给这两个consumer各自发送一条msg; 之后如果启动了 prefetchCountConsumer3(prefetch_count为20), 此时会发现队列中消息的最大 unacked 会为40, prefetchCountConsumer3的加入会使得队列中处于ready状态的消息直接骤减20个, 最后rabbitmq中的ready消息已经为0, 每个消费者还在继续消费各自未unacked的消息, 最终消费完成后, 整个队列中的 unacked 消息为0; Qos的设置只有在开启手动ack后才会生效 (即, prefetch_count 在 no_ask=false 的情况下生效) 测试 一般情况下, 同一队列绑定的多个消费者都是处理同一个业务, 而且如果在同一台机器启动, 消费能力应该都差不多, 但也难免出现如: 消费者资源分配不均 或者 两个消费者在处理业务时所请求的服务端机器配置有差异(假设SLB后又2台配置不均的机器), 这种情况还是应该考虑进来的! 本测试比较简单, 主要测试在默认不设置Qos的情况下, 两个消费能力不同的消费者在处理消息时存在的问题之一: 由于这种情况下, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是只顾自己轮发消息, 这样就会导致消息被轮发完成后, 消费能力高的消费者可能很快消费完消息并处于闲置状态, 而消费能力低的消费者却在很慢地进行消费, 这样就造成了资源的浪费; 准备 创建消费者1 ‘qosCustomer1’ (简单打印消息内容) , 代码参考, 启动消费者 php artisan qosConsumer1 创建消费者2 ‘qosCustomer2’ (sleep 5秒, 模拟处理能力比较差) , 代码参考, 启动消费者 php artisan qosConsumer2 创建生产者一次向队列 ‘qosQueue’ 中推送10条消息 , 代码参考, 请求一次生产者 http://www.rabbit.com/testQos 注意需要先启动消费者, 再请求生产者; (如果先请求了生产者, 可能在启动第一个消费者之后, 其会迅速消费完10条消息, 这样就无法模拟效果了) 测试发现 qosCustomer1 : 迅速打印出结果(1,3,5,7,9), 然后就处于闲置状态了 qosCustomer2 : 还在缓慢打印(2,4,6,8,10) 可以看到, 如果不设置Qos, Rabbitmq会尽快将消息从队列中轮发投递出去, 不会对消费者的消费能力进行任何评估! 所以: 为了避免这种浪费资源的情况, 你可能就需要根据上一篇讲解的 prefetch_count 来针对不同消费者进行设置; 问题答疑测试 根据上面的描述, 有个疑问: 在默认不设置Qos的情况下, 既然生产者发布的消息会尽可能全部推送给消费者进程, 队列中会尽可能将消息全部推出, 缓存在消费者本地, 那当消费者断开时, 消息是如何恢复到队列中的? 或者不会恢复到队列中? 为了答疑, 下面进行测试 准备测试代码 创建消费者1 ‘prefetchCountConsumer1’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 创建消费者2 ‘prefetchCountConsumer2’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 生产者一次向队列 ‘prefetchCountQueue’ 中推送100条消息 , 代码参考 测试: 在生产者请求一次之后(http://www.rabbit.com/prefetchCount), ready : 100, unack: 0, total : 100, 表示队列中已经有100条消息已经就绪, 等待发出 运行第一个php artisan prefetchCountConsumer1之后, ready : 0, unack : 100, total : 100 (也就是说, queue中已经没有 ready状态, 即准备好待发送的消息了, 消息都传递给消费者1了) 随着消费者的缓慢消费, ready : 0, unack : 94, total : 94 () 如果模拟 挂掉第一个消费者之后, 会发现, ready : 83， unack : 0, total : 83 (也就是说消费者意外宕掉之后, 队列中的消息会重新处于就绪状态, 等待着新的消费者来消费) 再次启动消费者2 php artisan testQosConsumerPrefetchCount2之后, ready : 0, unack : 80, total : 80 (消息又会被全量发送给消费者2) 注意: 如果此时启动消费者1, 你会发现, 它是无法帮助消费者2进行消费的, 因为消息都在消费者2的本地, 所以队列中并没有 ready状态的就绪消息; 测试注意: 上述测试过程如果先启动两个消费者, 然后再发布消息进行测试, 你会发现, 由于两个消费者都设置了预取值, 而且相等, 所以消息仍然会快速轮发给这两个消费者; 如果将两个消费者的 prefetch_count 都设置为10, 那么你会发现, unack最多也就是两个消费者的prefetch_count和, 即20个 小结 消费者的unack消息数量如果未达到Qos设置的 prefetch_count 量, Rabbit不会顾及消费者的消费能力, 会尽可能将queue中的消息全部推送出去给消费者; 因此, 当你发现消费者消费缓慢, 产生大量 unack 消息时, 即便增加新的消费者, 也无法帮助之前的消费者分担消息(除非消费者1的 unack达到了 prefetch_count 限制), 只能分担队列中处于 ready 状态的消息; 除非你断开之前的消费者, 然后启动一个新的消费者, 消费者中积压的消息才会重新放入队列中 (因为之前的消费者挂掉之后, 其处理后的剩余消息在 queue中会恢复为 ready 状态) 但是注意: 新启动的这个消费者如果设置额prefetch_count不合理的话, 假设与之前消费者的 预取值 设置一样大, 它很快也会产生大量 unack 消息 所以, 在新启消费者的时候, 需要设计好 prefetch_count 的大小, 然后可以启动多个消费者来共同进行消费; 扩展 rabbitmq对 basic.qos 信令的处理 首先, basic.qos 是针对 channel 进行设置的, 也就是说只有在channel建立之后才能发送basic.qos信令; RabbitMQ只支持通道级的预取计数, 而不是connection级的 或者 基于大小的预取;预取 在rabbitmq的实现中, 每个channel都对应会有一个rabbit_limiter进程, 当收到basic.qos信令后, 在rabbit_limiter进程中记录信令中prefetch_count的值, 同时记录的还有该channel未ack的消息个数; 在php-amqplib中, 可以使用 channel 的 basic_qos() 方法来进行控制, basic_qos() 有三个参数: prefetch_size : 限制预取的消息大小的参数, rabbitmq暂时没有实现 (如果prefetch_size字段不是默认值0, 则会通知客户端出错, 通知客户端RabbitMQ系统没有实现该参数的功能, 还可以参考此文)当你设置prefetch_size大于0的时候, 会出现如下报错 prefetch_count : 预取消息数量 global: 在3.3.0版本中对global这个参数的含义进行了重新定义, 即glotal=true时表示在当前channel上所有的consumer都生效(包括已有的), 否则只对设置了之后新建的consumer生效;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"19. 消费者预取 Consumer Prefetch - RabbitMQ关于 吞吐量, 延迟 和 带宽 的一些理论","slug":"rabbitmq/2018-06-12-rabbitmq-19","date":"2018-06-12T03:26:55.000Z","updated":"2018-06-30T05:51:29.000Z","comments":true,"path":"2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","link":"","permalink":"http://blog.renyimin.com/2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","excerpt":"","text":"RabbitMQ关于吞吐量,延迟和带宽的一些理论 译文: 网上虽然有很多版本的译文, 在此处还是重新做了翻译 你在Rabbit中有一个队列, 然后有一些客户端从这个队列中进行消费;如果你根本没有设置QoS(basic.qos), 那么Rabbit将尽可能快地按照网络和客户端允许的速度将所有队列的消息推送到客户端; 因此, 消费者所占用的内存将会激增, 因为它们将所有消息都缓存在自己的RAM中;同时, 值得注意的是: 此时如果你询问Rabbit, 队列可能会显示为空, 但是会有大量的未确认消息在客户端中, 准备被客户端应用程序处理;并且此时如果你添加新的消费者, 由于没有消息留在队列中, 所以队列也无法将消息发送给新的消费者的;尽管有其他消费者可用于更快地处理消息, 但由于消息已经在现有的客户端中缓存, 并且可能在那里很长一段时间, 所以这是相当次优的! 所以，默认的QoS预取给客户端(consumer)设置了无限的缓冲区, 这可能导致不良的行为和性能; 那么, 应该将QoS预取缓冲区大小设置为多少呢?目标是让消费者保持工作饱和状态, 但要尽量减少客户端的缓冲区大小, 以便让更多的消息保留在Rabbit的队列中, 这样就可以供新消费者来消费; 比方说, Rabbit从这个队列中拿出一条消息, 把它放到网络上，然后到达消费者, 需要50ms; 客户端处理消息需要4ms;一旦消费者处理了消息, 它就会发送一个ack给Rabbit, 这将再次花费50ms发送给Rabbit并被Rabbit进行处理; 所以我们总共有104ms的往返时间。如果我们消息设置了QoS预取值为1, 那么直到这个往返行程完成之前, Rabbit是不会发送下一个消息给客户端的;因此, 每次往返的104ms中, 客户端只有4ms,或者说只有3.8％的时间忙碌, 而我们希望百分之百的时间都在忙碌中; 如果我们在每个消息的客户端上执行 总的往返时间/处理时间, 会得到 104/4 = 26如果我们设置消息的QoS预取值为26, 那就解决了我们的问题: 假设客户端具有26个消息缓冲, 等待处理(这是一个明智的假设:一旦你设置了basic.qos, 然后从一个队列中进行消费, Rabbit将会尽可能多的将消息发送到你订阅该队列的客户端中, 直到QoS的限制; 如果你认为消息不是很大, 带宽也很高, 那么Rabbit很可能更快地发送消息到你的客户端, 因此, 从完整的客户端缓冲区的角度来做所有的数学运算是合理的(也更简单的) 如果每条消息需要4ms的处理来处理, 那么总共需要 26×4 = 104ms 来处理整个缓冲区(中的消息);第一个4ms是第一个消息的客户端处理消息的时间, 处理完成后, 客户端然后发出一个确认(这一点需要50ms才能到达代理), 然后继续处理缓冲区中的下一条消息, 代理向客户端发出一条新消息, 这需要50ms的时间, 所以到了104ms时间, 客户端已经完成缓冲区的处理, 代理的下一条消息已经到达, 并准备好等待客户端来处理它;因此, 客户端始终处于忙碌状态: 具有较大的QoS预取值也不会使其更快了, 但是我们最大限度地减少了缓冲区的大小, 并且减少了客户端消息的延迟; 事实上, 客户端能够在下一条消息到达之前完全排空缓冲区, 因此缓冲区实际上保持为空; 如果处理时间和网络行为保持不变, 此解决方案绝对没问题但考虑一下如果网络突然间速度减半会发生什么情况(rymuscle:显然, 网络传输时间就加长了): 此时你的预取缓冲区(也就是你设置的prefetch预取值)就不够大了, 现在客户端会就会闲置, 等待新消息到达, 因为客户端能够处理消息的速度比Rabbit能够提供新消息的速度要快; 为了解决这个问题, 我们可能会决定将QoS预取大小加倍(或接近两倍), 如果我们从26开始将它推到51, 那么如果客户端处理保持在每个消息4ms, 我们现在在缓冲区中会有51 4 = 204ms的消息处理时间, 其中4ms将用于处理消息, 而200ms用于发送消息回复rabbit并收到下一条消息, 因此, 我们现在可以应对网络速度的减半;但是, 如果网络正常运行, 现在将QoS预取加倍, 意味着每个消息都会驻留在客户端缓冲区中一段时间​​, 而不是在到达客户端时立即处理;再次分析: 从现在51条消息的完整缓冲区开始, 我们知道新消息将在客户端完成处理第一条消息之后的100ms处开始出现在客户端, 但在这100毫秒内, 客户只能处理100/4 = 25个消息, 这意味着当新消息到达客户端时, 它会在客户端从缓冲区头部移除时被添加到缓冲区的末尾;而缓冲区将始终保持(50 - 25 = 25)个消息长度, 因此每个消息将在缓冲区中保持 25 4 = 100ms所以有时候你会看到你的消费者虽然活着没有假死, 但是却有大量的unacked! 可以考虑一下这个原因!! 因此, 我们看到, 增加预取缓冲区大小, 以便客户端可以应对恶化的网络性能, 同时保持客户端繁忙, 大大增加网络正常运行时的延迟!! 同样, 如果不是网络性能的恶化, 而是客户端开始花费40ms来处理每条消息而不是之前的4ms, 会发生什么情况?假设原始的预取缓冲区大小设置的是26条消息, 客户端现在需要花40ms处理第一条消息, 然后将确认消息发送回Rabbit并移至下一条消息;ack仍然需要50ms才能到达Rabbit, 而Rabbit发出一条新的消息需要50ms, 但在100ms内, 客户端只处理了 100/40 = 2.5 条消息, 而不是剩余的25条消息;因此当新消息到来时, 缓冲区在这一点上仍然是有 25 - 3 = 22 个消息, 这样的话, 来自Rabbit的新消息就不会被立即处理, 而是位于第23位, 落后于其他22条仍在等待处理的消息;客户端(Consumer)将会有 22 * 40 = 880ms 的时间都不会触及到那个新到的消息, 鉴于从Rabbit到客户端的网络延迟仅为50ms, 这个额外的880ms延迟现在为延迟的95％ (880 / (880 + 50) = 0.946); 当你决定尝试通过添加更多消费者来处理这种增长的积压时, 需要注意, 现在有消息正在被现有客户端缓冲, 并不是说你增加消费者就能缓解这部分的压力! 更糟糕的是, 如果我们将缓冲区大小设置为可以预取51条消息以应对网络性能下降,会发生什么?处理第一条消息后, 将在客户端缓冲另外50条消息, 100ms后(假设网络运行正常), 一条新消息将从Rabbit到达客户端, consumer在100ms中只能处理这50条消息中的两条消息(缓冲区现在为47条消息长),因此新消息将会在缓冲区中是第48位, 这样的话, 知道 47 40 = 1880ms 之后, 消费者才会开始处理新来的消息, 同样, 考虑到向客户端发送消息的网络延迟仅为50ms, 现在这个1880ms的延迟意味着客户端缓冲占延迟的97％(1880/(1880 + 50)= 0.974);这可能是不可接受的: 数据只能在客户端收到后2秒内立即处理, 才能有效且有用！*如果其他消费客户端空闲, 他们无能为力: 一旦Rabbit向客户端发送消息, 消息就是客户端的责任, 直到他们拒绝或拒绝消息; 消息发送到客户端后，客户端不能窃取彼此的消息;您希望客户端保持繁忙状态, 但客户端尽可能少地缓存消息, 以便客户端缓冲区不会延迟消息, 因此新消费客户端可以快速接收来自Rabbit队列的消息; 因此, 如果网络变慢, 缓冲区太小会导致客户端空闲; 但如果网络正常运行, 缓冲区太大会导致大量额外的延迟;如果客户端突然开始花费更长时间来处理每个缓冲区, 则会导致大量额外的延迟;很明显, 你真正想要的是可以变化的缓冲区大小, 这些问题在网络设备中很常见, 并且一直是很多研究的主题;主动队列管理算法试图尝试放弃或拒绝消息，以避免消息长时间处于缓冲区。当缓冲区保持空闲时（每条消息只遭受网络延迟，并且根本不在缓冲区中），缓冲区在那里吸收峰值，从而实现最低延迟。从网络路由器的角度来看，Jim Gettys一直在研究这个问题：局域网和广域网性能之间的差异会遇到完全相同的问题。实际上，无论何时，在生产者（在我们的例子中为Rabbit）和消费者（客户端应用程序逻辑）之间都有一个缓冲区，双方的性能可以动态变化，您将会遇到这些问题。最近发布了一种名为Controlled Delay的新算法，该算法似乎在解决这些问题方面效果很好。 … 未完待续","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"13. Queue Length Limit (队列长度限制) - x-max-length, x-max-length-bytes, x-overflow","slug":"rabbitmq/2018-06-09-rabbitmq-13","date":"2018-06-09T07:26:36.000Z","updated":"2018-06-21T07:36:26.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-13/","excerpt":"","text":"理论 官方文档: https://www.rabbitmq.com/maxlength.html 译文: 一个队列的最大长度可以通过设置 消息的数量 或者 消息的字节数(所有消息体长度的总和, 忽略消息属性 和 任何杂项开销), 或者两者都设置, 来限制; 对于任何给定的队列, 其最大长度(包括以上说的两种类型), 可以使用队列的$arguments参数, 或者在服务器端使用policies策略来进行定义;在 policies策略 和 $arguments 都指定最大长度的情况下, 会应用这两个值中的最小值; 在所有情况下, 使用就绪(Ready状态)消息的数量, 未确认的(Unacked)消息不计入限制的数量, 如下简单展示了 queue中的几种消息状态:从 rabbitmqctl list_queues 中的 messages_ready 和 message_bytes_ready 字段 以及 管理API 会展示受到限制的值 123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...testQosQueue 162msgPrioityQueue 0renyimindeMacBook-Pro:~ renyimin$ - 默认最大队列长度行为 当设置最大队列 长度 或 大小 并达到最大值时, RabbitMQ的默认行为是将 队列前面(即队列中最早的消息) **丢弃** 或 **死信** 消息; 要修改此行为, 请使用下面描述的溢出设置; - 队列溢出行为 使用 `overflow`溢出设置 来配置队列溢出行为, 如果 `overflow` 设置为`reject-publish`, 则最近发布的消息将被丢弃; 否则, 如果publisher confirms(发布者确认)被启用, 则会通过 `basic.nack` 消息向发布者通知 reject拒绝; 如果一条消息被路由到多个队列并被至少一个队列拒绝, 该channel将通过 `basic.nack` 通知发布者, 该消息仍然会发布到所有其他可以入队的队列中; - 使用 策略 定义最大队列长度 要使用`策略`指定最大长度, 需要添加key `max-length` 和/或 `max-length-bytes` 到策略定义, 例如: 123456rabbitmqctl : rabbitmqctl set_policy my-pol &quot;^one-meg$&quot; &apos;&#123;&quot;max-length-bytes&quot;:1048576&#125;&apos; --apply-to queuesrabbitmqctl (Windows) : rabbitmqctl.bat set_policy my-pol &quot;^one-meg$&quot; &quot;&#123;&quot;&quot;max-length-bytes&quot;&quot;:1048576&#125;&quot; --apply-to queues``` &quot;my-pol&quot;策略确保 `one-meg` 队列包含不超过1MiB的消息数据, 达到1MiB限制时, 最旧的消息将从队列头部丢弃;- 要定义溢出行为 - 是从头删除消息还是拒绝新发布, 请将`overflow` key 添加到策略定义, 例如: rabbitmqctl : rabbitmqctl set_policy my-pol &quot;^two-messages$&quot; &apos;{&quot;max-length&quot;:2,&quot;overflow&quot;:&quot;reject-publish&quot;}&apos; --apply-to queues rabbitmqctl (Windows) : rabbitmqctl.bat set_policy my-pol &quot;^two-messages$&quot; &quot;{&quot;&quot;max-length&quot;&quot;:2,&quot;&quot;overflow&quot;&quot;:&quot;&quot;reject-publish&quot;&quot;}&quot; --apply-to queues ``` &quot;my-pol&quot;策略确保 &quot;two-messages&quot; 队列包含不超过2条消息, 并且只要队列中包含2条消息并启用发布商确认, 所有其他发布都会发送 `basic.nack` 响应; 还可以使用管理插件定义策略, 有关更多详细信息，请参阅[策略文档](https://www.rabbitmq.com/parameters.html#policies) - 使用 $arguments 定义最大队列长度 通过提供带有非负整数值的 `x-max-length`($ageuments中的选项) 队列声明参数, 可以设置最大消息数; 非负整数值的 `x-max-length-bytes` 则可以设置字节的最大长度; 如果两个参数都设置了, 那么两者都适用, 无论哪个限制首先被实施; 通过为队列声明参数$arguments的 `x-overflow` 选项提供一个字符串值, 可以设置溢出行为, 可能的值有 `drop-head`(默认) 或 `reject-publish`; 测试 默认溢出行为下, 当队列中消息放满时, 消息什么时候丢弃, 什么时候被死信? 测试代码参考;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"12. Lazy Queues 惰性队列(x-queue-mode) + 持久化","slug":"rabbitmq/2018-06-09-rabbitmq-12","date":"2018-06-09T02:14:30.000Z","updated":"2018-06-30T06:50:51.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-12/","excerpt":"","text":"Lazy Queues 从RabbitMQ 3.6.0开始m, Broker有了 Lazy Queues 的概念 – queues尽可能早地将其内容移动到磁盘的队列, 并且只在消费者请求时将其加载到RAM中, 因此命名为 Lazy Queues; Lazy Queues的主要目标之一是能够支持非常长的队列(数百万条消息), 出于各种原因, 队列可能变得很长: 消费者脱机/已经崩溃/因维护而停机 突然有消息进入高峰, 生产者超过消费者 消费者比平时慢 默认情况下, 消息被发布到RabbitMQ时, 队列将其保存在内存缓存中, 这能够尽可能快地向消费者传递消息; 请注意, 持久性消息可以在进入RabbitMQ时写入磁盘, 并同时保存在RAM中; 每当代理认为需要释放内存时, 缓存中的消息将被分页到磁盘; 将一批消息分页到磁盘会花费时间并阻塞队列进程, 从而无法在分页时收到新消息, 尽管最近版本的RabbitMQ改进了分页算法, 但对于队列中有数百万条消息可能需要分页的用例, 情况仍然不理想;而懒惰队列会尝试将消息尽可能早地移动到磁盘上, 这意味着在正常操作的大多数情况下, RAM中的消息数量明显减少, 这是以增加磁盘I / O为代价的; 队列具备两种模式: default 和 lazy 默认的为default模式, 在3.6.0之前的版本无需做任何变更 lazy模式即为惰性队列的模式, 可以通过调用 channel.queueDeclare 方法的时候在参数中设置 x-queue-mode 为 lazy 或者 default 来指明队列为惰性或者正常 也可以通过Policy的方式设置, 如果一个队列同时使用这两种方式设置的话, 那么队列参数优先于策略值 只能通过删除队列, 重新创建队列并使用不同的参数重新声明它, 来更改队列模式 当你想要优先保持节点内存使用率较低, 而较高的磁盘I/O和磁盘利用率可以接收时，所以惰性队列就比较合适; 惰性队列和持久化消息可谓是最佳拍档 如果消息是持久化的, 那么这样的I/O操作不可避免, 同时内存也会过高; 注意如果 惰性队列 中存储的是 非持久化 的消息, 内存的使用率会一直很稳定, 但是重启之后消息又会丢失; 所以如果惰性队列结合持久化消息, 这就比较合适了要持久化, 磁盘I/O是无法避免的, 但可以通过惰性队列保证内存降低! 代码参考 未完待续~~","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"11. Dead Letter Exchanges (DLX) 死信交换机","slug":"rabbitmq/2018-06-07-rabbitmq-11","date":"2018-06-07T06:36:51.000Z","updated":"2018-07-06T02:00:13.000Z","comments":true,"path":"2018/06/07/rabbitmq/2018-06-07-rabbitmq-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/07/rabbitmq/2018-06-07-rabbitmq-11/","excerpt":"","text":"关于死信 死信: 来自队列的消息可能会变成 “dead-lettered”(死信), 即, 当出现以下情况的时候, 会被重新发布到另一个Exchange中:123消息被拒绝(`basic.reject` 或 `basic.nack`) 并且 `requeue=false(即没有进行重排)`;消息TTL过期队列达到最大长度(队列满了, 无法再添加数据到mq中) 死信交换机(也叫死信邮箱): 在定义业务队列的时候, 你需要考虑指定一个死信交换机, 死信交换机可以和任何一个普通的队列进行绑定, 然后在 业务队列 出现 死信 的时候就会将数据发送到 死信队列; 死信Exchange(DLX)其实也是正常的Exchange, 它们可以是任何常见的type, 并且可以像通常那样进行声明; 对于任何给定的队列, 客户端可以使用队列的 $arguments 可选参数 或 在服务器中使用 策略 来定义DLX;(在 策略 和 $arguments 都指定的情况下, $arguments 将覆盖 策略) 要为队列设置死信交换, 需要队列 $arguments 参数的 x-dead-letter-exchange 选项来指定交换机的名称, testDlxRoutingKey 指定 死信路由键: 123456789$channel-&gt;exchange_declare(&apos;testDlxExchange&apos;, &apos;direct&apos;, false, true, false, false, false);$arguments = new AMQPTable([ // 指定死信交换机 &apos;x-dead-letter-exchange&apos; =&gt; &apos;testDlxExchange&apos;, // 指定死信路由键 &apos;x-dead-letter-routing-key&apos; =&gt; &apos;testDlxRoutingKey&apos;]);// 如下就可以把上面的普通交换机testDlxExchange设置为队列testDlxQueue的死信队列了$channel-&gt;queue_declare(&apos;testDlxQueue&apos;, false, true, false, false, false, $arguments); 使用策略配置DLX可参考官网文档 如果DLX不存在, 则出现死信之后, 这些死信将会被丢弃; 像上面那样, 你也可以指定一个死信routingkey, 以便在死信消息被重发时使用; 如果没有设置, 则会使用消息自己原本的routingkey: 假如你使用 routingkey=foo 将消息发布到交换机, 然后该消息被dead-lettered, 它将被发布到routingkey为 foo 的 死信exchange; 而如果消息最初着陆的队列已声明了 x-dead-letter-routing-key=bar, 则该消息将被发布到它的(routingkey为 “bar”)死信exchange; 死信队列 : 死信队列实际上就是一个普通的队列, 只是这个队列跟死信交换机进行了绑定, 用来存放死信而已; 当一个死信exchange被指定时, 除了声明队列上的通常配置权限之外, 用户还需要对该队列具有读权限, 并对死信exchange有写入权限, 权限在队列声明时被验证 如果死信消息被重发时, publisher confirms 是开启的, 则消息在从原始队列中删除之前, 必须得到 其最终到达的死信队列(DLX路由的目标) 的确认消息; 换句话说, “发布”(消息过期的那个)队列将不会在死信队列确认接收消息之前删除消息; 请注意, 如果不干净的代理关闭, 可能会在 原始队列 和 死信目标队列 上复制相同的消息; Dead-Lettered Messages 死信被re-publish后, 会在消息的header中增加一个叫做 x-death 的数组内容, 包含了以下字段内容: 123456queue 消息被重新路由前所在的queue的名称reason 消息成为死信的原因，有如下几种: 消息被拒绝并且requeue=false, message的TTL过期, 超过了queue允许的最大消息长度time 消息成为死信的日期时间exchange 消息被重新路由到exchange的名称routing-keys 消息被发布时的 routing-keysoriginal-expiration 消息的原始expiration属性 问题 一个queue可以绑定一个DLX, 那是否可以有多个DLX? 还是说所有的死信只能扔到一个DLX下的死信queue中? 如果queue既设置了 x-dead-letter-routing-key, 也进行了绑定并且设置了 bindingKey, 那么消息会如何路由? 死信队列如果也有超时时间, 此时消息再次超时之后, 还可以再次通过另一个 EXL 放入新的 死信队列么? 常见问题 ##https://www.cnblogs.com/williamwsj/p/8108970.html 可以设置消息的存活时间;https://my.oschina.net/u/3015099/blog/781970https://blog.csdn.net/qq_29778131/article/details/52536965 疑问死信消息重新发布，发布者确认已在内部启用，因此消息最终着陆的“死信队列”（DLX路由目标）必须在消息从原始队列中删除之前确认消息。换句话说，“发布”（邮件过期的队列）队列不会在死信队列确认接收邮件之前删除邮件（请参阅确认以了解有关所做保证的详细信息）。请注意，如果代理程序不正常关闭，则可能会在原始队列和死文字目标队列上复制相同的消息。有可能形成消息死书的循环。例如，当一个队列在没有指定死信路由密钥的情况下将消息发送到默认交换机时，会发生这种情况。如果在整个周期内没有拒绝，这些周期中的消息（即达到相同队列两次的消息）将被丢弃。","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"10. TTL (x-expires,x-message-ttl, expiration)和 延迟队列","slug":"rabbitmq/2018-06-07-rabbitmq-10","date":"2018-06-07T06:28:56.000Z","updated":"2018-07-07T02:13:19.000Z","comments":true,"path":"2018/06/07/rabbitmq/2018-06-07-rabbitmq-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/07/rabbitmq/2018-06-07-rabbitmq-10/","excerpt":"","text":"Time-To-Live Extensions RabbitMQ允许你为 Messages 和 Queues 设置 TTL(生存时间): 如果通过 队列的可选参数 设置队列/消息的TTL, 则队列中所有消息都有相同的过期时间 如果通过 消息的属性 为每条消息单独设置TTL, 则每条消息的TTL都可以不同; 如果上述两种方法同时使用, 则消息的过期时间以两者之间TTL较小的那个数值为准; Queue TTL 通过在队列声明时, 设置 $arguments 参数的 x-expires 选项 或 通过设置 expires 策略, 可以为给定队列设置过期时间 这可以控制队列在被自动删除之前有多长时间可以不被使用不被使用的意思是: 队列没有consumer, 队列尚未重新声明, 并且至少在有效期内未调用 basic.get, 待测试??例如, 这可以用于RPC的回复队列, 其中可以创建许多可能永远不会被耗尽的队列?? x-expires 以毫秒为单位, 与消息ttl不同, 它不能为0 (1000表示1秒内未使用的队列将会被删除) 此功能可以与自动删除队列属性一起使用 queue.declare 命令中的 x-expires 参数控制 queue 被自动删除前可以处于未使用状态的时间;未使用的意思是 queue 上没有任何 consumer, queue 没有被重新声明, 并且在过期时间段内未调用过 basic.get 命令;通俗的说就是: 一个被设置为自动删除的队列, 在没有消费者对其进行消费的情况下, 它可以存活的时间;该方式可用于, 例如, RPC-style 的回复 queue, 其中许多 queue 会被创建出来, 但是却从未被使用 ?? 这句话还未领会 服务器会确保在过期时间到达后 queue 被删除, 但是不保证删除的动作有多么的及时。在服务器重启后，持久化的 queue 的超时时间将重新计算。 队列过期后, 消息不会进入死信 Message TTL通过队列 设置消息TTL 消息TTL可以通过设置 队列的 $arguments 参数的 x-message-ttl 选项来设置队列中消息的TTL; 已经在队列中超过配置的TTL的消息被认为是dead(死亡的) 请注意: 路由到多个队列的消息可能会在其驻留的每个队列的不同时间死亡, 或根本不会死亡; 一个队列中消息的死亡对其他队列中同一消息的生命没有影响; 服务器保证死亡的消息不会使用 basic.deliver 进行投递(发送给消费者) 或 包含在 basic.get-ok 响应中(用于一次性提取操作);此外, 服务器将尝试在基于TTL的到期时或之后不久删除 “消息”; TTL参数 或 策略 的值必须是非负整数(0 &lt;= n), 以毫秒为单位因此, 值1000意味着添加到队列的消息将在队列中存在1秒 或者 直到它被传递给消费者; 使用 策略 定义消息TTL 通过消息 设置TTL 通过在使用 basic.publish 发布消息时, 在消息的属性中设置 expiration 字段, 这样就可以基于每条消息指定TTL; expiration 字段的值描述了以毫秒为单位的TTL周期, 与 x-message-ttl 的设置相同由于 expiration 字段必须是字符串, 因此代理将(仅)接受数字的字符串表示形式。 当同时指定 每个队列 和 每条消息 的TTL时, 将选择两者之间的较低值; 对比 对于第一种设置队列TTL属性的方法, 一旦消息过期, 就会从队列中抹去; 而第二种方法里, 即使消息过期, 也不会马上从队列中抹去, 因为每条消息是否过期是在即将投递到消费者之前判定的; 为什么两者得处理方法不一致? 因为第一种方法里, 队列中已过期的消息肯定在队列头部, RabbitMQ只要定期从队头开始扫描是否有过期消息即可; 而第二种方法里, 每条消息的过期时间不同, 如果要删除所有过期消息, 势必要扫描整个队列, 所以不如等到此消息即将被消费时再判定是否过期, 如果过期, 再进行删除; 因此, 如果使用了第二种方式, 建议让消费者在线以确保消息更快地被丢弃; 测试中会有体现 测试 Publisher代码参考 内容为业务队列绑定一个死信队列(死信队列可以先不存在, 即 你可以不用先运行Consumer来创建dlx) Consuemr代码参考 内容为死信队列与死信交换机的绑定 延迟队列TTL+DLX 首先, 应该清楚的是, 延迟队列存储的对象对应的肯定是延时消息, 所谓 延时消息 是指当消息被发送以后, 并不想让消费者立即拿到消息, 而是等待指定时间后, 消费者才拿到这个消息进行消费; 比如: 在订单系统中, 一个用户下单之后通常有30分钟的时间进行支付, 如果30分钟之内没有支付成功, 那么这个订单将进行一场处理, 这是就可以使用延时队列将订单信息发送到延时队列; 用户希望通过手机远程遥控家里的智能设备在指定的时间进行工作, 这时候就可以将用户指令发送到延时队列, 当指令设定的时间到了再将指令推送到只能设备; RabbitMQ延迟队列的实现: TTL+DLX https://blog.csdn.net/u014308482/article/details/53036770 死信延时的问题: https://blog.csdn.net/qq315737546/article/details/66475743 插件方式参考: https://www.cnblogs.com/haoxinyue/p/6613706.html","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"08. Publisher Confirms 发布者确认机制","slug":"rabbitmq/2018-06-05-rabbitmq-08","date":"2018-06-05T11:20:56.000Z","updated":"2018-06-30T05:50:02.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","excerpt":"","text":"前言 在RabbitMQ中, 为了 保证消息能安全发布到Broker, 保证消息被消费者成功处理, 因此, publishers 和 conmusers 需要有用于 delivery(交付)确认 和 处理确认 的机制; 从 consumers 到 RabbitMQ 的递送处理确认被称为 AMQP 0-9-1中的确认; 从 broker 到 publishers 的确认是一个扩展协议, 被称为 publisher confirms; 这两个功能都基于相同的想法, 并受TCP的启发; 它们对于 从publishers到RabbitMQ节点 以及 从RabbitMQ节点到consumers 的可靠传送都是至关重要的; 之前已经介绍了 consumers ack, 下面介绍 publisher confirm Publisher Confirms 当你向Broker(Rabbit Server)发布消息时, 由于网络常出现的不确定因素, 客户端(Publisher)需要有机制来确认该消息已到达服务器; 使用标准的AMQP 0-9-1, 保证消息不会丢失的唯一方法是使用事务 – 使channel事务化, 然后为 每个消息 或 消息集 发布和提交; 不过, 由于事务非常重量级, 会将吞吐量降低250倍; 为了解决这个问题, 引入了 Publisher Confirms, 它模仿协议中已经存在的 消费者确认机制; 要启用这个确认机制，客户端可以通过使用channel的 confirm.select 方法 如果设置了 confirm.select 方法的 no-wait, 代理会用 confirm.select-ok 进行响应, 不过这点你貌似也只能通过抓包来观察: 这里说的 confirm.select 和 php-amqplib包中的 confirm_select_ok() 方法可不是一个意思, 而且php-amqplib也没对confirm_select_ok做实现 上面也提到了, 该确认机制是模仿已经存在的 消费者确认机制, 所以, Broker也会使用类似 ack, nack 来响应Publisher: 可以通过为 set_ack_handler , set_nack_handler 设置回调, 来监测消息是否成功到达服务器, 成功则会触发 set_ack_handler, 失败则会触发 set_nack_handler 只有在负责队列的Erlang进程中发生内部错误时才会回应nack, 所以这个在测试中也一直没有使用set_nack_handler捕获到错误, 但是对于nack的消息, 可以设置进行重发; 注意: 这两监听函数是监听 publisher confirm 应答的, 可不是监听 consumer ack 应答的; 一旦在channel上使用 confirm.select 方法, 就说它处于确认模式, 事务通道不能进入确认模式, 一旦通道处于确认模式, 就不能进行事务处理; 也就是说 事务 和 Publisher Confirm 不能同时使用; 一旦通道处于确认模式, 代理和客户端都会对消息进行计数(在第一次confirm.select时从1开始计数), 然后, broker通过在相同channel上发送 basic.ack 来处理它们, 从而确认消息; delivery-tag 字段包含确认消息的序列号; 最大 Delivery Tag, 递送标签是一个64位长的值，因此其最大值为9223372036854775807.由于递送标签的范围是按每个通道划分的，因此发布商或消费者在实践中不太可能运行该值 Publisher Confirms 的顺序考虑 在大多数情况下, RabbitMQ将按发布顺序向publisher确认消息(这适用于在单个频道上发布的消息); 但是, 发布者确认是异步发出的, 并且可以确认一条消息或一组消息;由于消息确认可以以不同的顺序到达, 所以, 应用程序应尽可能不取决于确认的顺序; 未完~~~ https://yq.aliyun.com/articles/42206 测试代码publisher confirm 不需要消费者参与, 代码参考 预习本篇还涉及到了 Qos预取相关的内容: 针对Qos的提前预习(译文) 信道预取设置(QoS)由于消息是异步发送(推送)给客户端的, 因此在任何给定时刻通常都有不止一条消息在信道上运行; 此外, 客户的手动确认本质上也是异步的, 所以有一个 未确认的交付标签的滑动窗口, 开发人员通常会倾向于限制此窗口的大小, 以避免消费者端无限制的缓冲区问题。这是通过使用 basic.qos 方法设置 预取计数 值完成的, 该值定义了channel上允许的最大未确认递送数量, 一旦数字达到配置的计数, RabbitMQ将停止在通道上传送更多消息, 除非至少有一个未确认的消息被确认;例如, 假设在通道 “Ch” 上有未确认的交付标签5,6,7和8, 并且通道 “Ch” 的预取计数(后面会学到是prefetch_count)设置为4, 则RabbitMQ将不会在 “Ch” 上推送更多交付, 除非至少有一个未完成的交付被确认(当确认帧在 delivery_tag=8 的频道上到达时, RabbitMQ将会注意到并再发送一条消息) QoS预取设置对使用 basic.get(pull API) 获取的消息没有影响, 即使在手动确认模式下也是如此; 消费者确认模式, 预取和吞吐量(译文) 确认模式 和 QoS预取值 对消费者吞吐量有显着影响, 一般来说, 增加预取值将提高向消费者传递消息的速度, 当然, 自动确认模式可以产生最佳的传送速率 但是, 在上面两种情况下, 尚未完成交付处理的消息(unacked)数量也会增加, 从而增加消费者RAM消耗; 自动确认模式或带无限预取的手动确认模式应谨慎使用, 消费者在没有确认的情况下消耗大量消息将导致其所连接的节点上的内存消耗增长; 预取值1是最保守的, 但这将显着降低吞吐量, 特别是在消费者连接延迟较高的环境中, 对于许多应用来说, 更高的价值是合适和最佳的; 100到300范围内的Qos(prefetch_count)预取值通常提供最佳的吞吐量, 并且不会面临压垮consumer的重大风险, 而更高的值往往会遇到效率递减的规律;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"07. Consumer Acknowledgements 消费者确认机制","slug":"rabbitmq/2018-06-03-rabbitmq-07","date":"2018-06-05T09:45:51.000Z","updated":"2018-07-04T06:02:28.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-03-rabbitmq-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-03-rabbitmq-07/","excerpt":"","text":"前言 在RabbitMQ中, 为了 保证消息能安全发布到Broker, 保证消息被消费者成功处理, 因此, publishers 和 conmusers 需要有用于 delivery(交付)确认 和 处理确认 的机制; 从 consumers 到 RabbitMQ 的递送处理确认被称为 AMQP 0-9-1中的确认; 从 broker 到 publishers 的确认是一个扩展协议, 被称为 publisher confirms; 这两个功能都基于相同的想法, 并受TCP的启发; 它们对于 从publishers到RabbitMQ节点 以及 从RabbitMQ节点到consumers 的可靠传送都是至关重要的; Consumer Acknowledgements Delivery Identifiers: Delivery Tags(交付标签) 当消费者(订阅)被注册时, 消息将被RabbitMQ使用 basic.deliver 方法递送(推送), 该方法带有一个 delivery tag, 它唯一标识 channel 上的一个delivery(投递); 由于投放标签的范围是按每个channel划分的, 因此交付必须在同一channel上被确认; 确认交付的客户端库方法将交付标签作为参数, 如下, 可以在consumer中使用如下几种消息应答123$channel-&gt;basic_ack(8, false); // 有2个参数可用 delivery_tag, multiple$channel-&gt;basic_nack(8, false, true); // 有4个参数可用 delivery_tag, multiple, requeue$channel-&gt;basic_reject(8, false); // 有3个参数可用 delivery_tag, requeue 消费者确认模式和数据安全注意事项 当节点向consumer投递消息时, 它必须确定消息是被消费者处理(或至少是被接收到了); 由于这个过程中会有很多导致失败的情况, 比如consumer应用程序失败, 网络原因等; 消息传递协议通常提供一种确认机制, 允许消费者给他们所连接的节点发送 确认应答, 该机制是否被使用, 是在消费者订阅时决定的; RabbitMQ可以在消息被发出去之后立即就认为它已成功投递, 也可以在客户端手动发送确认之后才认为消息被成功投递, 可以使用以下协议方法之一来手动发送确认, 确认可以是 positive正面 或 negative负面: basic.ack is used for positive acknowledgements basic.nack is used for negative acknowledgements (note: this is a RabbitMQ extension to AMQP 0-9-1) basic.reject is used for negative acknowledgements 但是与 basic.nack 相比有一个限制 Positive acknowledgements 是告诉Rabbitmq消息已被成功投递并处理, 并且消息可以被丢弃了; 使用 basic.reject 的 Negative acknowledgements 也是相同的效果, 它与 Positive acknowledgements 主要是语义上的差异 positive acknowledgements 假定一条消息已被成功处理; 而 negative acknowledgements 则暗示交付未被处理, 但消息仍应被删除; (不过, negative acknowledgements 的两个方法, 都有个 requeue 参数可以将消息重新放到队列中) 发后即忘模式 而在 自动确认模式 下, 消息在发送后会立即被认为已成功delivery(投递), 这种模式可以实现更高的吞吐量(只要消费者能够跟上), 但会降低 delivery(交付) 和 消费者处理 的安全性; (无法保证消息能真正到达, 也无法保证被消费) 这种模式通常被称为 发后即忘模式, 与手动确认模式不同, 如果消费者的TCP连接或通道在成功交付之前关闭, 则服务器发送的消息将丢失, 因此, 自动消息确认应被视为不安全的; 使用自动确认模式时需要考虑的另一件事是消费者过载, 手动确认模式通常与 有限的通道预取(Qos-prefetch_count,后面会讲到)一起使用, 这限制了通道上unacked消息的数量; 然而, 对于自动确认, 根据定义默认没有Qos限制的, 消费者因此可能被delivery(投递)速度所压垮, 可能导致内存积压, 或者操作系统终止进程; 因此, 仅建议可以高效且稳定处理消息的消费者使用 发后即忘 模式; Negative AckNegative Ack 和 requeue 有时候, Consumers 并不能立即处理投递来的消息, 但其他实例可能能够处理, 在这种情况下, 你可能就需要重新安排并让其他消费者接收并处理它, basic.reject() 和 basic.nack() 是两种用于这种场景的方法; 这些方法通常用于 negatively ack (应答)一个投递, 这个投递可以被broker丢弃或重新入队列(默认是被丢弃的), 此行为由 requeue 字段控制(如 $channel-&gt;basic_nack(8, false, true);); 当该字段设置为true时, 代理将使用指定的 delivery tag 重新进行交付 如果可能, 当消息被 requeue(重排)时, 它将被放置在其队列中的原始位置, 如果不是(由于多个消费者共享队列时来自其他消费者的并发递送和确认), 则该消息将被重新排队到更接近队列头的位置; 已重排的消息可能会立即准备好重新发送, 具体取决于它们在队列中的位置, 以及具有活跃consumer的channel使用的预取值, 这意味着, 如果所有消费者都因为暂时状况 requeue 时, 它们将创建一个 requeue/redelivery(重新发货/重新递送)的循环, 就网络带宽和CPU资源而言, 这样的循环可能是昂贵的; 对于这种情况, 有建议说是在出现无法正确消费的消息时不要采用requeue的方式来确保消息可靠性, 而是重新投递到新的队列中,比如设定的死信队列中, 以此可以避免前面所说的死循环而又可以确保相应的消息不丢失, 对于死信队列中的消息可以用另外的方式来消费分析, 以便找出问题的根本; Nack 和 Reject 消费者实现可以跟踪重新传送的次数并拒绝消息(丢弃它们)或延迟计划重新计划; 可以使用 basic.nack() 方法可以一次性 拒绝 或 requeue 多个消息, 这是区别于 basic.reject()的; basic.nack() 接受一个额外的参数 multiple, 这两个方法的所有参数对比之前也已经介绍过了, 如下: 消费者失败或失去连接时: 自动requeue 使用手动确认时, 任何未收到应答的投递将在发送递送的通道(或连接)关闭时自动requeue 由于这种行为, 消费者可能已经执行了, 但是确认消息在发送途中由于网络问题, publisher没收到应答, 这就会导致消息被requeue(重发), 这样消费者必须做好准备来处理重新来的投递，这就得考虑到幂等性; 请注意, 因为有requeue机制, 所以消费者可以收到先前传送给其他消费者的消息。 一次确认多次交付 可以批量手动确认以减少网络流量, 这是通过将确认方法的 multiple 字段设置为true来完成的; 请注意: basic.reject() 在历史上并没有这个字段, 这就是为什么 basic.nack() 被RabbitMQ作为协议扩展引入的原因; 当 multiple 字段设置为true时, RabbitMQ将确认 到本次确认中指定的标签为止 所有未完成交付的标签 (如 $channel-&gt;basic_ack(8, true);) ; 与其他确认相关的内容一样, 这是在每个channel的范围的;例如, 假设在通道 “Ch” 上有未确认的交付标签 5,6,7,8, 当设置确认方法的 delivery_tag 为8并且 multiple 设置为true的情况下到达该channel时, 将确认从5到8的所有标签; 如果 multiple 设置为false, 那么值确认标签8; 客户端错误: 双重ack 和 未知Tags 如果消费者不止一次确认同一个递送标签, RabbitMQ将会出现channel错误, 例如 PRECONDITION_FAILED - unknown delivery tag 100, 如果使用未知的delivery tag(投递标签), 则会抛出同样的通道异常; Broker报出 “unknown delivery tag” 的另一种情况是, 无论是positive 还是negative 的应答, 在与收到投递的channel不同的channel上尝试确认; (不过这个一般我们都是使用当前channel应答当前channel) 扩展 - Negative Ack AMQP 0-9-1 中的消费者可以选择使用 手动确认机制; AMQP 0-9-1 规范定义了 basic.reject() 方法, 该方法允许consumer应答单条negatively确认消息，指示broker丢弃或重新发送消息; 不幸的是，basic.reject 不支持批量应答 negatively确认消息; 为了解决这个问题, RabbitMQ支持了 basic.nack() 方法, 该方法提供了 basic.reject 的所有功能, 同时也允许批量应答消息 要批量拒绝消息, 客户端将 basic.nack() 方法的 multiple 标志设置为true; 在这方面, basic.nack 补充了 basic.ack 的批量确认语义; 扩展 - consumer-cancle 当Consumer通过channel从queue中消费消息时, 有很多原因会导致消费停止; 其中一个很明显的原因是, consumer在该channel中发出 basic.cancel, 这将导致consumer被取消, 并且 Rabbit Server 回应 basic.cancel-ok; 其他事件(如queue被删除)或集群场景中队列所在的节点失败, 将导致消费被取消, 但不会通知客户端通道, 这通常是无益的; 测试 消费者代码参考 生产者代码参考","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"Restful","slug":"http/2018-03-06-restful","date":"2018-03-16T11:36:23.000Z","updated":"2018-05-09T06:31:19.000Z","comments":true,"path":"2018/03/16/http/2018-03-06-restful/","link":"","permalink":"http://blog.renyimin.com/2018/03/16/http/2018-03-06-restful/","excerpt":"","text":"简介REST本身并没有创造新的技术, 组件或服务, 主要指的是一组架构约束条件和原则, 隐藏在RESTful背后的理念就是使用Web的现有特征和能力, 更好地使用现有Web标准中的一些准则和约束; 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。 虽然REST本身受Web技术的影响很深, 但是理论上REST架构风格并不是绑定在HTTP上, 只不过目前HTTP是唯一与REST相关的实例; 所以通常描述的REST也是通过HTTP实现的REST; URI的设计 URI的设计应该遵循可寻址性原则, 具有自描述性, 需要在形式上给人以直觉上的关联; 比如: 用_或-来让URI可读性更好例如国内比较出名的开源中国社区, 它上面的新闻地址就采用这种风格, 如 http://www.oschina.net/news/38119/oschina-translate-reward-plan 使用/来表示资源的层级关系例如 https://github.com/rymuscle/chat/issues 就表示了一个多级的资源, 指的是rymuscle用户的chat项目的issues列表 使用?用来过滤资源 (如果记录数量很多，服务器不可能都将它们返回给用户, 比如分页等筛选条件)很多人只是把?简单的当做是参数的传递, 很容易造成URI过于复杂、难以理解; 其实可以把?用于对资源的过滤;例如 https://github.com/rymuscle/chat/pulls 用来表示git项目的所有推入请求;而 /pulls?state=closed 用来表示git项目中已经关闭的推入请求, 这种URL通常对应的是一些特定条件的查询结果或算法运算结果; ,或;可以用来表示同级资源的关系 URI里带上版本号, 如 https://api.example.com/v1/ (Github就是这样做的) 另一种做法是, 将版本号放在HTTP头信息中, 但不如放入URL方便和直观; URI中只应该描述清楚资源的名称, 而不应该包括资源的操作(因为统一资源接口要求使用标准的HTTP方法对资源进行操作, 方法都是有语义的, 所以已经明确描述了这次的操作含义) URI不应该再使用动作来描述, 如下就是一些不符合统一接口要求的URI (它们都在URI中对操作进行了描述):1234GET /getUser/1POST /createUserPUT /updateUser/1DELETE /deleteUser/1 HTTP动词接口应该使用标准的HTTP方法如GET，PUT和POST，并遵循这些方法的语义(通过明确的方法来描述操作), 可以参考博文 HTTP各请求方法详解 或者 Restful架构详解 状态码状态码应该使用HTTP标准状态码 可以参考博文 HTTP协议预览 更多参考","categories":[{"name":"Restful","slug":"Restful","permalink":"http://blog.renyimin.com/categories/Restful/"},{"name":"HTTP","slug":"Restful/HTTP","permalink":"http://blog.renyimin.com/categories/Restful/HTTP/"}],"tags":[{"name":"Restful","slug":"Restful","permalink":"http://blog.renyimin.com/tags/Restful/"},{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"08. 并行连接, 持久连接","slug":"http/2017-12-06-HTTP-08","date":"2017-12-06T12:10:11.000Z","updated":"2018-05-09T06:26:48.000Z","comments":true,"path":"2017/12/06/http/2017-12-06-HTTP-08/","link":"","permalink":"http://blog.renyimin.com/2017/12/06/http/2017-12-06-HTTP-08/","excerpt":"","text":"常被误解的Connection首部 HTTP允许在客户端和最终的源端服务器之间存在一串HTTP中间实体(代理, 高速缓存等)。可以从客户端开始, 逐跳地将HTTP报文经过这些中间设备, 转发到源端服务器上去(或者进行反向传递)。 HTTP的 Connection 首部字段中有一个由,分隔的连接标签列表; Connection首部可以承载3种不同类型的标签, 因此非常令人费解: HTTP首部字段名, 列出了只与此链接有关的首部; 任意标签值, 用于描述此链接的非标准选项; close, 说明操作完成之后需要关闭这条持久连接; 如果连接标签中包含了一个HTTP首部字段的名称, 那么这个首部字段就包含了一些连接有关的信息, 不能将其转发出去, 在将报文转发出去之前, 必须删除Connection首部列出的所有首部字段。 由于Connection首部可以防止无意中对本地首部的转发, 因此将逐跳字首部名放入Connection首部被称为”对首部的保护”。(Connection首部是个逐跳首部, 只适用于单条传输链路, 不应该沿着传输链路向下传输 (参考P101)) 并行连接 在串行请求时, 浏览器可以先完整地请求原始的HTML页面, 然后请求第一个嵌入对象, 然后请求第二个嵌入对象等, 以这种简单的方式对每个嵌入式对象进行串行处理, 很明显这样处理很慢!! HTTP允许客户端打开多条连接, 并行地执行多个HTTP事务, 如下图, 并行加载了四幅嵌入式图片, 每个事务都有自己的TCP连接: 并行连接可能会提高页面的加载速度 包含嵌入对象的组合页面如果能通过并行连接克服单条连接的空载时间和带宽限制, 加载速度也会有所提高。时延可以重叠起来, 而且如果单条连接没有充分利用客户端的因特网带宽, 可以将为用带宽分配来装载其他对象。 如下图, 串行和并行的对比, 并行情况下, 先装载的是封闭的HTML页面, 然后并行处理其余3个事务, 每个事务都有自己的连接。(图片的装载是并行的, 连接的时延也是重叠的) 由于软件开销的存在, 每个连接请求之间总会有一些小的时延, 但连接请求和传输时间基本上都是重叠起来的! 并行连接不一定更快 即使并行连接的速度可能会更快, 但是并不一定总是更快 因为在客户端的网络带宽如果不足时, 大部分的时间可能都是用来传送数据的。在这种情况下, 一个连接到速度较快服务器上的HTTP事务就会很容易耗尽所有可用的Modem带宽。 如果并行加载多个对象, 每个对象都会去竞争这有限的带宽, 每个对象都会以较慢的速度按比例加载, 这样带来的性能提升就很小, 甚至没什么提升。 而且打开大量连接会消耗很多内存资源, 从而引发自身性能问题。 复杂的Web有可能会有数十或数百个内嵌对象, 客户端可能可以打开数百个连接, 但Web服务器通常要同时处理很多其他用户的请求, 所以很少有Web服务器希望出现这样的情况。 一百个用户同时发出申请, 每个用户打开100个连接, 服务器就要负责处理1W个连接, 这会造成服务器性能的严重下降。对高负荷的代理来说也同样如此。 实际上, 浏览器确实使用了并行连接, 但它们会将并行连接的总数限制为一个较小的值(通常是四个)。服务器可以随意关闭来自特定客户端的超量连接。 并行连接可能让人”感觉”更快一些通过上面的介绍, 我们知道并行连接并不总是能使页面加载更快, 但即使实际上没有加快页面的传输速度, 并行连接通常也会让用户觉得页面加载的更快了,因为多个组件对象同时出现屏幕上时, 用户能够看到加载的进展。如果整个屏幕上有很多动作在进行, 即使实际上整个页面的下载时间更长, 用户也会认为Web页面加载得更快一些。 持久连接 HTTP/1.1(以及HTTP/1.0的各种增强版本)允许HTTP设备在事务处理结束之后将TCP连接保持在打开状态, 以便未来的HTTP请求能够重用现存的连接。在事务处理结束之后仍然保持在打开状态的TCP连接被称为持久连接。 非持久连接会在每个事务结束之后关闭, 持久连接会在不同事务之间保持打开状态, 直到客户端或服务器其决定将其关闭为止。 重用已对目标服务器打开的空闲持久连接, 就可以避开缓慢的连接建立阶段。而且已经打开的连接还可以避免慢启动的拥塞使用阶段, 以便更快速地进行数据的传输。 持久连接和并行连接 之前已经了解过”并行连接可以提高复合页面的传输速度, 但并行连接也有一些缺点”;而持久连接有一些比并行连接更好的地方,持久连接降低了时延和连接建立的开销, 将连接保持在已调谐状态, 而且减少了打开连接的潜在数量。但是, 管理持久连接时要特别小心, 不然就会积累大量的空闲连接, 耗费本地以及远程客户端和服务器上的资源。 持久连接与并行连接配合使用可能是更高效的方式。现在, 很多Web应用程序都会打开少量的并行连接, 其中的每一个都是持久连接。 持久连接有两种类型: 比较老的 HTTP/1.0+&quot;keep-alive&quot; 连接, 以及现代的 HTTP/1.1 &quot;persistent&quot; 连接。 HTTP/1.0+keep-alive连接 前言:大约从1996年开始, 很多HTTP/1.0浏览器和服务器都进行了扩展, 以支持一种被称为keep-alive连接的早期实验型持久连接。这些早期的持久连接收到了一些互操作性设计方面问题的困扰, 这些问题在后期的HTTP/1.1版本中都得到了修正, 但很多客户端和服务器仍然在使用这些早期的keep-alive连接。 下图在”串行连接上实现了4个HTTP事务的时间线” 与 “在一条持久连接上实现同样事务” 所需的时间线进行了比较, 显示了keep-alive连接的一些性能优点 由于去除了创建连接和关闭连接的开销, 所以时间线有所缩减 Keep-Alive操作客户端和服务器要配合 keep-alive已经不再使用了, 而且在当前的HTTP/1.1规范中也已经没有了对它的说明了。但浏览器和服务器对keep-alive握手的使用仍然相当广泛, 因此HTTP的实现者应该做好与之进行交互操作的准备. 实现HTTP/1.0 keep alive连接的客户端可以通过包含Connection: Keep-Alive首部请求将一条连接保持在打开状态。 如果服务器愿意为下一条请求将连接保持在打开状态, 就在响应中包含相同的首部。如果响应中没有Connection: Keep-Alive首部, 客户端就认为服务器不支持keep-alive, 会在发回响应报文之后关闭连接。 还有keep-alive首部 注意, keep-Alive首部只是请求将连接保持在活跃状态。发出keep-alive请求之后, 客户端和服务器并不一定会同意进行keep-alive会话。 它们可以在任意时刻关闭空闲的keep-alive连接, 并可随意限制keep-alive连接所处理事务的数量。 可以用Keep-Alive通用首部字段中指定的, 有逗号分隔的选项来调节keep-alive的行为: 参数timeout: 是在Keep-Alive响应首部发送的, 它估计了服务器希望将连接保持在活跃状态的时间。这并不是一个承诺值。 参数max: 是在Keep-Alive响应首部发送的, 它估计了服务器还希望为多少个事务保持此连接的活跃状态。这并不是一个承诺值。 Keep-Alive首部还可以支持任意未经处理的属性, 这些属性主要用于诊断和调试。语法为 name [=value]。 Keep-Alive首部完全是可选的, 但只有在提供了 Connection:Keep-Alive 时才能使用它。 下面这个例子说明服务器最多还会为另外5个事务保持连接的打开状态, 或者将打开状态保持到连接空闲了2分钟之后。 12Connection: Keep-AliveKeep-Alive: max=5, timeout=120 keep-alive连接的限制和规则 在HTTP/1.0中, keep-alive并不是默认使用的。客户端必须发送一个 Connection: Keep-Alive 请求首部来激活keep-alive连接。 Connection: Keep-Alive 首部必须随所有希望保持持久连接的报文一起发送。 如果客户端没有发送Connection: Keep-Alive首部, 服务器就会在那条请求之后关闭连接。 客户端如果探明响应中没有Connection: Keep-Alive响应首部, 就可以知道服务器发出响应之后是否会关闭连接了。 一般都是在检测到连接关闭之后, 就可以确定报文实体主体部分的长度。如果想”无需检测到连接关闭 就能确定报文实体主体部分的长度”, 那你的响应报文的实体主体部分必须有正确的Connect-Length, 有多部件媒体类型, 或者用分块传输编码的方式进行了编码。 在一条keep-alive信道中回送错误的 Connection-Length 是很糟糕的事, 这样的话, 事务处理的另一端就无法精确地检测出一条报文的结束和另一条报文的开始了。 代理和网关必须执行Connection首部的规则, 代理或网关必须在将报文转发出去或将其高速缓存之前, 删除在Connection首部中命名的所有首部字段以及Connection首部本身。 严格来说, 不应该与无法确定是否支持Connection首部的代理服务器建立keep-alive连接, 以防止出现下面要介绍的哑代理问题, 在实际应用中不是总能做到这一点的。 从技术上来讲, 应该忽略所有来自HTTP/1.0设备的Connection首部字段(包括Connection:Keep-Alive), 因为他们可能是由比较老的代理服务器误转发的。但是实际上, 尽管可能会有在老代理上挂起的危险, 有些客户端和服务器还是会违反这条规则。 除非重复发送请求会产生其他副作用, 否则 “如果在客户端受到完整响应之前连接就关闭了, 那么客户端一定要做好重试请求的准备”。 Keep-Alive和哑代理 正常情况下, 如果客户端与一台服务器对话, 客户端可以发送一个 Connection:Keep-Alive 首部来告知服务器它希望保持连接的活跃状态, 如果服务器支持keep-alive, 就回送一个 Connection:Keep-Alive 首部, 否则就不回送。 问题是出在代理上 — 尤其是那些不理解Connection首部, 而且不知道在沿着转发链路将报文转发出去之前应该将Connection首部删除的代理。 很多老式或简单的代理都是盲中继(blind relay), 他们只是将字节从一个连接转发到两一个连接中去, 不对Connection首部进行特殊处理。 下图就是一个Web客户端通过一个作为盲中继使用的哑代理与Web服务器进行对话的例子: 更多参考: P101 盲中继的更多问题参考 4.5.7 (??) 为了防止此类代理通信问题的发生, 现在的代理都决不能转发Connection首部和所有名字出现在Connection值中的首部。 另外还有几个不能作为Connection首部的值, 并且也不能被代理转发或作为缓存响应使用的首部: Proxy-Authenticate, Proxy-Connection, Transfer-Encoding 和 Upgrade; HTTP/1.1 persistent连接 HTTP/1.1主键停止了对keep-alive连接的支持, 用一种名为持久连接(persistent connection)的改进型设计取代了它。 持久连接的目的与keep-alive连接的目的相同, 但机制更优一些。 与HTTP/1.0的keep-alive连接不同, HTTP/1.1持久连接在默认情况下是激活的。除非特别指明, 否则HTTP/1.1假定所有连接都是持久的。 要在事务处理结束之后将连接关闭, HTTP/1.1应用程序必须向报文中显示地添加一个Connection:close首部。 这是与以前的HTTP协议很重要的区别, 在以前的版本中, keep-alive连接要么是可选的, 要么根本就不支持。 HTTP/1.1客户端假定在收到响应后, 除非响应中包含了 Connection:close首部, 不然HTTP/1.1连接就仍维持在打开状态。 但是, 客户端和服务器仍然可以随时关闭空闲的连接。 不发送 Connection:close 并不以为这服务器承诺永远将连接保持在打开状态。 persistent连接的限制和规则 (??) 发送了 Connection:close 请求首部之后, 客户端就无法在那条连接上发送更多的请求了。 如果客户端不想在连接上发送其他请求了, 就应该在最后一条请求中发送一个 Connection:close 请求首部。 只有当连接上所有的报文都有正确的, 自定义报文长度时 – 也就是, 实体主体部分的长度都和响应 Connect-Length 一致, 或者是用分块传输编码方式编码的 — 连接才能持久保持。 HTTP/1.1的代理必须能够分别管理与客户端和服务器的持久连接 — 每个持久连接都值适用于一跳传输。 (由于较老的代理会转发Connection首部, 所以)HTTP/1.1的代理服务器不应该与HTTP/1.0客户端建立持久连接, 除非他们了解客户端的处理能力。 实际上, 这一点是很难做到的, 很多厂商都违背了这一原则。 尽管服务器不应该试图在传输报文的过程中关闭连接, 而且在关闭连接之前至少应该响应一条请求, 但不管Connection首部取了什么值, HTTP/1.1设备都可以在任意时刻关闭连接。 HTTP/1.1应用程序必须能够从异步的关闭中恢复出来, 只要不存在可能会累积起来的副作用, 客户端都应该重试这条请求。(??) 除非重复发送请求会产生其他副作用, 否则 “如果在客户端收到完整响应之前连接就关闭了, 那么客户端必须要重新发送请求” 一个用户客户端对任何服务器或代理, 最多只能维护两条持久连接, 以防服务器过载。 代理可能需要更多到服务器的连接来支持并发用户的通信, 所以如果有N个用户试图访问服务器的话, 代理最多要维持2N条到任意服务器或父代理的连接。 管道化连接 HTTP/1.1允许在持久连接上可选地会用请求管道。这是在keep-alive连接上的进一步性能优化。在相应到达之前, 可以将多条请求放入队列。 当第一条请求通过网络流向地球另一端的服务器时, 第二条和第三条也可以开始发送了。 在高时延网络条件下, 这样做可以降低网络的回环时间, 提高性能。 如下图: 对管道化连接的限制 如果HTTP客户端无法确认连接是持久的, 就不应该使用管道。 必须按照与请求相同的顺序回送HTTP响应。HTTP报文中没有序列号标签, 因此如果收到的响应失序了, 就没办法将其与请求匹配起来了。 HTTP客户端必须做好连接会在任意时刻关闭的准备, 还要准备好重发所有未完成的管道化请求。 如果客户端打开了一条持久连接, 并立即发出了10条请求, 服务器可能在只处理了5条请求后关闭了连接, 剩下的5条请求会失败, 客户端必须能够应对这些过早关闭连接的情况, 重新发出这些请求。 HTTP客户端不应该用管道化的方式发送回产生副作用的请求(比如POST)。 总之, 出错的时候, 管道化方式会阻塞客户端了解服务器执行的是一系列管道化请求中的哪一些。由于无法安全地重试POST这样的非幂等请求, 所以出错时, 就存在某些方法永远不会被执行的风险。 关闭连接“任意”解除连接所有HTTP客户端, 服务器或代理都可以在任意时刻关闭一条TCP传输连接, 通常会在一条报文结束时关闭连接, 但出错的时候, 也可能在首部行中间, 或其他奇怪的地方关闭连接。对管道化持久连接来说, 这种情形是很常见的。HTTP应用程序可以在经过任意一段时间之后，关闭持久连接。比如，在持久连接空闲一段时间之后，服务器可能会决定将其关闭。但是，服务器永远都无法确定在它关闭”空闲”连接的那一刻，在线路的那一头的客户端有没有数据要发送。如果出现这种情况，客户端就会在写入半截请求报文时发现出现了连接错误。 Conetent-Length 及 截尾操作每条HTTP响应都应该有精确的Content-Length首部，用来描述响应主体的尺寸。如果老的HTTP服务器省略了Content-Length或者包含错误的长度指示，这样就要一来服务器发出连接关闭来说明数据的真是末尾。 连接关闭容限,重试及幂等性即使在非错误情况下,连接也可以在任意时刻关闭。HTTP应用程序要做好正确处理非预期关闭的准备。如果在客户端执行事务的过程中, 传输连接关闭了, 那么, 除非事务处理会带来一些副作用, 否则客户端就应该重新打开连接, 并重试一次。对管道化连接来说, 这种情况更加严重一些。客户端可以将大量请求放入队列中排队, 但源端服务器可以关闭连接, 这样就会留下大量未处理的请求, 需要重新调度。 副作用是很重要的问题, 如果在发送出一些请求数据之后, 收到返回结果之前, 连接关闭了, 客户端就无法百分之百地确定服务器端实际激活了多少事务。有些事务, 比如GET一个静态的HTML页面, 可以反复执行多次, 也不会有什么变化。而其他一些事务, 比如向一个在线书店POST一张订单, 就不能重复执行, 不然会有下多张订单的危险。 如果一个事务， 不管是执行一次还是很多次，得到的结果都相同, 这个事务就是幂等的。实现者们可以认为GET、HEAD、PUT、DELETE、TRACE和OPTIONS方法都共享这一特性。客户端不应该以管道化方式传送非幂等请求(比如POST)。否则，传输连接的过早终止就会造成一些不确定的后果。要发送一条非幂等请求，就需要等待来自前一条清求的响应状态。 尽管用户Agent代理可能会让操作员来选择是否对请求进行重试，但一定不能自动重试非幂等方法或序列。比如，大多数浏览器都会在重载一个缓存的POST响应时提供一个对话框，询问用户是否希望再次发起事务处理。 正常关闭连接 正常关闭连接TCP连接是双向的。TCP连接的每一端都有一个输入队列和一个输出队列, 用于数据的读或写。放入一端输出队列中的数据最终会出现在另一端的输入队列中。 完全关闭与半关闭应用程序可以关闭TCP输入和输出信道中的任意一个, 或者将两者都关闭了。套接字调用close()会将TCP连接的输入和输出信道都关闭了, 这被称作 “完全关闭”。还可以用套接字调用shutdown()单独关闭输入或输出信道。这被称为”半关闭”。 TCP关闭及重置错误…. 参考《HTTP权威指南》– 第四章《图解HTTP协议》https://developer.mozilla.org/en-US/docs/Web/HTTPhttps://tools.ietf.org/html/rfc2616","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"11. 内容协商与转码","slug":"http/2017-12-06-HTTP-11","date":"2017-12-06T10:50:27.000Z","updated":"2018-05-09T06:26:42.000Z","comments":true,"path":"2017/12/06/http/2017-12-06-HTTP-11/","link":"","permalink":"http://blog.renyimin.com/2017/12/06/http/2017-12-06-HTTP-11/","excerpt":"","text":"前言 一个URL常常需要代表若干不同的资源 例如那种需要以多种语言提供其内容的网站站点。 如果某个站点有 ‘说法语的’ 和 ‘说英语的’ 两种用户, 它可能想用这两种语言提供网站站点信息; 理想情况下，服务器应当向英语用户发送英文版，向法语用户发送法文版; 而用户只要访问网站主页就可以得到相应语言的内容。 HTTP提供了 内容协商 方法，允许客户端和服务器作这样的决定。 通过这些方法，单一的URL就可以代表不同的资源(比如，同一个网站页面的法语版和英语版)，这些不同的版本称为变体。 除了根据 内容协商 来决定URL代表的是那种版本的资源。另外, 对于有些特定的URL来说, 服务器还可以根据一些其他原则来决定发送什么内容给客户端最合适。在有些场合下, 服务器甚至可以自动生成定制的页面。比如，服务器可以为手持设备把HTML页面转换成WML页面，这类动态内容变换被称为转码。这些变换动作是HTTP客户端和服务器之间进行内容协商的结果。 内容协商技术 共有3种不同的方法可以决定服务器上哪个页面最适合客户端: 让客户端来选择, 服务器自动判定, 或 让中间代理来选。这3种技术分别称为客户端驱动的协商、服务器驱动的协商 以及 透明协商 内容协商技术摘要如下: 客户端驱动 (300)1.对于服务器来说，收到客户端请求时只是发回响应，在其中列出可用的页面，让客户端决定要看哪个，这是最容易的事情。 很显然，这是服务器最容易实现的方式，而且客户端很可能选择到最佳的版本(只要列表中有让客户端选择的足够信息)。 不利之处是每个页面都需要两次请求: 第一次获取列表，第二次获取选择的副本。这种技术速度很慢且过程枯燥乏味，让用户厌烦。 2.从实现原理上来说，服务器实际上有两种方法为客户端提供选项: 一是发送回一个HTML文档，里面有到该页面的各种版本的链接和每个版本的描述信息; 另一种方法是发送回HTTP/1.1响应时，使用 300 Multiple Choices 响应代码。客户端浏览器收到这种响应时，在前一种情况下(发回html文档的情况)，会显示一个带有链接的页面; 在后一种情况下，可能会弹出对话窗口，让用户做选择。不管怎么样，决定是由客户端的浏览器用户作出的 3.除了增加时延并且对每个页面都要进行繁琐的多次请求之外, 这种方法还有一个缺点: 它需要多个URL, 公共页面要一个, 其他每种特殊页面也都要一个。 服务器驱动1.之前已经知道了客户端驱动的协商存在的若干缺点。大部分缺点都涉及客户端和服务器之间通信量的增长, 这些通信量用来决定什么页面才是对请求的最佳响应。 2.而减少额外通信量的一种方法是让服务器来决定发送哪个页面回去，但为了做到这一点，客户端必须发送有关客户偏好的足够信息，以便服务器能够作出准确的决策。服务器通过 客户端请求的首部集 来获得这方面的信息(客户偏好)!! 有以下两种机制可供HTTP服务器评估发送什么响应给客户端比较合适： 检査 客户端请求中的内容协商首部集: 服务器察看客户端发送的 Accept内容协商首部集, 设法用相应的响应首部与之匹配; 根据其他(非内容协商)首部进行变通, 例如，服务器可以根据客户端发送的 User-Agent 首部来发送响应 客户端内容协商首部集1.客户端可以用下面列出的HTTP首部集发送用户的偏好信息 Accept : 告知服务器发送何种媒体类型Accept-Language : 告知服务器发送何种语言Accept-Charset : 告知服务器发送何种字符集Accept-Encoding : 告知服务器采用何种编码 2.实体首部集 和 内容协商首部集 注意: 内容协商首部集与实体首部非常类似(比如 Accept-Encoding 和 Content-Encoding)。不过, 这两种首部的用途截然不同: 实体首部集,像运输标签,它们描述了把报文从服务器传输给客户端的过程中必须的各种报文主体属性; 如下列出的实体首部集来匹配客户端的Accept内容协商首部集 12345Accet首部 实体首部Accept Content-TypeAccept-Language Content-LanguageAccept-Charset Content-TypeAccept-Encoding Content-Encoding （由于HTTP是无状态的协议，表示服务器不会在不同的请求之间追踪客户端的偏好，所以客户端必须在每个请求中都发送其偏好信息） 而内容协商首部集是由客户端发送给服务器用来告知其偏好信息的, 以便服务器可以从文档的不同版本中选择出最符合客户端偏好的那个来提供服务; 内容协商首部中的质量值1.HTTP协议中定义了质量值，允许客户端为每种偏好类别列出多种选项，并为每种偏好选项关联一个优先次序。 例如，客户端可以发送下列形式的Accept-Language首部：Accept-Language: en; q=0.5, fr; q=0.0 , nl; q=1.0, tr; q=0.0 其中q值的范围从0.0-1.0(0.0是优先级最低的，而1.0是优先级最高的)。 上面列出的那个首部，说明该客户端最愿意接收荷兰语(缩写为nl)文档，但英语(缩写为en)文档也行; 无论如何，这个客户端都不愿意收到法语(缩写为fr)或土耳 其语(缩写为tr)的版本; 2.注意: 偏好的排列顺序并不重要，只有与偏好相关的q值才是重要的; 客户端其它请求首部集1.服务器也可以根据客户端其他请求首部集来匹配响应, 比如 User-Agent 首部。例如, 服务器知道老版本的浏览器不支持JavaScript语言，这样就可以向其发送不含有JavaScript的页面版本。 2.由于缓存需要尽力提供所缓存文档中正确的”最佳”版本，HTTP协议定义了服务器在响应中发送的 Vary 首部。 这个首部告知缓存, 客户端, 和所有下游的代理, 服务器根据哪些首部来决定发送响应的最佳版本。 透明协商(vary首部)1.了支持透明内容协商，服务器必须有能力告知代理，服务器需要检査哪些请求首部，以便对客户端的请求进行最佳匹配。但是HTTP/1.1规范中没有定义任何透明协商机制, 不过却定义了 Vary 首部。服务器在响应中发送了Vary首部，以告知中间节点需要使用哪些请求首部进行内容协商 2.代理缓存可以为通过单个URL访问的文档保存不同的副本, 如果服务器把它们的决策过程传给代理,这些代理就能代表服务器与客户端进行协商。（缓存同时也是进行内容转码的好地方，因为部署在缓存里的通用转码器能对任意服务器，而不仅仅是一台服务器传来的内容进行转码） 3.对内容进行缓存的时候是假设内容以后还可以重用。然而，为了确保对客户端请求回送的是正确的已缓存响应, 缓存必须应用服务器在回送响应时所用到的大部分决策逻辑; 4.之前我们已经了解了客户端发送的Accept内容协商首部集; 也了解到, 为了给每条请求选择最佳的响应, 服务器使用了哪些与这些首部集匹配的相应实体首部集。其实, 代理缓存也必须使用相同的首部集来决定回送哪个已缓存的响应。 5.下图展示了涉及缓存的正确及错误的操作序列。 缓存把第一个请求转发给服务器，并存储其响应。 对于第二个请求，缓存根据URL査找到了匹配的文档。但是，这份文档是法语版的，而请求者想要的是西班牙语版的。如果缓存只是把文档的法语版本发给请求者的话，它就犯了错误; 像上面2中提到的, 代理缓存也必须要根据客户端发送来的内容协商首部来给客户端返回正确的响应 Vary首部1.下面是浏览器和服务器发送的一些典型的请求及响应首部: 2.然而, 如果服务器的决策不是依据Accept首部集，而是比如User-Agent首部的话，情况会如何？例如, 服务器可能知道老版本的浏览器不支持JavaScript语言, 因此可能会回送不包含JavaScript的页面版本。如果服务器是根据其他首部来决定发送哪个页面的话, 和Accept首部集一样, 缓存也必须知道这些首部是什么, 这样才能在选择回送的页面时做出同样的逻辑判断。 3.HTTP的 Vary 响应首部中列出了所有客户端请求首部, 服务器可用这些首部来选择文档或产生定制的内容(在常规的内容协商首部集之外的内容)。例如, 若所提供的文档取决于User-Agent首部, Vary首部就必须包含User-Agent; 小结 当新的请求到达时, 代理缓存会根据内容协商首部集来寻找最佳匹配。但在把文档提供给客户端之前, 它还必须检査服务器有没有在已缓存响应中发送Vary首部。 如果有Vary首部, 那么新请求中那些首部的值必须与旧的已缓存的响应的请求首部相同。(也就是说,代理缓存也会保存旧的请求的请求首部和响应首部, 下面一句话更加肯定这一点) 因为服务器可能会根据客户端请求的首部来改变响应, 为了实现透明协商, 代理缓存就必须为每个已缓存变体保存客户端请求首部和相应的服务器响应首部) 如果某服务器的Vary首部看起来像 Vary: User-Agent, Cookie 这样，大量不同的User-Agent和Cookie值将会产生非常多的变体, 而代理缓存必须为每个变体保存其相应的文档版本。当缓存执行査找时，首先会对内容协商首部集进行内容匹配，然后比较请求的变体与缓存的变体。如果无法匹配，缓存就从原始服务器获取文档 转码 我们已经讨论了一个机制, 该机制可以让客户端和服务器从某个URL的一系列文档中挑选出最适合客户端的文档。但是, 实现这些机制的前提是，存在一些满足客户端需求的文档—不管是完全满足还是在一定程度上满足; 然而, 如果服务器没有能满足客户端需求的文档会怎么样呢？服务器可以给出一个错误响应。但理论上，服务器可以把现存的文档转换成某种客户端可用的文档, 这种选项称为转码; 下面列出了一些假设的转码 1234567转换之前 转换之后HTML文档 WML文档高分辨率图像 低分辨率图像彩色图像 黑白图像有多个框架的复杂页面 没有很多框架或图像的简单文本页面有Java小应用程序的HTML页面 没有Java小应用程序的HTML页面有广告的页面 去除广告的页面 有3种类别的转码: 格式转换、信息综合以及内容注入 格式转换 格式转换是指将数据从一种格式转换成另一种格式, 使之可以被客户端査看。通过HTML到WML的转换, 无线设备就可以访问通常供桌面客户端査看的文档了。通过慢速连接访问Web页面的客户端并不需要接收高分辨率图像, 如果通过格式转换降低图像分辨率和颜色来减小图像文件大小的话, 这类客户端就能更容易地査看图像比较丰富的页面了。 格式转换可以由如下内容协商首部集来驱动, 但也能由 User-Agent 首部来驱动。注意: 内容转换或转码 与 内容编码 或 传输编码 是不同的, 后两者一般用于更高效或安全地传输内容, 而前两者则可使访问设备能够査看内容; Accet首部 实体首部 Accept Content-Type Accept-Language Content-Language Accept-Charset Content-Type Accept-Encoding Content-Encoding 信息综合 从文档中提取关键的信息片段称为信息综合(information synthesis), 这是一种有用的转码操作。这种操作的例子包括根据小节标题生成文档的大纲，或者从页面中删除广告和商标 根据内容中的关键字对页面分类是更精细的技术, 有助于总结文档的精髓。这种技术常用于Web页面分类系统中，比如门户网站的Web页面目录 内容注入参见P423 转码与静态预生成的对比 转码的替代做法是在Web服务器上建立Web页面的不同副本, 例如一个是HTML, 一个是WML, 一个图像分辨率高，一个图像分辨率低；一个有多媒体内容，一个没有。 但是，这种方法不是很切合实际，原因很多： 某个页面中的任何小改动都会牵扯很多页面，需要很多空间来存储各页面的不同版本，而且使页面编目和Web服务器编程(以提供正确的版本)变得更加困难。 有些转码操作，比如广告插入(尤其是定向广告插入)，就不能静态实现, 因为插入什么广告和请求页面的用户有关 对单一的根页面进行即时转换，是比静态的预生成更容易的解决方案。 但这样会在提供内容时增加时延。不过有时候其中一些计算可以由第三方进行，这样就减少了Web服务器上的计算负荷——比如可以由代理或缓存中的外部Agent完成转换 下图显示了在代理缓存中进行的转码 参考","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"05. 缓存","slug":"http/2017-11-30-HTTP-05","date":"2017-11-30T13:27:36.000Z","updated":"2018-05-15T09:20:25.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-05/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-05/","excerpt":"","text":"缓存关于缓存的概念本文就不详细展开说明了, 这里所说的web缓存其实就是: 为了降低服务器端的访问频率, 减少通信数量, 一些缓存设备, 如客户端或一些中继代理服务器将获取的数据保存下来, 当再次需要时, 就使用所保存的数据; (当请求抵达缓存设备时, 如果缓存设备本地有”已缓存的”副本, 就可以从本地设备而不是原始服务器中提取这个文档) 使用缓存的优点 减少了服务器端的通信数量, 可以提升用户访问速度 在网络断开的状态下, 也可以在某种程度上继续向用户提供服务 减少了冗余的数据传输, 节省了你的网络费用 缓解了网络本身的瓶颈问题, 不需要更多的带宽就能够更快地加载页面 降低了距离时延, 因为从较远的地方加载页面会更慢一些(即使带宽不是问题, 距离也可能成为问题。每台网络路由器都会增加因特网流量的时延。即使客户端和服务器之间没有太多的路由器, 光速自身也会造成显著的时延。) 综上所述, 缓存对用户体验和通信成本都会造成很大的影响, 所以我们要尽可能地去灵活使用缓存机制。 缓存命中如果一些请求到达缓存设备时, 缓存设备可以用本地已有的副本为这些请求提供服务, 就被称为缓存命中; 缓存未命中如果一些请求到达缓存设备时, 缓存设备本地没有副本提供给这些请求(或者由于某些策略需要无法将副本提供给请求), 而将请求转发给原始服务器, 这就被称为缓存未命中; 缓存再验证文档过期后, 缓存设备并不是直接去原始服务器重新请求文档, 而是尽可能先去原始服务器查看其缓存文档有没有发生变化, 这种 新鲜度检测 被称为 HTTP缓存再验证; 再验证命中参看下文 再验证未命中参看下文 公有缓存和私有缓存通用首部字段(general header fields)Cache-Control有两个缓存响应指令: public 和 private 缓存可以是单个用户专用的, 也可以是数千名用户共享的 专用缓存被称为私有缓存(private cache), 私有缓存是个人的缓存, 包含了单个用户最常用的页面 ; 共享缓存被称为公有缓存(public cache), 公有缓存包含了某个用户团体常用页面 ; 私有缓存私有缓存不需要很大的动力或存储空间, 这样就可以将其做的很小, 很便宜;Web浏览器中就有内建的私有缓存—大多数浏览器都会将常用文档缓存在你个人电脑的磁盘和内存中, 并且允许用户去配置缓存的大小和各种设置; 公有缓存 公有缓存是特殊的共享代理服务器, 被称为缓存代理服务器(caching proxy server), 或者更常见地被称为代理缓存(proxy cache); 代理缓存会从自己本地缓存中给用户提供缓存资源, 或者代表用户与服务器进行联系。公有缓存会接受来自多个用户的访问, 所以通过它可以更好地减少冗余流量 如下图: 每个客户端都会重复地访问一个(还不在私有缓存中的)新的”热门”文档。每个私有缓存都要获取同一份文档, 这样它就会多次穿过网络。 而使用共享的公有缓存时, 对于这个流行的对象, 缓存只要取一次就行了, 它会用共享的副本为所有的请求服务, 以降低网络流量。 缓存模型原始服务器的内容不可能是一成不变的, 所以缓存服务器还需要不时地去进行检测, 看看它们保存的副本是否是服务器上最新的副本; HTTP的缓存机制在RFC7234中进行了详细的定义, 分为过期模型(Expiration Model) 和 验证模型(Validation Model) 两类; 在HTTP协议中, 缓存处于可用状态时称为 fresh(新鲜) 状态, 而处于不可用的状态时则称为 stale(不新鲜) 状态; 过期模型 过期模型是指预先决定响应数据的保存期限, 当到达期限后, 就会再次访问原始服务器端来重新获得所需的数据; 过期模型可以通过在服务器的响应消息里包含何时过期的信息来实现, HTTP1.1中定义了两种实现方法: Cache-Control:max-age 响应消息首部 Cache-control 表示从当前时刻开始所经过的秒数; 该首部常用于控制各类缓存, 除了 max-age 外, 还可以进行各种指定; 稍后会进行介绍 Expires 响应消息首部 老式的HTTP/1.0的实体首部字段, Expires 是使用绝对时间, 并使用RFC 1123中定义的时间格式, 依赖于计算机时钟的正确设置 它们都可以给响应内容加一个过期时间, Cache-Control 首部能使用相对时间, 所以更倾向与使用比较新的 Cache-Control 首部 当以上两者同时使用的时候, Cache-Control 将获得优先 注意: 有些服务器会回送一个 Expires:0 响应头, 试图将文档置于永远过期的状态, 但这种语法是非法的, 可能给某个软件带来问题, 应该试着支持这种结构的输入, 但是不应该产生这种结构的输出; 而 Cache-Control 的 max-age 则可以设置 Cache-Control: max-age=0; Date 首部 max-age的计算会用到名为Date的首部, 该首部用来显示服务器端生成响应消息的时间信息, 从该时间开始计算, 当经过的时间超过 max-age 值时, 则可以认为缓存已过期; 文档过期算法为: expirationTime(过期时间) = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) 验证模型 文档过期后, 缓存设备并不是直接去服务器重新请求文档, 而是尽可能先去原始服务器查看其缓存文档有没有发生变化, 这种 新鲜度检测 被称为 HTTP缓存再验证; 当然, 在服务器没有返回”将缓存数据保存多久”的信息, 服务器就需要通过验证模型的两个响应首部字段(Last-Modified 和 ETag)来告知客户端更新相关的信息, 努力减少客户端不必要的访问; 所以说, 仅仅是已缓存文档过期了, 还不能说明该过期文档和原始服务器上的文档有实际的区别, 这只是意味着 到时间进行再验证了 ; 因为客户端还可以设置一些缓存控制项来进行影响; 在缓存文档过期之前, 缓存设备可以随意使用这些副本, 而且无需与服务器做任何联系, 除非 客户端请求中包含 “阻止提供缓存” 的首部 Cache-Control:no-store ; 或者客户端请求中包含 “只有经过验证才能返回缓存副本”的首部 Cache-Control:no-cache ; 但是一旦已缓存文档过期, 缓存设备就必须与服务器进行再验证核对, 除非你设置了 Cache-Control:only-if-cached, 要求只使用缓存; 再验证依据 为了有效地进行再验证, HTTP定义了一些特殊的请求, 不用从服务器上获取整个对象, 就可以快速检测出内容是否是最新的, HTTP的条件方法可以高效地实现再验证; HTTP定义了5个条件请求首部, 对 缓存再验证 来说有用的2个首部是 If-Mofified-Since 和 If-None-Match, 所有的条件首部都以前缀If-开头。 再验证 请求首部字段 If-Modified-Since 如果自If-Modified-Since指定日期之后, 文档被修改了, If-Modified-Since 条件就为真, 通常GET就会成功执行, 携带新首部的新文档会被返回给缓存, 新首部除了其他信息之外, 还包含了一个新的过期日期; 如果自If-Modified-Since指定日期之后, 文档没被修改, If-Modified-Since 条件就为假, 会向客户端返回一个小的 304 Not Modified响应报文, 为了提高有效性, 不会返回文档主体; 否则, 如果文档发生了变化, 就返回带有新主体的200响应; 请求首部字段 If-Modified-Since 和 实体首部字段 Last-Modified 配合工作; 再验证 请求首部字段 If-None-Match 有些文档可能会被周期性地重写, 但实际包含的数据常常却是一样的。尽管内容没有发生变化, 但是修改日期会发生变化; 有些文档可能内容被修改了, 但是所做的修改并不重要, 不需要让世界范围内的缓存都重装数据(比如对拼写或注释的修改), 涉及到弱验证器; 有些服务器无法准确地判定其页面的最后修改日期; 有些服务器提供的文档会在亚秒间隙发生变化(比如,实时监视器), 对这些服务器来说, 以秒为粒度的修改日期可能就不够用了; 为了解决上述问题, HTTP有一个被称为 实体标签(ETag) 的 版本标识符, 这个实体标签是附加到文档上的任意标签, 标签可能可能包含了文档序列号或版本名, 或是文档内容的校验及其他指纹信息。当对文档进行修改时, 可以修改文档的实体标签来说明这个新的版本。这样, 如果实体标签被修改了, 缓存就可以用 If-None-Match 条件首部来GET文档的新副本了; 如果If-None-Match 与 If-Modified-Since 同时存在, 两个都要进行验证; 请求首部字段 If-None-Match 和 响应首部字段 ETag 配合工作; 再验证命中: 缓存到期后, 在对副本进行再验证时, 会向原始服务器发送一个小的再验证请求。如果发现内容没有变化, 服务器会以一个小的 304 Not Modified 进行响应; 只要缓存设备知道副本仍然有效, 就会再次将副本标识为暂时新鲜的, 并将副本提供给客户端, 而不用原始服务器重新返回文档, 这被称为再验证命中(revalidate hit) 或 缓慢命中(slow hit); 缓存再验证命中还是需要与原始服务器进行核对, 所以会比单纯的缓存命中要慢, 但是它并没有从服务器中获取对象数据, 所以要比缓存未命中要快一些。 再验证未命中: 如果缓存发现原始服务器对象与已缓存副本不同, 则服务器会向客户端发送一条普通的, 带有完整内容的 HTTP 200 OK 响应; 这种方式 不仅需要与原始服务器进行核对, 而且会从服务器中获取对象数据, 所以理论上貌似要比缓存未命中要慢一些; 如果再验证发现服务器对象已经被删除, 服务器就回送一个 404 Not Found 响应, 缓存也会将其本地副本删除; 弱验证器 只要原始服务器内容发生变化, 则实体标签就会变化, 正常情况下, 强验证器就会对比失败, 导致服务器会在一个 200 OK 响应中返回新的内容以及新的Etag标签; 有时, 服务器希望对文档进行一些不重要的修改, 并且不需要使所有已缓存副本都失效HTTP1.1支持的”弱验证器”, 就允许对一些内容做修改, 此时服务器会用前缀 W/ 来标识弱验证器 不管相关的实体值以何种方式发生了变化, 强实体标签都要发生变化, 而相关实体在语义上发生了比较重要的变化时, 弱实体标签页应该发生变化 vary//todo 小结再验证命中 比 缓存未命中 要快再验证未命中 几乎和 缓存未命中 速度一样 Cache-Control 缓存控制字段详解1.之前在各通用首部字段详解已经对各选项进行了说明; 补充 (HTTP Cache-Control: max-age和max-stale=s的区别) immutable : 属于缓存控制的一个扩展属性, 参考 当一个支持immutable的客户端浏览器看到这个属性时, 它应该知道, 如果没有超过过期时间，那么服务器端该页面内容将不会改变, 这样浏览器就不应该再发送有条件的重新验证请求(比如通过If-None-Match 或 If-Modified-Since等条件再向服务器端发出更新检查);也就是说, 通常过去我们使用304回复客户端该页面内容没有变化，但是如果用户按浏览器的刷新或F5键，浏览器会再次向服务器端发出该页面内容请求，服务器端如果确认该页面没有变化，那么发回304给客户端，不再发送该页面的实体内容，虽然这样节省了来回流量，但是如果大型网站的很多用户为了得到及时信息，经常会刷新浏览器，这就造成了大量刷新请求，向服务器端求证该页面是否改变，这会影响网站的带宽，也增加服务器端验证压力。而新的选项immutable可以杜绝这种现象。immutable可以节省HTTP请求,缩短请求时间,这是因为服务器不必再处理304响应了。","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"04. 各通用首部字段详解","slug":"http/2017-11-30-HTTP-04","date":"2017-11-30T11:50:02.000Z","updated":"2018-05-09T06:26:56.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-04/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-04/","excerpt":"","text":"Cache-Control 缓存能力控制1.Cache-Control 缓存指令是单向的, 即请求中存在一个指令并不意味着响应中将存在同一个指令; 该指令是可选的; 指令可以有多个选项, 选项之间通过 , 分隔; 2.可以按请求和响应分为(P377) 缓存请求指令(选项如下) 缓存响应指令(选项如下) 3.注意 no-cache 和 must-revalidate 的区别 no-cache: 告诉浏览器、缓存服务器，不管本地副本是否过期，使用资源副本前，一定要到源服务器进行副本有效性校验; must-revalidate：告诉浏览器、缓存服务器，本地副本过期前，可以使用本地副本；本地副本一旦过期，必须去原始服务器进行有效性校验。(这应该是缓存系统的默认行为, 但must-revalidate指令使得这个要求是明确的参考; 可参考 更多参考后面的博文缓存相关中的介绍 Pragma 兼容Pragma是HTTP1.1之前版本的历史遗留字段, 仅作为与HTTP1.0做向后兼容;除了与只理解 Pragma:no-cache 的HTTP/1.0应用程序进行交互时使用; HTTP/1.1应用程序都应该使用Cache-Control:no-cache; Date 当前时间简单来说就是HTTP报文的创建日期, 它会参与到缓存的过期时间运算中; 公式: expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) 默认情况下, Date 的值为当前时间 而响应首部字段中的 Age 头字段, 是告诉客户端, 源服务器创建的缓存在代理服务器上已经保存了多久, 字段的单位为秒; 代理创建响应时必须加上首部字段Age; Connection 该首部字段具备两个作用: 控制不再转发给代理的首部字段 和 管理持久连接; 更多参考HTTP - 并行连接, 持久连接 未完待续… Via 使用首部字段Via, 是为了追踪客户端与服务器之间的请求和响应报文的传输路径;报文在经过代理或者网关时, 会先在首部字段Via中附加该服务器的信息, 然后再进行转发。 Via首部是为了追踪传输路径, 所以也经常会和TRACE方法一起使用, 比如代理服务器受到由TRACE方法发送过来的请求(其中Max-Forward:0)时, 代理服务器就不能再转发该请求了,这种情况下, 代理服务器会将自身的信息附加到Via首部后, 返回该请求的响应。 未完待续… Transfer-Encoding传输编码 该通用首部字段规定了传输报文主体时采用的编码方式; HTTP/1.1的传输编码方式仅对分块传输编码有效, 即只能设置为 Transfer-Encoding:chunked; 参考 MDN Doc; Content-Encoding 和 Transfer-Encoding 二者经常会结合来用, 其实就是针对 Transfer-Encoding 的分块再进行 Content-Encoding 压缩; 对比 请求首部字段Accept-encoding内容编码 (P369) 请求首部字段 Accept-encoding 是将客户端用户代理(浏览器)所支持的内容编码方式(通常是某种压缩算法) 及 内容编码方式的优先级顺序, 通知给服务器; 通过内容协商, 服务端会选择一个客户端支持的方式, 使用并在 响应报文的实体首部字段 Content-Encoding 中通知客户端, 服务器选择了哪种内容编码方式; 另外, 可以一次性指定多种内容编码! 也可以使用权重q值来表示相对优先级; 正常情况下, 主要采用4种编码方式: gzip, compress, deflate, identity; 即使客户端和服务器都支持某些相同的压缩算法，但如果Accept-encoding:identity, 表示客户端告诉服务器对响应主体不要进行压缩, 出现的两种情况的常见情形是： 要发送的数据已经经过压缩, 再次进行压缩不会导致被传输的数据量更小, 一些图像格式的文件会存在这种情况; 服务器超载, 无法承受压缩需求导致的计算开销, 通常, 如果服务器使用超过80%的计算能力, 微软建议不要压缩; 只要identity(表示不需要进行任何编码)没有被明确禁止使用(没有明确通过 identity;q=0 或是 *;q=0 指令明确设置 identity 的权重值为0), 一旦禁止就表示 服务端必须进行编码; 没有禁止identity, 则服务器禁止返回表示客户端错误的 406 Not Acceptable 响应; 禁止identity后, 服务器才可能因为实在没有合适的内容编码类型而, 返回表示客户端错误的406 Not Acceptable 参考MDN 对比 实体首部字段 Content-encoding内容编码实体首部字段 Content-encoding 会告诉客户端, 服务器对实体的主体部分选用的内容编码方式 对比 请求首部字段Accept-charset请求首部字段 Accept-charset 会告诉服务器, 用户代理(浏览器)所支持的字符集和字符集的相对优先顺序。(P388) 可以一次性指定多种字符集; 也可以使用权重q值来表示相对优先级; 对比 请求首部字段Accept-Language请求首部字段 Accept-charset 会告诉服务器, 用户代理(浏览器)所支持的语言和语言的相对优先顺序。 可以一次性指定多种字符集; 也可以使用权重q值来表示相对优先级; 小结 请求首部字段Accept-encoding 和 实体首部字段Content-encoding来决定内容压缩方式; 通用首部字段Transfer-Encoding用来决定响应实体是否分块; 请求首部字段Accept-charset, Accept-Language 是客户端告诉服务端自己能支持的字符编码和语言及其中的优先顺序; TrailerUpgradeWarning~~未完待续","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"03. HTTP状态码详解","slug":"http/2017-11-30-HTTP-03","date":"2017-11-30T06:30:12.000Z","updated":"2018-05-23T01:36:32.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03/","excerpt":"","text":"1xx 101: 参考博文WebSocket简单示例分析 (做协议升级, 还会响应: Connection: Upgrade)~~未完待续 2xx Web API的设计与开发 P109 200 OK : 200码非常出名, 似乎没有对它进一步说明的必要; 201 Created : 当在服务器端创建数据成功时, 会返回201状态码; 也就是使用 POST 请求方法的场景 (如:用户登录后添加了新用户, 上传了图片等新创建数据的场景) 202 Accepted : 在异步处理客户端请求时, 它用来表示服务器端已经接受了来自客户端的请求, 但处理尚未结束; 在文件格式转换, 处理远程通知(Apple Push Notification等)这类很耗时的场景中, 如果等到所有处理都结束后才向客户端返回响应消息, 就会花费相当长的时间, 造成应用可用性不高; 这时采用的方法是服务器向客户端返回一次响应消息, 然后立刻开始异步处理。 202状态码就被用于告知客户端服务器端已经开始处理请求, 但整个处理过程尚未结束; 比如: 以LinkedIn的参与讨论的API为例如果成功参与讨论并发表意见, 服务器端通常会返回201状态码;但如果需要得到群主的确认, 那么所发表的意见就无法立即在页面显示出来, 这时服务器端就需要返回202状态码; 从广义上来看, 该场景也属于异步处理, 但和程序设计里的异步执行当然不同; 204 No Content : 正如其字面意思, 当响应消息为空时会返回该状态码。 其实就是告诉浏览器, 服务端执行成功了, 但是没什么数据返回给你, 所以你不用刷新页面, 也不用导向新的页面; 在用DELETE方法删除数据时, 服务器端通常会返回204状态码(阮一峰博文也提到过, 对DELETE适用); 除此之外, 也有人认为在使用 PATCH 方法更新数据时, 因为只是更新已有数据, 所以返回204状态码更加自然; 关于204状态码的讨论可以参考 p111; 205 Reset Content : 告诉浏览器, 页面表单需要被重置; 205的意思是服务端在接收了浏览器POST请求以后, 处理成功以后, 告诉浏览器, 执行成功了, 请清空用户填写的Form表单, 方便用户再次填写; 206 Partial Content : 成功执行了一个部分或Range(范围)的请求; 206响应中, 必须包含 Content-Range, Date 以及 ETag或Content-Location首部; 3xx300 Multiple Choices : 客户端驱动方式进行内容协商时, 服务器可能返回多个连接供客户端进行选择 (比如多语言网站可能会出现); 301 Moved Permanently : 在请求的URL已经被移除时使用, 响应的Location首部中应该包含资源现在所处的URL; (比较适合永久重定向) 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是301; 则即便稍后取消了location.php中的跳转(或者修改了跳转地址), 由于浏览器还是会认为你之前的跳转是永久性的, 再次访问www.test.com/location.php仍然会跳转到之前的跳转链接(除非清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 302 Found: 与301类似, 但是客户端应该使用Location首部给出的URL来进行临时定位资源, 将来的请求仍应该使用老的URL; 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是302; 如果稍后取消了location.php中的跳转, 再次访问www.test.com/location.php, 会发现不会进行跳转, 而是访问到 location.php 修改后的代码 (不用清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 303 See Other : HTTP/1.1使用303来实现和302一样的临时重定向; 307 Temporary Redirect HTTP/1.1规范要求用307来取代302进行临时重定向; (302临时重定向留给HTTP/1.0) 所以他也具备302临时重定向的特点; 但是, 与 302, 303 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 308 Permanent Redirect 貌似不是rfc2616的标准 具备和301永久重定向的特点, 需要清除浏览器缓存才行; 但是, 与 301 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 304 Not Modified : 参考博文缓存相关 4xx Web API的设计与开发 P1134字头状态码主要用于描述因客户端请求的问题而引发的错误。也就是说, 服务器端不存在问题, 但服务器端无法理解客户端发送的请求, 或虽然服务器端能够理解但请求却没有被执行, 当遇到这些情况引发的错误时, 服务器端便会向客户端返回这一类别的状态码。因此, 当服务器端返回4字头的状态码时, 就表示客户端的访问方式发生了问题, 用户需要检查一下客户端的访问方式或访问的目标资源等。 400 Bad Request : 表示其他错误的意思, 即其他4字头状态码都无法描述的错误类型; 401 Unauthorized : 表示认证(Authentication)类型的错误 比如当需要先进行登录操作, 而却没有告诉服务器端所需的会话信息(比如token..), 服务器端就会返回401状态码, 告知客户端出错的大致原因; 403 Forbidden : 和401状态码比较相似, 所以也经常被混淆; 其实403表示的是授权(Authotization)类型的错误, 授权和认证的不同之处是: 认证表示”识别前来访问的是谁”, 而授权则表示”赋予特定用户执行特定操作的权限” 通俗地说: 401状态码表示”我不知道你是谁”, 403状态码表示”虽然知道你是谁, 但你没有执行该操作的权限” 404 Not Found : 表示访问的数据不存在, 但是 例如当客户端湿度获取不存在的用户信息时, 或者试图访问原本就不存在的端点时, 服务器就会返回404状态码; 所以, 如果客户端想要获取用户信息, 却得到服务器端返回的404状态码, 客户端仅凭”404 Not Found”将难以区分究竟是用户不存在, 还是端点URI错误导致访问了原本不存在的URI; 405 Method Not Allowed : 表示虽然访问的端点存在, 但客户端使用的HTTP方法不被服务器端允许; 比如客户端使用了POST方法来访问只支持GET方法的信息检索专用的API; 又比如客户端用了GET方法来访问更新数据专用的API等; 406 Not Acceptable : 服务器端API不支持客户端指定的数据格式时, 服务器端所返回的状态码; 比如, 服务器端只支持JSON和XML输出的API被客户端指定返回YAML的数据格式时, 服务器端就会返回406状态码; 408 Request Timeout : 当客户端发送请求至服务器端所需的时间过长时, 就会触发服务器端的超时处理, 从而使服务器端返回该状态码; 409 Conflict: 用于表示资源发生冲突时的错误 (est中就会有该错误码) 比如通过指定ID等唯一键值信息来调用注册功能的API时, 倘若已有相同ID的数据存在, 就会导致服务器端返回409状态码; 在使用邮箱地址及Facebook ID等信息进行新用户注册时, 如果该邮箱地址或者ID已经被其他用户注册, 就会引起冲突, 这时服务器端就会返回409状态码告知客户端该邮箱地址或ID已被使用; 410 Gone : 和 404状态码 相同, 都表示访问资源不存在, 只是410状态码不单表示资源不存在, 还进一步告知资源曾经存在, 只是目前已经消失了; 因此服务器端常在访问被删除的数据时返回该状态码, 但是为了返回该状态码, 服务器必须保存该数据已被删除的信息, 而且客户端也应该知晓服务器端保存了这样的信息; 但是在通过邮箱地址搜索用户信息的API中, 从保护个人信息的角度来说, 返回410状态码的做法也会受到质疑; (所以在此种资源不存在的情况下, 为了稍微安全一些, 返回410状态码需要慎重) 413 Request Entity Too Large : 413也是比较容易出现的一种状态码, 表示请求实体过大而引发的错误 请求消息体过长是指, 比如在上传文件这样的API中, 如果发送的数据超过了所允许的最大值, 就会引发这样的错误; 414 Request-URI Too Large : 414是表示请求首部过长而引发的错误 如果在进行GET请求时, 查询参数被指定了过长的数据, 就会导致服务器端返回414状态码 415 Unsupported Media Type : 和406比较相似 406我们知道是表示服务器端不支持客户端想要接收的数据格式 而415表示的是服务器端不支持客户端请求首部 Content-Type 里指定的数据格式, 也就是说, 当客户端通过POST,PUT,PATCH等方法发送的请求消息体的数据格式不被服务器支持时, 服务器端就会返回415状态码; 例如在只接收JSON格式的API里, 如果客户端请求时发送的是XML格式的数据去请求服务器端, 或者在 Content-Type 首部指定 application/xml, 都会导致该类型错误; 429 Too Many Requests : 是2012年RFC6585文档中新定义的状态码, 表示访问次数超过了所允许的范围; 例如某API存在一小时内只允许访问100次的访问限制, 这种情况下入股哦客户端视图进行第101次访问, 服务器便会返回该状态码; 表示在一定的时间内用户发送了太多的请求, 即超出了”频次限制”, 在响应中，可以提供一个 Retry-After 首部来提示用户需要等待多长时间之后再发送新的请求; 5xx 5字头状态码表示错误不发生在客户端, 而是由服务器自身问题引发的。 500 Internal Server Error : 是web应用程序开发里非常常见的错误, 当服务器代码里存在bug, 输出错误信息并停止运行等情况下, 就会返回该类型的错误; 因此, 不仅限于API, 对于5字头状态码的错误, 都要认真监视错误日志, 使系统在出错时及时告知管理员, 以便在错误发生时做好应对措施, 防止再次发生。 501 Not Implemented : ??? 502 Bad GateWay : ??? 503 Service Unavaliable : 用来表示服务器当前处于暂不可用状态 可以回送:响应首部 Retry-After 表示多久恢复; 不同的客户端与服务器端应用对于 Retry-After 首部的支持依然不太一致; 不过，一些爬虫程序，比如谷歌的爬虫程序Googlebot, 会遵循Retry-After响应首部的规则, 将其与503(Service Unavailable,当前服务不存在)响应一起发送有助于互联网引擎做出判断,在宕机结束之后继续对网站构建索引。 参考:https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After 504 Gateway Time-out: 复现这个错误码比较简单, 让你的php程序模拟耗时请求, 如下代码 123&lt;?phpsleep(70);//模拟耗时，睡70秒echo &quot;睡醒了&quot;; 就会返回 ``` 504 Gateway Time-out nginx/1.11.4 ``` 505 HTTP Version Not Supported: 服务器收到的请求, 使用的是它无法支持的HTTP协议版本; 参考:《HTTP权威指南》、《Web API的设计与开发》","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. HTTP各请求方法详解","slug":"http/2017-11-30-HTTP-02","date":"2017-11-30T03:29:12.000Z","updated":"2018-05-09T06:27:03.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-02/","excerpt":"","text":"前言 HTTP/1.1 中实现的method, 参考RFC2616, 可以看到有: OPTIONS, GET, HEAD, POST, PUT, DELETE, TRACE, CONNECT RFC2616中提到: PATCH，LINK，UNLINK方法被定义，但并不常见;(&lt;图解http协议&gt;中也提到 LINK, UNLINK已经被http1.1废弃); 不同应用各自的实现不同, 有些应用会完整实现, 有些还会扩展, 有些可能会实现一部分; PUT(对比POST) PUT: 对已有资源进行更新操作, 所以是 update 操作; put和post的区别 在HTTP中, PUT被定义为 idempotent(幂等性) 的方法，POST则不是，这是一个很重要的区别 应该用PUT还是POST ？ 取决于这个REST服务的行为是否是idempotent(幂等)的 假如发送两个请求, 希望服务器端是产生两个新数据，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用POST方法; 但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的, 那就应该使用PUT方法; 虽然POST和PUT差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦 POST POST: 上面已经提过了, POST是非幂等的; POST和PUT都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的; PATCH(对比PUT)PATCH不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题 对已有资源的操作 用于资源的部分内容进行更新 (例如更新某一个字段, 具体比如说只更新用户信息的电话号码字段); 而PUT则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变; 当资源不存在时: PATCH 可能会去创建一个新的资源, 这个意义上像是 saveOrUpdate 操作。 参考: https://segmentfault.com/q/1010000005685904/ https://unmi.cc/restful-http-patch-method/ http://restcookbook.com/HTTP%20Methods/patch/ https://tools.ietf.org/html/rfc5789 HEADHEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回，而仅仅返回HTTP头信息;比如: 欲判断某个资源是否存在, 我们通常使用GET, 但这里用HEAD则意义更加明确。 GET比较简单, 直接获取资源; OPTIONS这个方法使用比较少, 它用于获取当前URL所支持的方法;若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST;之前跨域相关博文 CORS方案 not-so-simple request 中的”预检”请求用的请求方法就是 OPTIONS; CONNECT要求用隧道协议连接代理, 如使用SSL TRACE~~未完待续 DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"01. HTTP协议预览","slug":"http/2017-11-30-HTTP-01","date":"2017-11-30T03:25:12.000Z","updated":"2018-05-09T06:27:09.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-01/","excerpt":"","text":"简介 HTTP(Hypertext Transfer Protocol 超文本传输协议), 是在万维网上进行通信时所使用的协议; 协议版本 HTTP/0.9 有很多设计缺, 很快就被HTTP/1.0所取代了;(…只支持GET方法…) HTTP/1.0 第一个得到广泛应用的HTTP版本; HTTP/1.O+ 在HTTP/1.0上扩展了很多非官方的特性, 是非正式的HTTP扩展版本;(支持持久的keep-alivel连接) HTTP/1.1 是当前使用的版本 HTTP-NG(HTTP/2.0) 关注的是性能提升, 目前还未普及 用于HTTP协议交互的信息被称为HTTP报文 请求端(客户端)的报文叫 请求报文; 响应端(服务器端)的叫 响应报文; 请求报文结构报文首部 请求行 : 包含了 请求方法, 请求URL, 客户端请求报文使用的HTTP协议版本 (如: GET / HTTP/1.1) 首部块 请求首部字段 通用首部字段 实体首部字段 其他 空行(CRLF)报文主体响应报文结构报文首部 状态行 : 包含了 服务端响应报文使用的HTTP协议版本, 状态码, 原因短语 (如: HTTP/1.1 200 OK) 首部块 响应首部字段 通用首部字段 实体首部字段 其他 空行(CRLF)报文主体请求方法上面已经了解到, 在 “请求报文” -&gt; “报文首部” -&gt; “请求行” 中, 包含了 请求方法, 具体可参考 各请求方法详解 首部字段从上面还可以看到, 报文中的首部块有如下几种首部字段 请求首部字段响应首部字段通用首部字段Cache-Control 缓存能力控制Pragma 兼容Cache-controlDate 当前时间Transfer-Encoding 传输编码Accept-encoding 内容编码 (请求首部字段) (可能会导致服务器返回 406 Not Acceptable)Content-encoding 内容编码 (实体首部字段)Accept-charset 请求首部字段可参考博文：各通用首部字段详解 实体首部字段状态码详解可参考博文：HTTP状态码详解 ~~未完待续","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"70. 查询性能优化","slug":"mysql/2017-09-27-mysql-70","date":"2017-09-27T12:50:37.000Z","updated":"2018-03-08T06:03:46.000Z","comments":true,"path":"2017/09/27/mysql/2017-09-27-mysql-70/","link":"","permalink":"http://blog.renyimin.com/2017/09/27/mysql/2017-09-27-mysql-70/","excerpt":"","text":"前言之前已经了解了索引优化的相关内容, 它对于高性能是必不可少的, 但还不够, 还需要合理设计查询; 如果查询写的很糟糕, 即使库表结构再合理, 索引再合适, 也无法实现高性能; 查询优化, 库表结构优化, 索引优化需要齐头并进, 一个不落; 慢查询基础优化数据访问 确认应用程序是否在检索大量超过需要的数据, 你可能访问了太多的行, 也可能是太多的列;比如: 总是返回全部的列; 只展示5条数据,你却查出100条; 确认MySQL服务器层是否在分析大量超过需要的数据行; (注意: 索引是在存储引擎层, 一旦服务器层分析的数据过多, 可能你的索引不太合适, 没有在存储引擎层过滤掉数据) 未完待续~~","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"查询性能优化","slug":"查询性能优化","permalink":"http://blog.renyimin.com/tags/查询性能优化/"}]},{"title":"50. EXPLAIN 分析","slug":"mysql/2017-09-25-mysql-50","date":"2017-09-25T13:23:08.000Z","updated":"2018-03-16T06:30:27.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-50/","excerpt":"","text":"准备环境1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB, DEFAULT CHARSET = utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`))ENGINE = InnoDB,DEFAULT CHARSET = utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) select_type select_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 (最常见的查询类别就是 SIMPLE 了) PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 如果使用了UNION查询, 那么EXPLAIN 输出结果类似如下: 123456789101112mysql&gt; EXPLAIN ( SELECT * FROM user_info WHERE id IN ( 1, 2, 3 ) ) UNION( SELECT * FROM user_info WHERE id IN ( 3, 4, 5 ) );+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+3 rows in set (0.00 sec) mysql&gt; type type 字段比较重要, 它提供了判断查询是否高效的重要依据依据; 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等; type 常用的取值有: system: 表中只有一条数据, 这个类型是特殊的 const 类型; ?? const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据(const 查询速度非常快, 因为它仅仅读取一次即可) eq_ref: 此类型通常出现在多表的join查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果, 并且查询的比较操作通常是 =, 查询效率较高, 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using where; Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | NULL |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询, 例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5;+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+2 rows in set (0.00 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录; 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL (没用到索引), 并且 key_len 字段是此次查询中使用到的索引的最长的那个 1234567mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+1 row in set (0.00 sec) 下面对比, 都使用了范围查询, 但是一个可以使用索引范围查询, 另一个不能使用索引 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 另外, 可参考 P185: in语句虽然有时候 type结果也是range (不过, 对于真正的范围查询, 确实是无法使用范围列后面的其他索引了, 但是对于”多个等值条件查询”则没有这个限制) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过ALL类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据, 即 做的是覆盖索引, 当是这种情况时, Extra 字段会显示 Using index 下面的例子中, 查询的 name 字段恰好是一个索引(做到了覆盖索引), 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据;因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index; 1234567mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 下面不但使用了全索引扫描, 而且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;nihao&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) 但是, 如果不使用索引的话, 下面type就是ALL, 表示使用了全表扫描, 并且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age=10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 下面 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一, 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. type小结type 类型的性能比较 : 通常来说, 不同的 type 类型的性能关系如: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的; 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快; 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了; possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引;注意: 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到;(MySQL 在查询时具体使用了哪些索引, 由 key 字段决定) key此字段是 MySQL 在当前查询时所真正使用到的索引 rowsrows 也是一个重要的字段, MySQL 查询优化器根据统计信息, 估算SQL要查找到结果集需要到表中扫描读取的数据行数(上面的例子可以看到, 基本上使用到了索引的话, 真正扫描的行数都很少); 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好 ExtraExplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 比如下面, 使用索引扫描做排序 和 不使用索引扫描做排序 的效果: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY name;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY age;+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using filesort |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+1 row in set (0.00 sec) Using index 与 Using index condition “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 比如下面, 第一个做到了覆盖索引扫描, 后面两个都没做到 mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name,age FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT * FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"49. 索引和锁","slug":"mysql/2017-09-25-mysql-49","date":"2017-09-25T13:10:40.000Z","updated":"2018-03-08T02:57:03.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-49/","excerpt":"","text":"索引可以让查询锁定更少的行 因为InnoDB只有在访问行的时候才会对其加锁, 而索引能够减少InnoDB访问的行数, 从而减少锁的数量;但这只有当InnoDB在存储引擎层就能过滤掉所有不需要的行时才行, 如果索引(处在存储引擎层)无法过滤掉无效的行, 那么在InnoDB检索到数据并发送给服务器层以后, 服务器层才能应用where子句, 这时已经无法避免锁定行了;虽然InnoDB的行锁效率很高, 内存使用也很少, 但是锁定行的时候仍然会带来额外开销;锁定超过需要的行会增加锁争用并减少并发性;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"48. 冗余和重复索引","slug":"mysql/2017-09-25-mysql-48","date":"2017-09-25T10:27:40.000Z","updated":"2018-03-08T02:39:49.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-48/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-48/","excerpt":"","text":"重复索引 是指在相同的列上按照相同的顺序创建的相同类型的索引; 应该避免这样的重复索引, 发现后也应该立即删除; MySQL允许在相同的列上创建多个索引, 但是MySQL需要单独维护重复的索引, 并且优化器在优化查询的时候也需要逐个地进行考虑, 这会影响性能; 冗余索引 和 重复索引 不同, 如果创建了索引(A,B), 在创建索引(A)就是冗余索引, 因为这只是(A,B)索引的前缀索引; 大多数情况下都不需要冗余索引; 因此索引(A,B)也可以当做索引(A)来使用 但是如果再创建索引(B,A), (B) 则都不是冗余索引 有时候为了让两个查询都变快, 也会需要冗余索引 (P179) 应该尽量扩展已有的索引而不是创建新的索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"47. 使用索引扫描来做排序","slug":"mysql/2017-09-25-mysql-47","date":"2017-09-25T10:25:11.000Z","updated":"2018-03-08T05:38:13.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-47/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-47/","excerpt":"","text":"简介 只有当索引的列顺序 和 ORDER BY 子句的顺序完全一致, 并且所有列的排序方向都一样时(要么都是正序, 要么都是倒序), MySQL才能够使用索引来对结果做排序; 如果查询需要关联多张表, 则只有当 ORDER BY 子句引用的字段全部为第一个表时, 才能使用索引做排序; ORDER BY 子句 和 查找型查询的限制是一样的, 需要满足索引的最左前缀的要求, 否则, MySQL都需要亲自去执行排序操作, 而无法利用索引排序; 有一种情况下, ORDER BY 子句可以不用满足最左前缀的要求, 那就是前导列为常量的时候; 比如一张表的索引是 key(a,b,c) , 而 查询语句是 ... where a=100 order by b,c, 即使 order by 不满足最左前缀的要求, 也可以使用索引做排序; P177 列出了很多不可以使用索引做排序的查询; 当查询同时有 ORDER BY 和 LIMIT 子句的时候 像select &lt;col...&gt; from profiles where sex=&#39;m&#39; order by rating limit 10;这种查询语句, 同时使用了order by和limit, 如果没有索引就会很慢; 即使有索引, 如果用户界面有翻页, 翻页比较靠后时, 也会非常慢, 因为随着偏移量的增加, MySQL需要花费大量的时间来扫描需要丢弃的数据; 但是sex的选择性又很低, 如何优化呢? 对于选择性非常低的列, 如果要做排序的话, 可以增加一些特殊的索引来做排序, 例如, 可以创建 (sex, rating)索引 然后采用 延迟关联 , 通过覆盖索引先查询返回需要的主键, 在根据这些主键关联原表获得需要的行;select &lt;col...&gt; from profiles INNER JOIN (select &lt;primart key&gt; from profiles where x.sex=&#39;m&#39; order by rating limit 100000, 10) as x using(primary key)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"46. 覆盖索引","slug":"mysql/2017-09-25-mysql-46","date":"2017-09-25T09:30:26.000Z","updated":"2018-03-08T02:29:53.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-46/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-46/","excerpt":"","text":"介绍 通常, 大家都会根据查询的WHERE条件来创建合适的索引, 不过这只是索引优化的一个方面, 设计优秀的索引应该考虑到整个查询, 而不单单是where条件部分。 覆盖索引 索引确实是一种查找数据的高效方式, 但是MySQL也可以使用索引就能直接获取列的数据, 这样就不需要再去读取数据行; 如果叶子节点中已经包含了要查询的数据, 那么就没有必要再回表查询了; 即, 如果一个索引包含(或者说覆盖)所有需要查询的字段值, 我们就称之为 覆盖索引; 覆盖索引是非常有用的工具, 能够极大地提高性能; 拿InnoDB来说, 覆盖索引就非常有用, 如果你的查询能够做到覆盖你的二级索引列, 那么只需要遍历一次B-Tree(可以直接在二级索引中找到数据), 可以避免对聚簇索引的二次查询; 其他更多参考 P171 注意: 覆盖索引必须要保存索引列的值, 而 哈希索引, 空间索引 和 全文索引 等都不存储索引列的值; 所以MySQL只能使用B-Tree索引做覆盖索引; 如果索引不能覆盖查询所需的全部列, 那就不得不每扫描一次索引记录, 就回表查询一次对应的行, 优化小案例 select * .... : 因为查询从表中选择了所有的列, 而一般你不会创建覆盖了所有列的二级索引, 所以这种局域肯定不会用到覆盖索引; .... where title LIKE &#39;%ren&#39;: Mysql只能在where条件中做索引的 最左前缀匹配的LIKE比较, 而这里的where条件是以通配符开头的LIKE查询; 查看 (P171) 的优化案例 (做表的自关联, 子句使用覆盖索引, 外部不用) 这样虽然无法使用索引覆盖整个查询, 但总算比完全无法利用覆盖索引要好","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"45. 聚簇索引","slug":"mysql/2017-09-24-mysql-45","date":"2017-09-24T12:10:31.000Z","updated":"2018-03-07T13:32:55.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-45/","excerpt":"","text":"简介 聚簇索引并不是一种单独的索引类型, 而是一种 数据存储方式; 因为是存储引擎负责实现索引, 所以不是所有的存储引擎都支持聚簇索引, 这里主要讨论的是 InnoDB引擎的聚簇索引; InnoDB的 聚簇索引 实际上在同一个结构中保存了 B-Tree索引 和 数据行 当表有聚簇索引时, 它的数据行实际上存放在索引的叶子页中(叶子页包含了数据行的 全部列数据) 因为无法同时把数据行存放在两个不同的地方, 所以一个表只能有一个聚簇索引 不过覆盖索引, 可以模拟多个聚簇索引的情况 InnoDB默认会创建聚簇索引: InnoDB通过主键来作为聚簇索引, 如果没有定义主键, 则会选择一个唯一的非空索引代替, 如果连非空索引都没有, InnoDB会隐式定义一个主键来作为聚簇索引; 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上，若使用where id = 14这样的条件查找数据; 则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据; InnoDB只聚集 在同一个磁盘页面中的记录, 因此, 如果数据在物理上是相邻的, 那么在索引上就也是相邻的; 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的; 但是包含相邻键值的页面可能物理上会相距甚远; 聚簇索引的优点 访问速度更快: 聚簇索引将索引和数据保存在同一个B-Tree中, 因此从聚簇索引中获取数据通常比在非聚簇索引中查询要快; 使用覆盖索引的查询, 可以直接使用页节点中的主键值; 聚簇索引缺点 聚簇索引最大限度地提高了I/O密集型应用的性能, 但如果数据全部都放在内存中, 则访问顺序就没那么重要了, 聚簇索引也就没什么优势了; 插入速度严重依赖于插入顺序, 按照主键的顺序插入是速度最快的方式, 但如果不是按照主键顺序, 在完成操作后最好执行 OPTIMIZE TABLE 命令重新组织一下表; 更新聚簇索引的代价很高, 因为会强制InnoDB将每个被更新的行移动到新的位置; 基于聚簇索引的表在插入新行, 或者主键被更新导致需要移动行的时候, 可能面临 “页分裂” 问题; 当前主键值要求必须将这一行插入到某个已满的页中时, 存储引擎会将该页分裂成两个页面来容纳该行, 这就是一次页分裂操作。 页分裂操作会导致表占用更多的磁盘空间 聚簇索引会导致全表扫描变慢, 尤其是行比较稀疏, 或者由于页分裂导致数据存储不连续的时候; 二级索引(非聚簇索引)可能比想象的要更大, 因为在二级索引的叶子节点包含了引用行的主键列; 二级索引访问需要两次索引查找, 而不是一次 二级索引叶子几点保存的 “行指针” 是行的主键; 这意味着通过二级索引查找行, 存储引擎需要找到二级索引叶子节点获得对应的主键值; 然后根据这个主键值去聚簇索引中查找对应的行数据; 这里做了重复工作, 两次 B-Tree 查找, 而不是一次。 InnoDB 和 MyISAM 索引对比 InnoDB支持聚簇索引, 而MyISAM不支持; MyISAM中主键索引和其他索引在索引结构上没有区别; 而InnoDB中 (主键)聚簇索引 和 二级索引(普通索引) 是有区别的;(P167) 上图总结","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"44. 高性能索引策略 -- 多列索引","slug":"mysql/2017-09-24-mysql-44","date":"2017-09-24T09:25:31.000Z","updated":"2018-03-07T12:23:09.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-44/","excerpt":"","text":"前言 在多个列上建立独立的单列索引, 在大部分情况下并不能提高MySQL的查询性能; 例如: film_actor 表在 film_id 和 actor_id 上各有一个单列索引, 但是对于下面这个查询WHERE条件, 这两个单列索引都不是好的选择 select film_id, actor_id from actor where actor_id=1 OR film_id=1; 对于上面的查询 老版本的MySQL会使用全表扫描; 而新版本会使用 索引合并策略(参考P158) 来进行优化, 但这更说明了表上的索引建的很糟糕 接下来除了 之前已经在博文: B-Tree索引中介绍过的多列索引的 左前缀策略; 你还需要关注的是创建索引时, 索引列的顺序 选择合适的索引列顺序 在一个多列索引中, 索引列的顺序首先决定了最左前缀策略在查询时是如何进行的; 其次还意味着索引首先按照最左列进行排序, 其次是第二列, 等等; 所以多列索引的列顺序至关重要; 在不需要考虑排序和分组的时候, 将选择性最高的列放在前面通常是很好的, 这时候索引的作用只是用于优化where条件的查找。 然而, 性能不知是依赖于所有索引列的选择性(整体基数), 也和查询条件的具体值有关, 也就是和值的分布有关; ~~未完待续","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"43. 高性能索引策略 - 独立的列, 前缀索引","slug":"mysql/2017-09-24-mysql-43","date":"2017-09-24T09:20:31.000Z","updated":"2018-03-08T05:22:03.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-43/","excerpt":"","text":"前言前面已经对常见的索引类型及其对应的优缺点有了一定的了解, 接下来要考虑的就是如何 高效正确地选择并使用索引 独立的列独立的列是指: 在查询条件中, 索引列 不能是表达式的一部分, 也不能是函数的参数; (如: select actor_id from actor where actor_id+1=5; 就无法使用索引, 应该始终将索引列单独放在比较符号的一侧, select actor_id from actor where actor_id=4;); 前缀索引 因为B-Tree索引中存储了实际的列值, 所以如果你需要索引的列的内容很长, 就会导致 索引变得大且慢; 对于BLOB, TEXT 或者很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 正常情况下, 不论是创建 单列索引 还是 多列索引, 创建的索引都是以 某个列完整的值 来创建索引, 而 前缀索引则是以 列开始的部分字符 来创建索引, 从而大大节约索引空间, 提高索引效率, 但是这样会降低索引的选择性; 索引选择性 索引选择性: 是指不重复的索引值(基数) 和 数据表的记录总数的比值(当然, 唯一索引的所有索引值都不同, 选择性是1, 这是最好的索引选择性, 性能也是最好的); 假设一张订单表, 按照 city(城市全名) 来分 和 按照 city(第一个字)来分组 , 那肯定前一种情况分出来的组比较多, 也就是不重复的索引值多; 如果按照 city 字段的前3个字符来分组的话, 效果如下 如果按照 city 字段的前7个字符来分组的话, 可以想到, 自然可能会是 分组会更多, 每组的数据会更少 分的组越多, 也就是如果以此长度的前缀创建索引的话, 不重复的索引值也就越多, 那么选择性就越高; 选择性越高, 则查询效率越高, 因为MySQL在查找时能够通过索引就过滤掉更多的行, 否则一个索引还是对应了很多的数据行, 那效率还是很低; 而我们要做的其实就是让我们的 前缀选择性 接近 完整列的选择性 简单点说, 让 city(n) 接近 city 的选择性; 计算完整列的选择性 下面给出了同一个查询中计算不同前缀长度的选择性 前缀索引是一种能使索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 有无法使用前缀索引做覆盖扫描 创建前缀索引: alter table city add key(city(7)), 表示以 city 字段的前7个字符 来创建索引; 参考p189: 根据传统经验, 不应该在选择性低的裂伤创建索引, 但是如果很过查询都用到该列, 比如一个表中的 gender 列, 考虑到使用的频率, 还是建议在创建不同组合的索引时, 将 (sex) 作为前缀!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"42. 哈希索引","slug":"mysql/2017-09-24-mysql-42","date":"2017-09-24T07:01:31.000Z","updated":"2018-03-07T10:04:36.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-42/","excerpt":"","text":"简介 哈希索引(hash index)基于哈希表实现, 只有精确匹配索引中所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code): 哈希索引将所有的哈希码存储在索引中 同时在哈希表中保存指向每个数据行的指针 在MySQL中, 只有Memory引擎显示支持哈希索引, 这也是Memory引擎表的默认索引类型, Memory引擎也支持B-Tree索引。 Memory引擎支持 非唯一哈希索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码, 如果出现这种情况, 索引会以链表的方式存放多个行指针到同一个哈希条目中 查找时, 会1. 先在索引中按照哈希码来找到指向数据行的指针, 2. 然后比较数据行的值是否是你查找的行 哈希索引的限制因为索引自身只要存储对应的哈希值和行指针, 所以索引的结构十分紧凑, 这也让哈希索引的查找速度非常快, 然而哈希索引也有它的限制: 哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快; 哈希索引数据并不是按照索引值顺序排序的, 所以无法用于排序; 不支持 部分索引列匹配查找 , 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 不支持范围查询 (只支持如 =, &lt;&gt;, in 等一些 等值比较) 哈希索引数据非常快, 除非有很多哈希冲突 (因为memory引擎支持非唯一索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码)当出现哈希冲突时, 存储引擎必须遍历 冲突的哈希值 所对应的 链表 中所有的行指针, 逐行到表中进行比较, 直到找到所有符合条件的行; 哈希冲突如果哈希冲突很多的话, 一些索引维护操作的代价也为很高, 如果表中删除一行数据, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的指针, 冲突越多代价越大;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"41. B-Tree索引","slug":"mysql/2017-09-24-mysql-41","date":"2017-09-24T06:50:19.000Z","updated":"2018-03-07T11:41:40.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-41/","excerpt":"","text":"简介 当人们谈论索引时, 如果没有特别指明索引类型, 多半说的就是 B-Tree 索引, 它使用 B-Tree 数据结构来存储数据; MySQL的大多数存储引擎都支持这种索引 (Archive引擎不支持) 存储引擎以不同的方式使用 B-Tree 索引, 性能也各有优劣: MyISAM使用 前缀压缩技术 使得索引更小 InnoDB则按照原数据格式进行存储 MyISAM的索引是通过 数据的物理位置 引用被索引的行叶子页中的值指向被索引的行的物理地址 InnoDB的索引则是通过 主键 引用被索引的行叶子页中的值指向被索引的行的主键 建立在B-Tree结构(从技术上来说是B+Tree)上的索引 注意: B-Tree索引中存储了 被索引的列实际的列值, 指向数据行的指针 (上面的图可能没体现出来, 结合下面多列索引的图找找感觉~~) B-Tree 通常意味着: 所有的值都是 按顺序 存储的;B-Tree对索引列是顺序组织存储的, 所以很适合查找范围数据; 每一个叶子页到根的距离都是相同的； 叶子页比较特别, 他们的指针指向的是被索引的数据, 而不是其他的节点页(不同引擎的”指针”类型不同) 下图显示了多列索引是如何组织数据存储的 对于下表中的每一行数据, 索引中包含了 last_name, first_name, dob(date of birth, 即出生日期) 列的值 1234567CREATE TABLE People last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(&apos;m&apos;, &apos;f&apos;) not null, key(last_name, first_name, dob)); 注意: 索引对多个列的值进行排序的依据是定义索引时列的顺序 (如上图中, 最后两个条目, 两个人的姓和名都一样, 则根据他们的出生日期来进行排列顺序) 最左前缀效应之前提到过 “索引可以包含一个或多个列的值。如果包含多个列, 那么列的顺序也十分重要, 因为MySQL只能高效地使用索引的最左前缀列“ 拿之前的 People 表来做参考(其创建的索引是 key(last_name, first_name, dob)) 全值匹配 : 指的是如果查询条件和某个索引中的所有列值进行匹配, 这样就可以利用到上面创建的索引; 比如, 查找 ‘姓为Allen,名为Cuba,出生日期为1960-01-01’ 的人时 并且指的注意的是, 如果你的查询条件做到了全值匹配, 那么即使你查询条件的顺序不是依照左前缀原则, MySQL也会做优化; 匹配最左前缀: 比如, 查找 ‘姓为Allen’ 的人, 即使用索引的第一列, 这样就可以利用到上面创建的索引; 匹配最左前缀列的前缀: 可以用来匹配索引中第一列的值的开头部分, 比如, 查找 姓以’J’开头 的人, 这样就可以利用到上面创建的索引; 匹配最左前缀范围值: 查找 ‘姓在Allen和Barrymore’ 之间的人, 这样就可以利用到上面创建的索引 精确匹配第一列 并 范围匹配第二列: 查找 ‘姓为Allen并且名字以K开头’ 的人, 这样就可以利用到上面创建的索引 只用访问索引的查询 : 这种查询只需要访问索引, 而无需访问数据行; 后面会单独讨论这种 覆盖索引 的优化。 B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了 B-tree索引的限制 如果查询时, 查询条件不是按照索引的最左列开始写, 则无法使用索引; 上面例子中, 索引就无法用于 ‘查找名字为Bill的人’, 也无法查找 ‘生日为1960-01-01的人’, 因为这两列都不是最左数据列。 也无法用于查找 ‘姓以某个字母结尾的人’ (你建索引时指定的列顺序, 列的值内容 都要符合最左前缀才能利用到索引) 查询条件不能跳过索引中的列 比如, 查询 ‘姓为Smith 并且 生日为1960-01-01’ 就无法使用到索引 如果查询条件中有某个列是范围查询, 则其右边的所有列都无法使用索引来优化查找 比如, where last_name=&#39;Smith&#39; and first_name LIKE &#39;J%&#39; AND dob=&#39;1976-12-23&#39; 这个查询只能使用索引的前两列 如果范围查询的列的值结果有限, 比如数据表中只有2个人是 ‘名字以J开头’ 的, 那你也别用范围查找了, 直接用多个等于条件来代替就行比如: where last_name=&#39;Smith&#39; and (first_name=&#39;Jack&#39; or first_name=&#39;Jieke&#39; ) AND dob=&#39;1976-12-23&#39; 到这里可以看到: 索引列的顺序是非常重要的! 在优化性能的时候, 可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"40. 索引基础","slug":"mysql/2017-09-24-mysql-40","date":"2017-09-24T06:30:25.000Z","updated":"2018-03-07T11:37:16.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-40/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-40/","excerpt":"","text":"索引基础 索引是存储引擎用于快速找到记录的一种数据结构; 索引优化是对查询性能优化最有效的手段了; 索引可以包含 一个 或 多个列 的值; 如果如果索引包含多个列, 索引中列的顺序也非常重要, 因为MySQL只能高效地使用索引的最左前缀列; 创建一个包含两个列的索引 和 创建两个只包含一个列的索引 是大不相同的; 在MySQL中, 索引是在 存储引擎层实现, 而不是在服务器层实现: 索引有很多种类型, 可以为不同场景提供更好的性能; 但并不是每个存储引擎都支持所有的索引类型; 不同存储引擎的索引, 其工作方式和底层实现也可能是不一样的; MySQL支持的索引类型B-Tree索引哈希索引数据空间索引全文索引其他索引类别索引优点索引可以让服务器快速地定位到表的指定位置; 但这并不是索引的唯一作用, 到现在可以看到, 根据创建索引的数据结构不同, 索引也有一些其他的附加组作用 最常见的B-Tree索引, 按照顺序存储数据, 所以MySQL可以用来做 ORDER BY 和 GROUP BY 操作; 因为数据是有序的, 所以B-Tree也就会将相关的列值都存储在一起 因为B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"27. REPEATABLE READ 可重复读","slug":"mysql/2017-09-17-mysql-27","date":"2017-09-17T14:10:52.000Z","updated":"2018-03-07T09:19:48.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-27/","excerpt":"","text":"前言 该隔离级别可以解决不可重复读问题, 脏读问题; 也就是它既可以让事务只能读其他事务已提交的的记录, 又能在同一事务中保证多次读取的数据即使被其他事务修改,读到的数据也是一致的。 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免 脏读, 不可重复读 这两个问题的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MVCC 多版本并发控制","slug":"mysql/2017-09-17-mysql-26","date":"2017-09-17T10:10:21.000Z","updated":"2018-03-07T09:19:44.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-26/","excerpt":"","text":"简介Multiversion Concurrency Control 阿里数据库内核’2017/12’月报中对MVCC的解释是: 多版本控制: 指的是一种 提高并发 的技术。最早的数据库系统, 只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。 &lt;高性能MySQL&gt;中对MVCC的部分介绍 MySQL的大多数事务型存储引擎实现的其实都不是简单的行级锁。基于提升并发性能的考虑, 它们一般都同时实现了多版本并发控制(MVCC)。不仅是MySQL, 包括Oracle,PostgreSQL等其他数据库系统也都实现了MVCC, 但各自的实现机制不尽相同, 因为MVCC没有一个统一的实现标准。可以认为MVCC是行级锁的一个变种, 但是它在很多情况下避免了加锁操作, 因此开销更低。虽然实现机制有所不同, 但大都实现了非阻塞的读操作，写操作也只锁定必要的行。MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。 相关概念 read view 主要是用来做可见性判断的, 比较普遍的解释便是”本事务不可见的当前其他活跃事务”, 但正是该解释, 可能会造成一节理解上的误区, 所以此处提供两个参考, 用来避开理解误区:read view中的高水位low_limit_id可以参考 对于read view快照的生成时机, 也非常关键, 也正是因为生成时机的不同, 造成了RC, RR两种隔离级别的不同可见性 可以参考; 在innodb(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来; 在innodb(默认repeatable committed级别), 事务中每条select语句都会创建一个快照(read view); 参考1234With REPEATABLE READ isolation level, the snapshot is based on the time when the first read operation is performed.使用REPEATABLE READ隔离级别，快照是基于执行第一个读操作的时间。With READ COMMITTED isolation level, the snapshot is reset to the time of each consistent read operation.使用READ COMMITTED隔离级别，快照被重置为每个一致的读取操作的时间。 undo-log 可以参考数据库内核月报2015/04/01 前言Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中, 但从5.6开始，也可以使用独立的Undo表空间。Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作。大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除, 而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo; 在回滚段中的undo logs分为: insert undo log 和 update undo loginsert undo log : 事务对insert新记录时产生的undolog, 只在本事务回滚时需要, 并且在事务提交后就可以立即丢弃;update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除。 InnoDB存储引擎在数据库每行数据的后面添加了三个字段 6字节的事务ID(DB_TRX_ID)字段: 用来标识最近一次对本行记录做修改(insert|update)的事务的标识符, 即最后一次修改(insert|update)本行记录的事务id。至于delete操作，在innodb看来也不过是一次update操作，更新行中的一个特殊位将行表示为deleted, 并非真正删除。 7字节的回滚指针(DB_ROLL_PTR)字段: 指写入回滚段(rollback segment)的 undo log record (撤销日志记录)。如果一行记录被更新, 则 undo log record 包含 ‘重建该行记录被更新之前内容’ 所必须的信息。 6字节的DB_ROW_ID字段: 包含一个随着新行插入而单调递增的行ID, 当由innodb自动产生聚簇索引时, 聚簇索引会包括这个行ID的值, 否则这个行ID不会出现在任何索引中。结合聚簇索引的相关知识点, 大概可以理解为: 如果我们的表中没有主键或合适的唯一索引, 也就是无法生成聚簇索引的时候, InnoDB会帮我们自动生成聚集索引, 但聚簇索引会使用DB_ROW_ID的值来作为主键; 如果我们有自己的主键或者合适的唯一索引, 那么聚簇索引中也就不会包含 DB_ROW_ID 了 , 如果有误, 希望指正, 谢谢。 可见性比较算法 假设要读取的数据行, 最后完成事务提交的事务id(即, 让当前数据行最后落地的事务id)为 trx_id_current ; 当前新开事务id为 new_id 当前新开事务创建的快照 read view 中最早的事务id为 up_limit_id, 最迟的事务id为 low_limit_id(注意 low_limit_id=未开启的事务id=当前最大事务id+1) 比较: 1.trx_id_current &lt; up_limit_id, 这种情况比较好理解, 表示, 新事务在读取该行记录时, 该行记录的最后稳定事务ID小于系统当前所有活跃的事务ID, 所以当前行稳定数据对新事务可见, 跳到步骤5; 2.trx_id_current &gt;= trx_id_last, 这种情况也比较好理解, 表示, 该行记录的 最后稳定事务ID 是在 本次新事务 创建之后才开启的,但是却在本次新事务执行第二个select前就commit了, 所以该行记录的当前值对本次新事务不可见(RR级别), 跳到步骤4; 3.trx_id_current &lt;= trx_id_current &lt;= trx_id_last, 表示: 该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见, 调到步骤4,否则表示可见。 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 trx_id_current，然后跳到步骤1重新开始判断; 5.将该可见行的值返回。 案例分析1.下面是一个非常简版的演示事务对某行记录的更新过程, 当然, InnoDB引擎在内部要做的工作非常多: 2.下面是一套比较算法的应用过程也可参考https://github.com/zhangyachen/zhangyachen.github.io/issues/68中的案例 当前读和快照读 MySQL的InnoDB存储引擎默认事务隔离级别是RR(可重复读), 是通过 “行排他锁+MVCC” 一起实现的, 不仅可以保证可重复读, 还可以部分防止幻读; 为什么是部分防止幻读, 而不是完全防止? 效果: 在如果事务B在事务A执行中, insert了一条数据并提交, 事务A再次查询, 虽然读取的是undo中的旧版本数据(防止了部分幻读), 但是事务A中执行update或者delete都是可以成功的!! 因为在innodb中的操作可以分为当前读(current read)和快照读(snapshot read): 快照读 和 当前读 可参考之前的博文幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read) 小结 InnoDB所谓的MVCC 一般我们认为MVCC有下面几个特点： 每行数据都存在一个版本，每次数据更新时都更新该版本 修改时Copy出当前版本, 然后随意修改，各个事务之间无干扰 保存时比较版本号，如果成功(commit)，则覆盖原记录, 失败则放弃copy(rollback) 就是每行都有版本号，保存时根据版本号决定是否成功，听起来含有乐观锁的味道, 因为这看起来正是，在提交的时候才能知道到底能否提交成功 而InnoDB实现MVCC的方式是: 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） 二者最本质的区别是: 当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ Innodb的实现真算不上MVCC, 因为并没有实现核心的多版本共存, undo log 中的内容只是串行化的结果, 记录了多个事务的过程, 不属于多版本共存。但理想的MVCC是难以实现的, 当事务仅修改一行记录使用理想的MVCC模式是没有问题的, 可以通过比较版本号进行回滚, 但当事务影响到多行数据时, 理想的MVCC就无能为力了。 比如, 如果事务A执行理想的MVCC, 修改Row1成功, 而修改Row2失败, 此时需要回滚Row1, 但因为Row1没有被锁定, 其数据可能又被事务B所修改, 如果此时回滚Row1的内容，则会破坏事务B的修改结果，导致事务B违反ACID。 这也正是所谓的 第一类更新丢失 的情况。 也正是因为InnoDB使用的MVCC中结合了排他锁, 不是纯的MVCC, 所以第一类更新丢失是不会出现了, 一般说更新丢失都是指第二类丢失更新。 MVCC 高并发特点写不影响读，读不影响写，写只影响写只有写写会阻塞!!! 参考 关于read view创建时机: http://www.sohu.com/a/194511597_610509 https://www.cnblogs.com/digdeep/p/4947694.html https://www.zhihu.com/question/265280455/answer/292022808 关于比较算法 low_limit_id 高水位事务: https://github.com/zhangyachen/zhangyachen.github.io/issues/68 https://www.zhihu.com/question/66320138 https://www.zhihu.com/question/265280455/answer/292022808 大咖问答: https://www.zhihu.com/inbox/4577674200 更多可以参考数据库内核月报: https://yq.aliyun.com/articles/303200?spm=5176.100240.searchblog.9.271fd153pQ9FgV 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"MVCC","slug":"MVCC","permalink":"http://blog.renyimin.com/tags/MVCC/"}]},{"title":"25. READ COMMITTED","slug":"mysql/2017-09-17-mysql-25","date":"2017-09-17T06:50:52.000Z","updated":"2018-03-07T09:19:39.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-25/","excerpt":"","text":"前言 READ COMMITTED 隔离级别可以解决高并发场景下, 事务 脏读 的问题; 也就是可以让事务只能读其他事务已完成(提交/回滚)的落地数据; 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免脏读的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"24. READ UNCOMMITTED 未提交读","slug":"mysql/2017-09-17-mysql-24","date":"2017-09-17T06:40:04.000Z","updated":"2018-03-07T09:19:35.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-24/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-24/","excerpt":"","text":"简介 READ UNCOMMITTED 隔离级别会造成 脏读(Dirty Read) 现象的出现:也就是说, 事务 可以读取 其他事务 未提交的数据(事务A中对数据所做的修改, 即使还没有被提交, 对其他事务也都是可见的); 这个级别会导致很多问题, 而且从性能上来说, READ COMMITTED 并不会比其他的级别好太多, 却缺乏其他级别的很多好处, 在实际应用中一般很少使用。 虽然该隔离级别很少使用, 但还是有必要了解一下, 它这个隔离级别究竟是如何进行隔离的, 竟还能容许很多问题的存在? 测试准备环境 先准备一张测试表test_transaction: 1234567891011121314DROP TABLE IF EXISTS `test_transaction`;CREATE TABLE `test_transaction` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `user_name` char(20) NOT NULL COMMENT &apos;姓名&apos;, `age` tinyint(3) NOT NULL COMMENT &apos;年龄&apos;, `gender` tinyint(1) NOT NULL COMMENT &apos;1:男, 2:女&apos;, `desctiption` text NOT NULL COMMENT &apos;简介&apos;, PRIMARY KEY (`id`), KEY `name_age_gender_index` (`user_name`,`age`,`gender`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;INSERT INTO `test_transaction` VALUES (1, &apos;金刚狼&apos;, 127, 1, &apos;我有一双铁爪&apos;);INSERT INTO `test_transaction` VALUES (2, &apos;钢铁侠&apos;, 120, 1, &apos;我有一身铁甲&apos;);INSERT INTO `test_transaction` VALUES (3, &apos;绿巨人&apos;, 0, 2, &apos;我有一身肉&apos;); 如下: 123456789mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 2 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 脏读效果 先查看 客户端1 事务的隔离级别: SELECT @@SESSION.tx_isolation;; 可以看到, InnoDB默认事务隔离级别为 REPEATABLE READ 123456789mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; 重新设置 会话端1 的事务隔离级别为 read uncommitted: SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; 注意, 此时只是当前会话端的隔离级别被改, 其余 会话端 还是默认的 REPEATABLE READ 隔离级别 接下来将 会话端2 的事务隔离级别也设置为read uncommitted; 客户端1 开启事务, 并执行一个查询 ‘读取数据’, 注意, 客户端1 的事务并未提交 1234567891011121314151617181920mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction where id=2;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 |+----+-----------+-----+--------+--------------------+1 row in set (0.00 sec) mysql&gt; 客户端2 开启事务, 并修改客户端1查询的数据 123456789101112131415mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-托尼&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 此时发现, 客户端2 可以对 客户端1 正在读取的记录进行修改 注意, 客户端2此时的事务也并未提交 回到 客户端1, 再次查询数据, 发现数据已经变成了’钢铁侠-托尼’; 然后客户端2 rollback 事务, 再到客户端1中查询, 发现user_name又变成了’钢铁侠’, 那之前读到 ‘钢铁侠-托尼’ 就是脏数据了, 这就是一次 脏读 小结: 可以用一张图来演示整个实验过程 分析该隔离级别如何加锁重新构造测试条件 客户端1开启事务, 然后对数据做修改 1234567mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rymuscle&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 注意, 客户端1此时的事务并未提交 客户端2开启事务, 对相同的数据行做修改 12345mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rym&apos; where id=2;....阻塞等待了 最终会如下: 注意: 在上面的过程, 在客户端2阻塞阶段, 你可以通过一个新的客户端来分析, 客户端2 在锁等待的情况下的 加锁情况 和 事务状态: 查看表的加锁情况: select * from information_schema.INNODB_LOCKS; 事务状态 select * from information_schema.INNODB_TRX; 小结 所以: READ UNCOMMITTED 隔离级别下, 写操作是会加锁的, 而且是X排他锁, 直到客户端1事务完成, 锁才释放, 客户端2才能进行写操作 接下来你肯定会纳闷 “既然该隔离级别下事务在修改数据的时候加的是x锁, 并且是事务完成后才释放, 那一次场测试中, 客户端2 在事务中修改完数据之后, 为什么还没提交事务, 也就是x锁还在, 结果客户端1却能读取到客户端2修改的数据”？ 这完全不符合排他锁的特性啊(排他锁会阻塞除当前事务之外的其他事务的读,写操作) 其实网上已经有人在sqlserver的官网上找到了相关资料: 12345ansactions running at the READ UNCOMMITTED level do not issue shared locks to prevent other transactions from modifying data read by the current transaction. READ UNCOMMITTED transactions are also not blocked by exclusive locks that would prevent the current transaction from reading rows that have been modified but not committed by other transactions. When this option is set, it is possible to read uncommitted modifications, which are called dirty reads. Values in the data can be changed and rows can appear or disappear in the data set before the end of the transaction. This option has the same effect as setting NOLOCK on all tables in all SELECT statements in a transaction. This is the least restrictive of the isolation levels. 翻译翻译, 在思考思考, 大概说的是在 READ UNCOMMITTED 级别运行的事务不会发出共享锁: 也就是事务A在读取数据时, 什么锁都不加; 这样的话, 事务B就可以对同样的数据进行修改(同时会加上排他锁);而事务A要读取事务B未提交的修改, 也不会被事务B所加的排他锁阻止, 因为排他锁会阻止其他事务再对其锁定的数据加读写锁, 但是可笑的是, 事务在该隔离级别下去读数据的话根本什么锁都不加, 这就让排他锁无法排它了; 所以可以得出: READ UNCOMMITTED隔离级别下, 读不会加任何锁。而写会加排他锁，并到事务结束之后释放。(读写不阻塞, 写写阻塞 (但会读到脏数据))","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"23. 事务隔离级别(READ UNCOMMITTED) 与 锁","slug":"mysql/2017-09-17-mysql-23","date":"2017-09-17T06:20:52.000Z","updated":"2018-03-07T09:19:29.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-23/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-23/","excerpt":"","text":"前言 之前几篇博文已经介绍了 Mysql事务, 高并发下事务将会面对的问题 及 解决方案;从之前的博文中可以了解到: MySQL在高并发场景下, 主要采用 事务隔离性中的4种隔离级别 及 MVCC机制 来解决事务可能会面临的一些问题; 接下来主要探讨一下, 在MySQL中, 事务的各隔离级别是如何实现的, 如何解决问题的, 隔离级别和锁之间是什么关系? 隔离级别 和 锁的关系 对于事务中各隔离级别与锁的关系, 先看下一参考美团技术博客中对事务隔离级别的介绍: 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 从上面的博文大概可以了解到: 事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略。 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的。 READ UNCOMMITTED 未提交读READ UNCOMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料:-《高性能MySQL》 MySQL官方文档 慕课mark_rock同学手记 美团技术博客","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"22. 幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read)","slug":"mysql/2017-09-16-mysql-22","date":"2017-09-16T06:10:07.000Z","updated":"2018-03-07T09:19:20.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-22/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-22/","excerpt":"","text":"前言RR + MVCC 虽然解决了 幻读 问题, 但严格来说, 只是部分解决幻读问题 演示 打开 客户端1 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 打开 客户端2 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 1234567891011121314mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) mysql&gt; 在 客户端2 之前开启的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了select幻读)!! 12345678910111213141516171819202122232425262728mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; //并且, 此时`update/delete`也是可以操作这条在事务中看不到的记录的! //( 后面会看到: update，delete也都是当前读) 问题的出现 虽然多次select读取,发现已经克服了幻读问题 但当 在客户端2事务中 insert插入一条id为4的新数据, 发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 – 一致性非阻塞读 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库状态的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果您插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。 如果事务更新或删除由不同事务提交的行, 则那些更改对当前事务就变得可见。 但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 小结 也就是RR隔离级别, 在同一事务中多次读取的话, 只是对 select 克服了幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的select都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 在RR级别下 快照读 是通过MVVC(多版本控制)和undo log来实现的 而当前读 是需要加 record lock(记录锁) 和 gap lock(间隙锁) 来实现的, 如果需要实时显示数据，还是需要通过加锁来实现, 这个时候会使用next-key技术来实现。 快照读, 读取专门的快照(对于RC，快照(ReadView)会在每个语句中创建, 对于RR, 快照是在事务启动时创建的), 快照读的操作如下: 1简单的select操作 (不包括: select ... lock in share mode, select ... for update) 当前读, 读取最新版本的记录, 没有快照, 在InnoDB中, 当前读取根本不会创建任何快照。当前读的操作如下, 可以看到 insert, update, delete都是快照读, 所以这几个操作会察觉到其他事务对数据做的更改, 而select察觉不到: 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后, 使用隔离性的最高隔离级别SERIALIZABLE也可以解决幻读, 但该隔离级别在实际中很少使用!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"21. MySQL高并发事务问题 及 解决方案","slug":"mysql/2017-09-16-mysql-21","date":"2017-09-16T05:40:53.000Z","updated":"2018-04-07T10:39:44.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-21/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-21/","excerpt":"","text":"前言上一篇MySQL事务简介简单介绍了MySQL事务的相关概念及特性; 但在实际场景中, 可能经常会面对一些高并发应用, 此时, 简单了解事务概念和特性就不足以应对问题了; 高并发的事务问题在并发量比较大的时候, 很容易出现 多个事务同时进行 的情况。 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以最终导致了一些问题的出现; 脏读 Mysql中, 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读。 因为 ‘事务A’ 此时读取到的是 尚未被持久化 的数据 (事务B中修改的数据还不具备事务的持久性特性) 事务A此时读取的数据也叫 脏数据事务B 最终可能会因为内部其他后续操作的失败或者系统后续突然崩溃等原因, 导致事务B最终整体提交失败, 从而导致事务A读取的数据被回滚;那么最终 事务A 拿到的自然就是脏的数据了, 因为 事务A 拿到的数据已经和数据表中持久化的真实数据不一致了。 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身特性 隔离性 的 – READ COMMITED或以上隔离级别 解决了这个问题; READ COMMITED 级别保证了: 在事务中, 某条语句执行前, 已经被其他事务提交的数据, 对该语句都是可见的。 不可重复读 现在, 上面的 脏读问题 已经被解决了, 那就意味着事务中每次读取到的数据都是 持久性 的数据(被别的事务最终 提交/回滚 的落地数据)。 但脏读问题的解决, 也仅仅只能保证你在事务中每次读到的数据都是持久性的数据而已。 试想, 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了; 那么, 多次读取数据不一致, 有什么危害呢? 首先, 一个事务中为什么要多次读取同一数据, 什么场景下需要这么做? 1234查询余额: 100给别人汇款: 20记录日志: ....展示余额(你可以直接计算然后返回80, 但如果你是查询的话, 就应该保证你查到的是80, 而不受其他事务干扰) 解决方案 : RR+ 在MySQL中, 事务已经用自身特性 隔离性 的 – REPEATABLE READ或以上隔离级别 解决了这个问题; REPEATABLE READ 级别保证了:1.在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的;2.在事务中, 多次读取同一个数据 (在两次读取操作之间, 无论数据被 提交/回滚 多少次(即无论落地过多少遍), 每次读取的结果都应该是一样的; 幻读 (区分不可重复读取)1.之前经常搞混 不可重复读 和 幻读 这两个概念（这两者确实非常相似） 但 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录)。(可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 2.解决方案: RR + MVCC 其实对于 幻读, 在Mysql的InnoDB存储引擎中, 事务默认的 RR 级别已经通过 MVCC机制 帮我们解决了(并非完全解决), 所以该级别下, 你也模拟不出幻读的场景; 退回到 RC 隔离级别的话, 你又容易把 幻读 和 不可重复读 搞混淆, 所以这可能就是比较头痛的点吧! 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 3.想了解更多, 可以参考下一篇幻读的延伸 更新丢失 最后聊一下 高并发事务 的另一个问题: 丢失更新问题, 该问题和之前几个问题需要区分开, 因为解决方案不是一类! 第一类丢失更新: A事务撤销时, 把已经提交的B事务的更新数据覆盖了不过这种情况在Mysql中不会出现, 因为即使是InnoDB事务的最低隔离级别(READ UNCOMMITED): 会在事务(假设事务A)中对写数据设置x锁, 其他事务(假设事务B)想对相同数据做修改, 要等到事务A数据修改完毕并提交(释放了x锁才行); 由于最低隔离级别会出现脏读, 所以即使事务B在事务A提交之后, 对数据做了修改, rollback回滚到的也是事务A最后的提交数据, 所以不会覆盖; (而更高的RC,RR隔离级别使用了MVCC技术, 事务B回滚的话, 也是回滚到事务A最后的那次提交) 第二类丢失更新: A事务 覆盖 B事务 已经提交的数据，造成B事务所做操作丢失 下图可以看到, 搞笑的是: 也正是因为RR隔离级别所保证的可重复读, 给此类问题的出现提供了基础条件; 这种丢失更新, 无法依靠前三种隔离级别来解决, 只能手动使用 乐观锁(在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制), 悲观锁 来解决。 注意注意: 最高隔离级别Serializable在实际应用场景中并不被采用 (并且, 它也无法解决更新丢失的问题) 因为虽然serialize将事务串行化了, 但对于事务, 只要开启事务的条件都满足, 事务A,B,C假设并行到来, 他们都会顺利依次开启事务(只不过以前是并行开启)); 但是开启之后, 虽然读写都会互相阻塞(经过测试), 但是这只是代表事务之间需要等待, 但最终都会执行, 所以还会造成超卖!; 所以还是会出现 更新丢失 的高并发事务问题: 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 参考资料: 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 《高性能MySQL》","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"20. MySQL事务简介","slug":"mysql/2017-09-16-mysql-20","date":"2017-09-16T03:01:07.000Z","updated":"2018-03-08T13:36:24.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-20/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-20/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的 工作单元, 在这个 独立的 工作单元中, 可以有 一组 操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败。 仍然通过最经典的银行转账应用来解释一下: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过该银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，其实就需要打包在一个事务中, 这样就可以保证一组操作可以作为一个 独立的工作单元 来执行。并且在 独立工作单元(即事务) 中的这三个操作, 只要有任何一个操作失败, 则事务整体就应该是失败的, 那就必须回滚所有已经执行了的步骤。 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚。(其实这里也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发场景离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元, 整个事务中的所有操作要么全部提交成功, 要么全部失败回滚。对于一个事务来说, 不能只成功执行其中的一部分操作, 这就是事务的原子性。 Consistency 一致性你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性总是能够保证 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态; 比如之前转账的例子:转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15)转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115)转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的;比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取A账户的余额, 那么这个程序读到的应该是没有被减100的余额才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务隔离性 的四个 隔离级别, 到时候就知道这里为什么说通常来说对其他事务是不可见的; (但确实也还有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 它仍然无法解决更新丢失的问题(可以参考) Durability 持久性一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久 保存到数据库中; (所谓永久可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方);","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"12. MySQL备份与恢复","slug":"mysql/2017-08-23-mysql-12","date":"2017-08-23T10:51:28.000Z","updated":"2018-03-18T07:29:49.000Z","comments":true,"path":"2017/08/23/mysql/2017-08-23-mysql-12/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/mysql/2017-08-23-mysql-12/","excerpt":"","text":"## ##","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"05. 复制","slug":"mysql/2017-08-18-mysql-05","date":"2017-08-18T10:51:28.000Z","updated":"2018-05-06T03:58:57.000Z","comments":true,"path":"2017/08/18/mysql/2017-08-18-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2017/08/18/mysql/2017-08-18-mysql-05/","excerpt":"","text":"概述 MySQL内建的复制功能是构建基于MySQL的大规模、高性能应用的基础, 这类应用使用所谓的水平扩展的架构。 可以通过为服务器配置一个或多个备库的方式来进行数据同步, 复制功能不仅有利于构建高性能的应用, 同时也是高可用、可扩展性、灾难恢复、备份以及数据仓库等工作的基础。 复制解决的基本问题是让一台服务器的数据与其他服务器的数据保持同步。一台主库的数据可以同步到多台备库上, 备库本身也可以被配置成另外一台服务器的主库。主库和备库之间可以有多种不同的组合方式。 MySQL版本对主从复制的影响: 新版本服务器可以作为老版本服务器的备库, 但是老版本服务器作为新版本服务器的备库通常是不可行的, 因为老版本可能无法解析新版本所采用的新特性或语法; 另外, 所使用的二进制文件格式也可能不相同; 小版本升级通常是兼容的; 开销 复制通常不会增加主库的开销, 主要是启用二进制带来的开销, 但出于对备份或及时从崩溃中恢复的目的, 这点开销也是必要的; 每个备库也会对主库增加一些负载(例如网络I/O开销), 尤其当备库请求从主库读取旧的二进制日志文件时, 可能会造成更高的I/O开销; 另外, 锁竞争也可能阻碍事务的提交; 最后, 如果是从一个高吞吐量的主库上复制到多个备库, 唤醒多个复制线程发送事件的开销将会累加; 复制解决的问题 数据分布: 可以在不同的地理位置来分布数据备份; 负载均衡: 通过MySQL复制可以将读操作分布到多个服务器上, 实现对读密集型应用的优化, 并且实现很方便, 通过简单的代码修改就能实现基本的负载均衡, 备份: 对于备份来说, 复制是一项很有意义的技术补充; 高可用性和故障切换: 复制能够帮助应用程序避免MySQL单点失败, 一个包含复制的设计良好的故障切换系统能够显著地缩短宕机时间; MySQL升级测试: 这种做法比较普遍, 使用一个更高版本的MySQL作为备库, 保证在升级全部实例前, 查询能够在备库按照预期执行; 复制原理概述 简单来说, MySQL的复制有如下三个步骤 在主库上把数据更改记录到二进制日志(Binary Log)中(这些记录被称为二进制日志事件) 备库将主库上的日志复制到自己的中继日志(Relay Log)中 备库读取中继日志中的事件, 将其重放到备库数据之上 高性能MySQL中用下图描述了上面三步 二进制日志记录格式 事实上, MySQL支持两种复制方式: 基于行的复制 和 基于语句的复制; 这两种复制方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制(这也就意味着, 在同一时间点, 备库上的数据可能与主库存在不一致, 并且无法保证主备之间的延迟, 一些大的语句可能导致备库产生几秒,几分钟甚至几个小时的延迟) 这两种方式主要是指在主库在记录二进制日志时所采用的日志格式(Binary Logging Formats), 其实有 STATEMENT, ROW, MIXED 三种配置; 基于语句的日志记录: 早在MySQL3.23版本中就存在; 可以通过使用 --binlog-format = STATEMENT 启动服务器来使用此格式; 基于行的日志记录: 在5.1版本中才被加进来(在5.0之前的版本中是只支持基于语句的复制); 可以通过以 --binlog-format = ROW 启动它来使服务器使用基于行的日志记录; 基于混合日志记录: 对于混合日志记录, 默认情况下使用基于语句的日志记录, 但在某些情况下, 日志记录模式会自动切换为基于行的; 当然, 您可以通过使用--binlog-format = MIXED选项启动mysqld来显式使用混合日志记录; 优缺点未完待续~~ 三个线程 MySQL使用3个线程来执行复制功能(其中1个在主服务器上, 另外两个在从服务器上); 当从服务器发出START SLAVE时, 从服务器创建一个I/O线程, 以连接主服务器并让它发送记录在其二进制日志中的语句; 主服务器创建一个binlog dump线程, 将二进制日志中的内容发送到从服务器; 从服务器I/O线程读取主服务器Binlog dump线程发送的内容, 并将该数据拷贝到从服务器的中继日志中; 第3个线程是从服务器的SQL线程, 是从服务器创建用于读取中继日志并执行日志中包含的更新; 问题?未完待续~~ 配置复制 master服务器上进行sql写操作的时候, 是会引起磁盘变化的; 所以slave服务器要想和master上的数据保持一致, 可以有两种办法: slave按照master服务器上每次的sql写语句来执行一遍; slave按照master服务器磁盘上的变化来做一次变化 ; 主服务器master上的写操作都会被记录到binlog二进制日志中, 从服务器slave去读主服务器的binlog二进制日志, 形成自己的relay中继日志, 然后执行一遍 ; 所以主从配置需要做到 主服务器要配置binlog二进制 从服务器要配置relaylog(中继日志) master要授予slave账号: 从服务器如何有权读取主服务器的 binlog (binlog非常敏感, 不可能让谁去随便读) 从服务器用账号连接master 从服务器一声令下开启同步功能 start slave 注意: 一般会在集群中的每个sql服务器中加一个server-id来做唯一标识 ; 配置启动主从 主服务器配置 1234#主从复制配置server-id=4 #服务器起一个唯一的id作为标识log-bin=mysql-bin #声明二进制日志文件名binlog-format= #二进制日志格式 mixed,row,statement 主服务器的 binlog二进制日志 有三种记录方式 mixed, row, statement ; statement: 二进制记录执行语句, 如 update….. row: 记录的是磁盘的变化 如何选择 binlog二进制日志 记录方式? update salary=salary+100; // 语句短, 但影响上万行, 磁盘变化大, 宜用statement update age=age+1 where id=3; // 语句长而磁盘变化小, 宜用row 你要是拿不准用哪个? 那就设置为mixed, 由系统根据语句来决定; 从服务器配置 首先从服务器肯定要开启relaylog日志功能 ; 从服务器一般也会开启binlog, 一方面为了备份, 一方面可能还有别的服务器作为这台从服务器的slave ; 主从之间建立关系 : 主服务器上建立一个用户: grant replication client,replication slave on *.* to repl@&#39;192.168.56.%&#39; identified by &#39;repl&#39; 告诉从服务器要连接哪个主服务器: 在从服务器上进入mysql执行如下语句 1234567reset slave #可以把之前的从服务器同步机制重置一下change master to master_host=&apos;192.168.56.4&apos;,master_user=&apos;repl&apos;,master_password=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,#当前主服务器产生的binlog走到哪儿了(需要在主服务器上`show maste status`来查看file名和position指针位置)master_log_pos=349 然后查看从服务器的slave状态 show slave status 发现已经连上主服务器了 从服务器中启动slave: start slave 注意: 可以一主多从, 但一个从有多个主就会傻逼了 ; 此时我们做的只是mysql主从复制, 但不是读写分离, 距离读写分离还差一小步; 因为读写分离还需要对sql语句进行判断(可以在php层面判断sql语句进行路由, 决定哪种sql去哪个服务器) 可以参考本人有道笔记上的记录","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]}]}