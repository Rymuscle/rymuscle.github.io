{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Java IO 体系","slug":"nginx/2019-11-08-IO-java","date":"2019-11-08T07:42:02.000Z","updated":"2019-11-12T02:33:14.000Z","comments":true,"path":"2019/11/08/nginx/2019-11-08-IO-java/","link":"","permalink":"http://blog.renyimin.com/2019/11/08/nginx/2019-11-08-IO-java/","excerpt":"","text":"BIO (同步阻塞) 在JDK1.4以前, 使用Java建立网络连接时, 只能采用BIO方式 采用BIO的方式进行网络连接时, 使用的是 java.io.* (即 Java 普通IO), java IO是面向流的(InputStream、OutputStream) 一个容易让初学者迷惑的 BIO小DEMO 启动服务器端后, 程序便阻塞等待了客户端连接了 然后启动客户端, 服务器连接成功后直接打印出了客户端发来的数据 所以整个过程非常容易让人误认为整个程序的阻塞点只是在 serverSocket.accept(), 即, 服务端等待客户端连接时 新DEMO: 可以直观地看到 (我们重点讨论的) 阻塞点, 即 数据从内核空间 拷贝 到用户空间时的 阻塞 (当内核空间数据未准备好时) 单线程 + 同步BIO 问题: 单线程+同步BIO 有两处阻塞: serverSocket.accept() 和 inputStream.read(b) 第二处阻塞会造成的问题: 客户端A连接上来后, 如果不发送数据并被服务端处理, 服务端会一直阻塞在 inputStream.read(b), 其他客户端是连接不上来的 第一处阻塞会造成的问题: 当客户端A连接上来后, 给服务端发送数据并被处理后, 他再次发送数据给服务端时, 由于服务端阻塞在 serverSocket.accept(), 是无法接收到该消息的 小结: 采用如上这种单线程的方式, 是无法处理并发的(客户端连接上之后,就得发送数据, 服务器端处理完一个客户端之后, 才能有新的客户端连接上来) 多线程 + BIO每个客户端连接到服务器时, 服务器都会为该客户端创建一个新的线程(用来接收客户端发来的数据并进行处理), 所以 同步阻塞的代码段 inputStream.read(b); 发生在了每个子线程内, 这样主线程就只负责阻塞等待客户端连接了 (使用 多线程+同步阻塞IO 的模式时, 此处的阻塞是自然要做的, 这确实不是我们重点关注的阻塞点)还可以使用可视化窗口查看进程中的线程数 在高并发场景下, 为每个任务(用户请求)创建一个进程或线程的开销非常大; 多线程编程的复杂度也比较高; 那么是否可以使用单线程进行并发处理? NIO概述这里的 非阻塞IO 其实说的是 IO多路复用, 因为传统非阻塞IO是在用户空间进行轮询, 而 IO多路复用 是在内核进行轮询, 所以看上去 IO多路复用 对用户程序来说是阻塞的, 而 非阻塞IO对用户程序是非阻塞的, 其实 IO多路复用是将非阻塞的位置移到了内核 要让单线程来并发处理多个客户端请求, 首先要知道, 之前的 单线程+同步BIO 有两处阻塞: serverSocket.accept() 和 inputStream.read(b) 要解决这个问题, 那就得让 以上两处都为非阻塞, 即, “没有新的客户端连接时,旧的客户端仍然能发送消息”, “旧的客户端不发送消息时, 新的客户端也可以连接上来” JAVA NIO 有两种解释: 一种叫 非阻塞IO (Non-blocking I/O), 另一种也叫 新的IO(New I/O), 它是一种同步非阻塞的I/O模型 NIO 是 Java 1.4 引入的 java.nio 包, 提供了 Channel、Selector、Buffer 等新的抽象, 可以构建多路复用的、同步非阻塞 IO 程序, 同时提供了更接近操作系统底层高性能的数据操作方式 由于这套API是JDK新提供的I/O API, 因此, 也叫New I/O, 这就是包名 nio 的由来 在理解NIO的时候, 需要区分, 说的是 New I/O 还是 非阻塞IO, New I/O是Java的包, NIO是非阻塞IO概念 可以将 NIO 简单区分为两种：普通的NIO, 和多路复用的NIO（加入了selector管理） 普通的NIO: 线程发起io请求后, 立即返回(非阻塞io), 用户线程不阻塞等待, 但是, 用户线程要定时轮询检查数据是否就绪, 当数据就绪后, 用户线程将数据从用户空间写入socket空间, 或从socket空间读取数据到用户空间(同步) 多路复用的NIO: 上述NIO实现中, 需要用户线程定时轮训, 去检查IO数据是否就绪, 占用应用程序线程资源。IO多路复用模型中, 将检查IO数据是否就绪的任务, 交给系统级别的select或poll模型, 由系统进行监控, 减轻用户线程负担 参考: (https://www.jianshu.com/p/8ad464ed516e, https://my.oschina.net/ljhlgj/blog/1811319) Java BIO 是面向流的, NIO是面向缓冲区的 Java IO面向流意味着每次从流中读一个或多个字节, 直至读取所有字节, 它们没有被缓存在任何地方 NIO则能前后移动流中的数据, 因为是面向缓冲区的 Java NIO 组件 NIO主要有三大核心部分: Channel(通道), Buffer(缓冲区), Selector(选择器) 传统IO是基于字节流和字符流进行操作(基于流), 而NIO是基于Channel和Buffer(缓冲区)进行操作, 数据总是从通道读取到缓冲区中, 或者从缓冲区写入到通道中 Selector(选择器) 用于监听多个通道的 事件(如连接打开, 数据到达), 因此, 单个线程可以监听多个数据通道 Buffer Buffer(缓冲区) 是一个用于存储特定基本类型数据的容器。除了boolean外, 其余每种基本类型都有一个对应的buffer类 Buffer类的子类有 ByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer Channel Channel(通道) 表示到实体, 如硬件设备、文件、网络套接字或可以执行一个或多个不同 I/O 操作(如读取或写入) 的程序组件的开放的连接 Channel接口的常用实现类有 FileChannel(对应文件IO) 、DatagramChannel(对应UDP) 、SocketChannel 和 ServerSocketChannel(对应TCP的客户端和服务器端) Channel和IO中的Stream(流)是差不多一个等级的。只不过Stream是单向的, 譬如：InputStream, OutputStream.而Channel是双向的, 既可以用来进行读操作, 又可以用来进行写操作 SelectorSelector(选择器) 用于监听多个通道的 事件(如连接打开, 数据到达), 因此, 单个的线程可以监听多个数据通道。即用选择器, 借助单一线程, 就可对数量庞大的活动I/O通道实施监控和维护 Java NIO 实现 单线程 非阻塞IONIO是针对服务端的, 客户端没有NIO的概念上述例子中, 一旦并发很高, 那么 socketChannelList 将会变得非常大, 所以其实不应该将轮询交给应用系统来进行, 而是交给操作系统 Java的NIO这块目前底层技术还是IO多路复用为主(Linux 中 epoll 是主要解决方案)Selector(选择器)是Java NIO中能够同时监测多个Channel通道, 并且还能知道Channel上读写事件是否准备好。这样一个Selector线程就可以管理多个Channel, 而不像Blocking IO那样一个线程对应一个监管一个IO事件。 NIO AIO (NIO 2)在 Java 7 中, NIO 有了进一步的改进, 也就是 NIO 2, 引入了异步非阻塞 IO 方式, 也有很多人叫它 AIO(Asynchronous IO) 。异步 IO 操作基于事件和回调机制, 可以简单理解为, 应用操作直接返回, 而不会阻塞在那里,当后台处理完成, 操作系统会通知相应线程进行后续工作。 异步非阻塞, 服务器实现模式为一个有效请求一个线程, 客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理, 深入理解NIO零拷贝及用户空间与内核空间切换https://blog.csdn.net/lemon89/article/details/78290389 https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&amp;mid=2247483941&amp;idx=1&amp;sn=97628f4d69d8607badf39bfeb7557457&amp;scene=21#wechat_redirect AIO(NIO2)—&gt;","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"07. IO 多路复用 select 、poll、epoll","slug":"nginx/2019-11-07-07-IO","date":"2019-11-07T11:31:15.000Z","updated":"2019-11-11T02:44:05.000Z","comments":true,"path":"2019/11/07/nginx/2019-11-07-07-IO/","link":"","permalink":"http://blog.renyimin.com/2019/11/07/nginx/2019-11-07-07-IO/","excerpt":"","text":"之前在学习Linux下的5种IO模型时, 已经简单介绍了 IO多路复用, 接下来对 select、poll、epoll 分别进行学习 linux系统调用表(system call table) 概述 Linux 支持 IO多路复用 的 系统调用有 select、poll、epoll, 这些调用都是内核级别的, 但 select、poll、epoll 本质上都是同步I/O, 先是 block 住等待就绪的 socket, 再是block住将数据从内核拷贝到用户内存 (select\\poll\\epoll 均属于实现多路复用的SystemCall(系统调用)) epoll跟select都能提供多路I/O复用的解决方案, 在现在的Linux内核里有都能够支持, 其中epoll是Linux所特有, 而select则应该是POSIX所规定, 一般操作系统均有实现 多路复用是通过 Linux 的 select\\poll\\epoll 模型实现的, 但它们本质上都是 同步 IO IO 多路复用通过把多个 IO 阻塞复用到同一个 select 的阻塞上, 从而使得系统在单线程的情况下, 可以同时处理多个 client 请求 IO 多路复用技术其实就是通过使用Linux的系统函数 select, poll, epoll 等, 通过其底层的系统调用在操作系统内核层面实现的 (tip: 所以一般说支不支持IO多路复用, 和你的代码关系不大, 主要看操作系统, 如 nginx 在 windows 只支持 select 不支持 epoll, epoll 是内核层面的东西, Windows 是不可支持的) select 说的通俗一点, 就是各个客户端连接的文件描述符也就是套接字, 都被放到了一个集合中, 调用select函数之后会一直监视这些文件描述符中有哪些可读, 如果有可读的描述符那么我们的工作进程就去读取资源 PHP 中有内置的函数来完成 select 系统调用, 函数原型: int socket_select(array &amp;$read , array &amp;$write , array &amp;$except , int $tv_sec [, int $tv_usec= 0 ]) (php 貌似不支持epoll, 需要使用 libevet 拓展) 特点: 最大缺陷是单个进程锁打开的 fd 是有限制的, 32位机器上是1024个, 64位机器上是2048个, 虽然可以改这个数值, 但是会造成性能下降 每次进行select调用都会线性扫描全部的fd集合, 不管哪个socket是活跃的, 都要遍历一遍fdset, 很耗时耗cpu, 当套接字比较多时, 效率就会呈现线性下降(如果能给套接字注册某个回调函数, 当本套接字活跃时, 自动完成相关操作, 就不用轮询, 实际上 epoll 就是改了这儿) 需要维护一个用来存放大量fd的数据结构, select在解决将fd消息传递给用户空间时采用了内存拷贝的方式, 在kernel缓冲区和用户缓冲区之间拷贝这个结构的开销很大, 这样, 其处理效率不高 时间复杂度O(n) pollpoll 本质上跟select没有区别, 只是 poll 没有最大连接数限制, 因为它是用基于链表来存储的时间复杂度O(n) epoll epoll 是当前在Linux下开发大规模并发网络程序的热门选择, epoll在Linux2.6内核中正式引入, 和select相似, 都是IO多路复用(IO multiplexing)技术 按照man手册的说法, epoll是为处理大批量句柄而做了改进的poll 对比于其他模型, epoll 做了如下改进: epoll 没有对描述符数目的限制, 它所支持的文件描述符上限是整个系统最大可以打开的文件数目 (例如, 在1GB内存的机器上, 这个限制大概为10万左右) IO效率不会随文件描述符(fd)的增加而线性下降不同于 忙轮询和无差别轮询, epoll 可以理解为 event poll, 它只会对活跃的socket进行操作, 这是因为在内核实现中, epoll是根据每个fd上面的callback函数实现的。因此, 只有活跃的socket才会主动去调用callback函数, 其他状态的socket则不会, 在这一点上, epoll实现了一个伪AIO, 其内部推动力在内核, 此时我们对这些流的操作都是有意义的 (复杂度降低到了O(1))传统的select/poll的一个致命弱点就是当你拥有一个很大的socket集合时, select/poll每次调用都会线性扫描整个socket集合, 这将导致IO处理效率呈现线性下降 使用 mmap 加速内核与用户空间的消息传递无论是select, poll还是epoll, 它们都需要内核把fd消息通知给用户空间, 因此, 如何避免不必要的内存拷贝就很重要了。对于该问题, epoll 利用 mmap()文件映射内存加速与内核空间的消息传递,即epoll使用mmap减少复制开销 Linux 底层的 epollLinux底层 epoll 的3个实现函数 int epoll_create(int size);epoll_create: 创建一个epoll对象参数size是内核保证能处理最大的文件句柄数, 在socket编程里面就是处理的最大连接数返回的int代表当前的句柄指针, 当然创建一个epoll对象的时候, 也会相应的消耗一个fd, 所以在使用完成的时候, 一定要关闭, 不然会耗费大量的文件句柄资源 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);epoll_ctl: 可以操作上面建立的 epoll例如, 将刚建立的socket加入到epoll中让其监控, 或者把 epoll正在监控的某个socket句柄移出epoll, 不再监控它等等epfd, 就是创建的文件句柄指针op是要做的操作, 例如删除, 更新等event 就是我们需要监控的事件 int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);epoll_wait: 在调用时, 在给定的timeout时间内, 当在监控的所有句柄中有事件发生时, 就返回用户态的进程epoll的高效就在于, 当我们调用epoll_ctl往里塞入百万个句柄时, epoll_wait仍然可以飞快的返回, 并有效的将发生事件的句柄发送给用户。这是由于我们在调用epoll_create时, 内核除了帮我们在epoll文件系统里建了个file结点, 在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外, 还会再建立一个list链表, 用于存储准备就绪的事件, 当epoll_wait调用时, 仅仅观察这个list链表里有没有数据即可。有数据就返回, 没有数据就sleep, 等到timeout时间到后即使链表没数据也返回。所以, epoll_wait非常高效。 那么, 这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时, 除了把socket放到epoll文件系统里file对象对应的红黑树上之外, 还会给内核中断处理程序注册一个回调函数, 告诉内核, 如果这个句柄的中断到了, 就把它放到准备就绪list链表里。所以, 当一个socket上有数据到了, 内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。（当网卡里面有数据的时候, 会发起硬件中断, 提醒内核有数据到来可以拷贝数据。当网卡通知内核有数据的时候, 会产生一个回调函数, 这个回调函数是epoll_ctl创建的时候, 向内核里面注册的。回调函数会把当前有数据的socket（文件句柄）取出, 放到list列表中。这样就可以把存放着数据的socket发送给用户态, 减少遍历的时间, 和数据的拷贝） 区别 Linux 内存映射 内存映射, 简而言之就是将 用户空间 的一段内存区域映射到 内核空间, 映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间, 相反, 内核空间对这段区域的修改也直接反映用户空间 那么, 对于内核空间&lt;——&gt;用户空间两者之间需要大量数据传输等操作的话效率是非常高的 首先, 驱动程序先分配好一段内存, 接着用户进程通过库函数 mmap() 来告诉内核要将多大的内存映射到内核空间 用户空间 mmap() 函数: void *mmap(void *start, size_t length, int prot, int flags,int fd, off_t offset) start: 用户进程中要映射的某段内存区域的起始地址, 通常为NULL（由内核来指定） length: 要映射的内存区域的大小 prot: 期望的内存保护标志 flags: 指定映射对象的类型 fd: 文件描述符（由open函数返回） offset: 要映射的用户空间的内存区域在内核空间中已经分配好的的内存区域中的偏移, 大小为PAGE_SIZE的整数倍","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"06. 网络IO 基础概念","slug":"nginx/2019-11-07-06-IO","date":"2019-11-07T07:10:53.000Z","updated":"2019-11-11T06:54:06.000Z","comments":true,"path":"2019/11/07/nginx/2019-11-07-06-IO/","link":"","permalink":"http://blog.renyimin.com/2019/11/07/nginx/2019-11-07-06-IO/","excerpt":"","text":"文件描述符 Linux 会把所有的外部设备都看成一个文件来操作, 对外部设备的操作可以看成是对文件的操作 当我们对外部设备(文件)进行读写时, 都会通过内核提供的系统调用, 内核会给我们返回一个 File Descriptor, 这个描述符是一个数字, 指向内核的一个结构体, 我们应用程序对文件的读写就是对描述符指向的结构体的读写 文件描述符(File descriptor): 其形式上是一个非负整数 ,它是一个索引值, 指向内核为每一个进程所维护的该进程打开文件的记录表 注意, 文件描述符 是一个抽象的概念 (这里的 文件 应该理解为 信息载体, 而不要直接理解为磁盘上的文件) Socket 是最初就作为UNIX操作系统的一部分而开发的, 所以其API和系统其他IO设备集成在一起 (套接字是一个抽象出来的概念,本质上也是一个文件描述符) 对于通常的文件IO来说, 其文件描述符最终是指向磁盘文件的 但是对于网络IO来说, socket的读写也会有相应的描述符, 称为 socketfd( socket 描述符), Socket描述符是一个指向内部数据结构的指针, 它指向描述符表入口, 调用Socket函数时, socket执行体将建立一个Socket, 实际上”建立一个Socket”意味着为一个Socket数据结构分配存储空间 简述 网络IO 主要工作 为了执行网络I/O, 一个进程必须做的第一件事就是调用 socket() 函数, 指定期望的通信协议类型。该函数只是作为一个简单的接口函数供用户调用, 调用该函数后将进入内核栈进行系统调用 sys_socket() (linux系统调用表(system call table)) 网络 IO 和 磁盘IO 类似 当进程读取磁盘上的文件时, 磁盘IO 的主要工作就是将 磁盘上的文件内容读取到 内核缓存中, 然后最终由 内核缓存 中拷贝到用户空间(缓存/无缓存), 最后被进程拿到数据 而网络IO与磁盘IO的读取过程类似, 他是将从网卡流入到内核空间的数据, 最终拷贝到用户空间的进程, 进而被进程所使用 小结: 简单来说, IO 读取的过程就是将 从外部设备来的数据 从 内核空间 拷贝到 用户空间的 过程! （写入则相反） Linux 五种 I/O 模型阻塞IO (blocking IO) 在Unix中,默认情况下所有的socket都是blocking, 一个典型的读操作流程大概如下 当用户进程调用了 recvfrom() 这个函数(其底层是进行 系统调用 sys_recvfrom() ), kernel内核 就开始了IO的第一个阶段: 准备数据 对于 网络IO 来说, 很多时候数据在一开始可能还没有到达(比如, 还没有收到一个完整的UDP包), 这个时候kernel就要等待足够的数据到来, 这个过程是需要等待的, 也就是说数据被拷贝到 操作系统内核的缓冲区 是需要一个过程的, 在这个过程中, 内核在做等待(被阻塞), 用户进程也在等待(被阻塞) kernel一直等到数据准备好了, 也就是数据从外设拷贝到了 操作系统内核缓冲区 中了, 接下来才会将数据从 kernel缓冲区 拷贝到 用户内存, 拷贝完成一组数据, 用户进程才暂时会解除block的状态, 重新运行起来 上述过程中有两处可能会被阻塞的地方: 外设(网卡) --&gt; 操作系统内核缓冲区, 操作系统内核缓冲区 --&gt; 用户空间tip: 外设数据准备好后, 内核就处于非阻塞新状态了(数据会被拷贝到操作系统的内核缓冲区), 但缓冲区只要没满, 对于用户空间进程来说, 其实仍是处于阻塞状态, 因为它没有等到从内和缓冲区拷贝来的数据 优点和缺点 优点: 一个线程处理一个任务, 编程模型比较简单 （另外, 进程处于 阻塞状态 时, 是不占用CPU资源的） 缺点: 一个线程只能处理一个任务 非阻塞IO (non-blocking IO) 进程把一个 套接字 设置成 非阻塞 是在告诉内核, 当 内核中的数据 因为某些原因(未准备好,或者内核区未满) 尚无法拷贝数据到用户内核空间 时, 需要返回一个error; 当对一个 non-blocking socket 执行读操作时, 流程如下: 从用户进程角度讲, 它发起一个 read 操作后, 并不需要等待, 而是马上就得到了一个结果, 用户进程判断结果是一个error时, 它就知道数据还没有准备好, 于是它可以再次发送read操作, 一旦kernel中的数据准备好了, 并且又再次收到了用户进程的system call, 那么它马上就将数据拷贝到了用户内存, 然后返回, 所以, 用户进程其实是需要不断的主动询问kernel数据好了没有 也就是我们的用户进程需要自己不断的测试数据是否已经准备好, 如果没有准备好, 继续测试, 直到数据准备好为止, 在这个不断测试的过程中, 会大量的占用CPU的时间 (Linux下, 可以通过设置 socket 使其变为 non-blocking, 当使用socket()函数和WSASocket()函数创建套接字时, 默认都是阻塞的。在创建套接字之后, 通过调用ioctlsocket()函数, 将该套接字设置为非阻塞模式。Linux下的函数是:fcntl()) 优点和缺点 优点: 相较于阻塞模型, 非阻塞模型下, 线程不用再等待任务, 而是可以把时间花费到其它任务上, 也就是说这个当前线程可以尝试去同时处理多个任务 (编程) 缺点: 导致任务完成的响应延迟增大了, 因为每隔一段时间才去执行询问的动作, 但是任务可能在两个询问动作的时间间隔内完成, 这会导致整体数据吞吐量的降低 (另外, 不停地轮询, 不是每次都成功, 所以可能会消耗大量CPU资源) IO 多路复用概述 IO 多路复用 (IO multiplexing) 就是使用 select, poll, epoll (有些地方也称这种IO方式为 event driven IO ) 这些方法 (linux系统调用表(system call table)) 可以找到这些方法及其底层对应的系统调用方法 有了I/O复用, 我们就可以调用 select, poll, epoll, 它们最终仍然是让用户进程处于阻塞状态, 只不过是内核在其内部对这几个方法所管辖的多个 socket fd 该进行轮询 在这些方法阻塞用户进程后, 内核会轮询检查每个IO连接的内核缓冲区的数据是否准备好了 如果内核管理的某个 socket fd 的内核缓冲区的数据准备好了, 则 select 会有返回值, 用户进程可以进一步去调用 recvfrom() 先看一下 select 函数原型 12#include &lt;sys/select.h&gt;int select(int nfds, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout); 参数解释: nfds: 需要监视的最大文件描述符+1； readfds: 监视可读文件描述符集合 （select在调用之前, 需要手动在应用程序里将要监控的文件描述符添加到 fed_set 集合中, 然后加载到内核进行监控) writefds: 监视可写文件描述符集合 exceptfds: 监视异常文件描述符集合 timeout: 设置select的等待时间 (也就是select的阻塞进程的时间, 设置为NULL则表示没有timeout, 用户进程会一直阻塞, 该参数直接就明示了 select 阻塞用户进程的特性)返回值:负值: select错误正值: 某些文件可读写或出错0: 等待超时, 没有可读写或错误的文件如果参数timeout设为NULL, 则表示select没有timeout IO 多路复用 VS 同步非阻塞 select 调用是内核级别的, select的轮询也是内核在其内部执行的 (你事先需要把你需要执行的多个 IO操作的 socket fd 通过 select 传给内核, 让内核帮你进行轮询), 对用户进程来说,一个进程虽然是阻塞的,但其内部可以对多个socket fd进行监控 (另外, 在io复用模型下, 对于每一个socket, 一般都设置成 non-blocking, 但其实整个用户进程是一直被block的, 只不过用户进程不是被 socket IO 给block的, 而是被 select 这个函数block住的) 而 非阻塞模型中的轮询 是 应用程序进程在 用户空间 进行的轮询 (你也可以在一个进程中对多个 socket fd 进行轮询 ), 对用户层来说, 进程确实是非阻塞的 其实 select 相比较 non-blocking 来说, 在单个任务的情况下可能要更差一些, 因为这里调用了 select 和 recvfrom 两个 system call, 而 non-blocking 只调用了一个 recvfrom , 但是用select的优势在于它可以帮你同时处理多个 socket fd, 而不用你自己去处理多个 socket fd IO 多路复用 VS 多线程+同步阻塞IO多路复用的图 其实看着和 blocking IO的图 没有太大的不同, 事实上, 还更差一些, 因为这里需要使用两个system call (select 和 recvfrom), 而 blocking IO 只调用了一个system call (recvfrom)但是, 用select的优势在于它可以同时处理多个socket fd, 所以, 如果处理的连结数目不高的话, 使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的web server性能好, 可能延迟还更大(因为阻塞可以保证没有延迟, 但是多路复用是处理先存在的数据, 所以数据的顺序则不管, 导致处理一个完整的任务的时间上有延迟) 同步非阻塞 VS 多线程+同步阻塞高并发的程序一般使用 同步非阻塞方式 而非 多线程 + 同步阻塞 方式因为在高并发场景下, 为每个任务(用户请求)创建一个进程或线程的开销非常大, 而 同步非阻塞 方式可以把多个 IO 请求丢到后台去, 这就可以在一个进程里服务大量的并发 IO 请求 异步IO (asynchronous IO)上面说的 阻塞, 非阻塞, IO多路复用 这几种方式, 对用户进程来说: 阻塞 和 IO多路复用 都是主动去等待; 非阻塞是用户进程主动去轮询; 总之都是用户进程都是自己主动去拿到了结果, 不是被回调通知的, 所以对用户进程来说都是同步的IO操作 Unix下的 asynchronous IO 其实用得很少 用户进程发起读取操作之后, 立刻就可以开始去做其它的事, 而另一方面, 从kernel的角度, 当它收到一个asynchronous read之后, 首先它会立刻返回, 所以不会对用户进程产生任何block, 然后, kernel会等待数据准备完成, 然后将数据拷贝到用户内存, 当这一切都完成之后, kernel会给用户进程发送一个signal, 告诉它read操作完成了 信号驱动式I/O模型…… 各 IO Model 对比 小结注意, 在上面讨论 Linux 五种 I/O 模型 时, 讨论的是Linux的系统调用函数是否实现了各模式(如图中的 recvfrom 就是Linux的系统调用函数)貌似在系统调用函数层面, 对 异步 的实现并非很成熟(可参考:https://www.zhihu.com/question/26943558); 而我们通常接触到的异步非阻塞, 只是用户态程序通过实现诸如 Proactor 模式来模拟的异步, 并不是真正的异步(操作系统层面(系统调用函数级别)的异步) 参考https://www.zhihu.com/question/26943558 https://www.cnblogs.com/diegodu/p/6823855.htmlhttps://blog.csdn.net/jay900323/article/details/18141217#t6https://blog.csdn.net/lemon89/article/details/78290389#Reactor_183 https://blog.csdn.net/u014730165/article/details/85044285 ？","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"05. 文件IO, 标准IO","slug":"nginx/2019-11-07-05-IO","date":"2019-11-07T07:03:29.000Z","updated":"2019-11-07T09:57:40.000Z","comments":true,"path":"2019/11/07/nginx/2019-11-07-05-IO/","link":"","permalink":"http://blog.renyimin.com/2019/11/07/nginx/2019-11-07-05-IO/","excerpt":"","text":"根据 有无用户态缓存 可以将IO分为以下两大类 (注意都有内核态缓存) 文件IO 文件I/O 又称为低级磁盘I/O(是操作系统提供的基本IO服务), 遵循POSIX相关标准, 与os绑定; 任何兼容POSIX标准的操作系统上都支持文件I/O (特定于 Linux 或 unix平台) 文件I/O 称之为不带缓存的IO(unbuffered I/O), 像 open(), read() 这些 posix 标准的会进行系统调用的函数, 主要是指其在用户空间没有缓冲, 在内核空间还是进行了缓存的 数据流转过程为: 数据---&gt;内核缓存区--&gt;磁盘 假设 内核缓存区 长度为100字节, 你调用 write() 进行写操作时, 设每次写入 10 个字节, 那么你要调用10次这个函数才能把 内核缓存区 写满, 没写满时数据还是在内核缓冲区中, 并没有写入到磁盘中, 内核缓存区满了之后或者执行了 fsync(强制写入硬盘)之后, 才进行实际的IO操作, 把数据写入磁盘上 标准IO (缓存IO) 标准I/O是 ANSI C 建立的一个标准I/O模型, 是一个 标准函数包 和 stdio.h 头文件中的定义, 具有一定的可移植性 标准I/O 被称为 高级磁盘I/O, 又被称作 缓存 IO, 大多数文件系统的默认 IO 操作都是缓存 IO, 像 fopen(), fwrite(), fget() 等, 是c标准库中定义的 数据流转过程为: 数据--&gt;流缓存区--&gt;内核缓存区--&gt;磁盘 假设 流缓存区 长度为50字节, 内核缓存区100字节, 我们用标准c库函数 fwrite() 将数据写入到这个流缓存中, 每次写10字节, 需要写5次流缓存区满后才会调用 write() (或调用 fflush() ), 将数据写到内核缓存区, 直到内核缓存区满了之后或者执行了 fsync(强制写入硬盘) 之后, 才进行实际的IO操作, 把数据写入磁盘上标准IO操作 fwrite() 最后还是要调用 无缓存的IO操作 write()如 标准IO函数 fopen() 底层用的还是 低级的文件IO函数 read() (这两个函数都是面向用户的用户编程接口API, 是用户空间的调用, 而 read() 底层则是进行 系统调用 (调用内核函数 sys_read()) ) tip: fsync: 是把内核缓冲刷到磁盘上 fflush: 是把C库中的缓冲调用 write() 函数写到磁盘 (其实是写到内核的缓冲区, 内核缓冲区满了之后才会真正写入磁盘) 标准IO的三种缓冲类型 (隐藏) 文件IO 与 标准IO 区别 有隐藏部分 标准IO 在用用户态下就有了缓存, 如调用 fopen(), fwrite() 的时候就减少了 用户态 和 内核态 的切换;而 文件IO 如 open(), write() 每次调用都会进行 内核态 和 用户态 的切换 所以, 如果顺序访问文件, fopen() 系列的函数要比直接调用 open() 系列快, 如果随机访问文件 open() 要比 fopen() 快 缓存 IO 的缺点:当然, 标准I/O库的并非没有缺点, 这与他需要复制的数据有关, 通常需要复制两次数据: 一次是在 内核 与 标准I/O缓冲区 之间(当标准IO缓冲区满后, 数据会被拷贝到内核缓冲区), 第二次是在 标准I/O缓冲区 和 应用程序行缓存 之间 参考https://www.cnblogs.com/orlion/p/6258691.htmlhttps://blog.csdn.net/m0_37542524/article/details/83663124#1IOIO_2https://blog.csdn.net/tanqiuwei/article/details/20641965https://www.jianshu.com/p/a77613045601","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"04. 基础概念 -- 用户态, 内核态, 系统调用","slug":"nginx/2019-11-07-04-IO","date":"2019-11-07T06:20:18.000Z","updated":"2019-11-07T09:42:01.000Z","comments":true,"path":"2019/11/07/nginx/2019-11-07-04-IO/","link":"","permalink":"http://blog.renyimin.com/2019/11/07/nginx/2019-11-07-04-IO/","excerpt":"","text":"在了解 IO 的相关概念之前, 有必要先了解一下 系统调用 的概念而在了解 系统调用 的概念前, 又不得不先了解Linux操作系统中 用户态与内核态 的概念 用户态 与 内核态 操作系统为什么需要分 内核空间 与 用户空间? 在 CPU 的所有指令中, 有些指令是非常危险的, 如果错用, 将导致系统崩溃, 比如清内存、设置时钟等 如果允许所有的程序都可以使用这些指令, 那么系统崩溃的概率将大大增加所以, CPU 将指令分为 特权指令 和 非特权指令, 对于那些危险的指令, 只允许操作系统及其相关模块使用, 普通应用程序只能使用那些不会造成灾难的指令(比如 Intel 的 CPU 将特权等级分为 4 个级别：Ring0~Ring3。其实 Linux 系统只使用了 Ring0 和 Ring3 两个运行级别(Windows 系统也是一样的)。 当进程运行在 Ring3 级别时被称为运行在用户态, 而运行在 Ring0 级别时被称为运行在内核态) Linux 从整体上分为 内核态(内核空间) 与 用户态(用户空间) 比如一个32位的操作系统, 寻址地址(虚拟内存空间)是2的32次方, 也就是4G 操作系统将较高的1G字节作为内核空间 (内核空间具有用户空间所不具备的操作权限) 将较低的3G字节作为用户空间 内核态 与 用户态 内核态 就是内核所处的空间, 内核负责调用底层硬件资源, 并为上层应用程序提供运行环境 用户态 即 应用程序的活动空间 (应用程序通常运行在用户空间, 当应用程序的某些操作需要内核权限时, 就需要通过 系统调用(System calls), 进入内核态执行, 这也就是一次 用户态 -&gt; 内核态 的转换) 系统调用(System calls) 应用程序的运行必须依托于内核提供的硬件资源(如cpu\\存储\\IO), 而内核是通过暴露外部接口来供应用程序进行调用的 （内核提供的这些外部接口就称为 SystemCall, 系统调用） 系统调用是操作系统的最小功能单位, 每次系统调用都会发生一次 用户态与内核态 的切换, 切换过程中涉及了各种函数的调用以及数据的复制 系统调用是每个操作系统必不可少的一部分, 操作系统上的每个应用程序都必须依靠 系统调用 来进入内核态,进而实现对硬件资源的操作 比如应用程序的 read() 的实现,其实是来源于内核空间的内核函数 sys_read(), 这样, 通过 read() 就形成一个系统调用 (更多系统调用可以搜索: Linux系统调用表(system call table) 去查看) 系统调用是 用户态 进入 内核态 的唯一入口 用户空间 与 操作系统内核空间 的系统调用 架构简图1 如下: 用户空间 与 操作系统内核空间 的系统调用 架构简图2 如下:","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"03. postman 测试 MIME类型","slug":"nginx/2019-11-06-03","date":"2019-11-07T02:45:11.000Z","updated":"2019-11-07T02:45:35.000Z","comments":true,"path":"2019/11/07/nginx/2019-11-06-03/","link":"","permalink":"http://blog.renyimin.com/2019/11/07/nginx/2019-11-06-03/","excerpt":"","text":"","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"02. nginx主配置文件(nginx.conf)结构","slug":"nginx/2019-11-06-02","date":"2019-11-06T07:32:11.000Z","updated":"2019-11-07T07:45:21.000Z","comments":true,"path":"2019/11/06/nginx/2019-11-06-02/","link":"","permalink":"http://blog.renyimin.com/2019/11/06/nginx/2019-11-06-02/","excerpt":"","text":"前言 整个conf文件分为 全局块、events块、http块、server块、location块 每个块有每个块的作用域, 外层块作用域就包含内部块的作用域, 如全局块作用域就包含events块、http块、server块和location块 修改nginx.conf后是必须重启nginx才会生效 nginx.conf 大体结构如下 12345678910111213141516171819202122232425... #全局块event&#123; #events块 ...&#125;http&#123; #http块 server&#123; #server块 ... #server全局块 location&#123; #location块 ... &#125; location&#123; #location块 ... &#125; &#125; server&#123; #server块 ... &#125; ... #http全局块&#125; 各块的作用 名称 作用 全局块 全局块是默认配置文件从开始到events块之间的一部分内容, 主要是设置一些影响Nginx服务器整体运行的配置指令。因此, 这些指令的作用域是Nginx服务器全局。作用通常包括: 配置Ngnix服务器的用户组、worker process数、Nginx进程PID存放路径、日志的存放路径和类型以及配置文件引入等 events块 events块涉及的指令主要是影响Nginx服务器与用户的网络链接。 常用的设置包括: 是否开启多worker process下的网络连接进行序列化, 是否允许同时接收多个网络连接, 选取那种事件驱动模型处连接请求, 每个worker process可以同时支持的最大连接数等 (该部分的指令对nginx的性能影响较大, 实际配置中应该根据实际情况进行灵活调整) http块 http块是Nginx服务器配置中的重要部分, 代理、缓存和日志定义等绝大多数的功能和第三方模块的配置都可以放在这模块中。可以在http全局块中配置的指令包括：文件引入、MIME-Type定义、日志自定义、是否使用sendfile传输文件、连接超时时间、单连接请求数上限等; 一个http块可以包含多个server块) server块 server块, 虚拟主机（虚拟服务器）。作用：使得Nginx服务器可以在同一台服务器上至运行一组Nginx进程, 就可以运行多个网站。(注意: 在http全局块中介绍过的部分指令可以在server块, location块中使用) location块 location块是server块的一个指令, 每个server块可以包含多个location块。作用：基于Nginx服务器接收到的请求字符串, 虚拟主机名称（ip, 域名）、url匹配, 对特定请求进行处理。 配置项介绍 配置运行nginx服务器的用户(组) user [user] [group] 如果希望所有用户都可以启动nginx, 注释掉该行或者 user nobody nobody 如果配置的用户或者用户组不存在, 则启动会报错 此指令只能在全局块中配置 worker_processes number/auto 该配置是服务器实现并发处理的关键所在 (rrc 是 auto) 设置 number 会指定nginx进程最多可以产生的 worker process 数 设置 auto 时, nginx进程将自动检测 此指令只能在全局块中配置 pid file 用来设置nginx进程的pid文件位置, 该文件用来保存当前运行程序的主进程号 此指令只能在全局块中配置 error_log file/stderr [debug|info|notice....] 配置错误日志的存放路径 (rrc是 error_log /mnt/logs/nginx/error.log notice) 注意 指定的文件需要对于运行nginx进程的用户具有写权限 配置文件的引入 : 在一些情况下, 我们可能需要将其他的nginx配置或者第三方模块的配置引入到当前的主配置文件中 include file 注意 引入的配置文件需要对于运行nginx进程的用户具有写权限 注意: 该指令可以放在配置文件的任意位置 (rrc 放在 http全局块中 include conf.d/*.conf;) accept_mutex on | off; : 设置网络连接的序列化 (当一个新网络连接来到时, 多个worker进程会被同时唤醒, 但仅仅只有一个进程可以真正获得连接并处理之。如果每次唤醒的进程数目过多的话, 其实是会影响一部分性能的) 如果accept_mutex on, 那么多个worker将是以串行方式来处理, 其中有一个worker会被唤醒；反之若accept_mutex off, 那么所有的worker都会被唤醒, 不过只有一个worker能获取新连接, 其它的worker会重新进入休眠状态 默认是开启(on)状态, 只能在 events块中进行配置 multi_accept on|off : 每个worker process 一次只能接收一个新到达的网络连接。若想让每个Nginx的workerprocess都有能力同时接收多个网络连接, 则需要开启此配置 该指令默认为off状态 只能在 events块中进行配置 use method 事件驱动模型的选择 (select/poll/kequeue/epoll…) 只能在 events块中进行配置 worker_connections number 主要用来设置允许每一个 worker process 同时开启的最大连接数 (默认值为512), (rrc 为 worker_connections 65535;) 这里的 number 不仅仅包括和前端用户建立的连接数, 而是包括所有可能建立的连接数 另外, number不能大于操作系统支持打开的最大文件句柄数 只能在 events块中进行配置 include全局块中的 MIME类型 配置, 貌似一般不动 12include mime.types;default_type application/octet-stream; 自定义服务日志 在全局块中, 我们介绍过error_log指令, 其用于nginx进程运行时的日志存放和级别。此处所指的日志与常规日志不同, 它是记录nginx服务器提供服务过程应答前端请求的日志, 我们将其称为服务日志以示区分。 access_log path [format [buffer=size]] 可以在 http,server,location中配置 log_format name string ... 只能在 http 全局块中进行配置 (rrc 把 log_format 放在 http全局块中, 然后通过 include 引入每个项目的server配置, 每个项目在自己的server配置中指定自己的 access_log ) keepalive_timeout imeout [header_timeout] 配置连接超时时间 （rrc : keepalive_timeout 30;） timeout 服务端对连接的保持时间, 默认为65s header_timeout 经测试, 会在响应头中加上 Keep-Alive: timeout=所设置的时间 可以在 http,server,location中配置 keepalive_requests number: 用于限制用户通过某一个连接向nginx发送请求的次数 默认为100 可以出现在server,location块中 配置网络监听, 有三种方式 (基于ip, port, UNIX Domain Socket), 直接看例子一般用在 server 块中 123listen *:80 | *:8000 (默认) 监听所有80和8000端口listen 8000 监听8000端口上所有ip,相当于 listen *:8000listen 192.168.1.10 监听具体IP的所有端口上的链接 虚拟主机（基于端口,域名,ip） 反向代理负载均衡算法 高可用keepalived openrestyluahttps://www.cnblogs.com/chenglc/p/8024994.htmlhttps://www.cnblogs.com/knowledgesea/p/5175711.html","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"01. nginx 安装","slug":"nginx/2019-11-06-01","date":"2019-11-06T03:31:23.000Z","updated":"2019-11-06T08:28:32.000Z","comments":true,"path":"2019/11/06/nginx/2019-11-06-01/","link":"","permalink":"http://blog.renyimin.com/2019/11/06/nginx/2019-11-06-01/","excerpt":"","text":"nginx下载nginx 下载 依赖准备由于nginx的一些模块需要依赖第三方库, 通常有pcre库(支持rewrite模块), zlib库(支持gzip模块) 和 openssl库(支持ssl (Secure Sockets Layer 安全套接层) 模块)等 :（可以使用yum安装即可）yum -y install gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel open openssl-devel 编译编译参数参考 http://nginx.org/en/docs/configure.html, 可以从如下几个角度来进行了解 Path选项 参数 用途 默认值 —prefix= Nginx的安装目录 /usr/local/nginx —sbin-path= Nginx可执行文件安装路径 /sbin/nginx —conf-path= 主配置文件安装位置 /conf/nginx.conf —error-log-path= 错误日志位置, 在nginx.conf配置文件指定 /logs/error.log —pid-path= Nginx pid文件路径, 在nginx.conf配置文件指定 /logs/nginx.pid —lock-path= nginx.lock文件路径 /logs/nginx.lock —with-perl_modules_path=… Perl模块位置 —with-perl=… Perl二进制文件路径 —http-log-path=… 访问日志路径, 可以在配置文件指定 /logs/access.log —http-client-body-temp-path=… 存放由客户端请求生成的临时文件路径 /client_body_temp —http-proxy-temp-path=… proxy产生的临时文件路径 /proxy_temp … … … 依赖选项依赖以库和二进制文件的形式出现, 现在, 它们应该已经全部安装在您的系统上了。但是, 即使它们存在于您的系统上, 也有可能出现配置脚本无法找到它们的情况。原因可能有所不同, 例如, 如果它们安装在非标准目录中。为了解决这些问题, 您可以使用以下选项来指定依赖的路径（其他依赖相关选项已组合在一起） 编译选项 描述 —with-cc=… 指定C编译器的备用位置 —with-cpp=… 指定C预处理器的备用位置 —with-cc-opt=… 定义要传递到C编译器命令行的其他选项 —with-ld-opt=… 定义要传递到C链接器命令行的其他选项 —with-cpu-opt=… 在以下值中指定不同的目标处理器体系结构：pentium, pentiumpro, pentium3, pentium4, athlon, opteron, sparc32, sparc64和ppc64 PCRE选项 描述 —without-pcre 禁用PCRE库的使用。 不建议使用此设置, 因为它将删除对正则表达式的支持, 从而禁用Rewrite模块 —with-pcre 强制使用PCRE库 —with-pcre=… 允许您指定PCRE库源代码的路径 —with-pcre-opt=… 构建PCRE库的其他选项 —with-pcre-jit=… 构建PCRE与JIT编译的支持 MD5选项 描述 —with-md5=… 指定MD5库源的路径 —with-md5-opt=… 用于构建MD5库的其他选项 —with-md5-asm 为MD5库指定汇编源 SHA1选项 描述 —with-sha1=… 指定SHA1库源的路径 —with-sha1-opt=… 构建SHA1库的其他选项 —with-sha1-asm 为SHA1库指定汇编器源 zlib选项 描述 —with-zlib=… 指定zlib library源的路径 —with-zlib-opt=… 用于构建zlib库的其他选项 —with-zlib-asm=… 为zlib库指定汇编器源 OpenSSL选项 描述 —with-openssl=… 指定OpenSSL库源的路径 —with-openssl-opt=… 用于构建OpenSSL库的其他选项 模块选项在编译程序之前, 需要指定要安装的模块。 有些是默认启用的, 有些需要手动启用 默认启用的模块 (以下参数允许您禁用默认情况下启用的模块) 1234567891011121314151617181920212223–without-http_charset_module–without-http_gzip_module–without-http_ssi_module–without-http_userid_module–without-http_access_module–without-http_access_module–without-http_autoindex_module–without-http_geo_module–without-http_map_module–without-http_referer_module–without-http_rewrite_module–without-http_proxy_module–without-http_fastcgi_module–without-http_uwsgi_module–without-http_scgi_module–without-http_memcached_module–without-http_limit_conn_module–without-http_limit_req_module–without-http_empty_gif_module–without-http_browser_module–without-http_upstream_ip_hash_module–without-http_upstream_least_conn_module–without-http_split_clients_module 默认禁用的模块 (以下参数允许您启用默认禁用的模块) 1234567891011121314151617181920–with-http_ssl_module–with-http_realip_module–with-http_addition_module–with-http_xslt_module–with-http_image_filter_module–with-http_geoip_module–with-http_sub_module–with-http_dav_module–with-http_flv_module–with-http_mp4_module–with-http_gzip_static_module–with-http_random_index_module–with-http_secure_link_module–with-http_stub_status_module–with-google_perftools_module–with-http_degradation_module–with-http_perl_module–with-http_spdy_module–with-http_gunzip_module–with-http_auth_request_module 示例: ./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module 安装然后 make &amp;&amp; make install 二进制nginx文件相关用法 -v: 打印版本号 并退出 -V: 打印版本号和 nginx的编译情况 并退出 -s signal: 向nginx的master进程发送信号 (signal可以是 stop, quit, reopen, reload) nginx的启停向 nginx主进程发送信号有两种方法: 一种是使用 Nginx二进制文件, 用法是: ./nginx -s signal; 另一种方法是使用kill命令发送信号, 用法是: kill -SIGNAL PID ./nginx -s stop = kill -INT/-TERM PID./nginx -s quit = kill -QUIT PID./nginx -s reload = kill -HUP PID./nginx -s reopen = kill -USR1 PID （重新打开日志文件, 切割日志时候用途较大）… https://www.cnblogs.com/chenglc/p/8024994.htmlhttps://www.cnblogs.com/knowledgesea/p/5175711.html","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"01.","slug":"math/2019-11-04-01","date":"2019-11-04T06:39:46.000Z","updated":"2019-11-04T06:40:19.000Z","comments":true,"path":"2019/11/04/math/2019-11-04-01/","link":"","permalink":"http://blog.renyimin.com/2019/11/04/math/2019-11-04-01/","excerpt":"","text":"","categories":[{"name":"math","slug":"math","permalink":"http://blog.renyimin.com/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"http://blog.renyimin.com/tags/math/"}]},{"title":"Chrome的Network分析 HTTP 报文","slug":"http/2019-10-28-chrome-network-http","date":"2019-10-28T03:37:56.000Z","updated":"2019-11-04T03:04:46.000Z","comments":true,"path":"2019/10/28/http/2019-10-28-chrome-network-http/","link":"","permalink":"http://blog.renyimin.com/2019/10/28/http/2019-10-28-chrome-network-http/","excerpt":"","text":"跨页面加载保存请求: Preserve log停用浏览器缓存: Disable cache模拟慢速网络连接: Throttling, 可自定义网速 过滤器, 属性过滤 domain: 仅显示来自指定域的资源 (你可以使用通配符字符 (*) 纳入多个域) has-response-header: 显示包含指定 HTTP 响应标头的资源 is: 使用 is:running 可以查找 WebSocket 资源, is:from-cache 可查找缓存读出的资源 larger-than: 显示大于指定大小的资源(以字节为单位), 将值设为 1000 等同于设置为1k method: 显示通过指定 HTTP 方法类型检索的资源 mime-type: 显示指定 MIME 类型的资源多属性间通过空格实现 AND 操作 scheme: 显示通过未保护 HTTP (scheme:http) 或受保护 HTTPS (scheme:https) 检索的资源 set-cookie-domain: 显示具有 Set-Cookie 标头并且 Domain 属性与指定值匹配的资源 set-cookie-name: 显示具有 Set-Cookie 标头并且名称与指定值匹配的资源 set-cookie-value: 显示具有 Set-Cookie 标头并且值与指定值匹配的资源 status-code: 仅显示 HTTP 状态代码与指定代码匹配的资源 (status-code:302) 请求列表 Waterfall 列: 各请求相关活动的直观分析图请求时间详细分布 Queueing: 浏览器在以下情况下对请求排队存在更高优先级的请求此源已打开六个 TCP 连接，达到限值，仅适用于 HTTP/1.0 和 HTTP/1.1浏览器正在短暂分配磁盘缓存中的空间 Stalled: 请求可能会因 Queueing 中描述的任何原因而停止 DNS Lookup: 浏览器正在解析请求的 IP 地址 Proxy Negotiation: 浏览器正在与代理服务器协商请求 Request sent: 正在发送请求 ServiceWorker Preparation: 浏览器正在启动 Service Worker Request to ServiceWorker: 正在将请求发送到 Service Worker Waiting (TTFB): 浏览器正在等待响应的第一个字节TTFB 表示 Time To First Byte (至第一字节的时间)。 此时间包括 1 次往返延迟时间及服务器准备响应所用的时间 Content Download: 浏览器正在接收响应 Receiving Push: 浏览器正在通过 HTTP/2 服务器推送接收此响应的数据 Reading Push: 浏览器正在读取之前收到的本地数据","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"","slug":"http/2019-10-24-OpenRestry","date":"2019-10-24T12:31:09.000Z","updated":"2019-10-28T02:47:48.000Z","comments":true,"path":"2019/10/24/http/2019-10-24-OpenRestry/","link":"","permalink":"http://blog.renyimin.com/2019/10/24/http/2019-10-24-OpenRestry/","excerpt":"","text":"OpenResty, 它是一个 “更好更灵活的 Nginx”","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"Wireshark 抓包分析 HTTPS","slug":"network/https-wireshark","date":"2019-10-24T09:10:36.000Z","updated":"2019-12-16T10:05:41.000Z","comments":true,"path":"2019/10/24/network/https-wireshark/","link":"","permalink":"http://blog.renyimin.com/2019/10/24/network/https-wireshark/","excerpt":"","text":"HTTPS 实现原理简介混合加密技术: 证书校验是使用CA机构提供的非对称加密, 然后使用非对称加密传递一个秘钥, 再使用对称加密 要使用HTTPS进行通信的服务端, 需要向CA机构申请数字证书, 来证明自己身份的有效性 (服务器会提供一对 公私钥) 当客户端请求的时候, 服务端把证书发给客户端, 客户端验证证书有效, 也就是认证了服务端的有效性, 并且拿出数字证书中包含的 (服务器)公钥 客户端生成一个随机数, 使用(服务器写入CA证书中的)公钥加密, 并且传递给服务端 服务端用私钥解密, 拿到随机数。之后客户端和服务端就用这个随机数进行加密通信 小结: HTTPS最关键的就是使用数字证书来证明服务器的有效性, 然后使用非对称加密来传递对称加密的密钥 (这里要说明的是, 实际使用中对称加密的密钥, 并不是直接由客户端生成的随机数, 这里为了说明方便而简单这样理解。实际上的对称加密密钥是要根据协商的加密算法组件来决定的) HTTPS 数字证书的验证 概述CA证书认证流程概述 浏览器通过URL请求后台服务器, 服务器接收到请求后, 会给浏览器发送一个自己的CA数字证书 浏览器接收到数字证书后 首先从证书的内容中获取证书的颁发机构, 然后从浏览器系统中去寻找此颁发机构是否为浏览器的信任机构 (这里解析一下, 世界上就几个权威的CA机构, 这几个机构的信息都是预先嵌入到我们的浏览器系统中的), 如果收到的一个数字证书但其颁发机构没有在我们浏览器系统中的, 那么就会有警告提示无法确认证书的真假, 如果是受信任的机构, 那么就到下一步 此时, 就可以从浏览器中找到CA机构的根公钥, 用这个根公钥去解析证书的签名(这个签名是证书发布之前CA机构用自己的根私钥加密而成的, 所以这里只能由根证书的根公钥去解密)得到一个hash值H1然后用证书的指纹算法对证书的内容再进行hash计算得到另一个hash值H2, 如果此时H1和H2是相等的, 就代表证书没有被篡改过在证书没有被修改过的基础上, 再检查证书上的使用者的URL (比如csdn.net) 和我们请求的URL是否相等, 如果相等, 那么就可以证明当前浏览器连接的网址也是正确的, 而不是一些钓鱼网之类的 假设, 浏览器的连接被某个钓鱼网截取了, 钓鱼网也可以发一个自己的证书给浏览器, 要知道如果钓鱼网站的证书和真实网站的整数是同一个CA机构的话, 前面也是会被内置在浏览器中的CA机构的根公钥解密的, 也会得到 H1, 然后也可以通过证书没有被篡改的验证但是在证书没有被篡改的情况下, 通过对比证书上的URL和我们请求的URL, 就可以发现这个证书的URL不是我们所要连接的网址, 所以说钓鱼网也骗不了我们 到这里, 已经验证了证书是没有被篡改的并且确认连接的URL也是正确的, 然后我们获取到了证书上的公钥 这个公钥是你访问的网站服务器在申请CA证书时, 所提供的服务器 公钥(私钥由服务器自己保管), 这个公钥会写在CA证书内, 该公钥和CA证书的根公钥不是一回事, 注意区分 对称秘钥的安全传递 接下来就是, 如何将一个对称加密算法的秘钥安全地发给服务器 首先随机生成一个字符串S作为我们的秘钥, 然后通过证书中(由网站服务器所提供)的 公钥 加密成密文, 将密文发送给服务器 (因为此密文是用公钥加密的, 这是一个非对称加密, 这个密文只有你访问的网站服务器上的 私钥 才能进行解密, 所以说任何第三方截取到密文也是没用的, 因为没有对应的私钥所以解析不出来)当然, 发送密文的时候也会对消息内容进行签名操作, 即对密文内容进行hash计算得到一个hash值, 将这个签名加密以后和消息内容一起发送出去接收方收到消息以后, 通过私钥解析出密文和签名的hash值, 同时也会对接收的消息内容进行同样的计算得到另一个hash值, 通过比对两个hash值是否相同来判断密文是否有修改过 过了以上步骤, 客户端和服务端都持有了对称加密算法的秘钥, 然后兄弟两就可以愉快地安全通信了 小结HTTPS采用的是 非对称加密 + 对称加密 这种混合加密方式数字证书的验证有两个重要的步骤, 第一是验证数字证书没有被篡改以及连接的URL是否正确, 第二是通过RSA机制的原理安全地将对称加密算法的秘钥发送给对方。这两步都完成以后, 整个HTTPS的数字证书的验证就算是成功了。 抓包分析 所用连接: https://cdn.staticfile.org/font-awesome/4.6.3/css/font-awesome.min.css 抓包 分析~~参考: https://blog.csdn.net/firefile/article/details/80537053#1__3 https://blog.csdn.net/u011803341/article/details/79708886https://blog.csdn.net/qq_32998153/article/details/80022489https://segmentfault.com/a/1190000014835279 https加密在应用层其实还是明文传输的!!!https://www.cnblogs.com/evan-blog/p/9867561.html https://blog.csdn.net/u014294681/article/details/86599741 https 传输的数据是否需要二次加密https 能保证的是连接的安全，当我们打开网站的时候，是需要和服务器进行通信的，那么 https 能够保证在通信时数据传输是安全的。 如果我们打开一个网页，在网址的前面有标明”https”的话，那么就说明这个网站有证书并且有进行加密的操作。但是，不少钓鱼网页其实也可以使用 https，这就说明了 https 并不能保证网站本身是安全的。举个例子，如果你在一个网页上输入账号密码的话，使用 https 的网页能够保证你的账号密码在传到网站服务器的过程中不会被其他的东西干扰或者盗取，但是，你输入的账号密码却有可能被你正在访问的这个网页盗用。 再举个例子：移动端手机种了木马，终端被攻击了，那么对于传输通道进行加密就毫无意义，试想人家都跑到你家里面去了，所有的东西在内存跟硬盘里面都有，再进行网络监听毫无意义。 所以 https 本身是安全的，至少在网络传输过程中，能够保证传输数据的完整性和一致性。但是并不代表它的宿主是安全的，所以如果是金融类，支付类等对安全等级要求很高的软件，最好进行二次加密。 可以用生活中的场景做个类比: 使用了https，可以这样认为: 你正在沟通的这个人, 是个合法的公民, 是被法律上认可的一个人, 他在公安局有备案,身份证等信息;也仅此而已 虽然是个合法公民, 你和他的沟通也要小心谨慎, 因为这只能保证你正在沟通的这个人是合法的, 但它未必是个守法的 另外, 你和这个合法公民的沟通也要防止被截获, 因为你两的沟通只是在传输层以下进行了保证, 应用层如果必要的话, 也是需要有一套安全措施的 不用https, 那你正在沟通的这个人可能根本就不是个合法公民, 连身份证都没有, 没有被政府所认可","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"},{"name":"HTTP","slug":"network/HTTP","permalink":"http://blog.renyimin.com/categories/network/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"},{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"HTTPS 证书申请","slug":"http/2019-10-24-HTTS","date":"2019-10-24T09:10:36.000Z","updated":"2019-11-04T10:59:46.000Z","comments":true,"path":"2019/10/24/http/2019-10-24-HTTS/","link":"","permalink":"http://blog.renyimin.com/2019/10/24/http/2019-10-24-HTTS/","excerpt":"","text":"","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTPS是什么?SSL/TLS又是什么?","slug":"http/2019-10-23-HTTS","date":"2019-10-23T12:31:09.000Z","updated":"2019-11-05T03:32:19.000Z","comments":true,"path":"2019/10/23/http/2019-10-23-HTTS/","link":"","permalink":"http://blog.renyimin.com/2019/10/23/http/2019-10-23-HTTS/","excerpt":"","text":"前言之前曾谈到过 HTTP 的一些缺点, 其中的 无状态 在加入 Cookie 后得到了解决, 而另两个缺点 — 明文 和 不安全 仅凭 HTTP 自身是无力解决的, 需要引 入新的 HTTPS 协议 由于 HTTP 天生 明文 的特点, 整个传输过程完全透明, 任何人都能够在链路中截获、 修改或者伪造请求 / 响应报文, 数据不具有可信性比如, “代理服务”, 它作为 HTTP 通信的中间人, 在数据上下行的时候可以添加或删除部分头字段, 也可以使用黑白名单过滤 body 里的关键字, 甚至直接发送虚假的请求、响应, 而浏览器和源服务器都没有办法判断报文的真伪因此 HTTP 明文这一特性, 对于网络购物、网上银行、证券交易等需要高度信任的应用场景来说是非常致命的。如果没有基本的安全保护, 使用互联网进行各种电子商务、电子政务就根本无从谈起对于安全性要求不那么高的新闻、视频、搜索等网站来说, 由于互联网上的恶意用户、恶意代理越来越多, 也很容易遭到 “流量劫持” 的攻击, 在页面里强行嵌入广告, 或者分流用户, 导致各种利益损失对于你我这样的普通网民来说, HTTP 不安全的隐患就更大了, 上网的记录会被轻易截获, 网站是否真实也无法验证, 黑客可以伪装成银行网站, 盗取真实姓名、密码、银行卡等敏感信息, 威胁人身安全和财产安全 什么是安全?通常认为, 如果通信过程具备了四个特性, 就可以认为是安全的, 这四个特性是: 机密性、完整性, 身份认证(真实性) 和 不可否认 (HTTPS 的出场, 为 HTTP 增加了这四大安全特性) 机密性(Secrecy/Confidentiality)机密性是指传输的数据是采用Session Key（会话密钥）加密的, 在网络上是看不到明文的 完整性(Integrity, 也叫一致性)是指为了避免网络中传输的数据被非法篡改, 使用MAC算法来保证消息的完整性 (每部分数据均有mac验证, 验证时计算数据的mac然后与接收到的mac比较, 即可确定数据是否完整) 身份认证(Authentication)真实性是指通信的对方是可信的, 利用了PKI（Public Key Infrastructure 即『公钥基础设施』）来保证公钥的真实性 不可否认(Non-repudiation/Undeniable)是这个消息就是你给我发的, 无法伪装和否认, 是因为使用了签名的技术来保证的session key 只有通信双方有, 并且不在网络上传输, 因此攻击者无法伪造使用session key加密的数据, 所以具有不可抵赖性https://zhidao.baidu.com/question/1435954777259099899.html （机密性由对称加密AES保证, 完整性由SHA384摘要算法保证, 身份认证和不可否认由RSA 非对称加密保证） HTTPSHTTPS 是如何做到这些安全特性呢?秘密就在于 HTTPS 名字里的 S, 它把 HTTP 下层的传输协议由 TCP/IP 换成了 SSL/TLS, 由 HTTP over TCP/IP 变成了 HTTP over SSL/TLS, 让 HTTP 运行在了安全的 SSL/TLS 协议上, 收发报文不再使用 Socket API, 而是调用专门的安全接口所以说, HTTPS 本身并没有什么 惊世骇俗的本事, 全是靠着后面的 SSL/TLS “撑腰”, 只要学会了 SSL/TLS, HTTPS 自然就OK了 SSL 即安全套接层(Secure Sockets Layer), 在 OSI 模型中处于第 5 层(会话层), 由 网景公司于 1994 年发明, 有 v2 和 v3 两个版本, 而 v1 因为有严重的缺陷从未公开过SSL 发展到 v3 时已经证明了它自身是一个非常好的安全通信协议, 于是互联网工程组 IETF 在 1999 年把它改名为 TLS(传输层安全, Transport Layer Security), 正式标准化, 版 本号从 1.0 重新算起, 所以 TLS1.0 实际上就是 SSLv3.1到今天 TLS 已经发展出了三个版本, 分别是 2006 年的 1.1、2008 年的 1.2 和去年 (2018)的 1.3, 每个新版本都紧跟密码学的发展和互联网的现状, 持续强化安全和性能, 已经成为了信息安全领域中的权威标准目前应用的最广泛的 TLS 是 1.2, 而之前的协议(TLS1.1/1.0、SSLv3/v2)都已经被认为是不安全的, 各大浏览器即将在 2020 年左右停止支持, 所以接下来的讲解都针对的是 TLS1.2 TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成, 综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术 浏览器 和 服务器在使用 TLS 建立连接时需要选择一组恰当的加密算法来实现安全通信, 这些算法的组合被称为 密码套件(cipher suite, 也叫加密套件)TLS 的密码套件命名非常规范, 格式很固定, 基本的形式是 密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法(比如 ECDHE-RSA-AES256-GCM-SHA384 的意思就是: “握手时使用 ECDHE 算法进行密钥交换, 用 RSA 签名和身份认证, 握手后的通信使用 AES 对称算法, 密钥长度 256 位, 分组模式是 GCM, 摘要算法 SHA384 用于消息认证和 产生随机数”) 说到 TLS, 就不能不谈到 OpenSSL, 它是一个著名的开源密码学程序库和工具包, 几乎支 持所有公开的加密算法和协议, 已经成为了事实上的标准, 许多应用软件都会使用它作为底层库来实现 TLS 功能, 包括常用的 Web 服务器 Apache、Nginx 等OpenSSL 目前有三个主要的分支, 1.0.2 和 1.1.0 都将在今年(2019)年底不再维护, 最新的长期支持版本是 1.1.1由于 OpenSSL 是开源的, 所以它还有一些代码分支, 比如 Google 的 BoringSSL、 OpenBSD 的 LibreSSL, 这些分支在 OpenSSL 的基础上删除了一些老旧代码, 也增加了一些新特性, 虽然背后有 大金主, 但离取代 OpenSSL 还差得很远 HTTPS如何为 HTTP 增加四个特性的机密性机密性是信息安全的基础; 实现机密性最常用的手段是 “加密(encrypt)”, 只有掌握特殊 “钥匙”(密钥(key)) 的人才能再转换出原始文本; 加密前的消息叫 “明文”(plain text/clear text), 加密后的乱码叫”密文”(cipher text), 加密解密的操作过程就是”加密算法”所有的加密算法都是公开的, 任何人都可以去分析研究, 而算法使用的 “密钥” 则必须保密, “密钥”就是一长串的数字, 但约定俗成的度量单位是 “位”(bit), 而不是”字节”(byte)按照密钥的使用方式, 加密可以分为两大类: 对称加密和非对称加密 对称加密 “对称加密”就是指加密和解密时使用的密钥都是同一个, 是 “对称” 的。只要保证了密钥的安全, 那整个通信过程就可以说具有了 机密性 TLS 里有非常多的对称加密算法可供选择, 比如 RC4、DES、3DES、AES、ChaCha20 等, 但前三种算法都被认为是不安全的, 通常都禁止使用, 目前常用的只有 AES 和 ChaCha20 AES 的意思是 高级加密标准(Advanced Encryption Standard), 密钥长度可以是 128、192 或 256; 它是 DES 算法的替代者, 安全强度很高, 性能也很好, 而且有的硬件还会做特殊优化, 所以非常流行, 是应用最广泛的对称加密算法 ChaCha20 是 Google 设计的另一种加密算法, 密钥长度固定为 256 位, 纯软件运行性能要超过 AES, 曾经在移动客户端上比较流行, 但 ARMv8 之后也加入了 AES 硬件优化, 所以现在不再具有明显的优势, 但仍然算得上是一个不错算法 加密分组模式对称算法还有一个 分组模式 的概念, 它可以让算法用固定长度的密钥加密任意长度的明文最早的分组模式有 ECB、CBC、CFB、OFB 等几种, 但都陆续被发现有安全漏洞, 所以现在基本都不怎么用了; 最新的分组模式被称为 AEAD(Authenticated Encryption with Associated Data),在加密的同时增加了认证的功能, 常用的是 GCM、CCM 和 Poly1305把上面这些组合起来, 就可以得到 TLS 密码套件中定义的对称加密算法。比如:AES128-GCM, 意思是密钥长度为 128 位的 AES 算法, 使用的分组模式是 GCM;ChaCha20-Poly1305 的意思是 ChaCha20 算法, 使用的分组模式是 Poly1305; 非对称加密对称加密看上去好像完美地实现了机密性, 但其中有一个很大的问题: 如何把密钥安全地传递给对方, 术语叫 “密钥交换”; 因为在对称加密算法中只要持有密钥就可以解密, 如果你和网站约定的密钥在传递途中被黑客窃取, 那他就可以在之后随意解密收发的数据, 通信过程也就没有机密性可言了, 这个问题该怎么解决呢? 这个问题该怎么解决呢?你或许会说:“把密钥再加密一下发过去就好了”, 但传输“加密密钥的密钥”又成了新问题。这就像是”鸡生蛋、蛋生鸡”, 可以无限递归下去。只用对称加密算法, 是绝对无法解决密钥交换的问题的 所以就出现了 非对称加密(也叫公钥加密算法) 非对称加密 有两个密钥, 一个叫 公钥(public key), 一个叫 私钥(private key), 两个密钥是不同的(不对称), 公钥可以公开给任何人使用, 而私钥必须严格保密 公钥和私钥有个特别的 单向 性, 虽然都可以用来加密解密, 但公钥加密后只能用私钥解密, 反过来, 私钥加密后也只能用公钥解密 非对称加密可以解决 密钥交换 的问题。网站秘密保管私钥, 在网上任意分发公钥, 你想要登录网站只要用公钥加密就行了, 密文只能由私钥持有者才能解密。而黑客因为没有私钥, 所以就无法破解密文 非对称加密算法的设计要比对称算法难得多, 在 TLS 里只有很少的几种, 比如 DH、 DSA、RSA、ECC 等 RSA 可能是其中最著名的一个, 几乎可以说是非对称加密的代名词, 它的安全性基于 “整数分解” 的数学难题, 使用两个超大素数的乘积作为生成密钥的材料, 想要从公钥推算出私钥是非常困难的10 年前 RSA 密钥的推荐长度是 1024, 但随着计算机运算能力的提高, 现在 1024 已经不安全, 普遍认为至少要 2048 位 ECC(Elliptic Curve Cryptography) 是非对称加密里的 “后起之秀”, 它基于 “椭圆曲线 离散对数” 的数学难题, 使用特定的曲线方程和基点生成公钥和私钥子算法 ECDHE 用于 密钥交换, ECDSA 用于数字签名(比起 RSA, ECC 在安全强度和性能上都有明显的优势。160 位的 ECC 相当于 1024 位的 RSA, 而 224 位的 ECC 则相当于 2048 位的 RSA。因为密钥短, 所以相应的计算量、消耗 的内存和带宽也就少, 加密解密的性能就上去了, 对于现在的移动互联网非常有吸引力。) 混合加密 虽然非对称加密没有 密钥交换 的问题, 但因为它们都是基于复杂的数学难题, 运算速度很慢(即使是 ECC 也要比 对称加密算法AES 差上好几个数量级), 如果仅用非对称加密, 虽然保证了安全, 但实用性就变成了零 因此就需要考虑, 是不是能够把对称加密和非对称加密结合起来呢, 两者互相取长补短, 即能高效地加密解密, 又能安全地密钥交换, 这就是现在 TLS 里使用的混合加密方式: 在通信刚开始的时候使用 非对称算法, 比如 RSA、ECDHE, 首先解决密钥交换的问题 然后用随机数产生对称算法使用的 会话密钥 (session key), 再用公钥加密。因为会话密钥很短, 通常只有 16 字节或 32 字节, 所以慢一点也无所谓 对方拿到密文后用私钥解密, 取出会话密钥。这样, 双方就实现了对称密钥的安全交换, 后续就不再使用非对称加密, 全都使用对称加密 从下往上如下图这样混合加密就解决了对称加密算法的密钥交换问题, 而且安全和性能兼顾, 完美地实现了机密性 不过这只是 “万里长征的第一步”, 后面还有完整性、身份认证、不可否认等特性没有实现, 所以现在的通信还不是绝对安全 TLS1.2连接过程解析HTTPS 建立连接 当你在浏览器地址栏里键入 https 开头的 URI, 再按下回车, 会发生什么呢?浏览器首先要从 URI 里提取出协议名和域名, 因为协议名是 https, 所以浏览器就知道了端口号是默认的 443, 它再用 DNS 解析域名, 得到目标的 IP 地址, 然后就可以使用三次握手与网站建立 TCP 连接了 在 HTTP 协议里, 建立连接后, 浏览器会立即发送请求报文。但现在是 HTTPS 协议, 它需要再用另外一个 握手 过程, 在 TCP 上建立安全连接, 之后才是收发 HTTP 报文。 这个 握手 过程与 TCP 有些类似, 是 HTTPS 和 TLS 协议里最重要、最核心的部分, 懂了它, 你就可以自豪地说自己 掌握了HTTPS TLS 协议的组成在讲 TLS 握手之前, 先简单介绍一下 TLS 协议的组成TLS 包含几个子协议, 你也可以理解为它是由几个不同职责的模块组成, 比较常用的有 记录协议、警报协议、握手协议、变更密码规范协议 等 记录协议(Record Protocol) 规定了 TLS 收发数据的基本单位: 记录(record), 它有点像是 TCP 里的 segment, 所有的其他子协议都需要通过记录协议发出。但多个记录数据可以在一个 TCP 包里一次性发出, 也并不需要像 TCP 那样返回 ACK 警报协议(Alert Protocol) 的职责是向对方发出警报信息, 有点像是 HTTP 协议里的状态码。比如, protocol_version 就是不支持旧版本, bad_certificate 就是证书有问题, 收到 警报后另一方可以选择继续, 也可以立即终止连接 握手协议(Handshake Protocol) 是 TLS 里最复杂的子协议, 要比 TCP 的 SYN/ACK 复 杂的多, 浏览器和服务器会在握手过程中协商 TLS 版本号、随机数、密码套件等信息, 然 后交换证书和密钥参数, 最终双方协商得到会话密钥, 用于后续的混合加密系统 最后一个是 变更密码规范协议(Change Cipher Spec Protocol), 它非常简单, 就是一个“通知”, 告诉对方, 后续的数据都将使用加密保护。那么反过来, 在它之前, 数据都是明文的 …………… 迁移HTTPS的必要性如果你做移动应用开发的话, 那么就一定知道, Apple、Android、某信等开发平台在 2017 年就相继发出通知, 要求所有的应用必须使用 HTTPS 连接, 禁止不安全的 HTTP在台式机上, 主流的浏览器 Chrome、Firefox 等也早就开始 强推 HTTPS, 把 HTTP 站 点打上 不安全 的标签, 给用户以 心理压力Google 等搜索巨头还利用自身的“话语权”优势, 降低 HTTP 站点的排名, 而给 HTTPS 更大的权重, 力图让网民只访问到 HTTPS 网站这些手段都逐渐 挤压 了纯明文 HTTP 的生存空间, 迁移到 HTTPS 已经不是要不要做的问题, 而是要怎么做的问题了, HTTPS 的大潮无法阻挡, 如果还是死守着 HTTP, 那么无疑会被冲刷到互联网的角落里目前国内外的许多知名大站都已经实现了 全站 HTTPS , 打开常用的某宝、某东、某 浪, 都可以在浏览器的地址栏里看到 小锁头 , 如果你正在维护的网站还没有实施 HTTPS, 那可要抓点紧了","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP 缓存代理","slug":"http/2019-10-23-HTTP-Proxy-cache","date":"2019-10-23T11:50:17.000Z","updated":"2019-10-23T11:33:42.000Z","comments":true,"path":"2019/10/23/http/2019-10-23-HTTP-Proxy-cache/","link":"","permalink":"http://blog.renyimin.com/2019/10/23/http/2019-10-23-HTTP-Proxy-cache/","excerpt":"","text":"把之前介绍的 HTTP 缓存控制 和 HTTP 的代理服务 结合起来就是这节所要说的 缓存代理, 也就是支持缓存控制的代理服务 之前谈到缓存时, 主要讲了客户端(浏览器)上的缓存控制, 它能够减少响应时间、节约带宽, 提升客户端的用户体验。但 HTTP 传输链路上, 不只是客户端有缓存, 服务器上的缓存也是非常有价值的, 可以让请求不必走完整个后续处理流程, 而是”就近”获得响应结果 HTTP 的服务器缓存功能主要由代理服务器来实现(即缓存代理), 而源服务器系统内部虽然也经常有各种缓存(如 Memcache、Redis、Varnish 等), 但与 HTTP 没有太多关系, 所以这里暂且不说 1。 代理服务收到源服务器发来的响应数据后需要做两件事 - 第一个当然是把报文转发给客户端, 而第二个就是把报文存入自己的 Cache 里 - 下一次再有相同的请求, 代理服务器就可以直接发送 304 或者缓存数据, 不必再从源服务器那里获取。这样就降低了客户端的等待时间, 同时节约了源服务器的网络带宽 作为中转站的 代理服务器, 它除了具备 客户端和服务器的双重角色, 可以使用之前的 Cache-Control 属性外, 还有一些特有的 Cache-Control 属性 因为它和客户端还是有一些不一样, 客户端的缓存只是用户自己使用, 而代理的缓存可能会为非常多的客户端提供服务。所以, 需要对它的缓存再多一些限制条件 代理服务器 Cache-Control 代理服务器首先要区分源服务器给他的缓存, 是让客户端缓存还是让代理缓存, 可以使用两个新属性 private 和 public private 表示缓存只能在客户端保存, 是用户 私有的, 不能放在代理上与别人共享 而 public 的意思就是缓存完全开放, 是存在代理服务器上, 谁都可以存, 谁都可以用 其次, 缓存失效后的重新验证也要区分开 (即 条件请求 Last-modified 和 ETag) must-revalidate 是只要过期就必须回源服务器验证 而新的 proxy-revalidate 只要求代理的缓存过期后必须验证, 客户端不必回源, 只验证到代理这个环节就行了 再其次, 缓存的生存时间可以使用新的 s-maxage (s 是 share 的意思, 注意 maxage 中间没有 “-“), 只限定在代理上能够存多久, 而客户端仍然使用 “max-age” 还有一个代理专用的属性 no-transform 代理有时候会对缓存下来的数据做一些优化, 比如把图片生成 png、webp 等几种格式, 方便今后的请求处理, 而 no-transform 就会禁止这样做, 不许“偷偷摸摸搞小动作” 注意: 源服务器在设置完“Cache-Control”后必须要为报文加上“Last- modified”或“ETag”字段。否则, 客户端和代理后面就无法使用条件请求来验证缓存是否有效, 也就不会有 304 缓存重定向 下面的流程图是完整的服务器端缓存控制策略, 可以同时控制客户端和代理","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP 代理服务","slug":"http/2019-10-23-HTTP-Proxy","date":"2019-10-23T08:41:43.000Z","updated":"2019-10-23T06:09:50.000Z","comments":true,"path":"2019/10/23/http/2019-10-23-HTTP-Proxy/","link":"","permalink":"http://blog.renyimin.com/2019/10/23/http/2019-10-23-HTTP-Proxy/","excerpt":"","text":"引入 HTTP 代理后, 原来简单的双方通信就变复杂了一些, 加入了一个或者多个中间人, 但整体上来看, 还是一个有顺序关系的链条, 而且链条里相邻的两个角色仍然是简单的一对一通信, 不会出现越级的情况链条的起点还是客户端(也就是浏览器), 中间的角色被称为代理服务器(proxy server), 链条的终点被称为源服务器(origin server) 代理服务 “代理” 这个词听起来好像很神秘, 有点“高大上”的感觉, 但其实 HTTP 协议里对它并没有什么特别的描述, 它就是在客户端和服务器原本的通信链路中插入的一个中间环节, 也是一台服务器, 但提供的是 “代理服务”所谓的“代理服务”就是指服务本身不生产内容, 而是处于中间位置转发上下游的请求和响应, 具有双重身份:面向下游的用户时, 表现为服务器, 代表源服务器响应客户端的请求;而面向上游的源服务器时, 又表现为客户端, 代表客户端发送请求; 代理有很多的种类, 例如 匿名代理、透明代理、正向代理和反 向代理, 这里主要是聊聊实际工作中最常见的 反向代理, 它在传输链路中更靠近源服务器, 为源服务器提供代理服务 代理的作用由于代理处在 HTTP 通信过程的中间位置, 相应地就对上屏蔽了真实客户端, 对下屏蔽了真实服务器。在这个中间层的里就可以做很多的事情, 为 HTTP 协议增加更多的灵活性, 实现客户端和服务器的 “双赢” 代理最基本的一个功能是 负载均衡因为在面向客户端时屏蔽了源服务器, 客户端看到的只是代理服务器, 源服务器究竟有多少台、是哪些 IP 地址 客户端都不知道, 于是代理服务器就可以掌握请求分发的“大权”, 决定由后面的哪台服务器来响应请求(代理中常用的负载均衡算法如轮询、一致性哈希等等, 这些算法的目标都是尽量把外部的流量合理地分散到多台源服务器, 提高系统的整体资源利用率和性能) 除了负载均衡, 代理服务还可以执行其他的更多功能, 如: 健康检查: 使用“心跳”等机制监控后端服务器, 发现有故障就及时“踢出”集群, 保证服务高可用; 安全防护: 保护被代理的后端服务器, 限制 IP 地址或流量, 抵御网络攻击和过载; 加密卸载: 对外网使用 SSL/TLS 加密通信认证, 而在安全的内网不加密, 消除加解密成本; 数据过滤: 拦截上下行的数据, 任意指定策略修改请求或者响应; 内容缓存: 暂存、复用服务器响应 代理相关头字段代理的好处很多, 但因为它“欺上瞒下”的特点, 隐藏了真实客户端和服务器, 如果双方想要获得这些“丢失”的原始信息, 该怎么办呢? via首先, 代理服务器需要用字段 Via 标明代理的身份Via 是一个通用字段, 请求头 或 响应头里都可以出现, 每当报文经过一个代理节点, 代理服务器就会把自身的信息追加到字段的末尾, 就像是经手人盖了一个章; 如果通信链路中有很多中间代理, 就会在 Via 里形成一个链表, 这样就可以知道报文究竟走过了多少个环节才到达了目的地。例如下图中有两个代理: proxy1 和 proxy2, 客户端发送请求会经过这两个代理, 依次添加就是 “Via: proxy1, proxy2”, 等到服务器返回响应报文的时候就要反过来走, 头字段就是 “Via: proxy2, proxy1” 不过, Via 字段只解决了客户端和源服务器判断是否存在代理的问题, 还不能知道对方的真实信息 X-Forwarded-For、X-Real-IP比如, 服务器的 IP 地址应该是保密的, 关系到企业的内网安全, 所以一般不会让客户端知道。 但反过来看, 服务器通常是需要知道客户端的真实 IP 地址, 方便做访问控制、用户画像、统计分析可惜的是 HTTP 标准里并没有为此定义头字段, 但已经出现了很多 “事实上的标准”, 最 常用的两个头字段是 X-Forwarded-For 和 X-Real-IP X-Forwarded-For 的字面意思是 “为谁而转发”, 形式上和 “Via” 差不多, 也是每经过一个代理节点就会在字段里追加一个信息; 但 Via 追加的是代理主机名(或者域名), 而 X-Forwarded-For 追加的是请求方的 IP 地址, 所以, 在字段里最左边的 IP 地址就客户端的地址 X-Real-IP 是另一种获取客户端真实 IP 的手段, 它的作用很简单, 就是记录客户端 IP 地址, 没有中间的代理信息, 相当于是 X-Forwarded-For 的简化版。如果客户端和源 服务器之间只有一个代理, 那么这两个字段的值就是相同的… 代理协议 有了 X-Forwarded-For 等头字段, 源服务器就可以拿到准确的客户端信息了, 但对于代理服务器来说它并不是一个最佳的解决方案 因为通过 “X-Forwarded-For” 操作代理信息必须要解析 HTTP 报文头, 这对于代理来说成本比较高, 原本只需要简单地转发消息就好, 而现在却必须要费力解析数据再修改数据, 会降低代理的转发性能 另一个问题是 “X-Forwarded-For” 等头必须要修改原始报文, 而有些情况下是不允许甚至不可能的(比如使用 HTTPS 通信被加密) 所以就出现了一个专门的“代理协议”(The PROXY protocol), 它由知名的代理软件 HAProxy 所定义, 也是一个“事实标准”, 被广泛采用(注意并不是 RFC) ……","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP 缓存控制 和 条件请求","slug":"http/2019-10-22-HTTP-CacheControl","date":"2019-10-22T06:30:12.000Z","updated":"2019-11-04T02:59:40.000Z","comments":true,"path":"2019/10/22/http/2019-10-22-HTTP-CacheControl/","link":"","permalink":"http://blog.renyimin.com/2019/10/22/http/2019-10-22-HTTP-CacheControl/","excerpt":"","text":"鉴于 HTTP “请求 - 应答” 模式的特点, 这里将 HTTP的缓存 大致分为 客户端缓存(也就是浏览器的缓存) 和 服务器端缓存 服务器端缓存控制服务器标记资源有效期使用的头字段是 Cache-Control Cache-Control 字段里的 max-age 属性 是用来标记资源的有效期 (Cache-Control: max-age=30 相当于告诉浏览器, 这个页面只能缓存30秒, 之后就算是过期) 注意: max-age 是 “生存时间” (又叫 “新鲜度” “缓存寿命”), 时间的计算起点是 响应报文的创建时刻(即 Date 字段, 也就是离开服务器的时刻), 而不是客户端收到报文的时刻, 也就是说包含了在链路传输过程中所有节点所停留的时间 (比如, 服务器设定 Cache-Control: max-age=5, 但因为网络质量很糟糕, 等浏览器收到响应报文已经过去了 4 秒, 那么这个资源在客户端就最多能够再存 1 秒钟, 之后就会失效) max-age 是 HTTP 缓存控制最常用的属性, 此外在响应报文里还可以用如下其他的属性来更精确地指示浏览器应该如何使用缓存 no_store: 不允许缓存, 用于某些变化非常频繁的数据, 例如秒杀页面 no_cache: 它的字面含义容易与 no_store 搞混, 实际的意思并不是不允许缓存, 而是可以缓存, 但在使用之前必须要去服务器验证是否过期, 是否有最新的版本 (貌似就是虽然存了, 但是不用) 如果验证没有过期, 则直接取客户端浏览器中的缓存; 否则, 客户端重新缓存新的数据, 然后再返回; “no-cache” 属性可以理解为 “max-age=0, must-revalidate” must-revalidate: 又是一个和 no_cache 相似的词, 它的意思是如果缓存不过期就可以继续使用, 但过期了如果还想用就必须去服务器验证 这个状态貌似比较正常, 缓存有效就用, 缓存没效就重新取~~ 除了 Cache-Control, 服务器也可以使用 Expires 字段来标记资源的有效期, 不过其优先级低于 Cache-Control; 另外还有一个历史遗留字段 Pragma: no-cache, 它相当于 Cache-Control: no-cache, 除非是为了兼容HTTP/1.0 否则不建议使用 小结服务器的缓存控制策略流程图大致如下, 对照着它就可以在今后的后台开发里明确 Cache-Control 的用法了 客户端缓存控制 注意: 仅仅当 服务器端 使用了 Cache-Control: max-age=30 时, 你点击浏览器 “刷新” 按钮会发现, 貌似缓存并未生效, 如下测试:~~这是因为: 不止是服务器可以发 Cache-Control 头, 客户端浏览器也可以发 Cache-Control, 也就是说 “请求-应答” 的双方都可以用这个字段进行缓存控制, 互相协商缓存的使用策略(而当你点 “刷新” 按钮的时候, 浏览器会在请求头里加一个 Cache-Control: max-age=0, 由于 max-age 是 “生存时间”, max-age=0 的意思就是 不进行缓存,而是每次都去服务器去最新数据, 而本地缓存里的数据至少保存了几秒钟) Ctrl+F5(MacOS是 shift+command+r) 的 强制刷新 又是什么样的呢?尝试发现, 它是发了一个 Cache-Control: no-cache, 含义和 max-age=0 基本一样, 就看后台的服务器怎么理解, 通常两者的效果是相同的 那么, 浏览器的缓存究竟什么时候才能生效呢? 试着点一下浏览器的 前进 后退 按钮, 再看开发者工具, 你就会惊喜地发现 from disk cache 的字样, 意思是没有发送网络请求, 而是读取的磁盘上的缓存~~ 如果用第 18 讲里的重定向跳转功能, 也可以发现浏览器使用了缓存~~ 条件请求HTTP 协议定义了一系列 If 开头的 条件请求 字段, 专门用来验证资源是否过期 (验证的责任是服务器的责任, 浏览器只需”坐享其成”) 1.客户端的 条件请求 一共有5个头字段, 我们最常用的是 if-Modified-Since 和 If-None-Match 这两个它们需要第一次的响应报文预先提供 last-modified 和 ETag, 然后第二次请求时就可以带上缓存里的原值, 验证资源是否是最新的, 如果资源没有变, 服务器就回应一个 304 Not Modified, 表示缓存依然有效, 浏览器就可以更新一下有效期, 然后放心大胆地使用缓存了 Last-modified 很好理解, 就是文件的最后修改时间 ETag 是 “实体标签(Entity Tag)” 的缩写, 是资源的一个唯一标识, 主要是用来解决 修改时间无法准确区分文件变化的问题比如, 一个文件在一秒内修改了多次, 但因为修改时间是秒级, 所以这一秒内的新版本无法区分再比如, 一个文件定期更新, 但有时会是同样的内容, 实际上没有变化, 如果用修改时间判断 就会误以为发生了变化, 传送给浏览器就会浪费带宽使用 ETag 就可以精确地识别资源的变动情况, 让浏览器能够更有效地利用缓存 ETag 还有 “强”、”弱” 之分强 ETag 要求资源在字节级别必须完全相符弱 ETag 在值前有个 W/ 标记, 只要求资源在语义上没有变化, 但内部可能会有部分发生了改变(例如 HTML 里的标签顺序调整, 或者多了几个空格) 小结 还是拿生鲜速递做比喻最容易理解:你打电话给超市, “我这个西瓜是 3 天前买的, 还有最新的吗?”。超市看了一下库存, 说:“没有啊, 我这里都是 3 天前的。”于是你就知道了, 再让超市送货也没用, 还是吃 冰箱里的西瓜吧。这就是“if-Modified-Since”和“Last-modified”但你还是想要最新的, 就又打电话:“有不是沙瓤的西瓜吗?”, 超市告诉你都是沙瓤的 (Match), 于是你还是只能吃冰箱里的沙瓤西瓜。这就是“If-None-Match”和“弱 ETag”第三次打电话, 你说 “有不是 8 斤的沙瓤西瓜吗?”, 这回超市给了你满意的答复:“有 个 10 斤的沙瓤西瓜”。于是, 你就扔掉了冰箱里的存货, 让超市重新送了一个新的大西瓜。这就是“If-None-Match”和“强 ETag” 2.条件请求里其他的三个头字段是 If-Unmodified-Since、If-Match 和 If-Range, 其实只要你掌握了 if-Modified-Since 和 If-None-Match, 可以轻易地举一反三 小结用好 HTTP 的缓存控制和条件请求, 可以减少响应时间、节约网络流量 服务器使用 Cache-Control 设置缓存策略, 常用的是 max-age, 表示资源的有效期 浏览器收到数据就会存入缓存, 如果没过期就可以直接使用, 过期就要去服务器验证是否仍然可用 验证资源是否失效需要使用 条件请求, 常用的是 if-Modified-Since 和 If-None-Match, 收到 304 就可以复用缓存里的资源 验证资源是否被修改的条件有两个: Last-modified 和 ETag, 需要服务器预先在响应报文里设置, 搭配条件请求使用 浏览器也可以发送 Cache-Control 字段, 使用 max-age=0 或 no_cache 刷新数据 客户端缓存控制补充客户端在 HTTP 缓存体系里要面对的是 代理 和 源服务器, 所以是必须区别对待的, 直接上图比如 如果服务端返回的响应头设置了 Cache-Control: public, max-age=10, s-maxage=30, 数据可以在浏览器里存 10 秒, 在代理上存 30 秒 可以看到, 关于缓存的生存时间, 多了两个新属性 max-stale 和 min-fresh max-stale 的意思是如果代理上的缓存过期了也可以接受, 但不能过期太多, 超过 x 秒就会不要了 min-fresh 的意思是缓存必须有效, 而且必须在 x 秒后依然有效(max-stale是可以接受的过期时间, min-fresh是可以接受的新鲜时间。 不好理解也没事, 这两个属性用的不多, 可以以后实际遇到了再体会) 有的时候客户端还会发出一个特别的 only-if-cached 属性, 表示只接受代理缓存的数据, 不接受源服务器的响应。如果代理上没有缓存或者缓存过期, 就应该给客户端返回一个 504(Gateway Timeout)","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"XSS(Cross Site Scripting) 跨站脚本攻击","slug":"http/2019-10-22-XSS","date":"2019-10-22T02:29:12.000Z","updated":"2019-11-05T09:10:09.000Z","comments":true,"path":"2019/10/22/http/2019-10-22-XSS/","link":"","permalink":"http://blog.renyimin.com/2019/10/22/http/2019-10-22-XSS/","excerpt":"","text":"XSS 简介 XSS(Cross Site Scripting)攻击 全称 跨站脚本攻击, 是为不和层叠样式表(Cascading Style Sheets, CSS)的缩写混淆, 故将跨站脚本攻击缩写为XSS, XSS是一种经常出现在web应用中的计算机安全漏洞 XSS 通常指黑客通过 “HTML注入” 篡改了网页, 插入了恶意的脚本, 从而在用户浏览网页时, 控制用户浏览器的一种攻击; XSS根据效果的不同可以分多种 XSS分类反射型 XSS反射型XSS: 反射型XSS又被称为非存储型XSS, 攻击者通常会通过URL参数传入恶意语句从而实现攻击由于我们的payload未经过一个存储的过程直接传到了用户浏览的页面上, 所以也称之为非存储型XSS (反射型XSS只是简单地把用户输入的数据 “反射” 给浏览器。也就是说, 黑客往往需要诱使用户”点击”一个恶意链接, 才能攻击成功) 最简单的反射型XSS拿cookie: https://blog.csdn.net/thislocal/article/details/51010021 存储型 XSS存储型XSS：攻击者在页面中插入XSS代码, 服务器将恶意代码传至数据库, 当受害者浏览页面时服务器将代码取出从而实现攻击。(存储型xss与反射型xss的区别在于存储型会将用户输入的数据存入服务器, 在用户下一次点击时便会触发, 由于其隐蔽性较高, 所以危害也普遍大于反射型xss) DOM Based XSSDOM型XSS: DOM型XSS与反射型XSS漏洞大同小异, 但是区别在于反射型XSS会将语句存储于后端再出现在前端页面, 而DOM型XSS漏洞直接将语句存储于前端 小结 http://www.imooc.com/article/252521 XSS 漏洞修复方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"CSRF","slug":"http/2019-10-22-CSRF","date":"2019-10-22T02:29:12.000Z","updated":"2019-11-06T02:38:44.000Z","comments":true,"path":"2019/10/22/http/2019-10-22-CSRF/","link":"","permalink":"http://blog.renyimin.com/2019/10/22/http/2019-10-22-CSRF/","excerpt":"","text":"","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"Cookie","slug":"http/2019-10-21-HTTP-Cookie","date":"2019-10-21T05:10:31.000Z","updated":"2019-11-04T02:59:21.000Z","comments":true,"path":"2019/10/21/http/2019-10-21-HTTP-Cookie/","link":"","permalink":"http://blog.renyimin.com/2019/10/21/http/2019-10-21-HTTP-Cookie/","excerpt":"","text":"HTTP协议 是 “无状态” 的, 但却是可扩展的, 后来发明的 Cookie 技术, 就给 HTTP 增加了 “记忆能力” Cookie 的工作过程Cookie 的传递主要用到两个字段: 响应头字段 Set-Cookie 和 请求头字段 Cookie 当用户通过浏览器第一次访问服务器的时候, 服务器肯定是不知道他的身份的。所以, 服务器就要创建一个独特的身份标识数据, 格式是 key=value, 然后放进 Set-Cookie 字段里, 随着响应报文一同发给浏览器 浏览器收到响应报文, 看到里面有 Set-Cookie, 知道这是服务器给的身份标识, 于是就保存起来, 下次再请求的时候就自动把这个值放进 Cookie 字段里发给服务器 浏览器第二次请求时, 请求头字段里面有了 Cookie 字段, 服务器就知道这个用户不是新人, 之前来过, 就拿出 Cookie 里的值, 识别出用户的身份, 然后提供个性化的服务 服务器可以在响应头里添加多个 Set-Cookie, 存储多个 “key=value”, 但浏览器这边发送时不需要用多个 Cookie 字段, 只要在一行里用 ; 隔开就行 Cookie 的属性Cookie 就是服务器委托浏览器存储在客户端里的一些数据, 而这些数据通常都会记录用户的关键识别信息。所以在 “key=value” 外, 还需要再用一些手段来保护Cookie, 防止外泄或窃取, 这些手段就是 Cookie 的属性 为了保护 Cookie, 还要给它设置有效期、作用域等属性, 常用的有 Max-Age、Expires、Domain、HttpOnly 等 Expires、Max-Age首先, 我们应该设置 Cookie 的生存周期, 也就是它的有效期, 让它只能在一段时间内可用, 就像是食品的” 保鲜期”, 一旦超过这个期限浏览器就认为是 Cookie 失效, 在存储里删除, 也不会发送给服务器Cookie 的有效期可以使用 Expires 和 Max-Age 两个属性来设置: Expires 俗称 “过期时间”, 用的是绝对时间点, 可以理解为 “截止日期”(deadline) Max-Age 用的是相对时间, 单位是秒, 浏览器用收到报文的时间点再加上 Max-Age, 就可以得到失效的绝对时间 Expires 和 Max-Age 可以同时出现, 两者的失效时间可以一致, 也可以不一致, 但浏览器会优先采用 Max-Age 计算失效期 Domain、Path其次, 需要设置 Cookie 的作用域, 让浏览器仅发送给特定的服务器 和 URI, 避免被其他网站盗用作用域的设置比较简单, Domain 和 Path 指定了 Cookie 所属的域名和路径, 浏览器在发送 Cookie 前会从 URI 中提取出 host 和 path 部分, 对比 Cookie 的属性, 如果不满足条件, 就不会在请求头里发送 Cookie 使用这两个属性可以为不同的 域名 和 路径 分别设置各自的 Cookie (比如 “/19-1” 用一个 Cookie, “/19-2” 再用另外一个 Cookie, 两者互不干扰, 不过现实中为了省事, 通常 Path 就用一个 “/“ 或者直接省略,表示域名下的任意路径都允许使用该Cookie) HttpOnly最后要考虑的就是Cookie 的安全性了, 尽量不要让服务器以外的人看到写过前端的同学一定知道, 在 JS 脚本里可以用 document.cookie 来读写 Cookie 数据,这就带来了安全隐患, 有可能会导致 “跨站脚本”(XSS)攻击窃取数据 属性 “HttpOnly” 会告诉浏览器, 此 Cookie 只能通过浏览器 HTTP 协议传输, 禁止其他方式访问, 浏览器的 JS 引擎就无法使用 document.cookie 等一切相关的 API, 脚本攻击也就无从谈起了 SameSite另一个属性 “SameSite”, 可以防范”跨站请求伪造”(XSRF)攻击, 设置 SameSite=Strict 可以严格限定 Cookie 不能随着跳转链接跨站发送, 而 SameSite=Lax 则略宽松一点, 允许 GET/HEAD 等安全方法, 但禁止 POST 跨站发送 Secure还有一个属性叫 “Secure”, 表示这个 Cookie 仅能用 HTTPS 协议加密传输, 明文的 HTTP 协议会禁止发送, 但 Cookie 本身不是加密的,浏览器里还是以明文的形式存在 Cookie 应用Cookie 最基本的一个用途就是身份识别, 保存用户的登录信息, 实现会话事务 (比如, 你用账号和密码登录某电商, 登录成功后网站服务器就会发给浏览器一个 Cookie, 内容大概是“name=yourid”, 这样就成功地把身份标签贴在了你身上) Cookie 的另一个常见用途是广告跟踪 TipCookie 是由浏览器负责存储的, 而不是操作系统。所以, 它是 浏览器绑定 的, 只能在本浏览器内生效 (如果你换个浏览器或者换台电脑, 新的浏览器里没有服务器对应的 Cookie, 只能再走一遍 Set-Cookie 流程)Cookie 这个词来源于计算机编程里的术语 Magic Cookie, 意思是不透明的数据, 并不是 “小甜饼” 的含义 (虽然字面意思如此)","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02.","slug":"nio-socket/2019-09-23-02","date":"2019-09-23T08:42:19.000Z","updated":"2019-09-23T08:42:24.000Z","comments":true,"path":"2019/09/23/nio-socket/2019-09-23-02/","link":"","permalink":"http://blog.renyimin.com/2019/09/23/nio-socket/2019-09-23-02/","excerpt":"","text":"","categories":[{"name":"NIO","slug":"NIO","permalink":"http://blog.renyimin.com/categories/NIO/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"http://blog.renyimin.com/tags/NIO/"}]},{"title":"01.","slug":"nio-socket/2019-09-23-01","date":"2019-09-23T08:41:23.000Z","updated":"2019-09-23T08:42:09.000Z","comments":true,"path":"2019/09/23/nio-socket/2019-09-23-01/","link":"","permalink":"http://blog.renyimin.com/2019/09/23/nio-socket/2019-09-23-01/","excerpt":"","text":"","categories":[{"name":"NIO","slug":"NIO","permalink":"http://blog.renyimin.com/categories/NIO/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"http://blog.renyimin.com/tags/NIO/"}]},{"title":"四次挥手","slug":"network/tcpip-4-wave","date":"2019-08-28T11:26:17.000Z","updated":"2019-11-01T08:52:38.000Z","comments":true,"path":"2019/08/28/network/tcpip-4-wave/","link":"","permalink":"http://blog.renyimin.com/2019/08/28/network/tcpip-4-wave/","excerpt":"","text":"TCP是全双工模式, 需要两边的连接全部关闭, 此TCP会话才算完全关闭, 四次挥手使得TCP的全双工连接能够可靠的终止另外 TIMED_WAIT 也使得连接终止后网络上残余的发送给该连接的数据被丢弃而不至于被新连接接收 第一次挥手当客户端认为数据发送完成, 它会向服务器发送 释放连接 的请求, 该请求只有报文头, 头中携带的主要参数为: FIN=1, seq=u (此时, 客户端 将进入 FIN-WAIT-1 状态) FIN=1: 表示该报文段是一个 连接释放 请求 seq=u: u-1 是 客户端 向 服务器 发送的最后一个字节的序号 第二次挥手服务器收到连接释放请求后, 会通知相应的应用程序, 告诉它 客户端-&gt;服务器 这个方向的连接已经释放 (此时 服务器 进入 CLOSE-WAIT 状态), 并向 客户端 发送连接释放的应答, 其报文头包含: ACK=1, seq=v, ack=u+1客户端收到该应答, 进入 FIN-WAIT-2 状态, 等待服务器发送连接释放请求(第二次挥手完成后, 客户端-&gt;服务端 方向的连接已经释放, 服务端不会再接收数据, 客户端也不会再发送数据; 但 服务端-&gt;客户端 方向的连接仍然存在, 服务端可以继续向客户端发送数据) 第三次挥手当服务器向客户端发完所有数据后, 向客户端发送连接释放请求, 请求头: FIN=1, ACK=1, seq=w, ack=u+1, 服务器便进入 LAST-ACK 状态 第四次挥手客户端收到释放请求后, 向服务器发送确认应答, 此时客户端进入 TIME-WAIT 状态, 该状态会持续2MSL时间, 若该时间段内没有服务器的重发请求的话, 就进入 CLOSED 状态; 当服务器收到确认应答后, 也便进入 CLOSED 状态 四次挥手抓包 服务端监听端口 客户端连接上来 客户端发送消息 + 客户端断开连接 + 服务器端断开连接 隐藏 RST 标志位 相关内容 问题1.为什么 ServerSocket 关闭后, ClientSocket 仍可以发送一次请求, 第二次发送才显示发送不出去客户端socket连接成功 (三次握手) 客户端第一次写数据成功 + 服务端-&gt;客户端单向socket连接关闭 客户端-&gt;服务器socket连接仍然存在, 可以发送, 但是发送后, 服务器端返回了 RST标志位再次发送才会发送失败 2.为什么客户端要先进入 TIME-WAIT 状态, 等待2MSL时间后才进入CLOSED状态?为了保证服务器能收到客户端的确认应答, 若客户端发完确认应答后直接进入CLOSED状态, 那么如果该应答丢失, 服务器等待超时后就会重新发送连接释放请求, 但此时客户端已经关闭了, 不会作出任何响应, 因此服务器永远无法正常关闭 参考https://www.cnblogs.com/cenglinjinran/p/8482412.htmlhttp://dy.163.com/v2/article/detail/ECQ04CUU05315U6Q.html","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"TCP 三次握手 -- 为什么TCP需要三次握手, 而不是两次, 四次或五次?","slug":"network/tcpip-questions-01","date":"2019-08-28T11:17:36.000Z","updated":"2019-11-01T09:38:42.000Z","comments":true,"path":"2019/08/28/network/tcpip-questions-01/","link":"","permalink":"http://blog.renyimin.com/2019/08/28/network/tcpip-questions-01/","excerpt":"","text":"三次握手简介注意: ACK(ACKNOWLEDGEMENT) 是TCP的标志位 (wireshark抓包可查看 Flags下的Acknowledgement字段) ack 不一样, 是应答的下一次的起始点 (wireshark抓包可查看 Acknowledgement number 字段) 第一次握手客户端向服务端发送连接请求报文段, 该报文段的头部中 SYN=1, ACK=0, seq=x; 请求发送后, 客户端便进入 SYN-SENT 状态 SYN=1, ACK=0 表示该报文段为连接请求报文 seq=x 为本次TCP通信的字节流的初始序号 第二次握手服务端收到连接请求报文段后, 如果同意连接, 则会发送一个应答: SYN=1, ACK=1, seq=y, ack=x+1 (ack 对应上一步的 seq); 该应答发送完成后便进入 SYN-RCVD 状态 SYN=1, ACK=1 表示该报文段为连接同意的应答报文 seq=y 表示服务端作为发送者时, 发送字节流的初始序号 ack=x+1 表示服务端希望下一个数据报发送序号从x+1开始的字节 第三次握手当客户端收到连接同意的应答后, 还要向服务端发送一个确认报文段, 表示: 服务端发来的连接同意应答已经成功收到; 该报文段的头部为: ACK=1, seq=x+1, ack=y+1 (此处的ack 对应上一步的 seq, 此处的seq对应上一步的ack)客户端发完这个报文段后便进入ESTABLISHED状态, 服务端收到这个应答后也进入 ESTABLISHED 状态, 此时连接的建立完成! 为什么两次握手不行?确保两方都有 发送/接收 消息的能力 TCP 建立连接时的 三次握手, 如果用一次电话通讯来描述, 过程大致如下第一步: server 知道 client 可以发消息第二步: client 知道 server 可以接收/发送消息第三步: server 知道 client 可以接收消息 如果少了第三步, 那 server 是不知道 client 是否可以接收消息 的 灰猫：”喂喂, 白猫通知, 你能听到我说话么?” 这是是第一次握手, 也就是说 灰猫(client)发送消息的能力没有问题 然后白猫回了一句 “灰猫同志, 我能听到你说话! 那你能听到我说话么?” 这是第二次握手 白猫回了黑猫一句, 说明server具有 发送消息的能力, 也同时具有接受消息的能力 灰猫到此时, 虽然具有发送消息的能力, 但服务端不确定其是否具有接收消息的能力, 最后灰猫说 “嗯, 白猫同志, 我也能听到你说话, 开始….”, 这是第三次握手 client可以回答server的问题, 证明client也具备接收消息的能力这样就可以进行通话了(建立了TCP连接) 小结, 对于两次握手来说 服务器收到了客户端的消息, 服务器知道了客户端是可以发送消息的 客户端从服务器接受到了消息, 客户端知道了服务器接受到了我的消息才回复, 说明服务器的接受消息能力和发送消息的能力没问题 但由于没有第三次握手, 所以服务器不知道客户端是否具有接受消息的能力 综上所述, 客户端确保了服务器的接受发送没问题, 但是服务器仅仅只知道客户端的发送消息没问题, 这并不是可靠的, 所以两次握手不可以 网络阻塞时, 两次握手出现问题 假设客户端和服务器进行TCP连接, 第一次握手时请求发生了阻塞 由于客户端没有收到服务器的应答报文, 客户端认为这个TCP连接请求丢失了; 于是会重新发送TCP连接请求, 这次没有阻塞, 成功连接了因为是讨论的两次握手, 所以只进行两次连接就可以进行通信了 通信结束后, 也正常断开了连接 谁曾想, 突然, 最开始的阻塞的连接请求(A客户端以为丢失了), 但其实并没有丢失, 只是阻塞了而已, 阻塞一段时间网络又畅通了, 于是TCP连接请求成功到达了服务器, 服务器又以为是客户端又要进行数据传输, 于是服务器就又对这个连接请求进行应答, 两次握手, 于是又成功建立了TCP连接但是由于客户端以为这个连接请求已经丢失了, 所以不会利用这个建立的连接进行数据通信, 虽然服务器分配给了资源给客户端, 但是客户端并不进行数据传输, 这样就白白浪费了服务器的资源, 试想一下如果网络很拥堵, 服务器岂不是浪费了一堆资源, 可能对于正常的连接请求都无法处理了 参考: http://dy.163.com/v2/article/detail/ECQ04CUU05315U6Q.html 为什么不是 四次, 五次握手?从经典的 红蓝军通讯问题 来分析, 两军为了实现协同出击, 在通讯上要想实现最终确认, 理论上其实是不可行的(双方对通信的确认是没有止境的), 也是无解的;在TCP协议中, 不使用四次, 五次握手, 是因为再多的握手和 三次握手 达到的效果其实都是一样的, 反而消耗更多的资源, 得不偿失所以 TCP/IP至少且比较恰当的是需要三次握手 主要是为了防止 失效的请求报文段 被服务端接收, 从而产生错误","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"tcp/ip -- questions","slug":"network/tcpip-questions-02","date":"2019-08-26T11:17:36.000Z","updated":"2019-11-04T02:47:32.000Z","comments":true,"path":"2019/08/26/network/tcpip-questions-02/","link":"","permalink":"http://blog.renyimin.com/2019/08/26/network/tcpip-questions-02/","excerpt":"","text":"现代浏览器在与服务器建立了一个 TCP 连接后是否会在一个 HTTP 请求完成后断开? 什么情况下会断开? 在 HTTP/1.0 中, 一个服务器在发送完一个 HTTP 响应后, 会断开 TCP 链接, 但是这样会导致每次请求都会重新建立和断开 TCP 连接, 代价过大; 所以虽然标准中没有设定, 但某些服务器对 Connection: keep-alive 的 Header 进行了支持, 意思是说, 完成这个 HTTP 请求之后, 不要断开 HTTP 请求使用的 TCP 连接, 这样的好处是连接可以被重新使用, 之后发送 HTTP 请求的时候不需要重新建立 TCP 连接, 并且如果维持连接, 那么 SSL 的开销也可以避免 持久连接: 既然维持 TCP 连接好处这么多, HTTP/1.1 就把 Connection 头写进标准, 并且默认开启持久连接, 除非请求中写明 Connection: close, 那么浏览器和服务器之间是会维持一段时间的 TCP 连接, 不会一个请求结束就断掉 所以第一个问题的答案是: 默认情况下建立 TCP 连接不会断开, 只有在请求报头中声明 Connection: close 才会在请求完成后关闭连接 下面两张图片是我短时间内两次访问 https://www.github.com 的时间统计 初始化连接和 SSL 开销消失了, 说明使用的是同一个 TCP 连接: 一次http请求, 谁会先断开TCP连接? 什么情况下客户端先断, 什么情况下服务端先断?一个tcp连接可以发送多少个http请求? 如果tcp连接保持长连接, 只要在tcp连接（默认两小时）不断开, 可以一直串行发送数量无上限 如果tcp连接不保持长连接(Connection:close), 则只能发一次http请求 如果http2, 采用多路复用技术Multiplexing, 一个tcp可以并发多个http请求, 同样也是无上限 如果和服务器建立多个tcp连接(chrome 浏览器一个host默认tcp连接并发数6, 这个限制是有原因的, Ipv4地址稀有资源, 涉及到NAT转换, 内网外网的端口映射详见https://blog.csdn.net/qfzhangwei/article/details/90614129), 底层还是长连接, 则发送数量无上限； 一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求, 再三个响应一起接收）? HTTP/1.1 存在一个问题, 单个 TCP 连接在同一时刻只能处理一个请求, 意思是说: 两个请求的生命周期不能重叠, 任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠 虽然 HTTP/1.1 规范中规定了 Pipelining 来试图解决这个问题, 但是这个功能在浏览器中默认是关闭的 但是, HTTP2 提供了 Multiplexing 多路传输特性, 可以在一个 TCP 连接中同时完成多个 HTTP 请求, 至于 Multiplexing 具体怎么实现的就是另一个问题了, 下面看一下使用 HTTP2 的效果 为什么有的时候刷新页面不需要重新建立 SSL 连接?在第一个问题的讨论中已经有答案了, TCP 连接有的时候会被浏览器和服务端维持一段时间, TCP 不需要重新建立, SSL 自然也会用之前的 浏览器对同一 Host 建立 TCP 连接到数量有没有限制? 假设我们还处在 HTTP/1.1 时代, 那个时候没有多路传输, 当浏览器拿到一个有几十张图片的网页该怎么办呢? 肯定不能只开一个 TCP 连接顺序下载, 那样用户肯定等的很难受, 但是如果每个图片都开一个 TCP 连接发 HTTP 请求, 那电脑或者服务器都可能受不了, 要是有 1000 张图片的话总不能开 1000 个TCP 连接吧, 你的电脑同意 NAT 也不一定会同意; 所以答案是: 有 (Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。) https://developers.google.com/web/tools/chrome-devtools/network/issues#queued-or-stalled-requestsdevelopers.google.com 那么回到最开始的问题, 收到的 HTML 如果包含几十个图片标签, 这些图片是以什么方式、什么顺序、建立了多少连接、使用什么协议被下载下来的呢？ 如果图片都是 HTTPS 连接并且在同一个域名下, 那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2, 如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取, 但是可以确定的是 Multiplexing 很可能会被用到。 如果发现用不了 HTTP2 呢? 或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的, 所以也就是只能使用 HTTP/1.1）。那浏览器就会在一个 HOST 上建立多个 TCP 连接, 连接数量的最大限制取决于浏览器设置, 这些连接会在空闲的时候被浏览器用来发送新的请求, 如果所有的连接都正在发送请求呢？那其他的请求就只能等等了。 https://www.cnblogs.com/qcloud1001/p/9591172.html 原文链接：https://blog.csdn.net/qq_36865108/article/details/84885506","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"TCP/IP 协议简介","slug":"network/2019-08-26-TCPIP","date":"2019-08-26T10:28:05.000Z","updated":"2019-11-04T02:47:19.000Z","comments":true,"path":"2019/08/26/network/2019-08-26-TCPIP/","link":"","permalink":"http://blog.renyimin.com/2019/08/26/network/2019-08-26-TCPIP/","excerpt":"","text":"TCP/IP五层协议 和 OSI的七层协议 对应关系如下 在每一层都工作着不同的设备, 比如我们常用的交换机就工作在数据链路层的, 一般的路由器是工作在网络层的 在每一层实现的协议也各不同, 即每一层的服务也不同.下图列出了每层主要的协议: 各分层的协议 (在OSI模型中ARP协议属于链路层；而在TCP/IP四层模型中, ARP协议属于网络层)OSI:12345678910111213物理层：EIA/TIA-232, EIA/TIA-499, V.35, V.24, RJ45, Ethernet, 802.3, 802.5, FDDI, NRZI, NRZ, B8ZS数据链路层：Frame Relay, HDLC, PPP, IEEE 802.3/802.2, FDDI, ATM, IEEE 802.5/802.2网络层：IP, IPX, AppleTalk DDP, 【ARP,RARP】传输层：TCP, UDP, SPX会话层：RPC,SQL,NFS,NetBIOS,names,AppleTalk,ASP,DECnet,SCP表示层:TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML应用层：FTP,WWW,Telnet,NFS,SMTP,Gateway,SNMP 参考极客时间: https://time.geekbang.org/course/detail/100026801-93590","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"OSI参考模型 (七层模型)","slug":"network/2019-08-26-OSI","date":"2019-08-26T07:17:23.000Z","updated":"2019-12-31T10:56:11.000Z","comments":true,"path":"2019/08/26/network/2019-08-26-OSI/","link":"","permalink":"http://blog.renyimin.com/2019/08/26/network/2019-08-26-OSI/","excerpt":"","text":"OSI 七层模型 应用层 (七层)应用层 是 OSI参考模型中最靠近用户的一层, 是为计算机用户提供应用接口, 也为用户直接提供各种网络服务 (常见应用层的网络服务协议有：HTTP, HTTPS, FTP, POP3、SMTP 等) 表示层 (六层)表示层 (也有叫 表现层), 提供各种用于应用层数据的编码和转换功能, 确保一个系统的应用层发送的数据能被另一个系统的应用层识别 (数据压缩 和 加密 也是表示层可提供的转换功能之一) 对于A,B两个不同国家的公司, 他们之间可以商定统一用英语作为交流的语言, 此时表示层, 就可以用来将应用层传递的信息转成英语, 同时为了防止别的公司看到, 公司A的人也会对这份报价单做一些加密的处理, 这就是表示的作用, 将应用层的数据转换翻译等 (我们的远程调用也经常会涉及到数据的加密、Json化) 会话层 (五层)会话层 就是负责建立、管理和终止表示层实体之间的通信会话。该层的通信由不同设备中的应用程序之间的服务请求和响应组成(和 HTTP Session 、telnet远程登录session 有异曲同工之妙) 传输层 (四层)传输层 建立了主机端到端的链接, 传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务, 包括处理差错控制和流量控制等问题;该层向高层屏蔽了下层数据通信的细节, 使高层用户看到的只是在两个传输实体间的一条主机到主机的、可由用户控制和设定的、可靠的数据通路; 我们通常说的, TCP、UDP 就是在这一层; 另外传输层还会指明应用层所提供的目标端口号; 四层传输层传输的叫 段(segment); 网络层 (三层) 本层通过 IP寻址 来建立两个节点之间的连接, 为源端的运输层送来的分组, 选择合适的路由和交换节点, 正确无误地按照地址传送给目的端的运输层 (也是通常说的IP层) 网络层就相当于快递公司庞大的快递网络, 全国不同的集散中心, 比如说, 从深圳发往北京的顺丰快递(陆运为例啊, 空运好像直接就飞到北京了), 首先要到顺丰的深圳集散中心, 从深圳集散中心再送到武汉集散中心, 从武汉集散中心再寄到北京顺义集散中心 (这个每个集散中心, 就相当于网络中的一个IP节点) 通常, 第三层网络层, 路由器传输的数据的叫 数据包(package) 路由器中包含了三层: 物理层、数据链路层、网络层路由器接收到的数据包package (其数据部分 包装了数据链路层以太网mac地址信息), 而路由器作为网络层设备, 使用的是因特网的ip协议, 所以需要拆包, 拿到ip地址, 并在自己内部使用 数据链路层的 ARP协议找到下一条的mac地址, 并再次包装上数据链路层的头信息, 发送给下一跳的设备(设备和设备之间还是使用的mac地址, 不过在路由器设备中会通过ip来寻找mac) 如下图: 数据链路层 (二层) 该层会将比特组合成字节, 再将字节组合成 帧(交换机转发的以太网数据叫 帧(frame)) , 然后使用链路层地址(以太网使用MAC地址)来访问介质, 并进行差错检测 通常, 数据链路层的数据称为 帧(frame) (第三层网络层, 路由器传输的数据的叫 数据包(package); 四层传输层传输的叫 段(segment)) 交换机 也叫 第二层交换机 (以太网交换机): 第二层交换机是对应于 OSI/RM 的第二协议层来定义的, 因为它只能工作在 OSI/RM 开放体系模型的第二层 －－ 数据链路层;第二层交换机依赖于链路层中的信息 (如MAC地址) 完成不同端口数据间的线速交换, 主要功能包括物理编址、错误校验、帧序列以及数据流控制, 这是最原始的交换技术产品;目前桌面型交换机一般是属于这类型, 因为桌面型的交换机一般来说所承担的工作复杂性不是很强, 又处于网络的最基层, 所以也就只需要提供最基本的数据链接功能即可 (目前第二层交换机应用最为普遍(主要是价格便宜), 功能符合中、小企业实际应用需求, 一般应用于小型企业或中型以上企业网络的桌面层次) 三层交换机 三层交换技术其实就是: 二层交换技术+三层转发技术它解决了局域网中网段划分之后, 网段中子网必须依赖路由器进行管理的局面, 解决了传统路由器低速、复杂所造成的网络瓶颈问题;第三层交换是在网络交换机中引入路由模块而取代传统路由器实现交换与路由相结合的网络技术。它根据实际应用时的情况, 灵活地在网络第二层或者第三层进行网络分段。具有三层交换功能的设备是一个带有第三层路由功能的第二层交换机。(在京东购买交换机时, 可以搜索 二层交换机 或者 三层交换机(相比二层交换机自然要贵一些, 毕竟至少多了路由功能)) 以太网 以太网(Ethernet) 是一种计算机局域网技术, IEEE组织的IEEE 802.3标准制定了以太网的技术标准, 它规定了包括物理层的连线、电子信号和介质访问层协议的内容 (以太网是目前应用最普遍的局域网技术, 取代了其他局域网技术如令牌环、FDDI 和 ARCNET) 以太网是现实世界中最普遍的一种计算机网络。以太网有两类: 第一类是经典以太网, 第二类是交换式以太网, 使用了一种称为交换机的设备连接不同的计算机。 经典以太网是以太网的原始形式, 运行速度从3~10 Mbps不等; 而交换式以太网正是广泛应用的以太网, 可运行在100、1000和10000Mbps那样的高速率, 分别以快速以太网、千兆以太网和万兆以太网的形式呈现; 交换机在构建局域网时, 主要就是使用以太网MAC地址确定目标 (所以 交换机, 以太网适配器(网卡) 都是工作在数据链路层) tips 阿里云的 SLB负载均衡(Server Load Balancer) 产品, 其文档中提到 阿里云当前提供 四层 和 七层 的负载均衡服务 四层采用开源软件LVS(Linux Virtual Server) + keepalived的方式实现负载均衡, 并根据云计算需求对其进行了个性化定制如果相应的负载均衡实例服务端口使用的是四层协议（TCP或UDP）, 那么LVS集群内每个节点都会根据负载均衡实例负载均衡策略, 将其承载的服务请求按策略直接分发到后端ECS服务器 七层采用Tengine实现负载均衡, Tengine是由淘宝网发起的Web服务器项目, 它在 Nginx 的基础上, 针对有大访问量的网站需求, 添加了很多高级功能和特性如果相应的负载均衡实例服务端口使用的是七层HTTP协议, 那么LVS集群内每个节点会先将其承载的服务请求均分到Tengine集群, Tengine集群内的每个节点再根据负载均衡策略, 将服务请求按策略最终分发到后端ECS服务器如果相应的负载均衡实例服务端口使用的是七层HTTPS协议, 与上述HTTP处理过程类似, 差别是在按策略将服务请求最终分发到后端ECS服务器前, 先调用Key Server进行证书验证及数据包加解密等前置操作 对于常听到的 二层交换机、三层交换机 这些词中所谓的 层, 也都是根据OSI参考模型的分层来说的第x层; 参考https://www.cnblogs.com/qishui/p/5428938.htmlhttps://blog.csdn.net/wdkirchhoff/article/details/43915825","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"","slug":"PHP/2018-03-10-mac-env","date":"2019-07-31T03:26:02.000Z","updated":"2019-07-31T03:26:02.000Z","comments":true,"path":"2019/07/31/PHP/2018-03-10-mac-env/","link":"","permalink":"http://blog.renyimin.com/2019/07/31/PHP/2018-03-10-mac-env/","excerpt":"","text":"http://laravel-china.github.io/php-the-right-way/","categories":[],"tags":[]},{"title":"","slug":"distributed-consensus/tcc","date":"2019-07-30T08:45:56.000Z","updated":"2019-07-30T08:45:56.000Z","comments":true,"path":"2019/07/30/distributed-consensus/tcc/","link":"","permalink":"http://blog.renyimin.com/2019/07/30/distributed-consensus/tcc/","excerpt":"","text":"https://www.cnblogs.com/soundcode/p/5590710.html","categories":[],"tags":[]},{"title":"","slug":"distributed-consensus/3pc","date":"2019-07-30T08:02:01.000Z","updated":"2019-07-30T08:02:01.000Z","comments":true,"path":"2019/07/30/distributed-consensus/3pc/","link":"","permalink":"http://blog.renyimin.com/2019/07/30/distributed-consensus/3pc/","excerpt":"","text":"https://github.com/benbjohnson/thesecretlivesofdata","categories":[],"tags":[]},{"title":"","slug":"distributed-consensus/2pc","date":"2019-07-30T08:02:01.000Z","updated":"2019-07-30T08:02:01.000Z","comments":true,"path":"2019/07/30/distributed-consensus/2pc/","link":"","permalink":"http://blog.renyimin.com/2019/07/30/distributed-consensus/2pc/","excerpt":"","text":"https://github.com/benbjohnson/thesecretlivesofdata","categories":[],"tags":[]},{"title":"","slug":"distributed-consensus/raft","date":"2019-07-30T08:02:01.000Z","updated":"2019-07-30T08:02:01.000Z","comments":true,"path":"2019/07/30/distributed-consensus/raft/","link":"","permalink":"http://blog.renyimin.com/2019/07/30/distributed-consensus/raft/","excerpt":"","text":"https://github.com/benbjohnson/thesecretlivesofdata","categories":[],"tags":[]},{"title":"","slug":"SpringBoot/2019-06-08-01","date":"2019-06-21T11:05:41.000Z","updated":"2019-08-22T09:09:05.000Z","comments":true,"path":"2019/06/21/SpringBoot/2019-06-08-01/","link":"","permalink":"http://blog.renyimin.com/2019/06/21/SpringBoot/2019-06-08-01/","excerpt":"","text":"","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SSM 整合","slug":"SpringMVC/2019-06-08-04","date":"2019-06-16T02:45:17.000Z","updated":"2019-08-15T04:15:58.000Z","comments":true,"path":"2019/06/16/SpringMVC/2019-06-08-04/","link":"","permalink":"http://blog.renyimin.com/2019/06/16/SpringMVC/2019-06-08-04/","excerpt":"","text":"SpringFramework (整合另外两个框架)SpringMVCMyBatis","categories":[{"name":"SSM","slug":"SSM","permalink":"http://blog.renyimin.com/categories/SSM/"}],"tags":[{"name":"SSM","slug":"SSM","permalink":"http://blog.renyimin.com/tags/SSM/"}]},{"title":"SpringBoot 整合 Redis","slug":"SpringBoot/2019-06-10-06","date":"2019-06-09T06:17:47.000Z","updated":"2019-08-28T06:29:55.000Z","comments":true,"path":"2019/06/09/SpringBoot/2019-06-10-06/","link":"","permalink":"http://blog.renyimin.com/2019/06/09/SpringBoot/2019-06-10-06/","excerpt":"","text":"添加redis的起步依赖 12345&lt;!--redis 的起步依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置redis的连接信息 注入RedisTemplate测试redis操作 demo 注入RedisTemplate测试redis操作","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SpringBoot 整合 Spring Data JPA","slug":"SpringBoot/2019-06-10-05","date":"2019-06-09T03:27:19.000Z","updated":"2019-08-28T05:51:05.000Z","comments":true,"path":"2019/06/09/SpringBoot/2019-06-10-05/","link":"","permalink":"http://blog.renyimin.com/2019/06/09/SpringBoot/2019-06-10-05/","excerpt":"","text":"Spring Data JPA 概述https://www.jianshu.com/p/c23c82a8fcfc 整合 添加Spring Data JPA的起步依赖 12345&lt;!--springBoot JPA的起步依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 添加数据库驱动依赖 12345&lt;!-- 添加数据库驱动坐标--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 在 application.yml 中配置数据库和jpa的相关属性 demo","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SpringBoot 整合 Junit","slug":"SpringBoot/2019-06-10-04","date":"2019-06-09T03:11:08.000Z","updated":"2019-08-28T03:21:16.000Z","comments":true,"path":"2019/06/09/SpringBoot/2019-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2019/06/09/SpringBoot/2019-06-10-04/","excerpt":"","text":"添加Junit的起步依赖 123456&lt;!--测试的起步依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; SpringRunner 继承自 SpringJUnit4ClassRunner，使用哪一个Spring提供的测试引擎都可以; @SpringBootTest 的属性指定的是引导类的字节码对象 Demo","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SpringBoot 集成 Mybatis","slug":"SpringBoot/2019-06-08-03","date":"2019-06-09T03:02:19.000Z","updated":"2019-08-28T03:10:45.000Z","comments":true,"path":"2019/06/09/SpringBoot/2019-06-08-03/","link":"","permalink":"http://blog.renyimin.com/2019/06/09/SpringBoot/2019-06-08-03/","excerpt":"","text":"添加 Mybatis 的起步依赖 123456&lt;!-- 添加 Mybatis 的起步依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 添加数据库驱动坐标 (有些包是不需要指定版本的) 12345&lt;!-- 添加数据库驱动坐标 (不需要指定版本, 本配置文件的parent的parent中已经指定了)--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 在 application.yml 中添加数据库的连接信息 123456spring: datasource: driverClassName: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 username: root password: 123456 在 application.yml 中添加mybatis的信息 123456#spring集成Mybatis环境#pojo别名扫描包mybatis: type-aliases-package: com.spring.study.dto mapper-locations: classpath: mapper/*Mapper.xml 编写测试Controller 123456789101112@Controllerpublic class UserController &#123; @Autowired private UserMapper userMapper; @RequestMapping(&quot;/queryUser&quot;) @ResponseBody public Object queryUser() &#123; List&lt;UserDTO&gt; users = userMapper.queryUserList(); return users; &#125;&#125; 启动后, 测试访问 http://localhost:8888/demo/queryUser","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SpringBoot 起步","slug":"SpringBoot/2019-06-08-02","date":"2019-06-08T09:09:53.000Z","updated":"2019-08-23T07:37:41.000Z","comments":true,"path":"2019/06/08/SpringBoot/2019-06-08-02/","link":"","permalink":"http://blog.renyimin.com/2019/06/08/SpringBoot/2019-06-08-02/","excerpt":"","text":"环境搭建 创建maven工程使用idea工具创建一个maven工程, 该工程为普通的java工程即可 添加SpringBoot的起步依赖 SpringBoot要求, 项目要继承SpringBoot的起步依赖 spring-boot-starter-parentspring-boot-starter-parent 是一个特殊的starter, 它用来提供相关的Maven默认依赖。使用它之后, 常用的包依赖可以省去version标签 SpringBoot要集成SpringMVC进行Controller的开发, 所以项目要导入web的启动依赖 编写SpringBoot引导类 编写Controller在引导类MySpringBootApplication同级包或者子级包中创建QuickStartController 如下:(可以看到, 整个过程并没有配置tomcat, 但启动引导类的main方法后（一般都把main方法放在引导类中）, 可以看到tomcat监听了8080端口, 英文springBoot已经内嵌了tomcat服务器) 热部署 (一般也不用) 热部署: 修改源码后会自动重新部署, 我们可以在修改代码后不重启就能生效 在 pom.xml 中添加如下配置就可以实现这样的功能 123&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; Intellij IEDA 默认情况下不会自动编译, 因此还需要对IDEA进行自动编译的设置, 如下: IDEA快速创建SpringBoot项目下面注意, 此处由于只是创建web工程, 所以只勾选了web最终, 会帮我们生成所需的配置及启动类: 起步依赖原理 分析 spring-boot-starter-parent 按住Ctrl点击pom.xml中的spring-boot-starter-parent, 跳转到了 spring-boot-starter-parent 的 pom.xml 从上面可以看到 spring-boot-starter-parent 又继承了 spring-boot-dependencies, 从 spring-boot-starter-dependencies 的pom.xml中还可以发现, 一部分坐标的版本、依赖管理、插件管理已经定义好, 所以我们的SpringBoot工程继承spring-boot-starter-parent后已经具备版本锁定等配置了, 所以 起步依赖的作用就是进行依赖的传递依赖包的版本不用我们自己去找了, SpringBoot的版本定了之后, 很多包的版本就自动帮我们锁定了最合适的 分析 spring-boot-starter-web 按住Ctrl点击pom.xml中的spring-boot-starter-web, 跳转到了 spring-boot-starter-web 的pom.xml 从上面的spring-boot-starter-web的pom.xml中我们可以发现, spring-boot-starter-web就是将web开发要使用的 spring-web、spring-webmvc等坐标进行了“打包”, 这样我们的工程只要引入spring-boot-starter-web起步依赖的 坐标就可以进行web开发了, 同样体现了依赖传递的作用 SpringBoot配置文件及类型 SpringBoot是基于约定的, 所以很多配置都有默认值, 但如果想使用自己的配置替换默认配置的话, 就可以使用 application*.properties 或者 application*.yml 、application*.yaml 进行配置 SpringBoot默认会从 Resources目录 下加载 application*.properties 或者 application*.yml 、application*.yaml 文件 application.properties文件是键值对类型的文件 除了properties文件外, SpringBoot还可以使用yml文件进行配置, 下面对yml文件进行讲解 yml 配置文件简介 YAML是一种直观的能够被电脑识别的的数据数据序列化格式, 并且容易被人类阅读, 容易和脚本语言交互的, 可以被支持YAML库的不同的编程语言程序导入, 比如: C/C++, Ruby, Python, Java, Perl, C#, PHP等 YML文件是以数据为核心的, 比传统的xml方式更加简洁 YML文件的扩展名可以使用 .yml 或者 .yaml yml配置文件的语法 配置普通数据, 语法: key: value (注意: value 之前有一个空格) 配置对象数据, 语法: 123key: key1: value1 key2: value2 (注意: key1前面的空格个数不限定, 在yml语法中, 相同缩进代表同一个级别) 或者: key: {key1: value1,key2: value2} 配置Map数据 (同上面的对象写法) 配置数组(List、Set)数据, 语法: 12345city: - beijing - tianjin - shanghai - chongqing 或者: city: [beijing,tianjin,shanghai,chongqing] 集合中的元素是对象形式 student: 123456789student: - name: zhangsan age: 18 score: 100 - name: lisi age: 28 score: 88 - name: wangwu age: 38 score: 90 (注意: value 与 - 之间也存在一个空格) 上面提及过, SpringBoot配置文件的主要目的就是对配置信息进行修改的, 但在配置时的key从哪里去查询呢?我们可以查阅SpringBoot的官方文档 简单配置: 配置文件与配置类的属性映射@Value 注解可以通过 @Value 注解将配置文件中的值映射到一个Spring管理的Bean的字段上 @ConfigurationProperties 注解使用注解 @ConfigurationProperties 映射 通过注解 @ConfigurationProperties(prefix=&quot;配置文件中的key的前缀&quot;) 可以将配置文件中的配置自动与实体进行映射 注意: 使用 @ConfigurationProperties 方式可以进行配置文件与实体字段的自动映射, 但需要字段必须提供 set方法 才可以, 而使用 @Value 注解修饰的字段不需要提供set方法 上述有个警告, 可以引入包(消除上述警告, 并且可以在yml中配置你的bean中的属性时, 会自动提示(貌似没试成功))12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://blog.renyimin.com/tags/SpringBoot/"}]},{"title":"SSM 整合","slug":"SSM/2019-06-16-01","date":"2019-06-08T02:45:17.000Z","updated":"2019-08-14T11:06:20.000Z","comments":true,"path":"2019/06/08/SSM/2019-06-16-01/","link":"","permalink":"http://blog.renyimin.com/2019/06/08/SSM/2019-06-16-01/","excerpt":"","text":"","categories":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/categories/SpringMVC/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/tags/SpringMVC/"}]},{"title":"","slug":"SpringMVC/2019-06-07-03","date":"2019-06-07T09:20:09.000Z","updated":"2019-08-15T04:12:17.000Z","comments":true,"path":"2019/06/07/SpringMVC/2019-06-07-03/","link":"","permalink":"http://blog.renyimin.com/2019/06/07/SpringMVC/2019-06-07-03/","excerpt":"","text":"forward 和 redirect 进行页面跳转不能使用视图解析器, 需要自己写路径 @ResponseBody文件上传异常处理及拦截器","categories":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/categories/SpringMVC/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/tags/SpringMVC/"}]},{"title":"SpringMVC 常用注解","slug":"SpringMVC/2019-06-07-02","date":"2019-06-07T07:18:35.000Z","updated":"2019-08-14T11:06:04.000Z","comments":true,"path":"2019/06/07/SpringMVC/2019-06-07-02/","link":"","permalink":"http://blog.renyimin.com/2019/06/07/SpringMVC/2019-06-07-02/","excerpt":"","text":"@RequestParam之前已经尝试过, 当你表单中的参数名与控制器方法中的形参名不一致时, 控制器的方法是获取不到参数值的, 此时就可以使用 @RequestParam 注解常用属性: name 、value 用来指明传入的参数的名称 required 默认为true, 表示必传参数, 可以设置为 false @RequestBody用于获取请求体内容, 直接使用会得到 key=value&amp;key=value... 结构的数据 (注意: get 请求方式不适合使用, get没有请求体)常用属性: required 默认为true, 表示必传参数 (当值为true, get请求会报错, 值为false, get请求得到的是null) @PathVariable用于绑定url中的占位符, 例如url中的 /delete/{id}, 这个id就是url占位符 (url支持占位符是spring3.0之后加入的, 是springmvc支持rest风格url的一个重要标志)属性: value: 用于指定url中占位符名称 requested: 是否必须提供占位符 @HiddentHttpMethodFilter由于浏览器 form 表单只支持 GET 与 POST 请求, 而DELETE、PUT 等 method 并不支持, Spring3.0 添 加了一个过滤器, 可以将浏览器请求改为指定的请求方式, 发送给我们的控制器方法, 使得支持 GET、POST、PUT、DELETE 请求使用方法:第一步: 在 web.xml 中配置该过滤器第二步: 请求方式必须使用 post 请求第三步: 按照要求提供 _method 请求参数, 该参数的取值就是我们需要的请求方式 (一般不用) @RequestHeader用于获取请求消息头属性: value: 提供消息头名称 required: 是否必须有此消息头 在实际开发中一般不怎么用 @CookieValue用于把指定 cookie 名称的值传入控制器方法参数属性: value: 指定 cookie 的名称 required: 是否必须有此 cookie 1234public String useCookieValue(@CookieValue(value=&quot;JSESSIONID&quot;,required=false) String cookieValue)&#123; System.out.println(cookieValue); return &quot;success&quot;; &#125; @ModelAttribute该注解是 SpringMVC 4.3 版本以后新加入的, 它可以用于修饰 方法 和 参数 用于 方法上, 表示当前方法会在控制器的方法执行之前, 先执行 (它可以修饰没有返回值的方法, 也可 以修饰有具体返回值的方法) 用于 参数上, 获取指定的数据给参数赋值 属性: value: 用于获取数据的 key (key 可以是 POJO 的属性名称, 也可以是 map 结构的 key)应用场景: 当表单提交数据不是完整的实体类数据时, 保证没有提交数据的字段使用数据库对象原来的数据例如: 我们在编辑一个用户时, 用户有一个创建信息字段, 该字段的值是不允许被修改的, 在提交表单数据是肯定没有此字段的内容, 一旦更新会把该字段内容置为 null, 此时就可以使用此注解解决问题 @SessionAttributes用于多次执行控制器方法间的参数共享属性: value: 用于指定存入的属性名称 type: 用于指定存入的数据类型","categories":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/categories/SpringMVC/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/tags/SpringMVC/"}]},{"title":"SpringMVC 入门","slug":"SpringMVC/2019-06-02-01","date":"2019-06-02T07:20:11.000Z","updated":"2019-08-14T08:52:01.000Z","comments":true,"path":"2019/06/02/SpringMVC/2019-06-02-01/","link":"","permalink":"http://blog.renyimin.com/2019/06/02/SpringMVC/2019-06-02-01/","excerpt":"","text":"基于maven搭建环境 前端控制器的配置、服务器的部署 入门 配置服务器 在 web.xml 中配置 最前端的控制器(第一个servlet: org.springframework.web.servlet.DispatcherServlet), 并设置 &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; 服务器启动后即会创建该Servlet, 该 servlet 就像一个指挥中心 紧接着配置该 Servlet 的 init-param 标签, 初始化即会去加载我们自定义的配置文件 springmvc.xml 如下 执行流程各组件分析 DispatcherServlet: 前端控制器 相当于 mvc 模式中的 c, dispatcherServlet 是整个流程控制的中心, 由它调用其它组件处理用户的请求, dispatcherServlet 的存在降低了组件之间的耦合 HandlerMapping: 处理器映射器 HandlerMapping 负责根据用户请求找到 Handler 即处理器, SpringMVC 提供了不同的映射器实现不同的映射方式, 例如: 配置文件方式, 实现接口方式, 注解方式等 Handler: 处理器 它就是我们开发中要编写的具体业务控制器, 由 DispatcherServlet 把用户请求转发到 Handler。由 Handler 对具体的用户请求进行处理。 HandlAdapter: 处理器适配器 通过 HandlerAdapter 对处理器进行执行, 这是适配器模式的应用, 通过扩展适配器可以对更多类型的处理 View Resolver: 视图解析器 View Resolver 负责将处理结果生成 View 视图, View Resolver 首先根据逻辑视图名解析成物理视图名, 即具体的页面地址, 再生成 View 视图对象, 最后对 View 进行渲染将处理结果通过页面展示给用户 View: 视图 SpringMVC 框架提供了很多的 View 视图类型的支持, 包括: jstlView、freemarkerView、pdfView 等。我们最常用的视图就是 jsp 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户, 需要由程序员根据业务需求开发具体的页面 &lt;mvc:annotation-driven&gt; 说明 在 SpringMVC 的各个组件中, 处理器映射器、处理器适配器、视图解析器 称为 SpringMVC 的三大组件 使用 &lt;mvc:annotation-driven&gt; 自动加载 RequestMappingHandlerMapping(处理映射器) 和 RequestMappingHandlerAdapter (处理适配器), 可用在 SpringMVC.xml 配 置 文 件 中 使 用 &lt;mvc:annotation-driven&gt; 替代注解处理器和适配器的配置 @RequestMapping 作用: 用于建立 请求URL 和 处理请求方法 之间的对应关系 该注解可以用于 方法、类 上 作用于类上: 可以设置请求 URL 的第一级访问目录, 此处不写的话, 就相当于应用的根目录写的话需要以 / 开头, 它出现的目的是为了使我们的 URL 可以按照模块化管理 作用与方法上: 则为设置请求 URL 的第二级访问目录 属性: value: 用于指定请求的 URL, 它和 path 属性的作用是一样的 method: 用于指定请求的方式 params: 用于指定限制请求参数的条件, 它支持简单的表达式, 要求请求参数的 key 和 value 必须和配置的一模一样 123例如:params = &#123;&quot;accountName&quot;&#125;, 表示请求参数必须有 accountNameparams = &#123;&quot;moeny!100&quot;&#125;, 表示请求参数中money不能是100 headers: 用于指定限制请求消息头的条件 注意: 以上四个属性只要出现 2 个或以上时, 他们的关系是与的关系 请求参数绑定 当参数比较多时, 一般会考虑把参数封装到javabean中 请求参数中包含引用类型 通过配置过滤器, 解决表单中参数乱码的问题 请求参数绑定集合类型 自定义类型转换器 表单提交的都是String类型, SpringMVC中是有自动类型转换的, 所以之前的例子中, javabean的属性都有自己的类型, 但不会报错 但是需要注意的是, 比如传递的是字符串 2019/02/10, 可能会自动转换为Date类型不会报错 但如果你传递的是 2019-02-10 可能就会报错了 自定义类型转换器 获取Servlet原生API","categories":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/categories/SpringMVC/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://blog.renyimin.com/tags/SpringMVC/"}]},{"title":"String常用操作 (StringUtils)","slug":"SpringFramework5/2019-05-27-15","date":"2019-05-27T11:20:11.000Z","updated":"2019-09-03T03:52:53.000Z","comments":true,"path":"2019/05/27/SpringFramework5/2019-05-27-15/","link":"","permalink":"http://blog.renyimin.com/2019/05/27/SpringFramework5/2019-05-27-15/","excerpt":"","text":"StringUtils类中的方法其实真的还是很多, 可能平时我们用的比较多的还是一些普通的方法, 其实类似文件路径, 文件名等相关操作, 以前还会专门引入common-io的FilenameUtils等额外的工具类, 原来在StringUtils中都有, 而且根据其设计的这些方法, 我们也能大概的猜出一些方法在Spring中哪些地方可能有用；最后, 其中有些方法, 还是非常常见的面试题, 比如替换字符串, 查询子串个数等, 有兴趣也可以看看Spring的具体实现。 org.apache.commons.lang3.StringUtils参考https://www.jianshu.com/p/da7d986cf19e","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"","slug":"SpringFramework5/2019-05-27-16","date":"2019-05-27T10:23:39.000Z","updated":"2019-09-03T06:18:25.000Z","comments":true,"path":"2019/05/27/SpringFramework5/2019-05-27-16/","link":"","permalink":"http://blog.renyimin.com/2019/05/27/SpringFramework5/2019-05-27-16/","excerpt":"","text":"","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"","slug":"SpringFramework5/2019-05-27-17","date":"2019-05-27T10:23:39.000Z","updated":"2019-09-03T06:22:12.000Z","comments":true,"path":"2019/05/27/SpringFramework5/2019-05-27-17/","link":"","permalink":"http://blog.renyimin.com/2019/05/27/SpringFramework5/2019-05-27-17/","excerpt":"","text":"","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"Spring 事务控制","slug":"SpringFramework5/2019-05-25-11","date":"2019-05-25T07:20:11.000Z","updated":"2019-08-12T09:45:19.000Z","comments":true,"path":"2019/05/25/SpringFramework5/2019-05-25-11/","link":"","permalink":"http://blog.renyimin.com/2019/05/25/SpringFramework5/2019-05-25-11/","excerpt":"","text":"注解AOP的问题AOP 的 后置通知 和 最终通知 在注解配置时, 其执行顺序是有问题, 会导致事务出现问题, 先释放了事务, 然后再提交或者回滚, 是有问题的; 可以自己写环绕通知来解决 声明式事务 Spring为我们提供了一组事务控制的API (即在 spring-tx 包中)","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"JdbcTemplate","slug":"SpringFramework5/2019-05-19-10","date":"2019-05-19T12:19:36.000Z","updated":"2019-08-12T09:15:56.000Z","comments":true,"path":"2019/05/19/SpringFramework5/2019-05-19-10/","link":"","permalink":"http://blog.renyimin.com/2019/05/19/SpringFramework5/2019-05-19-10/","excerpt":"","text":"概述 JdbcTemplate 是 spring 框架中提供的一个对象, 是对原始 Jdbc API 对象的简单封装 Spring 框架为我们提供了很多 操作模板类 操作关系型数据的: JdbcTemplate、HibernateTemplate 操作 nosql 数据库的: RedisTemplate 操作消息队列的: JmsTemplate 在导包的时候，除了要导入这个 spring-jdbc-5.1.4.RELEASE.jar 包外, 还需要导入一个 spring-tx-5.1.4.RELEASE.jar (它是和事务相关的) 下图仅供参考 JdbcTemplate JdbcTemplate 最基本的使用 基于 ioc, JdbcTemplate 最基本的使用 JdbcTemplate 基本的 crud 操作 JdbcTemplate 在Dao中的使用 JdbcDaoSupport 使用, 解决各个 Dao 中的重复代码","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"AOP (Aspect Oriented Programming) 面向切面编程","slug":"SpringFramework5/2019-05-18-09","date":"2019-05-18T09:57:29.000Z","updated":"2019-08-30T03:49:37.000Z","comments":true,"path":"2019/05/18/SpringFramework5/2019-05-18-09/","link":"","permalink":"http://blog.renyimin.com/2019/05/18/SpringFramework5/2019-05-18-09/","excerpt":"","text":"概述 AOP 是使用 动态代理的方式 实现的 (在程序运行期间, 不用去修改源码即可对原有方法进行增强, 满足 开闭原则 ) Spring中的AOP 是通过配置来实现上一篇中的动态代理功能的 (Spring同样提供了 基于xml配置 和 基于注解 两种方式来实现AOP) Spring的AOP可以手动选择动态代理是 基于接口的 还是 基于子类的 术语 Join point 连接点: 在Spring中指的就是 委托类中的所有方法 Point cut 切入点: 代理类在 invoke 中还可以决定对 委托类 的哪些方法进行增强, 被增强的连接点就是切入点 (切入点一定是连接点, 连接点不一定是切入点) Advice 通知: 即拦截到连接点时要做的事情, 通知的类型分为 前置通知, 后置通知, 异常通知 (在 catch 块中的), 最终通知 (在 finally 块中的), 环绕通知 … 基于XML的AOP项目准备 Demo 切入点表达式写法（后置通知和异常通知只会执行一个, 最终通知无论有没有异常都会执行） 环绕通知除了通过配置的方式来实现对方法的增强, 还可以使用Spring为我们提供的 环绕通知 基于注解的AOP下面仍包含了少量的xml配置 基于注解AOP的问题发现通过注解的方式使用AOP时, 后置通知的顺序有问题, 跑到了最终通知的后面 基于注解环绕通知此时可以通过 环绕通知 解决上述问题 纯注解AOP","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"静态代理, 动态代理","slug":"SpringFramework5/2019-05-18-08","date":"2019-05-18T09:36:13.000Z","updated":"2019-08-30T03:44:22.000Z","comments":true,"path":"2019/05/18/SpringFramework5/2019-05-18-08/","link":"","permalink":"http://blog.renyimin.com/2019/05/18/SpringFramework5/2019-05-18-08/","excerpt":"","text":"代理 Proxy Proxy代理模式 是一种 结构型 设计模式, 是一种常用的设计模式, 主要解决的问题是: 在直接访问对象时所带来的问题 其目的就是为某个对象提供一个代理, 以控制对该对象的访问, 代理类负责为委托类 预处理消息、过滤消息并转发消息、以及进行 被委托类执行后的 后续处理 为了保持行为的一致性, 代理类 和 委托类 通常会实现相同的接口, 所以在访问者看来两者没有丝毫的区别; 通过代理类这中间一层, 能有效控制对委托类对象的直接访问, 也可以很好地隐藏和保护委托类对象, 同时也为实施不同控制策略预留了空间, 从而在设计上获得了更大的灵活性 更通俗的说, 当两个类需要通信时, 引入第三方代理类, 将两个类的关系解耦, 让我们只了解代理类即可, 而且代理的出现还可以让我们完成与另一个类之间的关系的统一管理, 但是切记, 代理类和委托类要实现相同的接口, 因为代理真正调用的还是委托类的方法 按照代理的创建时期, 代理类可以分为两种: 静态: 由程序员创建代理类或特定工具自动生成源代码再对其编译, 在程序运行前代理类的.class文件就已经存在了 动态: 在程序运行时运用反射机制动态创建而成 下面分别用静态代理与动态代理演示一个示例: 添加打印日志的功能, 即每个方法调用之前和调用之后写入日志 静态代理DEMO 静态代理优点通过 代理 使 客户端 不需要知道实现类是什么, 怎么做的, 而客户端只需知道代理即可(解耦合), 对于如上的客户端代码, newUserManagerImpl() 可以应用 工厂 或 IOC 将它隐藏, 如上只是举个例子而已 静态代理缺点 代理类 和 委托类 实现了相同的接口, 代理类 和 委托类 也都会实现相同的方法, 这样就出现了大量的代码重复, 如果接口增加一个方法, 除了所有实现类需要实现这个方法外, 所有代理类也需要实现此方法, 增加了代码维护的复杂度 代理对象只服务于一种类型的对象, 如果要服务多类型的对象, 势必要为每一种对象都进行代理, 静态代理在程序规模稍大时就无法胜任了如上的代码是只为UserManager类的访问提供了代理, 但是如果还要为其他类如Department类提供代理的话, 就需要我们再次添加代理Department的代理类 即静态代理类只能为特定的接口(Service)服务, 如想要为多个接口服务则需要建立很多个代理类 静态代理小结代理可以对实现类进行统一的管理, 如在调用具体实现类之前, 需要打印日志等信息, 这样我们只需要添加一个代理类, 在代理类中添加打印日志的功能, 然后调用实现类, 这样就避免了修改具体实现类, 满足开闭原则; 但是如果想让不同类型的其他实现类都添加打印日志的功能的话, 就需要添加多个代理类, 并且代理类中各个方法都需要添加打印日志功能(如上的代理方法中删除, 修改, 以及查询都需要添加上打印日志的功能) 动态代理 (基于接口)根据如上的介绍, 你会发现每个代理类只能为一个接口服务, 这样程序开发中必然会产生许多的代理类, 所以我们需要的是 通过一个代理类完成全部的代理功能, 那么我们就需要用动态代理 在上面的示例中, 一个代理只能代理一种类型, 而且是在编译期就已经确定被代理的对象, 而动态代理是在运行时 通过反射机制实现动态代理, 并且能够代理各种类型的对象 在Java中要想实现 动态代理 机制, 需要 java.lang.reflect.InvocationHandler接口 和 java.lang.reflect.Proxy类 的支持 可以看到, 我们可以通过 LogHandler 代理不同类型的对象, 如果我们把对外的接口都通过动态代理来实现, 那么所有的函数调用最终都会经过 invoke函数 的转发, 因此我们就可以在这里做一些自己想做的操作, 比如日志系统、事务、拦截器、权限控制等, 这也就是AOP(面向切面编程)的基本原理 小结: 动态代理与静态代理相比较, 最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理（InvocationHandler.invoke）这样, 在接口方法数量比较多的时候, 我们可以进行灵活处理, 而不需要像静态代理那样每一个方法进行中转;而且动态代理的应用使我们的类职责更加单一, 复用性更强 动态代理(基于子类)基于接口的动态代理, 委托类必须实现接口 (上面已经演示过了), 下面看基于子类的动态代理 提供者：第三方cglib库 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.5&lt;/version&gt;&lt;/dependency&gt; 涉及的类：Enhancer 使用 Enhancer 类中的 create 方法创建代理对象 创建代理对象的要求: 被代理类不能是最终类(不能被 final 修饰) Demo Demo 转载https://blog.csdn.net/hejingyuan6/article/details/36203505","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"Spring 整合 Junit","slug":"SpringFramework5/2019-05-14-07","date":"2019-05-14T08:47:31.000Z","updated":"2019-08-30T03:32:28.000Z","comments":true,"path":"2019/05/14/SpringFramework5/2019-05-14-07/","link":"","permalink":"http://blog.renyimin.com/2019/05/14/SpringFramework5/2019-05-14-07/","excerpt":"","text":"使用Junit进行之前的测试如下, 直接加上对junit的依赖即可 问题出现 在测试类中, 每个测试方法都会有以下代码: 12// ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); ApplicationContext ac = new AnnotationConfigApplicationContext(SpringConfiguration.class); 针对上述问题, 我们需要的是程序能自动帮我们创建容器 而 junit 是无法实现的, 它甚至无法知晓我们是否使用了 spring 框架, 更不用说帮我们创建 spring 容器了 不过, junit 给我们暴露了一个注解, 可以让我们替换掉它的运行器, 这时, 我们需要依靠 spring 框架, 因为它提供了一个运行器, 可以读取配置文件(或注解)来创建容器, 我们只需要告诉它配置文件在哪就行了 Spring 整合 Junit1.使用 @RunWith 注解替换原有运行器 2.使用 @ContextConfiguration 注解 指定 spring 配置文件的位置locations属性: 用于指定xml配置文件的位置(如果是类路径下, 需要用 classpath: 表明)classes属性: 用于指定注解的类 (当不使用 xml 配置时, 需要用此属性指定注解类的位置) 3.使用 @Autowired 给测试类中的变量注入数据 demo 转载https://blog.csdn.net/hejingyuan6/article/details/36203505","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"基于注解的IOC配置","slug":"SpringFramework5/2019-05-12-05","date":"2019-05-12T11:36:11.000Z","updated":"2019-08-30T02:53:00.000Z","comments":true,"path":"2019/05/12/SpringFramework5/2019-05-12-05/","link":"","permalink":"http://blog.renyimin.com/2019/05/12/SpringFramework5/2019-05-12-05/","excerpt":"","text":"Spring2.0 开始引入基于注解的配置方式, 即 Bean的定义信息可以通过在 Bean的实现类上 标注注解来实现 注解配置和 xml 配置要实现的功能都是一样的, 都是要降低程序间的耦合, 只是配置的形式不一样, 关于实际的开发中到底使用 xml 还是注解, 每家公司有着不同的使用习惯 (基于注解的配置需要 aop jar 包) 创建 spring 的 xml 配置文件并开启对注解的支持 (基于注解整合时, 导入约束时需要多导入 context 名称空间下的约束) 下面开始介绍 Spring IOC 的常用注解 用于创建Bean的注解作用和之前学的使用xml实现ioc时, 配置一个 &lt;bean id=&quot;accountService&quot; class=&quot;com.spring.study.service.impl.AccountServiceImpl&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 是类似的 @Component该注解用于把当前类对象存入Spring容器中属性 value: 指定 bean 的 id (如果不指定 value 属性, 默认 bean 的 id 是当前类的类名, 首字母小写) tips: 要知道, 只在类上加了 @Component 并不能完成 “将类对象存入Spring容器因为我们目前仍然是通过 new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); 来创建容器的, 因此 bean.xml 需要知道你在哪些地方使用了 @Component所以会用到新的xml标签 &lt;context:component-scan base-package=&quot;要扫描的包&quot;&gt;&lt;/context:component-scan&gt;, 用来指定将要扫描的注解的位置 另外几个注解@Controller、 @Service、 @Repository, 他们三个注解都是针对 @Component 的衍生注解, 作用及属性都一样, 只不过是提供了更加明确的语义化 @Controller: 一般用于表现层的注解 @Service: 一般用于业务层的注解 @Repository: 一般用于持久层的注解 小结相比 使用xml配置来创建bean, xml + annotation 的方式只用在xml中配置很少的内容, 然后对于需要被Spring容器管理的对象, 在其类上加上以上注解即可（但此时仍然需要在xml中进行配置, 为了说明你使用注解的地方） 用于 注入依赖bean 的注解作用和之前学的使用xml实现 ioc 时, 在 &lt;bean&gt; 下通过 &lt;property name=&quot;&quot; value=&quot;&quot;&gt;&lt;/property&gt; 或 &lt;property name=&quot;&quot; ref=&quot;&quot;&gt;&lt;/property&gt; 是类似的 @Autowired其作用是自动按照类型注入, 只要容器中有唯一的一个bean对象的类型 和 @Autowired 所修饰的要注入的变量的类型一致, 就可以注入成功; 该注解可以修饰 成员变量、方法 …… 如果容器中没有任何对象的类型与当前变量类型匹配, 则注入失败会报错 如果容器中有多个对象(的类型与当前变量类型匹配时, 会继续使用当前变量名称作为 bean的id进行匹配, 如果id都不匹配则报错) tip: 值得注意的是, 在 AccountServiceImpl 中注入 AccountDaoImpl 时, 并没有使用之前xml配置ioc时的三种方式 (并没有 setter), 这是肿么回事儿?@Autowired 方式是通过万能而无节操的反射设置属性值的…. （有空可以看看源码） @Qualifier相比于 @Autowired, @Qualifier 会同时按照 类型 和 bean的id 进行匹配属性 value : 用于指定注入bean的id @Qualifier 略微闹心, 在修饰类成员变量时, 不能单独使用, 还得使用 @Autowired tips: 它在给类成员注入时不能单独使用, 但是在给方法参数注入时可以单独使用 (如: public QueryRunner createQueryRunner(@Qualifier(&quot;ds2&quot;) DataSource dataSource){, 指明当Spring容器中有多个类型都为 DataSource 的bean对象时, 此处取name为ds2的) @Resource相比之前两种, @Resource 可以直接按照bean的id进行注入, 并且可以单独使用 但是它的属性是 name, 用来指明bean的id 基本类型 的注入 以上注解都只能注入其他bean类型的数据, 而 基本类型 和 String类型 无法使用上述注解实现 另外, 集合类型的注入只能使用 xml 配置的方式进行注入 @Value其作用用于注入 基本类型 和 String类型 的数据 属性value用于指定数据的值, 同时它还可以使用 Spring 中的 SpEL表达式 (el表达式的写法 ${表达式}) 用于改变作用范围的注解@Scope作用 和 之前学的使用xml实现ioc时, 在 &lt;bean&gt; 中使用 scope 属性是类似的 属性value: 用于指定范围, 取值为 (singleton: 默认值, 单例的、prototype: 多例的 、…) 可以用在类上(当类使用了创建bean的注解时,可以一起用), 也可以用在返回bean对象的方法上(当方法上使用了 @Bean 将bean交给Spring容器进行管理时, 可以一起使用) 用于生命周期的注解作用和之前学的使用xml实现ioc时, 在 &lt;bean&gt; 中使用 init-method、destroy-method 属性是类似的用于方法上, 指定方法为 初始化方法 或者 销毁方法 (如果是多例对象的话, 调用销毁方法没效, 因为多例的销毁是JVM进行的) @PostConstruct指定初始化方法 @PreDestroy指定销毁方法 xml + 注解 的IOC到目前为止我们仍未脱离 xml配置: 对于上面的示例中的操作, 相比 使用xml配置来 创建bean 或者 依赖注入, xml+annotation 的方式只用在xml中配置很少的内容（为了说明你使用注解的地方）, 然后对于需要被Spring容器管理的对象, 在其类上加上以上注解即可 几个问题 注解配置Ioc的问题： 试想一下, 一旦当我们所依赖的对象是其他 jar 包中的类时, 我们是无法使用 注解 方法完成bean的创建 和 注入 的, 因为我们无法在别人jar包的代码中加注解 如何将 xml配置 直接取消, 完全使用纯注解进行 IOC 配置呢 ? 纯注解配置IOC Spring的几个新注解在纯注解配置IOC时, 如果要对配置类进行拆分, 比如 公共配置类, 和多个专项配置类, 如果拆开的话, 由于主配置类文件中的扫描路径不包含 config路径, 所以会报错, 可以如下:或者 如果不希望每个配置类都配置 @Configuration, 并且在主配置类中还要修改 @ComponentScan 设置多个路径或者也不希望在 AnnotationConfigApplicationContext 中传递多个配置类该怎么办? @Import用于导入其他配置类; 属性value, 用于指定其他配置类的字节码 @PropertySource用于指定 properties 文件的路径, 属性value用来指定文件的路径 （value中的关键字classpath:表示类路径下）当使用 @Configuration 修饰的配置类, 并通过在其中的方法上使用 @Bean 来将bean对象交给Spring容器管理时, 比如在操作数据库时, 由于需要设置一些datasource的数据参数, 这些参数就写死到了 方法中 此时就需要使用 @PropertySource 并且配合 @Value来将这些基本类型进行注入 关于 Spring 注解和 XML 的选择问题 注解的优势: 配置简单, 维护方便 (我们找到类, 就相当于找到了对应的配置) XML 的优势: 修改时, 不用改源码。不涉及重新编译和部署","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"基于 XML 的 IOC 配置","slug":"SpringFramework5/2019-05-12-04","date":"2019-05-12T07:07:48.000Z","updated":"2019-08-29T04:01:20.000Z","comments":true,"path":"2019/05/12/SpringFramework5/2019-05-12-04/","link":"","permalink":"http://blog.renyimin.com/2019/05/12/SpringFramework5/2019-05-12-04/","excerpt":"","text":"认识 Spring 的配置文件 Spring的配置文件 其实就是Spring容器对Bean进行生产以及关系注入的图纸, 它是Spring的基础, 如果没有配置文件的话, 则 Spring 的容器将无从谈起Spring 的配置文件 是用于指导 Spring 工厂进行 Bean 的生产、依赖关系注入及 Bean 实例分发的”图纸”, 我们必须学会并灵活应用这份”图纸”, 准确地表达自己的”生产意图”, 它是一个或多个标准的XML文档 (ApplicationContext.xml 是Spring的默认配置文件, 当容器启动时找不到其他的配置文件时, 则会尝试加载这个默认的配置文件) Spring启动时会读取 配置文件 中提供的Bean的配置信息, 并在Spring容器中生成一份相应的Bean的配置注册表, 然后根据这张注册表来实例化Bean, 装配好Bean之间的依赖关系, 为上层应用提供准备就绪的运行环境 bean的配置信息就是Bean的元数据信息, 由以下五个方面来组成: Bean的一些属性信息: 比如 数据源的连接数, 用户名和密码等等 Bean的依赖关系: Spring根据依赖关系配置完成Bean之间的装配 Bean的行为配置: 比如, 生命周期范围以及生命周期各个过程的回调函数等 Bean的创建方式定义: 主要说明是通过构造器还是工厂方法来构造Bean 有时, 一个项目中可能存在多个配置文件, 那么Spring项目加载多个配置文件的方法如下: 在配置文件中使用 import 来导入所需的配置文件 将多个配置文件构造为一个数组, 然后传递给 ApplicationContext 这两种方式都是通过调用 BeanDefinitionReader 来读取定义文件的, 在内部实现上没有任何的区别 在大型的Spring项目当中, 所有的bean配置在一个配置文件当中很不容易管理且也不利于团队的开发; 因此, 通常在开发过程当中, 我们会按照 功能模块和开发人员 来将配置文件分成多个, 这样会有利与模块的划分, 接下来我们需要使用 import 属性来引入多个配置文件到项目当中 xml 配置 ioc在项目根目录下创建任意名称的 xml 文件 (此处为 bean.xml): 给配置文件导入约束 (查看文档/spring-framework-5.1.4.RELEASE/docs/spring-framework-reference/core.html)如下就做好了准备工作 配置 &lt;bean&gt; 标签 其作用就是配置对象以让 spring 进行创建, 默认情况下它调用的是类中的无参构造函数, 如果没有无参构造函数则不能创建成功 属性: id: 给对象在容器中提供一个唯一标识, 用于获取对象 class: 指定类的全类名, 用于反射创建对象 (默认情况下调用无参构造函数) scope: 指定对象的作用范围 12345singleton: 默认值, 单例的prototype: 多例的request: WEB项目中, Spring创建一个Bean的对象, 将对象存入到request域中session: WEB项目中, Spring创建一个Bean的对象, 将对象存入到session域中global session: 作用于集群环境的会话范围, 当不是集群环境时, 则作用于session init-method: 指定类中的初始化方法名称 destroy-method: 指定类中销毁方法名称 bean 的作用范围和生命周期单例对象: scope=&quot;singleton&quot; 一个应用只有一个对象的实例, 它的作用范围就是整个应用 生命周期: 对象出生: 当容器创建时(配置文件被加载时), 对象立刻就被创建了 对象活着: 只要容器在, 对象一直活着 对象死亡: 容器销毁时, 对象就被销毁了 小结: 单例对象的生命周期和容器相同 多例对象: scope=&quot;prototype&quot; 每次访问对象时, 都会重新创建对象实例 生命周期: 对象出生: 当获取对象时, 才会创建新的对象实例 对象活着: 只要对象在使用中, 就一直活着 对象死亡: 当对象长时间不用时, 且没有别的对象使用时, 会被 java 的垃圾回收器回收了 ApplicationContext 接口的实现类 ClassPathXmlApplicationContext: 可以加载类路径下的配置文件, 否则无法加载 （实际开发中, 相比第二种而言, 这种比较常用） FileSystemXmlApplicationContext: 可以加载磁盘任意路径下的配置文件 (必须有访问权限) AnnotationConfigApplicationContext: 用于读取注解创建容器 BeanFactory 和 ApplicationContext BeanFactory 才是 Spring 容器中的顶层接口, ApplicationContext 是它的子接口下面还可以查看其实现类 两者创建对象的时间点不一样BeanFactory: 创建对象采取的是延迟加载, 什么时候get, 什么时候创建对象 （所以 多例 貌似比较适用）ApplicationContext: 只要一读取配置文件, 默认情况下就会创建对象 （所以 单例 貌似比较适用） 实际开发中更多适用 ApplicationContext 创建 Bean 的三种配置方式使用默认无参构造函数12&lt;!--在默认情况下: 它会根据默认无参构造函数来创建类对象。如果 bean 中没有默认无参构造函数, 将会创建失败--&gt;&lt;bean id=&quot;accountService&quot; class=&quot;com.spring.study.service.impl.AccountServiceImpl&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 使用静态工厂的方法创建对象1234567891011121314/*** 模拟一个静态工厂, 创建业务层实现类*/public class StaticFactory &#123; public static IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125; &#125;&lt;!-- 此种方式是: 使用 StaticFactory 类中的静态方法 createAccountService 创建对象, 并存入 spring 容器id 属性: 指定 bean 的 id, 用于从容器中获取 class 属性: 指定静态工厂的全限定类名 factory-method 属性: 指定生产对象的静态方法--&gt;&lt;bean id=&quot;accountService&quot; class=&quot;com.spring.study.factory.InstanceFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt; 使用实例工厂的方法创建对象1234567891011121314151617/*** 模拟一个实例工厂, 创建业务层实现类* 此工厂创建对象, 必须现有工厂实例对象, 再调用方法 */public class InstanceFactory &#123;public IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125;&#125;&lt;!-- 此种方式是:先把工厂的创建交给 spring 来管理然后在使用工厂的 bean 来调用里面的方法 factory-bean 属性: 用于指定实例工厂 bean 的 idfactory-method 属性: 用于指定实例工厂中创建对象的方法--&gt;&lt;bean id=&quot;instancFactory&quot; class=&quot;com.spring.study.factory.InstanceFactory&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;accountService&quot; factory-bean=&quot;instancFactory&quot; factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt; Ioc (注入 Bean) 的三种方式构造函数注入 (constructor-arg)顾名思义, 就是使用类中的构造函数, 给成员变量赋值 (注意: 赋值的操作不是我们自己做的, 还是通过配置的方式, 让 spring 框架来为我们注入) set 方法注入 (property) p 名称空间注入数据 (p:)使用 p名称空间 注入数据(本质还是调用 set 方法, 即 类的属性需要有对应的setter)参考文档 /spring-framework-5.1.4.RELEASE/docs/spring-framework-reference/core.html 在xml中导入 P命名空间测试 如何注入集合属性?","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"最基础的 Spring 项目","slug":"SpringFramework5/2019-05-12-03","date":"2019-05-12T06:35:09.000Z","updated":"2019-08-29T03:38:45.000Z","comments":true,"path":"2019/05/12/SpringFramework5/2019-05-12-03/","link":"","permalink":"http://blog.renyimin.com/2019/05/12/SpringFramework5/2019-05-12-03/","excerpt":"","text":"手动创建1.创建普通Java项目 2.在项目目录下创建 libs 目录 ( 右击 项目目录 —&gt; “Add as library”) 3.导入所需的 Spring 包 核心容器四个包spring-beans-5.1.4.RELEASE.jarspring-context-5.1.4.RELEASE.jarspring-core-5.1.4.RELEASE.jarspring-expression-5.1.4.RELEASE.jar 核心容器依赖的日志包commons-logging-1.2.jar ( commons-logging 相当于一个日志接口, log4j相当于该接口的实现, 如果不添加log4j包也可以, 因为commons-logging也有一个简单的实现会自动使用)log4j-1.2.17.jar 测试类包spring-test-5.1.4.RELEASE.jarjunit-4.12.jar(高于4.10版本还需要hamcrest-core.jar + hamcrest-library.jar) 测试类用到了AOP必须导入aop包spring-aop-4.1.3.RELEASE.jar 4.这样一个Spring项目所需要的最基础的环境就搭建完成了 当然, 现在一般不像上面那样创建项目, 而是选择 Maven 等构件工具来创建项目 Maven创建 (推荐)1.创建maven空项目 2.导入所需的 Spring 包Spring框架提供了很多服务, 但这些服务并不是默认为应用打开的, 应用可以按需指明使用的服务 和手动创建时一样, 不过此处是在maven项目的 pom 文件中指定对应的包坐标 3.注意: 创建Spring基础项目的话, 其实只用导入 spring-context 即可自动拉取创建基本Spring项目所需的jar包(apo:是基于注解配置IOC所要依赖的jar, spring不知道你将来会使用xml还是注解配置IOC, 所以会将aop包也导入)可以查看 spring-context 的依赖 (我们的项目依赖了 spring-context, 而spring-context依赖的包如下) 参考:https://www.cnblogs.com/mibloom/p/9871652.html","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"Spring 核心 之 DI、IOC","slug":"SpringFramework5/2019-05-11-02","date":"2019-05-11T03:15:31.000Z","updated":"2019-08-29T03:29:18.000Z","comments":true,"path":"2019/05/11/SpringFramework5/2019-05-11-02/","link":"","permalink":"http://blog.renyimin.com/2019/05/11/SpringFramework5/2019-05-11-02/","excerpt":"","text":"概述 控制反转(Inversion of Control, IOC) 不是什么技术, 而是一种设计思想传统的创建对象的方法是直接通过 new 关键字, 而 Spring 则是通过 IOC 容器来创建对象, 也就是说我们将创建对象的控制权交给了 IOC 容器, 控制权发生了转移而 IOC 让程序员不再关注怎么去创建对象, 而是关注与对象创建之后的操作, 把对象的创建、初始化、销毁等工作交给Spring容器来做, 所有的类的创建、销毁都由 spring来控制, 也就是说控制对象生存周期的不再是引用它的对象, 而是spring, 对于某个具体的对象而言, 以前是它控制其他对象, 现在是所有对象都被spring控制, 所以这叫控制反转 对于 Spring框架 来说, Ioc 就是由 Spring 来负责控制对象的生命周期和对象间的关系 在传统的程序开发中, 在一个对象中, 如果要使用另外的对象, 就必须得到它 (自己new一个, 或者从JNDI中查询一个), 使用完之后还要将对象销毁(比如Connection等), 这样会导致当前对象 和 其他的接口或类藕合起来 而对于IoC, 有点像通过婚介找女朋友, 在我和女朋友之间引入了一个第三者: 婚姻介绍所。婚介管理了很多男男女女的资料, 我可以向婚介提出一个列表, 告诉它我想找个什么样的女朋友, 比如长得像李嘉欣, 身材像林熙雷, 唱歌像周杰伦, 速度像卡洛斯, 技术像齐达内之类的, 然后婚介就会按照我们的要求, 提供一个mm, 我们只需要去和她谈恋爱、结婚就行了。简单明了, 如果婚介给我们的人选不符合要求, 我们就会抛出异常。整个过程不再由我自己控制, 而是有婚介这样一个类似容器的机构来控制。 Spring所倡导的开发方式就是如此, 所有的类都会在spring容器中登记, 告诉Spring你是谁, 你需要什么, 然后spring会在系统运行到适当的时候, 把你要的东西主动给你, 同时也把你交给其他需要你的东西 Ioc 和 DI 大致可以认为是一个意思, 不同的叫法 (使用 Spring容器进行 依赖注入 后, 控制权就发生了反转 😆!) Ioc 的三种依赖注入方式后面会讲到","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"Spring Framework 5 入门","slug":"SpringFramework5/2019-05-11-01","date":"2019-05-11T03:12:31.000Z","updated":"2020-03-12T10:38:52.000Z","comments":true,"path":"2019/05/11/SpringFramework5/2019-05-11-01/","link":"","permalink":"http://blog.renyimin.com/2019/05/11/SpringFramework5/2019-05-11-01/","excerpt":"","text":"Spring 体系架构 下载并解压 spring-framework-5.1.4.RELEASE-dist.zip 后, 可以看到 Spring Framework 有 docs、libs 和 schema 文件夹 其中libs文件夹下是所需要的jar包(从Spring 5 RELEASE版本开始, Spring 提供了21个模块, 可以看到每一模块都有三个jar包, 分别为类库、文档和源码, 所以总共会有63个jar包), 在项目中引用Spring Framework时, 应该按需引用所需的类库 下图是从 spring-framework-5.1.4.RELEASE 解压包下的 docs/spring-framework-reference/images 中拿到的 Spring 总共大约有 21 个模块, 而这些组件被分别整合在 核心容器(Core Container)、AOP(Aspect Oriented Programming)、设备支持(Instrmentation)、 数据访问及集成(Data Access/Integeration)、Web、报文发送(Messaging)、Test 7个模块集合中 Spirng 各模块之间的依赖关系该图是 Spring5 的包结构, 可以从中清楚看出 Spring 各个模块之间的依赖关系 另外, 因为 spring-core 依赖了 commons-logging, 而其他模块都依赖了 spring-core, 所以整个spring框架都依赖了commons-logging日志框架有多种, 也不一定使用commons-logging, 如果有自己的日志实现如 log4j, 可以排除对 commons-logging 的依赖, 否则会编译报错 参考 Spring历史版本变迁 https://www.cnblogs.com/tuhooo/p/5883277.html https://www.cnblogs.com/mokingone/p/9108995.html","categories":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.renyimin.com/tags/Spring/"}]},{"title":"三次握手","slug":"network/tcpip-3-handshark","date":"2019-05-06T12:07:49.000Z","updated":"2019-12-03T07:30:32.000Z","comments":true,"path":"2019/05/06/network/tcpip-3-handshark/","link":"","permalink":"http://blog.renyimin.com/2019/05/06/network/tcpip-3-handshark/","excerpt":"","text":"在抓包分析TCP/IP, 需要了解几个简单概念 位码, 即tcp标志位, 通常用大写, 有6种: SYN(synchronous建立联机)、ACK(acknowledgement 确认)、PSH(push传送)、FIN(finish结束)、RST(reset重置)、URG(urgent紧急) Sequence number(顺序号码)、Acknowledge number(确认号码) 三次握手 1.第一次握手主机A发送位码为 SYN(syn＝1), 随机产生 seq number=x 的数据包到服务器, 主机B由 SYN(syn=1) 知道, A要求建立联机客户端进入 SYN SENT 状态 2.第二次握手主机B收到请求后要确认联机信息，向A发送位码为 ACK+SYN (ack=1, syn=1)ACK: ack number = x+1(即主机A的seq+1)SYN: 随机产生 seq number=y 服务器进入 SYN RECV 状态 3.第三次握手主机A收到后检查 ack number 是否正确,即第一次发送的 seq number+1 (x+1), 以及位码 ACK(ack=1)若正确, 主机A会再发送 ack number=(主机B的seq+1,即 Y+1), 此时自然是ACK(ack=1), 同时还会将 seq number=x+1 发送到服务端主机B收到后确认 ACK, ack = y+1, seq = x+1 值, 客户端和服务器进入 ESTABLISHED (TCP连接成功) 状态, 完成三次握手 tip10 帧是客户端向服务端发送 tcp window update, 表示buffer已经清空。并提示服务端现在已经有足够的window, 大小为 408288","categories":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://blog.renyimin.com/tags/network/"}]},{"title":"ServletContext","slug":"JavaWeb/2019-04-21-07","date":"2019-04-21T08:02:32.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/21/JavaWeb/2019-04-21-07/","link":"","permalink":"http://blog.renyimin.com/2019/04/21/JavaWeb/2019-04-21-07/","excerpt":"","text":"ServletContext 对象 ServletContext 对象: 代表整个web应用, 可以和程序的容器(服务器)来通信 获取: 通过 Request 对象获取 request.getServletContext(); 通过 HttpServlet 获取 this.getServletContext(); 功能: 获取 MIME 类型 域对象 (共享数据: setAttribute()、getAttribute()、removeAttribute())ServletContext 域对象的范围: 所有用户所有请求的数据 获取文件的真实路径(服务器路径) Demo获取文件的真实路径(服务器路径) 文件下载案例…","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"ServletResponse","slug":"JavaWeb/2019-04-21-06","date":"2019-04-21T07:27:19.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/21/JavaWeb/2019-04-21-06/","link":"","permalink":"http://blog.renyimin.com/2019/04/21/JavaWeb/2019-04-21-06/","excerpt":"","text":"Request 和 Response 对象都是由Tomcat服务器创建的, ( Request 对象是来获取请求消息, Response对象是来设置响应消息 ) 转发 重定向建议 动态获取虚拟目录 (虚拟目录不能写死): resp.sendRedirect(req.getContextPath() + &quot;/demo01&quot;); 服务器输出字符数据到浏览器 服务器输出字节数据到浏览器 验证码","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"51","slug":"JavaSE/2019-04-21-51","date":"2019-04-21T03:25:07.000Z","updated":"2019-09-04T03:10:28.000Z","comments":true,"path":"2019/04/21/JavaSE/2019-04-21-51/","link":"","permalink":"http://blog.renyimin.com/2019/04/21/JavaSE/2019-04-21-51/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"50. 多线程入门","slug":"JavaSE/2019-04-21-50","date":"2019-04-21T03:19:36.000Z","updated":"2019-09-04T03:10:17.000Z","comments":true,"path":"2019/04/21/JavaSE/2019-04-21-50/","link":"","permalink":"http://blog.renyimin.com/2019/04/21/JavaSE/2019-04-21-50/","excerpt":"","text":"并发 vs 并行 并发: 指的是同一时间段内, 有两个或多个事件在执行 并行: 指的是同一时刻, 有两个或多个事件在执行 创建线程的 类 Java 使用 java.lang.Thread 类代表线程类, 所有的线程对象都必须是 Thread类 或 其子类的实例; 每个线程的作用是完成一定的任务(即 执行一段程序流, 一段代码) Java 中有两种方式来创建线程: 一种是继承 Thread类 的方式, 另一种是实现 Runnable接口 的方式 继承 Thread类 创建线程类 通过继承 Thread类 来创建并启动多线程, 步骤如下: 定义 Thread类 的子类, 并重写该类的 run() 方法, 方法体就代表了线程需要执行的任务 ( run() 方法也称为线程的执行体) 创建线程对象 (实例化你的线程类) 调用线程对象的 start() 方法来启动该线程 Java程序属于 抢占式调度, 哪个线程的优先级高, 哪个线程就优先执行, 同一个优先级会随机选择一个执行 可以看到main线程和新创建的thread线程会出现抢占调度的情况 Thread类常用方法 构造方法: public Thread() 分配一个新的线程对象 public Thread(String name) 分配一个指定名字的新的线程对象 public Thread(Runnable target) 分配一个带有指定目标的新的线程对象 public Thread(Runnable target, String name) 分配一个带有指定目标的新的线程对象, 并指定线程名字 常用方法 public void start() 导致此线程开始执行, JVM调用此线程的 run() 方法 public void run() 此线程要指定的任务在此处定义代码 public String getName() 获取当前线程名称 public static void sleep(long millis) 使当前正在执行的线程以指定的毫秒数暂停 public static Thread currentThread() 返回对当前正在执行的线程对象的引用 两种方法获取线程名1.可以直接使用 Thread类 中的方法 String getName() 返回线程的名称 2.也可以使用 Thread类 中的静态方法 先获取到正在执行的线程 Thread.currentThread() 返回当前正在执行的线程对象的引用 然后使用线程中的方法 String getName() 获取线程的名称 (当然, 如果在子线程对象中, 也可以不用Thread调用, 而是直接 currentThread().getName() 也可以) DEMO 两种方法设置线程名设置线程的名称有两种方法: 一种是子类自己给自己起名字, 一种是子类让父类Thread给起名字 子类自己给自己起名字: 直接使用Thread类的方法 setName(名字) 父类 Thread 给自定义的线程类起名字: 为你的线程类创建一个带参数的构造方法, 参数传递线程的名称 在构造方法中调用父类的带参构造方法, 把线程名称传递给父类, 让父类(Thread)给子线程起一个名字 线程的 sleep() 方法Thread.sleep(毫秒数)如下, 为了明显地看到线程抢占式调度的效果, 可以适当地将线程任务进行sleep 多线程内存浅析 tip: 自定义线程对象直接调用 run() 和 调用 start() 是有区别的 线程 与 堆栈栈是一块和线程紧密相关的内存区域, 每个线程都有自己的栈内存, 用于存储本地变量, 方法参数和栈调用, 一个线程中存储的变量对其它线程是不可见的;而堆是所有线程共享的一片公用内存区域, 对象都在堆里创建, 为了提升效率线程会从堆中弄一个缓存到自己的栈, 如果多个线程使用该变量就可能引发问题, 这时 volatile 变量就可以发挥作用了, 它要求线程从主存中读取变量的值 如下图: 当我们直接调用线程对象的 run() 方法时, 该方法只是存在于 main线程栈空间的一个普通方法; 只有调用线程对象的 start() 方法时, 才会开辟新的栈空间并执行 run方法中的线程任务 实现 Runnable接口 创建线程 采用 java.lang.Runnable 也是非常常见的一种创建线程的方式, 只需要重写 run() 方法即可 定义 Runnable接口 的实现类, 并重写该接口的 run() 方法, run() 方法的方法体同样是该线程的线程执行体 创建 Runnable 实现类的实例, 并以此实例作为 Thread类 的 target参数 来创建Thread对象, 该Thread对象才是真正的线程对象 最后调用线程对象的 start() 方法来启动线程 示例 扩展在Java中, 每次程序运行至少启动2个线程, 一个是main线程, 一个是垃圾收集线程; 因为每当使用java命令执行一个类的时候, 实际上都会启动一个JVM, 每一个JVM其实就是在操作系统中启动了一个进程;","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"Servlet 体系结构","slug":"JavaWeb/2019-04-20-04","date":"2019-04-20T08:21:11.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/20/JavaWeb/2019-04-20-04/","link":"","permalink":"http://blog.renyimin.com/2019/04/20/JavaWeb/2019-04-20-04/","excerpt":"","text":"之前写的 ServletDemo 类实现了Servlet接口的所有方法, 但通常只用 service 方法, 因此没必要每个 Servlet 类都实现 Servlet 接口的所有方法 其实 Servlet 接口下其实还有两个实现类 GenericServlet 抽象类 (它将Servlet接口中其他的方法做了空实现, 只将 service 方法作为抽象方法, 因此定义Servlet类时, 可以继承这个抽象类即可, 当然其他方法需要时也可以复写; 不过在实际开发中也不用这个类, 而用下面的 HttpServlet 类) HttpServlet 抽象类 (对http协议进行了封装) 当用户请求到达后, 在你的每个servlet的service方法中首先要做的就是获取用户请求参数(get 和 post 获取数据的方式可是不太一样的), HttpServlet帮我们做了 继承 GenericServlet 来创建一个Servlet类 继承 HttpServlet 来创建一个Servlet类 (可以通过 get请求 和 表单模拟post请求 进行测试)","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"Servlet 入门","slug":"JavaWeb/2019-04-20-03","date":"2019-04-20T07:41:25.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/20/JavaWeb/2019-04-20-03/","link":"","permalink":"http://blog.renyimin.com/2019/04/20/JavaWeb/2019-04-20-03/","excerpt":"","text":"第一个Servlet类 创建JavaWeb项目: Javaee -&gt; Web Application 第一个Servlet类（实现 Servlet 接口） 在项目的 web/WEB-INF/web.xml 中配置 Servlet, 需要告诉 tomcat, 当浏览器访问 哪个资源名可以映射到哪个 Servlet类 先关注 Servlet 接口 中的 service 这个提供服务的方法 (在IDE中集成了tomcat, 可以直接在中对项目虚拟目录进行配置) 然后访问 localhost:8080/demo01/demo01 即可看到idea控制台打印了 “Hello Servlet!” (注意: 如果你在idea中配置tomcat时, Edit configur.. -&gt; demploy -&gt; Application Context 中配置了虚拟路径) 访问时需要加上虚拟路径, 即 localhost:8080/demo/demo01 servlet 执行原理 localhost:8080/demo01/demo01 先找到服务器, 当服务器接收到客户端浏览器的请求后, 会解析url路径, 先找到虚拟目录对应的项目, 最后获取访问的Servlet的资源路径名 demo01 根据资源路径名, 去web.xml中对应的 &lt;url-pattern&gt;, 找到映射的 &lt;servlet-class&gt; 全类名 tomcat会将Servlet类的字节码文件加载进内存, 并且创建其对象 最后调用其方法 servlet 中的方法 init 初始化方法: 在Servlet被创建时执行, 即第一次访问资源时执行 (只会执行一次) servlet 的 init() 方法只执行一次, 说明 Servlet在内存中只存在一个对象, Servlet是单例, 多用户同时访问时, 可能存在线程安全问题 因此, 我们需要注意: 尽量不要再servlet中定义成员变量, 可以定义局部变量, 即使定义了成员变量, 也不要对其进行修改, 只获取就行了(另外, 需要注意如果强制给service代码加同步代码块, 效率影响会非常大) service 提供服务的方法: 每次servlet被访问都会执行 destroy: 在servlet被杀死前执行, 即服务器正常关闭前执行一次, 一般用于释放资源 getServletInfo: 获取servlet的一些信息 版本, 作者 等等; 一般不会实现 servlet 生命周期servlet 什么时候被创建? 默认情况下, 第一次被访问时, servlet 被创建, 可以配置Servlet的创建时机 servlet 注解配置 将来开发中, 可能需要上百个servlet, 如果都像上面那样在 web.xml 中进行配置, 不敢想象… Servlet3.0 开始支持注解配置 (可以不需要web.xml)创建javaee, 选择Servlet3.0以上版本(即勾选 Web Application 时, 括号中的版本), 由于要使用注解配置Servlet, 所以其实在创建项目时可以不勾选 创建web.xml 如下: @WebServlet 注解 相关配置 @WebServlet 注解的 urlPatterns 参数可以给一个 Servlet 配置多个访问路径, 如 @WebServlet({&quot;/demo03&quot;, &quot;/demo003&quot;}) urlPatterns 的定义规则 /xxx /xxx/xxxx : 多层路径 ( @WebServlet({&quot;/d1/d2/d3&quot;, &quot;/demo4/*&quot;} ) /* : 表示写什么都可以访问到该Servlet (不常见) *.xxx : 不能加 /","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"Servlet 简介","slug":"JavaWeb/2019-04-20-02","date":"2019-04-20T07:29:04.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/20/JavaWeb/2019-04-20-02/","link":"","permalink":"http://blog.renyimin.com/2019/04/20/JavaWeb/2019-04-20-02/","excerpt":"","text":"前言 通过服务器可以访问 静态资源 和 动态资源: 静态资源就是一些图片、视频等文件, 访问流程: 读取磁盘文件到内存中—-&gt;启动IO流—&gt;将文件内容以字符串形式返回给浏览器 动态资源则是一些代码, 需要执行后将结果返回给浏览器端, 而要在本地执行对应的代码, 就需要有执行环境和标准接口规范 (服务器要和程序对话, 就需要有响应的规范, 如 CGI) CGI模式的特点: 是将服务端的动态资源基于进程(process)方法来运行, 由于进程的执行非常耗费时间, 且内存空间浪费, 所以效率极其低下 FastCGI模式: 所谓的fastCGI的模式其实就是当前的CGI模式中添加的一个pooling(资源池)的概念, 在服务器启动时初始化固定的进程来提高处理客服端的请求的速度, 但是该方法治标不治本 Servlet模式服务器 123技术特点：与传统的CGI模式不同, Servlet运行模式改为单进程多线程的模式(线程程运行在进程中)单进程: 服务器多线程：服务端的Servlet 这样就大大提高了运行效率 Java Servlet 如上面介绍, Java Servlet 是运行在 Web 服务器或应用服务器上的程序, 它是作为来自 “Web浏览器或其他 HTTP 客户端的请求” 和 “HTTP服务器上的数据库或应用程序” 之间的 中间层 使用 Servlet, 您可以收集来自网页表单的用户输入, 呈现来自数据库或者其他源的记录, 还可以动态创建网页 浏览器请求服务器动态资源时, 需要通过java代码来进行一些逻辑的动态判断, 但是这类型的java类并没有main方法, 而是依赖于服务器 tomcat 去执行的 要知道的是, 不是所有的类 tomcat 都能够认识, 所以这些类需要遵守一定的规则才能被tomcat所识别 Servlet就是一个接口, 定义了java类被浏览器访问到的规则(被tomcat识别的规则), 可以到 JavaEE Api 中查询Servlet接口 Java Servlet 通常情况下与使用 CGI（Common Gateway Interface, 公共网关接口）实现的程序可以达到异曲同工的效果。但是相比于 CGI, Servlet 有以下几点优势： 性能明显更好 Servlet 在 Web 服务器的地址空间内执行。这样它就没有必要再创建一个单独的进程来处理每个客户端请求。 Servlet 是独立于平台的, 因为它们是用 Java 编写的。 服务器上的 Java 安全管理器执行了一系列限制, 以保护服务器计算机上的资源。因此, Servlet 是可信的 Java 类库的全部功能对 Servlet 来说都是可用的。它可以通过 sockets 和 RMI 机制与 applets、数据库或其他软件进行交互","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"Nginx VS Tomcat","slug":"JavaWeb/2019-04-20-01","date":"2019-04-20T07:21:17.000Z","updated":"2019-09-03T10:20:45.000Z","comments":true,"path":"2019/04/20/JavaWeb/2019-04-20-01/","link":"","permalink":"http://blog.renyimin.com/2019/04/20/JavaWeb/2019-04-20-01/","excerpt":"","text":"Nginx VS Tomcat Nginx 常用做静态内容服务 和 代理服务器, 直面外来请求转发给后面的 应用服务( 如 tomcat 等); Tomcat 更多用来做一个应用容器, 让 javaweb应用 跑在里面(对应同级别的有 jboss, jetty 等) 严格的讲, Apache/Nginx 应该叫做 HTTP Server; 而 Tomcat 则是一个 Application Server, 或者更准确的来说, 是一个 Servlet/JSP 应用的容器; (不过, nginx 也可以通过模块开发来提供应用功能; 而 Tomcat 也可以直接提供http服务, 通常用在内网和不需要流控等小型服务的场景) 一个 HTTP Server 关心的是 HTTP 协议层面的传输和访问控制, 所以在 Apache/Nginx 上你可以看到 代理、负载均衡 等功能; 客户端通过 HTTP Server 访问服务器上存储的资源(HTML 文件、图片文件等等) 通过 CGI 技术, 也可以将处理过的内容通过 HTTP Server 分发, 但是一个 HTTP Server 始终只是把服务器上的文件如实的通过 HTTP 协议传输给客户端 而应用服务器, 则是一个应用执行的容器, 它首先需要支持开发语言的 Runtime (对于 Tomcat 来说, 就是 Java), 保证应用能够在应用服务器上正常运行; 其次, 需要支持应用相关的规范, 例如类库、安全方面的特性, 对于 Tomcat 来说, 就是需要提供 JSP/Sevlet 运行需要的标准类库、Interface 等 不过, 为了方便, 应用服务器往往也会集成 HTTP Server 的功能, 但是不如专业的 HTTP Server 那么强大, 所以应用服务器往往是运行在 HTTP Server 的背后, 执行应用, 将动态的内容转化为静态的内容之后, 通过 HTTP Server 分发到客户端","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"88. EXPLAIN -- Extra","slug":"MySQL 暂停/2019-04-15-mysql-88","date":"2019-04-15T08:53:49.000Z","updated":"2019-04-26T10:53:32.000Z","comments":true,"path":"2019/04/15/MySQL 暂停/2019-04-15-mysql-88/","link":"","permalink":"http://blog.renyimin.com/2019/04/15/MySQL 暂停/2019-04-15-mysql-88/","excerpt":"","text":"Extra列包含有关MySQL如何解析查询的其他信息, 该列显示的是不在其他列显示的额外信息; 此字段能够给出让我们深入理解执行计划进一步的细节信息, 比如是否使用ICP, MRR等 Using filesort 当 SQL 中包含 ORDER BY, 而且无法利用索引完成排序操作的时候, MySQL Query Optimizer 不得不选择相应的排序算法来实现; 数据较少时从内存排序, 否则从磁盘排序; Explain不会显示的告诉客户端用哪种排序 官方解释: “MySQL需要额外的一次传递, 以找出如何按排序顺序检索行, 通过根据联接类型浏览所有行并为所有匹配WHERE子句的行保存排序关键字和行的指针来完成排序。然后关键字被排序, 并按排序顺序检索行” 示例: 使用的排序字段并没有为其创建索引 1234567mysql&gt; explain select `name` from t3 where id in (1,2) order by name;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+| 1 | SIMPLE | t3 | NULL | range | PRIMARY | PRIMARY | 4 | NULL | 2 | 100.00 | Using where; Using filesort |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+1 row in set, 1 warning (0.00 sec) Using index 主键-覆盖索引 直接在主键索引上完成查询和所有数据的获取, 一般是只获取主键ID (主键 覆盖索引) 示例: 1234567mysql&gt; explain select id from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) 示例: 如果使用的是主键, 但是select的字段列表不止主键id, Extra 为 NULL 123456789101112131415mysql&gt; explain select id,goods_name from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) mysql&gt; explain select * from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 小结: where条件使用了主键索引, 且 select 只有主键 Using where; Using index 二级索引-覆盖索引 直接在二级索引(覆盖索引)上获取全部数据 示例: 1234567891011121314mysql&gt; explain select id from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using where; Using index |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; explain select id,goods_name from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using where; Using index |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+ 小结: where条件用到了二级索引, 并且 select 做到覆盖索引 Using index condition 二级索引-非覆盖索引 where条件用到了二级索引, 但无法提供所有select字段: 先在二级索引上使用索引查找到主键ID, 然后在主键索引上通过主键ID进行查找 这里之所以需要去主键索引上查, 是因为 select 需要的数据, 二级索引不能完全提供 示例: 123456789101112131415mysql&gt; explain select * from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) mysql&gt; explain select id,goods_name,goods_status from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) 小结: where条件用到了二级索引, 只不过 select 未做到覆盖索引 注意: 如果是范围查找, 优化器会在索引存在的情况下，通过符合 RANGE 范围的条数和总数的比例来选择是使用索引还是进行全表遍历 1234567891011121314151617// 正常情况下mysql&gt; explain select stock,goods_status from explain_goods where stock&gt;1;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_stock | idx_stock | 4 | NULL | 2 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec)// 而如下一张实战的表, `car_id varchar(45)` 为单列普通索引, 表数据量比较大, mysql优化器最终变成了全表扫描 (全表扫描, 参考 type 列)mysql&gt; explain select * from cm_bid_history where car_id&gt;&quot;1000&quot;;+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history | ALL | car_id | NULL | NULL | NULL | 141048 | Using where |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+1 row in set (0.00 sec) 注意: 二级索引貌似为 字符类型 时, 才会是上述结果, 如果为int, Extra 可能会是 null 123456789101112131415mysql&gt; explain select stock,goods_name from explain_goods where stock=&quot;1&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_stock | idx_stock | 33 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) mysql&gt; explain select stock,goods_name from explain_goods where stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | ref | idx_stock | idx_stock | 4 | const | 1 | NULL |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+1 row in set (0.00 sec) Using index condition; Using where where 条件有二级索引, 也有 非索引字段时 12345678910111213141516mysql&gt; explain select * from explain_goods where goods_name=&quot;华为&quot; and stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition; Using where |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+1 row in set (0.00 sec) mysql&gt; explain select id from explain_goods where goods_name=&quot;华为&quot; and stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition; Using where |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+1 row in set (0.00 sec) 小结: where 条件有二级索引, 也有 非索引字段时, 和覆盖不覆盖没有关系 Using where 未用到索引 普通where条件, 无索引 (全表扫描, 参考 type 列) 示例: 1234567891011121314151617mysql&gt; explain select * from explain_goods where stock=1;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 2 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; explain select * from explain_goods where goods_status=1;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 2 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) ………… 未完待续const row not found For a query such as SELECT … FROM tbl_name, the table was empty (类似于select … from tbl_name, 而表记录为空) no matching row in const table 表为空或者表中根据唯一键查询时没有匹配的行 示例: MySQL 5.7.25 1234567mysql&gt; explain select * from t1 where id= 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set, 1 warning (0.00 sec) 示例: MySQL 5.6.35 1234567mysql&gt; explain select `name` from t1 where id = 12;+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+1 row in set (0.00 sec) 问题问题参考: https://segmentfault.com/q/1010000004197413","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"87. EXPLAIN -- key_len 检测多列索引的使用情况","slug":"MySQL 暂停/2019-04-15-mysql-87","date":"2019-04-15T05:13:36.000Z","updated":"2019-04-26T10:48:39.000Z","comments":true,"path":"2019/04/15/MySQL 暂停/2019-04-15-mysql-87/","link":"","permalink":"http://blog.renyimin.com/2019/04/15/MySQL 暂停/2019-04-15-mysql-87/","excerpt":"","text":"key_len表示索引使用的字节数，根据这个值可以判断索引的使用情况, 特别是在组合索引的时候, 判断该索引有多少部分被使用到非常重要 在不损失精确性的情况下, 长度越短越好 在计算key_len时，下面是一些需要考虑的点: 参考 索引字段的附加信息: 可以分为变长和定长数据类型讨论, 当索引字段为定长数据类型时, 如 char, int, datetime 需要有是否为空的标记, 这个标记占用1个字节 (对于not null的字段来说,则不需要这1字节); 对于变长数据类型,比如varchar,除了是否为空的标记外,还需要有长度信息,需要占用 1-2 个字节 对于 char、varchar、blob、text 等字符集来说, key len的长度还和字符集有关, latin1一个字符占用1个字节, gbk一个字符占用2个字节, utf8一个字符占用3个字节 注意: key_len只指示了where中用于条件过滤时被选中的索引列，是不包含order by/group by这一部分被选中的索引列的例如, 有个联合索引idx(c1,c2,c3), 3列均是int not null, 那么下面的SQL执行计划中, key_len的值是8而不是12:select ... from tb where c1=? and c2=? order by c1; 示例表结构 123456789101112131415161718CREATE TABLE `explain_goods` ( `id` int(11) NOT NULL AUTO_INCREMENT, `goods_number` varchar(50) NOT NULL, `goods_name` varchar(100) NOT NULL, `goods_weight` int(11) NOT NULL, `goods_brand` varchar(50) NOT NULL, `goods_desc` text NOT NULL, `stock` int(11) NOT NULL, `goods_status` tinyint(1) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_goods_number` (`goods_number`) USING BTREE, KEY `idx_goods` (`goods_name`,`goods_brand`,`goods_weight`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;INSERT INTO `explain_goods` VALUES (1, &apos;xs335671&apos;, &apos;华为plus&apos;, 11, &apos;荣耀&apos;, &apos;国产最强&apos;, 110, 1, &apos;2019-04-15 13:16:11&apos;, &apos;2019-04-15 13:16:37&apos;);INSERT INTO `explain_goods` VALUES (2, &apos;cxf77890&apos;, &apos;华为Meta20&apos;, 13, &apos;Meta&apos;, &apos;相机莱卡&apos;, 76, 1, &apos;2019-04-15 13:17:15&apos;, &apos;2019-04-15 13:17:15&apos;); 示例1, 多列索引中的三个字段都被使用到时: goods_name varchar(100) NOT NULL : 100 3(utf8) = 300字节 2个字节(长度, 貌似无论varchar(M)的M是多少, 此处都是按照2个字节来计算长度, 而且一个varchar字段此处最大是 3255+2=767) goods_brand varchar(50) NOT NULL: 50 3(utf8) = 150 字节 2个字节(长度, 貌似无论varchar(M)的M是多少, 此处都是按照2个字节来计算长度, 而且一个varchar字段此处最大是 3255+2=767 ) goods_weight int(11) NOT NULL: 4个字节 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 458 | const,const,const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+1 row in set (0.00 sec) 当使用到其中两列: 少了一个 “goods_weight int(11) NOT NULL”, 非空 int 列, 也就少了4个字节 (458-4=454) 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 454 | const,const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+1 row in set (0.00 sec) 当使用到其中一列: 又少了一个 “goods_brand varchar(50) NOT NULL,”, 非空 varchar 列, 也就少了 50*3+2 = 152 个字节 (454-152) 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) 如下: 当多列索引中某一列使用了范围查询, 则右边的列无法使用索引优化 少了 goods_weight int(11) NOT NULL 的 4个字节 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand&gt;&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 454 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec) 少了 goods_brand varchar(50) NOT NULL 152 + goods_weight int(11) NOT NULL 的 4 = 156个字节 1234567mysql&gt; explain select * from explain_goods where goods_name &gt; &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 302 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec) 不是最左前缀匹配, 不会使用索引: 12345678910111213141516// 最左前缀：mysql&gt; explain select * from explain_goods where goods_name like &quot;华为Plus%&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 458 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec)// 非最左前缀, 未使用索引:mysql&gt; explain select * from explain_goods where goods_name like &quot;%华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"86. EXPLAIN -- type 列","slug":"MySQL 暂停/2019-04-13-mysql-86","date":"2019-04-13T11:31:21.000Z","updated":"2019-04-26T10:49:02.000Z","comments":true,"path":"2019/04/13/MySQL 暂停/2019-04-13-mysql-86/","link":"","permalink":"http://blog.renyimin.com/2019/04/13/MySQL 暂停/2019-04-13-mysql-86/","excerpt":"","text":"type 列介绍这一列描述了MySQL是如何查找表中的行的, 下面的类型从最优到最差: system system 其实是 const 的特例, 当表只有一行记录(等于系统表)时会出现 (业务上几乎不会出现只有一条记录的表, 所以这种情况并不多见) system 貌似只能用于 MyISAM 和 Memory, InnoDB 模拟不出来 MyISAM: t2表只有一条id为1, name 为Lant2的记录 (如下两条查询, 不管用没用到索引, 由于只有一条记录, 所以 type 就是 system) 123456789101112131415mysql&gt; explain SELECT * from t2 where id=1;+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | t2 | system | PRIMARY | NULL | NULL | NULL | 1 | NULL |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+1 row in set (0.00 sec) mysql&gt; explain SELECT * from t2 where name=&quot;Lant2&quot;;+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | t2 | system | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+1 row in set (0.00 sec) InnoDB示例: t2表只有一条id为1的记录, 查询不仅使用了主键索引, 查询的内容还做到了覆盖索引, 但发现 type列 结果是 const, 而不是 system 1234567mysql&gt; explain SELECT id from t2 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | t2 | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) const const 表示通过索引一次就找到了, 用于 primary key 或者 unique 索引, 因为只匹配一行数据, 所以很快 (MyISAM和InnoDB使用主键或唯一索引在where条件, 都可以模拟出来) 示例: (注意, 覆盖索引和非覆盖索引的Extra可不同) 1234567891011121314151617mysql&gt; explain SELECT id from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) mysql&gt; explain SELECT * from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) mysql&gt; eq_ref 读取本表表中和 关联表 表中的每行组合成的一行, 除 了 system 和 const 类型之外, 这是最好的联接类型。当连接使用索引的所有部分时, 索引是 主键 或 唯一非NULL索引 时, 将使用该值 (InnoDB 和 MyISAM 略有不同) InnoDB示例: 当使用主键或唯一索引, 并且查询的内容是覆盖索引, 会出现 eq_ref: 12345678mysql&gt; explain SELECT t1.id,t3.id FROM t1 join t3 on t1.id=t3.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| 1 | SIMPLE | t1 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | Using index |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+2 rows in set (0.00 sec) InnoDB示例: 当使用主键或唯一索引, 但查询的内容没有做到覆盖索引时, 就不会出现 eq_ref, 如下: 12345678910111213141516171819// 可以看到 t3 使用了 index, 而 t1 使用了 ALL 全表扫描mysql&gt; explain SELECT t1.id,t3.id,t1.name FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+| 1 | SIMPLE | t3 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+2 rows in set (0.00 sec)// 如下的话, 则都是全表扫描mysql&gt; explain SELECT * FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | NULL || 1 | SIMPLE | t3 | ALL | PRIMARY | NULL | NULL | NULL | 3 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+2 rows in set (0.00 sec) InnoDB示例: 需要注意的是, 当 InnoDB 数据量比较多的情况下, 有时即使没有做到覆盖所有, 也会出现 eq_ref 12// cm_bid_history 表中有10万数据explain SELECT * FROM cm_bid_history a join cm_bid_history as b on a.id=b.id; MyISAM示例 : 只要像官网说的那样, 用到了 主键 或唯一索引, 就会出现 eq_ref 12345678mysql&gt; explain SELECT * FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | NULL || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | NULL |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+2 rows in set (0.00 sec) ref 这是一种索引访问, 它可能会找到多个符合条件的行, 因此它是查找和扫描的混合体; 此类索引访问只有当使用 非唯一性索引 或者 唯一性索引的非唯一性前缀 时才会发生; 不像 eq_ref 那样要求连接顺序, 也没有主键和唯一索引的要求, 只要使用相等条件检索时就可能出现, 常见于辅助索引的等值查找, 或者多列主键、唯一索引中, 使用第一个列之外的列作为等值查找也会出现, 总之, 返回数据不唯一的等值查找就可能出现 如 explain select num from goods where num=11; (num列是个普通索引) 1234567mysql&gt; explain select num from goods where num=11;+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | goods | ref | idx_num | idx_num | 5 | const | 1 | Using index |+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) ref_ro_null 是ref之上的一个变体, 与ref方法类似, 只是增加了null值的比较, 实际用的不多 1234567mysql&gt; explain select id,num from goods where num=11 or num is null;+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+| 1 | SIMPLE | goods | ref_or_null | idx_num | idx_num | 5 | const | 2 | Using index condition |+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+1 row in set (0.00 sec) range 范围扫描是一个有限制的索引扫描, 它开始于索引里的某一点, 返回匹配这个值域的行; 这比全索引扫描好一些, 因为它用不着遍历全部索引; 显而易见的范围扫描是where子句里带有 BETWEEN 或 &gt;、&lt;、in 等的查询 MyISAM in() 示例: (goods设置了num做BTree索引) 1234567mysql&gt; explain select id,num from goods where num in (10,11);+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| 1 | SIMPLE | goods | range | idx_num | idx_num | 5 | NULL | 3 | Using index condition |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+1 row in set (0.00 sec) MyISAM or 示例: 1234567mysql&gt; explain select id,num from goods where num=10 or num=11;+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| 1 | SIMPLE | goods | range | idx_num | idx_num | 5 | NULL | 3 | Using index condition |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+1 row in set (0.00 sec) 注意: 当MySQL使用索引 用 in() 和 or 去查找一系列值时, 虽然都会显示为范围扫描, 但这两者其实是相当不同的访问类型, 在性能上有重要的差异 … index Full Index Scan, 和全表扫描一样, 不过index只遍历索引树, 通常比ALL快 (虽说都是读全表, 但是index是从索引中读取的, 而all是从硬盘中读取的) 如果在Extra列中看到 “Using index”, 说明MySQL正在使用覆盖索引, 它只扫描索引的数据, 比按索引次序全表扫描的开销要少很多; 示例: 1234567891011121314151617// InnoDBmysql&gt; explain select id from cm_bid_history;+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history | index | NULL | deleted | 1 | NULL | 142485 | Using index |+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+1 row in set (0.00 sec)// MyISAMmysql&gt; explain select id from cm_bid_history_copy1;+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history_copy1 | index | NULL | PRIMARY | 4 | NULL | 142267 | Using index |+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+1 row in set (0.00 sec) … ALL 全表扫描, 通常意味着Mysql必须扫描整张表, 从头到尾去找到需要的行 (也有例外, 例如在查询里使用了LIMIT, 或者在 Extra 列中显示 “Using distinct/not exists”) 示例: 1234567891011121314151617// InnoDBmysql&gt; explain select * from cm_bid_history;+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+| 1 | SIMPLE | cm_bid_history | ALL | NULL | NULL | NULL | NULL | 142485 | NULL |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+1 row in set (0.00 sec)// MyISAMmysql&gt; explain select * from cm_bid_history_copy1;+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+| 1 | SIMPLE | cm_bid_history_copy1 | ALL | NULL | NULL | NULL | NULL | 142267 | NULL |+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+1 row in set (0.00 sec) NULLNULL是最好的, 不用访问索引或表, 直接获得数据…","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"","slug":"Maven/2019-04-11-maven-02","date":"2019-04-11T09:10:12.000Z","updated":"2019-08-22T08:40:48.000Z","comments":true,"path":"2019/04/11/Maven/2019-04-11-maven-02/","link":"","permalink":"http://blog.renyimin.com/2019/04/11/Maven/2019-04-11-maven-02/","excerpt":"","text":"","categories":[{"name":"Maven","slug":"Maven","permalink":"http://blog.renyimin.com/categories/Maven/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://blog.renyimin.com/tags/Maven/"}]},{"title":"Maven 入门","slug":"Maven/2019-04-11-maven-01","date":"2019-04-11T08:23:07.000Z","updated":"2019-08-22T09:04:07.000Z","comments":true,"path":"2019/04/11/Maven/2019-04-11-maven-01/","link":"","permalink":"http://blog.renyimin.com/2019/04/11/Maven/2019-04-11-maven-01/","excerpt":"","text":"概述 Maven 是一个采用纯Java编写的开源项目管理工具; 是当前最受欢迎的Java项目管理构建自动化综合工具, 类似以前Java中的Ant、node.js中的npm、dotNet中的nuget、PHP中的Composer Maven 基于POM(POM project object model 项目对象模型)对项目进行管理(使用 pom.xml 来对项目进行管理) Maven 解决的问题 大量jar包的引入, jar包冲突, 版本冲突 可以和IDEA一样 将代码编译成字节码文件 批量(可以指定目录)单元测试 项目打包, 发布 (简化) 节省项目空间: 传统web工程, jar包会包含在项目中, 项目会很大 (如果有10个项目, 每个项目都要在自己项目中放入所用到的jar包) 核心功能依赖管理上面描述maven所解决的问题时, 无不体现了maven的这一核心功能 一键构建 传统的Java项目, 往往都要经历 编译、测试、运行、打包、安装、部署等一系列过程 (每一步都是在构建只不过构建的成都不一样) Maven 一键构建 指的就是项目从 编译、测试、运行、打包、安装、部署整个过程都交给 maven 进行管理 (整个构建过程, 使用 maven 一个命令可以轻松完成整个工作) maven已经在内部继承了tomcat插件 之前我们手动创建的项目, 需要手动打包, 然后将项目部署到tomcat的webapp目录下, 然后启动tomcat, 才可以访问项目; 虽然将tomcat集成到IDEA后, 让此过程变得简单了一些, 但让需要一些步骤; 而如果使用maven来构建, 只用执行一个命令 mvc tomcat:run maven仓库 maven项目中只是在pom.xml中存放了依赖jar包的坐标, 当maven初次启动时, 会从远程仓库把pom文件中所标明的 jar包 拿下来, 放到本地仓库 maven 默认将jar包放置的位置: 可以查看maven的配置文件 maven安装路径/conf/settings.xml (mac在 ${user.home}/.m2/ 下) 在settings.xml中找到 localRepository, 可以看到默认位置为 ${user.home}/.m2/repository 重新指定本机仓库地址 &lt;localRepository&gt;新指定的位置&lt;/localRepository&gt; 仓库种类 (项目启动后, maven找包的位置依次是 从上到下) 本机的本地仓库 远程仓库(公司私服) 中央仓库: http://mvnrepository.com/ 、 https://search.maven.org/、http://repo2.maven.org/maven2/ 安装配置 其实主流的开发工具如IDEA、Eclipse都集成了Maven(可见重要性), 但为了更加深刻的学习与管理该工具(比如多个IDE共享的问题), 建议还是单独安装比较好 下载地址 (Maven3必须JDK1.7以上) Mac 使用 brew install maven 安装后不用做什么配置 1234567➜ ~ mvn -versionApache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-18T02:33:14+08:00)Maven home: /usr/local/Cellar/maven/3.5.4/libexecJava version: 11.0.1, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/HomeDefault locale: zh_CN_#Hans, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;10.12.6&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;➜ ~ 本地仓储配置 如果不进行配置, 默认会在 /Users/renyimin/.m2/repository 存放从远程下载到的包 mac上brew安装maven后, setting.xml文件位于 /Users/renyimin/.m2/ （如果需要重新设置本地仓库的位置(打开maven安装目录, 打开conf目录下的setting.xml文件)进行配置）配置本地仓储的的位置: `` 你还可以在运行时指定本地仓库位置 mvn clean install -Dmaven.repo.local=yourpath 中央仓库配置 当构建一个Maven项目时, 首先检查 pom.xml 文件以确定依赖包的下载位置, 执行顺序如下： 从本地资源库中查找并获得依赖包, 如果没有, 执行第2步 从Maven默认中央仓库中查找并获得依赖包 (http://repo1.maven.org/maven2/), 如果没有, 执行第3步 如果在 pom.xml 中定义了自定义的远程仓库, 那么也会在这里的仓库中进行查找并获得依赖包, 如果都没有找到, 那么Maven就会抛出异常 可以修改默认中央仓库地址或增加新的镜像地址 常用地址 12345678- http://maven.aliyun.com/nexus/content/groups/public/ 阿里云 （强力推荐）- http://www.sonatype.org/nexus/ 私服nexus工具使用- http://mvnrepository.com/ （推荐）- http://repo1.maven.org/maven2- http://repo2.maven.org/maven2/- http://uk.maven.org/maven2/- http://repository.jboss.org/nexus/content/groups/public- http://maven.oschina.net/content/groups/public/ maven 项目标准目录结构传统Java项目中, 像是 测试部分, 配置部分可能并不需要被打包, 但是每个公司都有自己对这两部分的命名以便打包时可以不打包这些内容, 但这样比较混乱, 而maven有自己的标准规范: src/main/java 核心代码部分 src/main/resources 配置文件部分 src/test/java 测试代码部分 src/test/resources 测试代码的配置文件 src/main/webapp 页面资源, js,css,图片等 (Maven的web工程会有该目录) maven常用命令 mvn clean: 删除项目中的 target 目录 一般拿到别人的代码, 由于环境不一定一致, 所以一般先执行这个命令, 用自己的环境重新编译 mvn compile: 编译 src/main 下的代码, 生成 target 目录 mvn test: 该命令除了会编译 src/main 下的代码, 也会把 src/test/java 下的代码编译 mvn package: 会执行 mvn test, 然后会默认将项目打成 jar 包 (也可以在pom文件中配置, 打成 war 包) mvn install: 除了干了 mvn package 的活儿, 还把项目打好的包放到了本地 maven 仓库 maven deploy: 发布","categories":[{"name":"Maven","slug":"Maven","permalink":"http://blog.renyimin.com/categories/Maven/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://blog.renyimin.com/tags/Maven/"}]},{"title":"85. EXPLAIN -- select_type 列","slug":"MySQL 暂停/2019-04-11-mysql-85","date":"2019-04-11T03:21:53.000Z","updated":"2019-04-26T10:52:43.000Z","comments":true,"path":"2019/04/11/MySQL 暂停/2019-04-11-mysql-85/","link":"","permalink":"http://blog.renyimin.com/2019/04/11/MySQL 暂停/2019-04-11-mysql-85/","excerpt":"","text":"select_type列介绍 这一列显示了对应行是简单还是复杂SELECT, 如果是复杂SELECT, 则是三种复杂类型中的一种 上一篇已经介绍过: MySQL将select查询分为 简单 和 复杂 两种类型; 复杂类型又可以分成三大类: 简单子查询、所谓的派生表(在FROM子句中的子查询), 以及UNION查询 (DERIVED:派生, 衍生) select_type 列的常见值有: SIMPLE: 简单的select查询, 意味着查询中不包含 UNION 或 子查询 (比较常见: 参考EXPLAIN — id 列 简单查询示例) PRIMARY: 如果查询有任何复杂的子部分, 则最外层的查询会被标记为 PRIMARY (参考EXPLAIN — id 列 简单子查询示例) SUBQUERY: 在SELECT或WHERE列表中包含的子查询 (注意不是 FORM的子句) (参考EXPLAIN — id 列 简单子查询示例) DERIVED: 在FROM列表中包含的子查询被标记为DERIVED(衍生), MySQL会把结果放在临时表里 (参考EXPLAIN — id 列 派生表示例) UNION: 若第二个select出现在 UNION 之后, 则被标记为 UNION; 若UNION包含在FROM子句的子查询中, 外层SELECT将被标记为 DERIVED; (参考EXPLAIN — id 列 派生表示例) UNION RESULT: 用来从 UNION 的匿名临时表检索结果的SELECT被标记为 UNION RESULT (参考EXPLAIN — id 列 UNION查询示例) 除了上面的值, SUBQUERY 和 UNION 还可以被标记为 DEPENDENT 和 UNCACHEABLEDEPENDENT 意味着SELECT依赖于外层查询中发现的数据UNCACHEABLE 意味着SELECT中的某些特性组织结果被缓存与一个 Item_cache 中 DEPENDENT SUBQUERY: 参考P224关联子查询的优化 UNCACHEABLE SUBQUERY DEPENDENT UNION UNCACHEABLE UNION 测试表准备为了后面的测试工作, 接下来简单创建几张测试表 (表本身没有业务意义, 只是为了做测试) 测试接下来将会用几个示例来演示id列几种值的效果, 对于EXPLAIN结果, 我们先只关注 select_type 这一列 DEPENDENT SUBQUERY 参考 P224 关联子查询的优化 P223: MySQL5.6之前, 在子查询方面比较糟糕 如果是 select * from a where a.id in (1,2,3), 速度不会慢 但是如果写为 select * from a where a.id in (select id from b where b.id in (1,2,3)), mysql不会将 select id from b where b.id in (1,2,3) 这个内容查询出来, 放到in里 如果用explain分析, 则会看到 DEPENDENT SUBQUERY, mysql会将外层的结果先查询出来, 然后逐条执行子查询, 如果外层结果集很大, 这个就会很慢(可以参考下高性能mysql的查询性能优化章节, 有详细描述) 一般出现这种情况, 是需要进行优化的 123456789101112131415161718192021222324mysql&gt; show variables like &quot;version&quot;;+---------------+--------+| Variable_name | Value |+---------------+--------+| version | 5.5.62 |+---------------+--------+1 row in set (0.00 sec)mysql&gt; EXPLAIN SELECT customer_id FROM explain_order WHERE id = ( SELECT goods_id FROM explain_goods WHERE goods_number = &quot;693822310030&quot;);+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | explain_order | ALL | NULL | NULL | NULL | NULL | 1 | Using where || 2 | DEPENDENT SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+2 rows in set (0.00 sec)mysql&gt; EXPLAIN SELECT customer_id FROM explain_order WHERE id in ( SELECT goods_id FROM explain_goods WHERE goods_number = &quot;693822310030&quot;);+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | explain_order | ALL | NULL | NULL | NULL | NULL | 1 | Using where || 2 | DEPENDENT SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+2 rows in set (0.00 sec) 不过, 在 5.6+版本, 已经对上面的子查询做了很大的改进, 一般不会再出现 DEPENDENT SUBQUERY (但不是一定不出现, 如果出现需要进一步分析), 而是 SUBQUERY","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"","slug":"MySQL 暂停/50-bak","date":"2019-04-10T08:15:46.000Z","updated":"2019-04-10T08:15:46.000Z","comments":true,"path":"2019/04/10/MySQL 暂停/50-bak/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL 暂停/50-bak/","excerpt":"","text":"bak准备环境1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB, DEFAULT CHARSET = utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`))ENGINE = InnoDB,DEFAULT CHARSET = utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) select_type select_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 (最常见的查询类别就是 SIMPLE 了) PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 如果使用了UNION查询, 那么EXPLAIN 输出结果类似如下: 123456789101112mysql&gt; EXPLAIN ( SELECT * FROM user_info WHERE id IN ( 1, 2, 3 ) ) UNION( SELECT * FROM user_info WHERE id IN ( 3, 4, 5 ) );+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+3 rows in set (0.00 sec) mysql&gt; type type 字段比较重要, 它提供了判断查询是否高效的重要依据依据; 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等; type 常用的取值有: system: 表中只有一条数据, 这个类型是特殊的 const 类型; ?? const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据(const 查询速度非常快, 因为它仅仅读取一次即可) eq_ref: 此类型通常出现在多表的join查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果, 并且查询的比较操作通常是 =, 查询效率较高, 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using where; Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | NULL |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询, 例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5;+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+2 rows in set (0.00 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录; 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL (没用到索引), 并且 key_len 字段是此次查询中使用到的索引的最长的那个 1234567mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+1 row in set (0.00 sec) 下面对比, 都使用了范围查询, 但是一个可以使用索引范围查询, 另一个不能使用索引 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 另外, 可参考 P185: in语句虽然有时候 type结果也是range (不过, 对于真正的范围查询, 确实是无法使用范围列后面的其他索引了, 但是对于”多个等值条件查询”则没有这个限制) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过ALL类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据, 即 做的是覆盖索引, 当是这种情况时, Extra 字段会显示 Using index 下面的例子中, 查询的 name 字段恰好是一个索引(做到了覆盖索引), 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据;因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index; 1234567mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 下面不但使用了全索引扫描, 而且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;nihao&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) 但是, 如果不使用索引的话, 下面type就是ALL, 表示使用了全表扫描, 并且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age=10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 下面 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一, 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. type小结type 类型的性能比较 : 通常来说, 不同的 type 类型的性能关系如: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的; 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快; 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了; possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引;注意: 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到;(MySQL 在查询时具体使用了哪些索引, 由 key 字段决定) key此字段是 MySQL 在当前查询时所真正使用到的索引 rowsrows 也是一个重要的字段, MySQL 查询优化器根据统计信息, 估算SQL要查找到结果集需要到表中扫描读取的数据行数(上面的例子可以看到, 基本上使用到了索引的话, 真正扫描的行数都很少); 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好 ExtraExplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 比如下面, 使用索引扫描做排序 和 不使用索引扫描做排序 的效果: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY name;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY age;+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using filesort |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+1 row in set (0.00 sec) Using index 与 Using index condition “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 比如下面, 第一个做到了覆盖索引扫描, 后面两个都没做到 mysql&gt; EXPLAIN SELECT name FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name,age FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT * FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.","categories":[],"tags":[]},{"title":"84. EXPLAIN -- id 列","slug":"MySQL 暂停/2019-04-10-mysql-84","date":"2019-04-10T06:03:18.000Z","updated":"2019-04-26T10:49:39.000Z","comments":true,"path":"2019/04/10/MySQL 暂停/2019-04-10-mysql-84/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL 暂停/2019-04-10-mysql-84/","excerpt":"","text":"id 列介绍 EXPLAIN 的执行结果会返回一行或多行信息, 显示出执行计划中的的每一部分和执行顺序, 简单点说, 就是会显示出查询中执行表及其顺序, id列就是表示查询中 ‘表’ 的执行顺序 (‘表’的意思在这里比较广义, 可以是一个子查询, 一个UNION结果等等;) 如果查询是两个表的联接, 那么输出中将有两行, 别名表会单算为一个表 (如果把一个表与自己联接, 输出中也会有两行); MySQL将select查询分为 简单 和 复杂 两种类型; 复杂类型又可以分成三大类: 简单子查询、所谓的派生表(在FROM子句中的子查询), 以及UNION查询 (DERIVED:派生, 衍生) id列一般会有如下几种值: 简单查询: id 编号相同, 执行顺序由上至下 简单子查询: id的序号会递增, id值越大优先级越高, 越先被执行 派生表(FROM子句中的子查询): id 有相同的, 也有不同的, 相同的id可以认为是同一组, 从上往下顺序执行; id值大的, 优先级越高 UNION查询: id 为null, 当引用其他查询结果做union时, 该值为null, 且table列的值为 union(m,n), 意思是把id为m和n的查询结果做union 测试表准备为了后面的测试工作, 接下来简单创建几张测试表 (表本身没有业务意义, 只是为了做测试) t1表 12345CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; t2表 12345CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; t3表 12345CREATE TABLE `t3` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; 测试接下来将会用几个示例来演示id列几种值的效果, 对于EXPLAIN结果, 我们先只关注 id 这一列 简单查询 并行的表查询, id都是1 123456789mysql&gt; EXPLAIN SELECT * FROM t1, t2, t3 WHERE t1.id = t2.id and t1.id=t3.id and t1.name=&quot;test&quot;;+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | Using where || 1 | SIMPLE | t2 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | NULL || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | NULL |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) 简单子查询 id会递增 123456789mysql&gt; EXPLAIN SELECT * FROM t1 WHERE id=(SELECT id FROM t2 WHERE id = (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables || 2 | SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 3 | SUBQUERY | t3 | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+3 rows in set (0.00 sec) in 非简单子查询?? 注意, 这里只是把上面语句的 = 改为 in, 却发现 id 没有递增 123456789mysql&gt; EXPLAIN SELECT * FROM t1 WHERE id in (SELECT id FROM t2 WHERE id in (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | NULL || 1 | SIMPLE | t2 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | Using where |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) 派生表(FROM子句中的子查询) 下面 &lt;derived2&gt; 和 t2 都是1, 是因为 t2 和 是并行的一组, 而 &lt;derived2&gt;(derived是衍生的意思, 2是t2这一行的id值) 是衍生临时表 123456789mysql&gt; EXPLAIN SELECT * FROM (SELECT * FROM t2 WHERE name = &quot;test&quot;) as tmp2, t1 where t1.id=tmp2.id;+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+| 1 | PRIMARY | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | NULL || 1 | PRIMARY | &lt;derived2&gt; | ref | &lt;auto_key0&gt; | &lt;auto_key0&gt; | 4 | explain_test.t1.id | 2 | NULL || 2 | DERIVED | t2 | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) UNION查询 id 为null 123456789mysql&gt; explain select 1 union all select 1;+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || 2 | UNION | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+3 rows in set (0.00 sec) 疑问 in 非简单子查询?? (通过 explain extended + show warnings 分析, mysql的优化语句为 join 结构)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"83. EXPLAIN","slug":"MySQL 暂停/2019-04-10-mysql-83","date":"2019-04-10T05:20:29.000Z","updated":"2019-05-07T03:31:10.000Z","comments":true,"path":"2019/04/10/MySQL 暂停/2019-04-10-mysql-83/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL 暂停/2019-04-10-mysql-83/","excerpt":"","text":"参考 https://segmentfault.com/a/1190000017278335 进行补充 ExplainMySQL中的 explain 命令, 其主要功能是用来分析 select 语句的运行效果, 例如它可以获取select语句使用的索引情况、排序情况等等 语法: EXPLAIN [EXTENDED] SELECT select_options EXPLAIN 只能解释select查询, 并不会对存储程序调用和INSERT、UPDATE、DELETE或其他语句做解释 EXPLAIN 结果的表头如下MySQL 5.6.35123+----+-------------+--------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+------+-------+ MySQL 5.7.25 (多了两列)123+----+-------------+--------+------------+-------+---------------+-----+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+-------+---------------+-----+---------+------+------+----------+-------+ 测试准备 几张简陋的测试表 1234567891011121314151617CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8;CREATE TABLE `t3` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 几张简陋的测试表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253DROP TABLE IF EXISTS `explain_customer`;CREATE TABLE `explain_customer` ( `id` int(11) NOT NULL AUTO_INCREMENT, `customer_name` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_age` tinyint(3) NOT NULL DEFAULT &apos;0&apos;, `gender` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `identity_card_type` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `identity_card_no` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_phone` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_level` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `customer_email` varchar(50) NOT NULL DEFAULT &apos;&apos;, `status` int(11) NOT NULL DEFAULT &apos;0&apos;, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_unique_identity_card_no` (`identity_card_no`) USING BTREE COMMENT &apos;身份证号唯一&apos;, UNIQUE KEY `idx_unique_phone` (`customer_phone`) USING BTREE COMMENT &apos;手机号唯一&apos;, KEY `idx_unique_custome` (`customer_name`,`gender`,`customer_level`) USING BTREE COMMENT &apos;cms常用查询条件&apos;) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `explain_goods` ( `id` int(11) NOT NULL AUTO_INCREMENT, `goods_number` varchar(50) NOT NULL, `goods_name` varchar(100) NOT NULL, `goods_weight` int(11) NOT NULL, `goods_brand` varchar(50) NOT NULL, `stock` int(11) NOT NULL, `goods_status` tinyint(1) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_goods_number` (`goods_number`) USING BTREE, KEY `idx_goods` (`goods_name`,`goods_brand`,`goods_weight`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;CREATE TABLE `explain_order` ( `id` int(11) NOT NULL AUTO_INCREMENT, `customer_id` int(11) NOT NULL, `seller_id` int(11) NOT NULL, `goods_id` int(11) NOT NULL, `order_number` varchar(50) NOT NULL, `order_status` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `pay_status` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `seller_status` tinyint(1) NOT NULL COMMENT &apos;卖家备货状态&apos;, `delivery_type` tinyint(1) NOT NULL COMMENT &apos;配送类型&apos;, `delivery_id` int(11) NOT NULL COMMENT &apos;配送方id&apos;, `order_amount` decimal(12,2) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_order_number` (`order_number`) USING BTREE, KEY `idx_ocreated_at` (`created_at`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8; EXPLAIN EXTENDED其实除了 explain 以外, 其变种 explain extended 命令也非常有用, 它能够在原本explain的基础上额外的提供一些查询优化的信息, 这些信息可以通过mysql的 show warnings 命令得到, 执行分为两步: explain extended 你的SQL; show warnings; EXPLAIN EXTENDED 命令的表头如下: MySQL 5.6.35 (多了 filtered 列) 123+----+-------------+-------+------+---------------+-----+---------+------+------+----------+--------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------+---------------+-----+---------+------+------+----------+---------+ MySQL 5.7.25 (相对 EXPLAIN 来说, 表头没有变化) 123+----+-------------+-------+------------+--------+---------------+-----+---------+-----+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+-----+---------+-----+------+----------+-------+ 从 explain extend 的输出中, 你可以看到sql的执行方式, 对于分析sql还是很有帮助的 explain extended 除了能够告诉我们mysql的查询优化能做什么, 同时也能告诉我们mysql的查询优化做不了什么 比如 mysql的查询优化器不能将 id&gt;5 和 id&gt;6 这两个查询条件优化合并成一个 id&gt;6 (参考)1234567891011121314mysql&gt; EXPLAIN EXTENDED SELECT * FROM t1 WHERE id &gt; 5 AND id &gt; 6;+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 1 | 100.00 | Using where |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+1 row in set (0.00 sec)mysql&gt; show warnings;+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Note | 1003 | /* select#1 */ select `explain_test`.`t1`.`id` AS `id`,`explain_test`.`t1`.`name` AS `name` from `explain_test`.`t1` where ((`explain_test`.`t1`.`id` &gt; 5) and (`explain_test`.`t1`.`id` &gt; 6)) |+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 注意: 从 EXPLAIN extended + show warnings 得到 优化以后 的查询语句可能还不是最终优化执行的sql, 或者说explain extended看到的信息还不足以说明mysql最终对查询语句优化的结果 (?? https://www.jianshu.com/p/7656b114e783) EXPLAIN EXTENDED 示例如下在一个 简单的子查询中, 使用 = 和 使用 in 的结果截然不同 使用 = 时, 可以看到, 3条执行记录的id是递增的, 是有两个子句的执行, 最后才执行了外层的查询 (表中无数据) 12345678910111213141516171819// 当表中没有数据时mysql&gt; EXPLAIN extended SELECT * FROM t1 WHERE id = (SELECT id FROM t2 WHERE id = (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 2 | SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 3 | SUBQUERY | t3 | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+3 rows in set, 2 warnings (0.00 sec)mysql&gt; show warnings;+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Warning | 1681 | &apos;EXTENDED&apos; is deprecated and will be removed in a future release. || Note | 1003 | /* select#1 */ select NULL AS `id`,NULL AS `name` from `test`.`t1` where multiple equal((/* select#2 */ select NULL from `test`.`t2` where multiple equal((/* select#3 */ select `test`.`t3`.`id` from `test`.`t3` where (`test`.`t3`.`name` = &apos;Lant3&apos;)), NULL)), NULL) |+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) 而在使用 in 时, 可以看到, id 列的值都是1, 即为并行执行的, 使用 show warnings 查看后发现, MySQL将语句优化成了 join 结构 123456789101112131415161718mysql&gt; EXPLAIN extended SELECT * FROM t1 WHERE id in (SELECT id FROM t2 WHERE id in (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+| 1 | SIMPLE | t1 | NULL | ALL | PRIMARY | NULL | NULL | NULL | 1 | 100.00 | NULL || 1 | SIMPLE | t2 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | 100.00 | Using index || 1 | SIMPLE | t3 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | 100.00 | Using where |+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+3 rows in set, 2 warnings (0.00 sec)mysql&gt; show warnings;+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Warning | 1681 | &apos;EXTENDED&apos; is deprecated and will be removed in a future release. || Note | 1003 | /* select#1 */ select `test`.`t1`.`id` AS `id`,`test`.`t1`.`name` AS `name` from `test`.`t3` join `test`.`t2` join `test`.`t1` where ((`test`.`t2`.`id` = `test`.`t1`.`id`) and (`test`.`t3`.`id` = `test`.`t1`.`id`) and (`test`.`t3`.`name` = &apos;Lant3&apos;)) |+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) EXPLAIN PARTITIONSEXPLAIN 结果列说明跳转到 id 列跳转到 select_type列table 列 显示这一行显示了对应行正在访问哪个表, 在通常情况下, 它相当明了: 它就是那个表, 或是该表的别名(如果sql中定义了别名) 当FROM子句中有子查询或者有UNION时, table列会变得复杂得多, 在这些场景下, 确实没有一个 “表” 可以参考到, 因为 MySQL 创建的匿名临时表仅在查询执行过程中存在 当在FROM子句中有子查询时, table列是 的形式, 其中 N 是子查询的id 当有 UNION 时, UNION RESULT 的table列包含一个参与UNION的 id 列表 ( ) 跳转到 type 列 这一列显示了 MySQL 是决定如何查找表中的行, 下面是最重要的访问方法, 依次从最差到最优 possible_keys, key possible_keys: 显示可能应用在这张表中的索引, 一个或多个; 查询涉及到的字段上若存在索引, 则该索引将被列出, 但不一定被查询实际使用 key : 实际使用的索引, 如果为NULL, 表示没有使用索引; 如果查询中使用了覆盖索引, 则该索引仅出现在key列表中 跳转到 key_lenref 显示索引的哪一列被使用了, 如果可能的话, 是一个常数 (哪些列或常量被用于查找索引列上的值) 示例 12345678mysql&gt; explain SELECT t1.id,t3.id FROM t1 join t3 on t1.id=t3.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| 1 | SIMPLE | t1 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | Using index |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+2 rows in set (0.00 sec) 示例 1234567mysql&gt; explain SELECT * from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) rows 根据表统计信息及索引选用情况, 大致估算出找到所需的记录所需要读取的行数, 这是MySQL认为它要检查的行数, 而不是结果集里的行数; 对于InnoDB表, 此数字是估计值, 可能并不总是准确的 … 跳转到 Extra 列","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"47. Java IO 体系","slug":"JavaSE/2019-04-09-47","date":"2019-04-09T12:17:56.000Z","updated":"2019-11-08T10:42:49.000Z","comments":true,"path":"2019/04/09/JavaSE/2019-04-09-47/","link":"","permalink":"http://blog.renyimin.com/2019/04/09/JavaSE/2019-04-09-47/","excerpt":"","text":"普通IO普通IO? BIO (同步阻塞) 在JDK1.4以前, 使用Java建立网络连接时, 只能采用BIO方式 BIO 即 Block-IO (阻塞IO), 主要是说, 当一个线程调用 read() 或 write() 时, 直到有数据被读取或写入到操作系统内核前, 该线程不能执行其他任务, 即 线程会被阻塞; 因此, 在这种模式下, 要进行并发, 我们只能增加线程数, 服务器端必须为每个客户端都提供一个独立的线程进行处理, 当服务器端需要处理大量客户端时, 性能就会急剧下降(线程的开销比较大, 数量有限, 因此服务器同时能处理的连接数很低); 一个容易让初学者迷惑的 BIO小DEMO 启动服务器端后, 程序便阻塞等待了客户端连接了 然后启动客户端, 服务器连接成功后直接打印出了客户端发来的数据 所以整个过程非常容易让人误认为整个程序的阻塞点只是在 serverSocket.accept(), 即, 服务端等待客户端连接时 新DEMO: 可以直观地看到 (我们重点讨论的) 阻塞点, 即 数据从内核空间 拷贝 到用户空间时的 阻塞 (当内核空间数据未准备好时) 单线程 + 同步BIO 问题: 单线程+同步BIO 有两处阻塞: serverSocket.accept() 和 inputStream.read(b) 第一处阻塞会造成: 客户端1连接上来后, 给服务端发送数据并被处理后, 他再次发送数据给服务端时, 由于服务端阻塞在 serverSocket.accept(), 是无法接收到该消息的 第二处阻塞会造成: 客户端1连接上来后, 如果不发送数据并被服务端处理, 其他客户端是连接不上来的 小结: 当采用如上这种单线程的方式, 是无法处理并发的 (客户端连接上之后,就得发送数据, 服务器端处理完一个客户端之后, 才能有新的客户端连接上来) 多线程 + 同步阻塞每个客户端连接到服务器时, 服务器都会为该客户端创建一个新的线程(用来接收客户端发来的数据并进行处理), 所以 同步阻塞的代码段 inputStream.read(b); 发生在了每个子线程内, 这样主线程就只负责阻塞等待客户端连接了 (使用 多线程+同步阻塞IO 的模式时, 此处的阻塞是自然要做的, 这确实不是我们重点关注的阻塞点)还可以使用可视化窗口查看进程中的线程数 在高并发场景下, 为每个任务(用户请求)创建一个进程或线程的开销非常大; 多线程编程的复杂度也比较高; 那么是否可以使用单线程进行并发处理? 单线程 非阻塞IO这里的 非阻塞IO 其实说的是 IO多路复用, 因为传统非阻塞IO是在用户空间进行轮询, 而 IO多路复用 是在内核进行轮询, 所以看上去 IO多路复用 对用户程序来说是阻塞的,而 非阻塞IO对用户程序是非阻塞的, 其实 IO多路复用是将非阻塞的位置移到了内核 要让单线程来并发处理多个客户端请求, 首先要知道, 之前的 单线程+同步BIO 有两处阻塞: serverSocket.accept() 和 inputStream.read(b) 第一处阻塞会造成: 客户端1连接上来后, 给服务端发送数据并被处理后, 他再次发送数据给服务端时, 由于服务端阻塞在 serverSocket.accept(), 是无法接收到该消息的 第二处阻塞会造成: 客户端1连接上来后, 如果不发送数据并被服务端处理, 其他客户端是连接不上来的 要解决这个问题, 那就得让 以上两处都为非阻塞, 即, “没有新的客户端连接时,旧的客户端仍然能发送消息”, “旧的客户端不发送消息时, 新的客户端也可以连接上来” Java NIO概述JAVA NIO有两种解释: 一种叫 非阻塞IO (Non-blocking I/O), 另一种也叫 新的IO(New I/O), 其实是同一个概念, 它是一种同步非阻塞的I/O模型, 也是I/O多路复用的基础(IO多路复用里面也得是非阻塞才行啊), 已经被越来越多地应用到大型应用服务器, 成为解决高并发与大量连接、I/O处理问题的有效方式 可以将NIO简单区分为两种：普通的NIO，和多路复用的NIO（加入了selector管理）(https://www.jianshu.com/p/8ad464ed516e, https://my.oschina.net/ljhlgj/blog/1811319) IO多路复用阻塞不阻塞的问题 https://cloud.tencent.com/developer/article/1121736 NIO vs IO区别 IO是面向流的, NIO是面向缓冲区的 Java IO面向流意味着每次从流中读一个或多个字节, 直至读取所有字节, 它们没有被缓存在任何地方 NIO则能前后移动流中的数据, 因为是面向缓冲区的 IO流是阻塞的, NIO流是不阻塞的 Java IO的各种流是阻塞的。这意味着, 当一个线程调用read() 或 write()时, 该线程被阻塞, 直到有一些数据被读取, 或数据完全写入。该线程在此期间不能再干任何事情了 Java NIO的非阻塞模式, 使一个线程从某通道发送请求读取数据, 但是它仅能得到目前可用的数据, 如果目前没有数据可用时, 就什么都不会获取NIO可让您使用单线程即可管理多个通道(网络连接或文件), 但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂 选择器Java NIO的选择器允许一个单独的线程来监视多个输入通道, 你可以注册多个通道使用一个选择器, 然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入, 或者选择已准备写入的通道。这种选择机制, 使得一个单独的线程很容易来管理多个通道。 Java NIO 组件 NIO主要有三大核心部分: Channel(通道), Buffer(缓冲区), Selector(选择器) 传统IO是基于字节流和字符流进行操作(基于流), 而NIO是基于Channel和Buffer(缓冲区)进行操作, 数据总是从通道读取到缓冲区中, 或者从缓冲区写入到通道中 Selector(选择器) 用于监听多个通道的 事件(如连接打开, 数据到达), 因此, 单个线程可以监听多个数据通道 Buffer Buffer(缓冲区) 是一个用于存储特定基本类型数据的容器。除了boolean外, 其余每种基本类型都有一个对应的buffer类 Buffer类的子类有 ByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer Channel Channel(通道) 表示到实体, 如硬件设备、文件、网络套接字或可以执行一个或多个不同 I/O 操作(如读取或写入) 的程序组件的开放的连接 Channel接口的常用实现类有 FileChannel(对应文件IO) 、DatagramChannel(对应UDP) 、SocketChannel 和 ServerSocketChannel(对应TCP的客户端和服务器端) Channel和IO中的Stream(流)是差不多一个等级的。只不过Stream是单向的, 譬如：InputStream, OutputStream.而Channel是双向的, 既可以用来进行读操作, 又可以用来进行写操作 SelectorSelector(选择器) 用于监听多个通道的 事件(如连接打开, 数据到达), 因此, 单个的线程可以监听多个数据通道。即用选择器, 借助单一线程, 就可对数量庞大的活动I/O通道实施监控和维护 Java NIO 实现 单线程 非阻塞IONIO是针对服务端的, 客户端没有NIO的概念 上述例子中, 一旦并发很高, 那么 socketChannelList 将会变得非常大, 所以其实不应该将轮询交给应用系统来进行 (交给操作系统吧) Java NIO 实现 多路复用Selector(选择器)是Java NIO中能够同时监测多个Channel通道, 并且还能知道Channel上读写事件是否准备好。这样一个Selector线程就可以管理多个Channel, 而不像Blocking IO那样一个线程对应一个监管一个IO事件。 BIO, NIO, AIO区别http://www.imooc.com/article/265871","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"46. 两种I/O多路复用模式 Reactor, Proactor","slug":"JavaSE/2019-04-09-46","date":"2019-04-09T11:52:37.000Z","updated":"2019-11-07T10:45:15.000Z","comments":true,"path":"2019/04/09/JavaSE/2019-04-09-46/","link":"","permalink":"http://blog.renyimin.com/2019/04/09/JavaSE/2019-04-09-46/","excerpt":"","text":"概述 在高性能的I/O设计中, 有两个比较著名的模式 Reactor 和 Proactor (其中 Reactor 模式用于同步I/O操作, 而 Proactor 模式则运用于异步I/O操作) 这两种模式的 中心思想 即是众所周知的 I/O 多路复用 (select、poll、epoll、kqueue 等) 这两种模式中都需要一个 事件多路分解器(Event Demultiplexer) 的支持, 它可以将来自事件源的I/O事件 分发到对应的 事件处理器(Event Handler)上 (比如Java中的 Selector 支持, 操作系统的 select 系统调用支持, 如果要自己实现Synchronous Event Demultiplexer可能不会有那么高效) 这里完全搞不清楚, 事件多路分解器 和 dispatcher 到底哪个是用来分发事件的 ? Reactor Reactor模式是处理并发I/O比较常见的一种模式, 中心思想就是 将所有 需要处理的事件 及 事件处理器(或回调函数) 注册到一个中心I/O多路复用器上, 同时主线程阻塞在多路复用器上; 一旦有I/O事件到来或是准备就绪(区别在于多路复用器是边沿触发还是水平触发), 多路复用器返回并将相应I/O事件分发到对应的处理器中 Reactor模型有三个重要的组件: 事件多路分解器: 将多路复用器中返回的就绪事件分到对应的处理函数中 多路复用器(事件多路分解器内部使用的): 由操作系统提供, 在linux上一般是select, poll, epoll等系统调用 事件处理器: 负责处理特定事件的处理函数 Reactor模型类图 http://www.blogjava.net/DLevin/archive/2015/09/02/427045.html ? Handle: 即操作系统中的句柄, 是对资源在操作系统层面上的一种抽象, 它可以是打开的文件、一个连接(Socket)、Timer等 (由于Reactor模式一般使用在网络编程中, 因而这里一般指Socket Handle, 即一个网络连接(Connection, 在Java NIO中的Channel), 这个Channel注册到 Synchronous Event Demultiplexer(事件分解器) 中, 以监听Handle中发生的事件 对ServerSocketChannnel可以是CONNECT事件, 对SocketChannel可以是READ、WRITE、CLOSE等事件 Synchronous Event Demultiplexer: 阻塞等待一系列的 Handle 中的事件到来, 这个模块一般使用操作系统的 select, poll, epoll 来实现, 在Java NIO中用Selector来封装, 当Selector.select()返回时, 可以调用Selector的selectedKeys()方法获取Set, 一个SelectionKey表达一个有事件发生的Channel以及该Channel上的事件类型 同步事件多路分解器一般都是由操作系统来支持, 如支持IO复用的select,epoll,poll等, 通过监听IO事件集合, 根据事件类型将具体事件交由具体 (如, nginx 在 windows 只支持 select 不支持 epoll, epoll 是内核层面的东西,Windows 是不可支持的) Initiation Dispatcher: 用于管理 Event Handler, 即 EventHandler 的容器, 用以注册、移除EventHandler等, 另外, 它还作为Reactor模式的入口调用 Synchronous Event Demultiplexer 的select方法以阻塞等待事件返回, 当阻塞等待返回时, 根据事件发生的Handle将其分发给对应的Event Handler处理, 即回调EventHandler中的handle_event()方法 Event Handler: 定义事件处理方法 handle_event(), 以供InitiationDispatcher回调使用 Concrete Event Handler: 事件EventHandler接口, 实现特定事件处理逻辑 模块交互 1）我们注册Concrete Event Handler到Initiation Dispatcher中。 2）Initiation Dispatcher调用每个Event Handler的get_handle接口获取其绑定的Handle。 3）Initiation Dispatcher调用handle_events开始事件处理循环。在这里，Initiation Dispatcher会将步骤2获取的所有Handle都收集起来，使用Synchronous Event Demultiplexer来等待这些Handle的事件发生。 4）当某个（或某几个）Handle的事件发生时，Synchronous Event Demultiplexer通知Initiation Dispatcher。5）Initiation Dispatcher根据发生事件的Handle找出所对应的Handler。 6）Initiation Dispatcher调用Handler的handle_event方法处理事件。 单Reactor单线程模型 Reactor线程负责多路分离套接字, accept新连接, 并分派请求到handler (Redis使用单Reactor单进程的模型) 消息处理流程： Reactor 对象通过 select 监控连接事件, 收到事件后通过 dispatch 进行转发 如果是连接建立的事件, 则由acceptor接受连接, 并创建handler处理后续事件 如果不是建立连接事件, 则Reactor会分发调用Handler来响应 handler会完成 read-&gt;业务处理-&gt;send的完整业务流程 单Reactor单线程模型只是在代码上进行了组件的区分, 但是整体操作还是单线程, 不能充分利用硬件资源。handler业务处理部分没有异步。 对于一些小容量应用场景, 可以使用单Reactor单线程模型。但是对于高负载、大并发的应用场景却不合适, 主要原因如下： 即便Reactor线程的CPU负荷达到100%, 也无法满足海量消息的编码、解码、读取和发送 当Reactor线程负载过重之后, 处理速度将变慢, 这会导致大量客户端连接超时, 超时之后往往会进行重发, 这更加重Reactor线程的负载, 最终会导致大量消息积压和处理超时, 成为系统的性能瓶颈 一旦Reactor线程意外中断或者进入死循环, 会导致整个系统通信模块不可用, 不能接收和处理外部消息, 造成节点故障为了解决这些问题, 演进出 单Reactor多线程模型 单Reactor多线程模型 该模型在事件处理器(Handler)部分采用了多线程(线程池) 消息处理流程 Reactor对象通过Select监控客户端请求事件, 收到事件后通过dispatch进行分发 如果是建立连接请求事件, 则由acceptor通过accept处理连接请求, 然后创建一个Handler对象处理连接完成后续的各种事件 如果不是建立连接事件, 则Reactor会分发调用连接对应的Handler来响应 Handler只负责响应事件, 不做具体业务处理, 通过Read读取数据后, 会分发给后面的Worker线程池进行业务处理 Worker线程池会分配独立的线程完成真正的业务处理, 将响应结果发给Handler进行处理 Handler收到响应结果后通过send将响应结果返回给Client 相对于第一种模型来说, 在处理业务逻辑, 也就是获取到IO的读写事件之后, 交由线程池来处理, handler收到响应后通过send将响应结果返回给客户端, 这样可以降低Reactor的性能开销, 从而更专注的做事件分发工作了, 提升整个应用的吞吐 但是这个模型存在的问题： 多线程数据共享和访问比较复杂, 如果子线程完成业务处理后, 把结果传递给主线程Reactor进行发送, 就会涉及共享数据的互斥和保护机制 Reactor承担所有事件的监听和响应, 只在主线程中运行, 可能会存在性能问题 (例如并发百万客户端连接, 或者服务端需要对客户端握手进行安全认证, 但是认证本身非常损耗性能) 为了解决性能问题, 产生了第三种主从Reactor多线程模型 主从Reactor多线程模型 比起第二种模型, 它是将Reactor分成两部分: mainReactor 负责监听 server socket, 用来处理网络IO连接建立操作, 将建立的socketChannel指定注册给subReactor subReactor 主要做和建立起来的socket做数据交互和事件业务处理操作, 通常, subReactor 个数上可与CPU个数等同 (Nginx、Swoole、Memcached和Netty都是采用这种实现) 消息处理流程： 从主线程池中随机选择一个Reactor线程作为acceptor线程, 用于绑定监听端口, 接收客户端连接 acceptor 线程接收客户端连接请求之后创建新的 SocketChannel, 将其注册到主线程池的其它 Reactor 线程上, 由其负责接入认证、IP黑白名单过滤、握手等操作 步骤2完成之后, 业务层的链路正式建立, 将 SocketChannel 从主线程池的 Reactor线程的多路复用器上摘除, 重新注册到Sub线程池的线程上, 并创建一个Handler用于处理各种连接事件 当有新的事件发生时, SubReactor 会调用连接对应的Handler进行响应 Handler 通过Read读取数据后, 会分发给后面的Worker线程池进行业务处理 Worker 线程池会分配独立的线程完成真正的业务处理, 如何将响应结果发给Handler进行处理 Handler 收到响应结果后通过Send将响应结果返回给Client 小结Reactor模型具有如下的优点: 响应快, 不必为单个同步时间所阻塞, 虽然Reactor本身依然是同步的 编程相对简单, 可以最大程度的避免复杂的多线程及同步问题, 并且避免了多线程/进程的切换开销 可扩展性, 可以方便地通过增加 Reactor 实例个数来充分利用CPU资源 可复用性, Reactor 模型本身与具体事件处理逻辑无关, 具有很高的复用性 常见架构的进程/线程模型Netty 的线程模型Tomcat 的线程模型Nginx 的进程模型Redis 的线程模型Swoole 的进程模型参考https://www.cnblogs.com/wskwbog/p/10725372.htmlhttps://cloud.tencent.com/developer/article/1488120 Reactor 模式 说明你可以进行读写操作了 Reactor关注的是I/O操作的就绪事件, 而Proactor关注的是I/O操作的完成事件Proactor Proactor模式下说明已经完成读写操作了, 具体内容在给定缓冲区中, 可以对这些内容进行其他操作了 在Reactor中, 事件分离器负责等待文件描述符或socket为读写操作准备就绪, 然后将就绪事件传递给对应的处理器, 最后由处理器负责完成实际的读写工作。 而在Proactor模式中, 处理器或者兼任处理器的事件分离器, 只负责发起异步读写操作。IO操作本身由操作系统来完成。传递给操作系统的参数需要包括用户定义的数据缓冲区地址和数据大小, 操作系统才能从中得到写出操作所需数据, 或写入从socket读到的数据。事件分离器捕获IO操作完成事件, 然后将事件传递给对应处理器。比如, 在windows上, 处理器发起一个异步IO操作, 再由事件分离器等待IOCompletion事件。典型的异步模式实现, 都建立在操作系统支持异步API的基础之上, 我们将这种实现称为“系统级”异步或“真”异步, 因为应用程序完全依赖操作系统执行真正的IO工作。 Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的, Reactor中需要应用程序自己读取或者写入数据, 而Proactor模式中, 应用程序不需要进行实际的读写过程, 它只需要从缓存区读取或者写入即可, 操作系统会读取缓存区或者写入缓存区到真正的IO设备. php 实现 reactorhttps://www.jianshu.com/p/0bc0830875bc https://blog.csdn.net/u014730165/article/details/85044285 ？ https://blog.csdn.net/u014730165/article/details/85089085 proactor 不是真正意义上的异步https://blog.csdn.net/rain_qingtian/article/details/11826617","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"46. 两种I/O多路复用模式 Reactor, Proactor","slug":"nginx/2019-11-07-08-IO","date":"2019-04-09T11:52:37.000Z","updated":"2019-11-08T10:41:39.000Z","comments":true,"path":"2019/04/09/nginx/2019-11-07-08-IO/","link":"","permalink":"http://blog.renyimin.com/2019/04/09/nginx/2019-11-07-08-IO/","excerpt":"","text":"概述 在高性能的I/O设计中, 有两个比较著名的模式 Reactor 和 Proactor 常见的说法是: Reactor 模式用于同步I/O操作, 而 Proactor 模式则运用于异步I/O操作; 但事实上 Proactor 并不是实际意义上的异步(并不是UNIX IO模型中的aio); 这两种模式的 中心思想 都是众所周知的 I/O 多路复用 (select、poll、epoll、kqueue 等) Reactor Reactor模式是处理并发I/O比较常见的一种模式, 中心思想就是 将所有 需要处理的事件 及 事件处理器(或回调函数) 注册到一个中心I/O多路复用器上, 同时主线程阻塞在这个 中心多路复用器上; 一旦有I/O事件到来或是准备就绪(区别在于多路复用器是边沿触发还是水平触发), 多路复用器返回并将相应I/O事件分发到对应的处理器中 Reactor模型有三个重要的组件: 事件多路分解器: 将多路复用器中返回的就绪事件分到对应的处理函数中 多路复用器(事件多路分解器内部使用的): 由操作系统提供, 在linux上一般是select, poll, epoll等系统调用 事件处理器: 负责处理特定事件的处理函数 Reactor模型类图 http://www.blogjava.net/DLevin/archive/2015/09/02/427045.html ? Handle: 即操作系统中的句柄, 是对资源在操作系统层面上的一种抽象, 它可以是打开的文件、一个连接(Socket)、Timer等 (由于Reactor模式一般使用在网络编程中, 因而这里一般指Socket Handle, 即一个网络连接(Connection, 在Java NIO中的Channel), 这个Channel注册到 Synchronous Event Demultiplexer(事件分解器) 中, 以监听Handle中发生的事件 对ServerSocketChannnel可以是CONNECT事件, 对SocketChannel可以是READ、WRITE、CLOSE等事件 Synchronous Event Demultiplexer: 阻塞等待一系列的 Handle 中的事件到来, 这个模块一般使用操作系统的 select, poll, epoll 来实现, 在Java NIO中用Selector来封装, 当Selector.select()返回时, 可以调用Selector的selectedKeys()方法获取Set, 一个SelectionKey表达一个有事件发生的Channel以及该Channel上的事件类型 同步事件多路分解器一般都是由操作系统来支持, 如支持IO复用的select,epoll,poll等, 通过监听IO事件集合, 根据事件类型将具体事件交由具体 (如 nginx 在 windows 只支持 select 不支持 epoll, epoll 是内核层面的东西,Windows 是不可支持的) Initiation Dispatcher: 用于管理 Event Handler, 即 EventHandler 的容器, 用以注册、移除EventHandler等, 另外, 它还作为Reactor模式的入口调用 Synchronous Event Demultiplexer 的select方法以阻塞等待事件返回, 当阻塞等待返回时, 根据事件发生的Handle将其分发给对应的Event Handler处理, 即回调EventHandler中的handle_event()方法 Event Handler: 定义事件处理方法 handle_event(), 以供InitiationDispatcher回调使用 Concrete Event Handler: 事件EventHandler接口, 实现特定事件处理逻辑 模块交互 1）我们注册Concrete Event Handler到Initiation Dispatcher中。 2）Initiation Dispatcher调用每个Event Handler的get_handle接口获取其绑定的Handle。 3）Initiation Dispatcher调用handle_events开始事件处理循环。在这里，Initiation Dispatcher会将步骤2获取的所有Handle都收集起来，使用Synchronous Event Demultiplexer来等待这些Handle的事件发生。 4）当某个（或某几个）Handle的事件发生时，Synchronous Event Demultiplexer通知Initiation Dispatcher。5）Initiation Dispatcher根据发生事件的Handle找出所对应的Handler。 6）Initiation Dispatcher调用Handler的handle_event方法处理事件。 单Reactor单线程模型 Reactor线程负责多路分离套接字, accept新连接, 并分派请求到handler (Redis使用单Reactor单进程的模型) 消息处理流程： Reactor 对象通过 select 监控连接事件, 收到事件后通过 dispatch 进行转发 如果是连接建立的事件, 则由acceptor接受连接, 并创建handler处理后续事件 如果不是建立连接事件, 则Reactor会分发调用Handler来响应 handler会完成 read-&gt;业务处理-&gt;send的完整业务流程 单Reactor单线程模型只是在代码上进行了组件的区分, 但是整体操作还是单线程, 不能充分利用硬件资源。handler业务处理部分没有异步。 对于一些小容量应用场景, 可以使用单Reactor单线程模型。但是对于高负载、大并发的应用场景却不合适, 主要原因如下： 即便Reactor线程的CPU负荷达到100%, 也无法满足海量消息的编码、解码、读取和发送 当Reactor线程负载过重之后, 处理速度将变慢, 这会导致大量客户端连接超时, 超时之后往往会进行重发, 这更加重Reactor线程的负载, 最终会导致大量消息积压和处理超时, 成为系统的性能瓶颈 一旦Reactor线程意外中断或者进入死循环, 会导致整个系统通信模块不可用, 不能接收和处理外部消息, 造成节点故障为了解决这些问题, 演进出 单Reactor多线程模型 单Reactor多线程模型 该模型在事件处理器(Handler)部分采用了多线程(线程池) 消息处理流程 Reactor对象通过Select监控客户端请求事件, 收到事件后通过dispatch进行分发 如果是建立连接请求事件, 则由acceptor通过accept处理连接请求, 然后创建一个Handler对象处理连接完成后续的各种事件 如果不是建立连接事件, 则Reactor会分发调用连接对应的Handler来响应 Handler只负责响应事件, 不做具体业务处理, 通过Read读取数据后, 会分发给后面的Worker线程池进行业务处理 Worker线程池会分配独立的线程完成真正的业务处理, 将响应结果发给Handler进行处理 Handler收到响应结果后通过send将响应结果返回给Client 相对于第一种模型来说, 在处理业务逻辑, 也就是获取到IO的读写事件之后, 交由线程池来处理, handler收到响应后通过send将响应结果返回给客户端, 这样可以降低Reactor的性能开销, 从而更专注的做事件分发工作了, 提升整个应用的吞吐 但是这个模型存在的问题： 多线程数据共享和访问比较复杂, 如果子线程完成业务处理后, 把结果传递给主线程Reactor进行发送, 就会涉及共享数据的互斥和保护机制 Reactor承担所有事件的监听和响应, 只在主线程中运行, 可能会存在性能问题 (例如并发百万客户端连接, 或者服务端需要对客户端握手进行安全认证, 但是认证本身非常损耗性能) 为了解决性能问题, 产生了第三种主从Reactor多线程模型 主从Reactor多线程模型 比起第二种模型, 它是将Reactor分成两部分: mainReactor 负责监听 server socket, 用来处理网络IO连接建立操作, 将建立的socketChannel指定注册给subReactor subReactor 主要做和建立起来的socket做数据交互和事件业务处理操作, 通常, subReactor 个数上可与CPU个数等同 (Nginx、Swoole、Memcached和Netty都是采用这种实现) 消息处理流程： 从主线程池中随机选择一个Reactor线程作为acceptor线程, 用于绑定监听端口, 接收客户端连接 acceptor 线程接收客户端连接请求之后创建新的 SocketChannel, 将其注册到主线程池的其它 Reactor 线程上, 由其负责接入认证、IP黑白名单过滤、握手等操作 步骤2完成之后, 业务层的链路正式建立, 将 SocketChannel 从主线程池的 Reactor线程的多路复用器上摘除, 重新注册到Sub线程池的线程上, 并创建一个Handler用于处理各种连接事件 当有新的事件发生时, SubReactor 会调用连接对应的Handler进行响应 Handler 通过Read读取数据后, 会分发给后面的Worker线程池进行业务处理 Worker 线程池会分配独立的线程完成真正的业务处理, 如何将响应结果发给Handler进行处理 Handler 收到响应结果后通过Send将响应结果返回给Client 小结Reactor模型具有如下的优点: 响应快, 不必为单个同步时间所阻塞, 虽然Reactor本身依然是同步的 编程相对简单, 可以最大程度的避免复杂的多线程及同步问题, 并且避免了多线程/进程的切换开销 可扩展性, 可以方便地通过增加 Reactor 实例个数来充分利用CPU资源 可复用性, Reactor 模型本身与具体事件处理逻辑无关, 具有很高的复用性 常见架构的进程/线程模型Netty 的线程模型Tomcat 的线程模型Nginx 的进程模型Redis 的线程模型Swoole 的进程模型参考https://www.cnblogs.com/wskwbog/p/10725372.htmlhttps://cloud.tencent.com/developer/article/1488120 Reactor 模式 说明你可以进行读写操作了 Reactor关注的是I/O操作的就绪事件, 而Proactor关注的是I/O操作的完成事件Proactor Proactor模式下说明已经完成读写操作了, 具体内容在给定缓冲区中, 可以对这些内容进行其他操作了 在Reactor中, 事件分离器负责等待文件描述符或socket为读写操作准备就绪, 然后将就绪事件传递给对应的处理器, 最后由处理器负责完成实际的读写工作。 而在Proactor模式中, 处理器或者兼任处理器的事件分离器, 只负责发起异步读写操作。IO操作本身由操作系统来完成。传递给操作系统的参数需要包括用户定义的数据缓冲区地址和数据大小, 操作系统才能从中得到写出操作所需数据, 或写入从socket读到的数据。事件分离器捕获IO操作完成事件, 然后将事件传递给对应处理器。比如, 在windows上, 处理器发起一个异步IO操作, 再由事件分离器等待IOCompletion事件。典型的异步模式实现, 都建立在操作系统支持异步API的基础之上, 我们将这种实现称为“系统级”异步或“真”异步, 因为应用程序完全依赖操作系统执行真正的IO工作。 Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的, Reactor中需要应用程序自己读取或者写入数据, 而Proactor模式中, 应用程序不需要进行实际的读写过程, 它只需要从缓存区读取或者写入即可, 操作系统会读取缓存区或者写入缓存区到真正的IO设备. php 实现 reactorhttps://www.jianshu.com/p/0bc0830875bc https://blog.csdn.net/u014730165/article/details/85044285 ？ https://blog.csdn.net/u014730165/article/details/85089085 proactor 不是真正意义上的异步https://blog.csdn.net/rain_qingtian/article/details/11826617 I/O模型之四：Java 浅析I/O模型（BIO、NIO、AIO、Reactor、Proactor）https://www.cnblogs.com/duanxz/p/5150973.html","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.renyimin.com/tags/nginx/"}]},{"title":"45. IO 多路复用 select 、poll、epoll","slug":"JavaSE/2019-04-09-45","date":"2019-04-09T11:31:15.000Z","updated":"2019-09-12T09:15:39.000Z","comments":true,"path":"2019/04/09/JavaSE/2019-04-09-45/","link":"","permalink":"http://blog.renyimin.com/2019/04/09/JavaSE/2019-04-09-45/","excerpt":"","text":"之前在学习Linux下的5种IO模型时, 已经简单介绍了 IO多路复用, 接下来对 select、poll、epoll 分别进行学习 linux系统调用表(system call table) 概述 Linux 支持 IO多路复用 的 系统调用有 select、poll、epoll, 这些调用都是内核级别的, 但 select、poll、epoll 本质上都是同步I/O, 先是 block 住等待就绪的 socket, 再是block住将数据从内核拷贝到用户内存 (select\\poll\\epoll 均属于实现多路复用的SystemCall(系统调用)) epoll跟select都能提供多路I/O复用的解决方案, 在现在的Linux内核里有都能够支持, 其中epoll是Linux所特有, 而select则应该是POSIX所规定, 一般操作系统均有实现 多路复用是通过 Linux 的 select\\poll\\epoll 模型实现的, 但它们本质上都是 同步 IO IO 多路复用通过把多个 IO 阻塞复用到同一个 select 的阻塞上, 从而使得系统在单线程的情况下, 可以同时处理多个 client 请求 IO 多路复用技术其实就是通过使用Linux的系统函数 select, poll, epoll 等, 通过其底层的系统调用在操作系统内核层面实现的 (tip: 所以一般说支不支持IO多路复用, 和你的代码关系不大, 主要看操作系统, 如nginx 在 windows 只支持 select 不支持 epoll, epoll 是内核层面的东西, Windows 是不可支持的) select 说的通俗一点, 就是各个客户端连接的文件描述符也就是套接字, 都被放到了一个集合中, 调用select函数之后会一直监视这些文件描述符中有哪些可读, 如果有可读的描述符那么我们的工作进程就去读取资源 PHP 中有内置的函数来完成 select 系统调用, 函数原型: int socket_select(array &amp;$read , array &amp;$write , array &amp;$except , int $tv_sec [, int $tv_usec= 0 ]) (php 貌似不支持epoll, 需要使用 libevet 拓展) 特点: 最大缺陷是单个进程锁打开的 fd 是有限制的, 32位机器上是1024个, 64位机器上是2048个, 虽然可以改这个数值, 但是会造成性能下降 每次进行select调用都会线性扫描全部的fd集合, 不管哪个socket是活跃的, 都要遍历一遍fdset, 很耗时耗cpu, 当套接字比较多时, 效率就会呈现线性下降(如果能给套接字注册某个回调函数, 当本套接字活跃时, 自动完成相关操作, 就不用轮询, 实际上 epoll 就是改了这儿) 需要维护一个用来存放大量fd的数据结构, select在解决将fd消息传递给用户空间时采用了内存拷贝的方式, 在kernel缓冲区和用户缓冲区之间拷贝这个结构的开销很大, 这样, 其处理效率不高 时间复杂度O(n) pollpoll 本质上跟select没有区别, 只是 poll 没有最大连接数限制, 因为它是用基于链表来存储的时间复杂度O(n) epoll epoll 是当前在Linux下开发大规模并发网络程序的热门选择, epoll在Linux2.6内核中正式引入, 和select相似, 都是IO多路复用(IO multiplexing)技术 按照man手册的说法, epoll是为处理大批量句柄而做了改进的poll 对比于其他模型, epoll 做了如下改进: epoll 没有对描述符数目的限制, 它所支持的文件描述符上限是整个系统最大可以打开的文件数目 (例如, 在1GB内存的机器上, 这个限制大概为10万左右) IO效率不会随文件描述符(fd)的增加而线性下降不同于 忙轮询和无差别轮询, epoll 可以理解为 event poll, 它只会对活跃的socket进行操作, 这是因为在内核实现中, epoll是根据每个fd上面的callback函数实现的。因此, 只有活跃的socket才会主动去调用callback函数, 其他状态的socket则不会, 在这一点上, epoll实现了一个伪AIO, 其内部推动力在内核, 此时我们对这些流的操作都是有意义的 (复杂度降低到了O(1))传统的select/poll的一个致命弱点就是当你拥有一个很大的socket集合时, select/poll每次调用都会线性扫描整个socket集合, 这将导致IO处理效率呈现线性下降 使用 mmap 加速内核与用户空间的消息传递无论是select, poll还是epoll, 它们都需要内核把fd消息通知给用户空间, 因此, 如何避免不必要的内存拷贝就很重要了。对于该问题, epoll 利用 mmap()文件映射内存加速与内核空间的消息传递,即epoll使用mmap减少复制开销 Linux 底层的 epollLinux底层 epoll 的3个实现函数 int epoll_create(int size);epoll_create: 创建一个epoll对象参数size是内核保证能处理最大的文件句柄数, 在socket编程里面就是处理的最大连接数返回的int代表当前的句柄指针, 当然创建一个epoll对象的时候, 也会相应的消耗一个fd, 所以在使用完成的时候, 一定要关闭, 不然会耗费大量的文件句柄资源 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);epoll_ctl: 可以操作上面建立的 epoll例如, 将刚建立的socket加入到epoll中让其监控, 或者把 epoll正在监控的某个socket句柄移出epoll, 不再监控它等等epfd, 就是创建的文件句柄指针op是要做的操作, 例如删除, 更新等event 就是我们需要监控的事件 int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);epoll_wait: 在调用时, 在给定的timeout时间内, 当在监控的所有句柄中有事件发生时, 就返回用户态的进程epoll的高效就在于, 当我们调用epoll_ctl往里塞入百万个句柄时, epoll_wait仍然可以飞快的返回, 并有效的将发生事件的句柄发送给用户。这是由于我们在调用epoll_create时, 内核除了帮我们在epoll文件系统里建了个file结点, 在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外, 还会再建立一个list链表, 用于存储准备就绪的事件, 当epoll_wait调用时, 仅仅观察这个list链表里有没有数据即可。有数据就返回, 没有数据就sleep, 等到timeout时间到后即使链表没数据也返回。所以, epoll_wait非常高效。 那么, 这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时, 除了把socket放到epoll文件系统里file对象对应的红黑树上之外, 还会给内核中断处理程序注册一个回调函数, 告诉内核, 如果这个句柄的中断到了, 就把它放到准备就绪list链表里。所以, 当一个socket上有数据到了, 内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。（当网卡里面有数据的时候, 会发起硬件中断, 提醒内核有数据到来可以拷贝数据。当网卡通知内核有数据的时候, 会产生一个回调函数, 这个回调函数是epoll_ctl创建的时候, 向内核里面注册的。回调函数会把当前有数据的socket（文件句柄）取出, 放到list列表中。这样就可以把存放着数据的socket发送给用户态, 减少遍历的时间, 和数据的拷贝） 区别 Linux 内存映射 内存映射, 简而言之就是将 用户空间 的一段内存区域映射到 内核空间, 映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间, 相反, 内核空间对这段区域的修改也直接反映用户空间 那么, 对于内核空间&lt;——&gt;用户空间两者之间需要大量数据传输等操作的话效率是非常高的 首先, 驱动程序先分配好一段内存, 接着用户进程通过库函数 mmap() 来告诉内核要将多大的内存映射到内核空间 用户空间 mmap() 函数: void *mmap(void *start, size_t length, int prot, int flags,int fd, off_t offset) start: 用户进程中要映射的某段内存区域的起始地址, 通常为NULL（由内核来指定） length: 要映射的内存区域的大小 prot: 期望的内存保护标志 flags: 指定映射对象的类型 fd: 文件描述符（由open函数返回） offset: 要映射的用户空间的内存区域在内核空间中已经分配好的的内存区域中的偏移, 大小为PAGE_SIZE的整数倍","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"44. 同步/异步/阻塞/非阻塞 (可忽略)","slug":"JavaSE/2019-04-09-44","date":"2019-04-09T11:19:36.000Z","updated":"2019-09-09T08:41:32.000Z","comments":true,"path":"2019/04/09/JavaSE/2019-04-09-44/","link":"","permalink":"http://blog.renyimin.com/2019/04/09/JavaSE/2019-04-09-44/","excerpt":"","text":"一直依赖总是搞不太清楚 同步,异步,阻塞,非阻塞的概念, 总感觉同步就是阻塞, 异步就是非阻塞的, 搞得晕乎乎的, 于是就重新查了些资料进行了梳理, 如有不对欢迎大家指正 简单来说同步: 同步体现在, 在等待一件事情的处理结果时, 对方是否提供通知服务, 如果对方不提供通知服务, 则为 同步; 异步: 异步体现在, 在等待一件事情的处理结果时, 对方是否提供通知服务, 如果对方提供通知服务, 则为 异步; (常见的Ajax请求, 前端在ajax请求后通常还会执行一些其他加载操作(非阻塞); 在server端处理完后, 会将数据’通知’到前端页面) 阻塞: 阻塞体现在, 在等待一件事情的处理结果时, 你是否还去干点其他的事情, 如果不去, 则为 阻塞; 非阻塞: 非阻塞体现在, 在等待一件事情的处理结果时, 你是否还去干点其他的事情, 如果去了, 则为 非阻塞; 结合例子说明此处找了一位网友写的例子, 感觉很不错同步阻塞: 你去 甜在心馒头 店买太极馒头, 阿梅说:”暂时没, 正在蒸呢, 你自己看着点儿!”, 于是你就站在旁边只等馒头, 此时的你, 是阻塞的, 也是同步的; 阻塞表现在你除了等馒头，别的什么都不做了; 同步表现在等馒头的过程中, 阿梅不提供通知服务, 你不得不自己主动检查 ＂馒头出炉＂ 的消息; 同步非阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 你自己看着点儿!＂, 于是你就站在旁边发发微信, 然后问一句:＂好了没？＂, 然后玩玩QQ游戏, 然后再问一句:＂好了没？＂, 此时的你, 是非阻塞的, 不过却还是同步的; 非阻塞表现在你除了等馒头, 自己还在干别的事情; 同步表现在等馒头的过程中, 由于阿梅不提供通知服务, 你不得不自己主动检查 ＂馒头出炉＂ 的消息; 异步非阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 蒸好了我打电话告诉你!＂, 于是你就走了, 去买了双新球鞋, 看了看武馆, 总之, 从此不再过问馒头的事情, 一心只等阿梅电话, 此时的你, 是非阻塞的, 是异步的 (异步天然就是非阻塞的, 没啥好说的) 非阻塞表现在你除了等馒头, 自己还去干点别的事情; 异步表现在等馒头的过程中, 阿梅提供电话通知＂馒头出炉＂的消息, 你只需要等阿梅的电话; 异步阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 蒸好了我打电话告诉你!＂, 但你依然站在旁边只等馒头, 此时的你, 是阻塞的, 是异步的; (所以事实上这种模式一般不存在) 阻塞表现在你除了等馒头, 也没去干点别的什么(比如玩玩手机啥的); 异步表现在等馒头的过程中, 阿梅提供电话通知＂馒头出炉＂的消息, 你只需要等阿梅的电话; 参考","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"43. 网络IO 基础概念","slug":"JavaSE/2019-04-07-43","date":"2019-04-07T07:10:53.000Z","updated":"2019-09-11T10:30:43.000Z","comments":true,"path":"2019/04/07/JavaSE/2019-04-07-43/","link":"","permalink":"http://blog.renyimin.com/2019/04/07/JavaSE/2019-04-07-43/","excerpt":"","text":"文件描述符 Linux 会把所有的外部设备都看成一个文件来操作, 对外部设备的操作可以看成是对文件的操作 我们对一个文件的读写, 都会通过内核提供的系统调用, 内核会给我们返回一个 File Descriptor, 这个描述符是一个数字, 指向内核的一个结构体, 我们应用程序对文件的读写就是对描述符指向的结构体的读写 文件描述符(File descriptor): 其形式上是一个非负整数 ,它是一个索引值, 指向内核为每一个进程所维护的该进程打开文件的记录表 注意: 文件描述符 是一个抽象的概念 (这里的 文件 应该理解为 信息载体, 而不要直接理解为磁盘上的文件) Socket也是最初就作为UNIX操作系统的一部分而开发的, 所以其API和系统其他IO设备集成在一起 (套接字是一个抽象出来的概念,本质上也是一个文件描述符) 对于通常的文件IO来说, 其文件描述符最终是指向磁盘文件的 但是对于网络IO来说, socket的读写也会有相应的描述符, 称为 socketfd( socket 描述符), Socket描述符是一个指向内部数据结构的指针, 它指向描述符表入口, 调用Socket函数时, socket执行体将建立一个Socket, 实际上”建立一个Socket”意味着为一个Socket数据结构分配存储空间 简述 网络IO 主要工作 为了执行网络I/O, 一个进程必须做的第一件事就是调用 socket() 函数, 指定期望的通信协议类型。该函数只是作为一个简单的接口函数供用户调用, 调用该函数后将进入内核栈进行系统调用 sys_socket() (linux系统调用表(system call table)) 网络 IO 和 磁盘IO 类似 当进程读取磁盘上的文件时, 磁盘IO 的主要工作就是将 磁盘上的文件内容读取到 内核缓存中, 然后最终由 内核缓存 中拷贝到用户空间(缓存/无缓存), 最后被进程拿到数据 而网络IO与磁盘IO的读取过程类似, 他是将从网卡流入到内核空间的数据, 最终拷贝到用户空间的进程, 进而被进程所使用 小结: 简单来说, IO 读取的过程就是将 从外部设备来的数据 从 内核空间 拷贝到 用户空间的 过程! （写入则相反） Linux 五种 I/O 模型阻塞IO (blocking IO) 在Unix中,默认情况下所有的socket都是blocking, 一个典型的读操作流程大概如下 当用户进程调用了 recvfrom() 这个函数(其底层的是进行 系统调用 sys_recvfrom() ), kernel 就开始了IO的第一个阶段: 准备数据 对于 网络IO 来说, 很多时候数据在一开始可能还没有到达(比如, 还没有收到一个完整的UDP包), 这个时候kernel就要等待足够的数据到来, 这个过程是需要等待的, 也就是说数据被拷贝到 操作系统内核的缓冲区 是需要一个过程的, 在这个过程中, 内核在做等待(被阻塞), 用户进程也在等待(被阻塞) kernel一直等到数据准备好了, 也就是数据从外设拷贝到了 操作系统内核缓冲区 中了, 接下来才会将数据从 kernel缓冲区 拷贝到 用户内存, 拷贝完成一组数据, 用户进程才暂时会解除block的状态, 重新运行起来 上述过程中有两处可能会被阻塞的地方: 外设(网卡) --&gt; 操作系统内核缓冲区, 操作系统内核缓冲区 --&gt; 用户空间tip: 外设数据准备好后, 虽然会向操作系统内核缓冲区进行拷贝, 对于内核来说, 处于非阻塞状态, 因为它一直在工作, 但缓冲区只要没满, 而对于用户空间进程来说, 其实是处于阻塞状态, 因为它没有等到从内和缓冲区拷贝来的数据 优点和缺点 优点: 一个线程处理一个任务, 编程模型比较简单 （另外, 进程处于 阻塞状态 时, 是不占用CPU资源的） 缺点: 一个线程只能处理一个任务 非阻塞IO (non-blocking IO) 进程把一个 套接字 设置成 非阻塞 是在告诉内核, 当 内核中的数据 因为某些原因(为准备好,或者内核区未满) 尚无法拷贝数据到用户内核空间 时, 需要返回一个error; 当对一个 non-blocking socket 执行读操作时, 流程如下: 从用户进程角度讲, 它发起一个 read 操作后, 并不需要等待, 而是马上就得到了一个结果, 用户进程判断结果是一个error时, 它就知道数据还没有准备好, 于是它可以再次发送read操作, 一旦kernel中的数据准备好了, 并且又再次收到了用户进程的system call, 那么它马上就将数据拷贝到了用户内存, 然后返回, 所以, 用户进程其实是需要不断的主动询问kernel数据好了没有 也就是我们的用户进程需要自己不断的测试数据是否已经准备好, 如果没有准备好, 继续测试, 直到数据准备好为止, 在这个不断测试的过程中, 会大量的占用CPU的时间 (Linux下, 可以通过设置 socket 使其变为 non-blocking, 当使用socket()函数和WSASocket()函数创建套接字时, 默认都是阻塞的。在创建套接字之后, 通过调用ioctlsocket()函数, 将该套接字设置为非阻塞模式。Linux下的函数是:fcntl()) 优点和缺点 优点: 相较于阻塞模型, 非阻塞模型下, 线程不用再等待任务, 而是可以把时间花费到其它任务上, 也就是说这个当前线程可以尝试去同时处理多个任务 (编程) 缺点: 导致任务完成的响应延迟增大了, 因为每隔一段时间才去执行询问的动作, 但是任务可能在两个询问动作的时间间隔内完成, 这会导致整体数据吞吐量的降低 (另外, 不停地轮询, 不是每次都成功, 所以可能会消耗大量CPU资源) IO 多路复用概述 IO 多路复用 (IO multiplexing) 就是使用 select, poll, epoll (有些地方也称这种IO方式为 event driven IO ) 这些方法 (linux系统调用表(system call table)) 可以找到这些方法及其底层对应的系统调用方法 有了I/O复用, 我们就可以调用 select, poll, epoll, 它们最终仍然是让用户进程处于阻塞状态, 只不过是内核在其内部对这几个方法所管辖的多个 socket fd 该进行轮询 在这些方法阻塞用户进程后, 内核会轮询检查每个IO连接的内核缓冲区的数据是否准备好了 如果内核管理的某个 socket fd 的内核缓冲区的数据准备好了, 则 select 会有返回值, 用户进程可以进一步去调用 recvfrom() 先看一下 select 函数原型 12#include &lt;sys/select.h&gt;int select(int nfds, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout); 参数解释: nfds: 需要监视的最大文件描述符+1； readfds: 监视可读文件描述符集合 （select在调用之前, 需要手动在应用程序里将要监控的文件描述符添加到 fed_set 集合中, 然后加载到内核进行监控) writefds: 监视可写文件描述符集合 exceptfds: 监视异常文件描述符集合 timeout: 设置select的等待时间 (也就是select的阻塞进程的时间, 设置为NULL则表示没有timeout, 用户进程会一直阻塞, 该参数直接就明示了 select 阻塞用户进程的特性)返回值:负值: select错误正值: 某些文件可读写或出错0: 等待超时, 没有可读写或错误的文件如果参数timeout设为NULL, 则表示select没有timeout``` IO 多路复用 VS 同步非阻塞 select 调用是内核级别的, select的轮询也是内核在其内部执行的 (你事先需要把你需要执行的多个 IO操作的 socket fd 通过 select 传给内核, 让内核帮你进行轮询), 对用户进程来说,一个进程虽然是阻塞的,但其内部可以对多个socket fd进行监控 (另外, 在io复用模型下, 对于每一个socket, 一般都设置成 non-blocking, 但其实整个用户进程是一直被block的, 只不过用户进程不是被 socket IO 给block的, 而是被 select 这个函数block住的) 而 非阻塞模型中的轮询 是 应用程序进程在 用户空间 进行的轮询 (你也可以在一个进程中对多个 socket fd 进行轮询 ), 对用户层来说, 进程确实是非阻塞的 其实 select 相比较 non-blocking 来说, 在单个任务的情况下可能要更差一些, 因为这里调用了 select 和 recvfrom 两个 system call, 而 non-blocking 只调用了一个 recvfrom , 但是用select的优势在于它可以帮你同时处理多个 socket fd, 而不用你自己去处理多个 socket fd IO 多路复用 VS 多线程+同步阻塞IO多路复用的图 其实看着和 blocking IO的图 没有太大的不同, 事实上, 还更差一些, 因为这里需要使用两个system call (select 和 recvfrom), 而 blocking IO 只调用了一个system call (recvfrom)但是, 用select的优势在于它可以同时处理多个socket fd, 所以, 如果处理的连结数目不高的话, 使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的web server性能好, 可能延迟还更大(因为阻塞可以保证没有延迟, 但是多路复用是处理先存在的数据, 所以数据的顺序则不管, 导致处理一个完整的任务的时间上有延迟) 同步非阻塞 VS 多线程+同步阻塞高并发的程序一般使用 同步非阻塞方式 而非 多线程 + 同步阻塞 方式因为在高并发场景下, 为每个任务(用户请求)创建一个进程或线程的开销非常大, 而 同步非阻塞 方式可以把多个 IO 请求丢到后台去, 这就可以在一个进程里服务大量的并发 IO 请求 异步IO (asynchronous IO)上面说的 阻塞, 非阻塞, IO多路复用 这几种方式, 对用户进程来说: 阻塞 和 IO多路复用 都是主动去等待; 非阻塞是用户进程主动去轮询; 总之都是用户进程都是自己主动去拿到了结果, 不是被回调通知的, 所以对用户进程来说都是同步的IO操作 Unix下的 asynchronous IO 其实用得很少 用户进程发起读取操作之后, 立刻就可以开始去做其它的事, 而另一方面, 从kernel的角度, 当它收到一个asynchronous read之后, 首先它会立刻返回, 所以不会对用户进程产生任何block, 然后, kernel会等待数据准备完成, 然后将数据拷贝到用户内存, 当这一切都完成之后, kernel会给用户进程发送一个signal, 告诉它read操作完成了 信号驱动式I/O模型…… 各 IO Model 对比 参考https://www.cnblogs.com/diegodu/p/6823855.htmlhttps://blog.csdn.net/jay900323/article/details/18141217#t6https://blog.csdn.net/lemon89/article/details/78290389#Reactor_183 https://blog.csdn.net/u014730165/article/details/85044285 ？","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"42. Linux 文件IO操作的分类 - 文件IO, 标准IO","slug":"JavaSE/2019-04-07-42","date":"2019-04-07T07:03:29.000Z","updated":"2019-09-06T10:38:19.000Z","comments":true,"path":"2019/04/07/JavaSE/2019-04-07-42/","link":"","permalink":"http://blog.renyimin.com/2019/04/07/JavaSE/2019-04-07-42/","excerpt":"","text":"根据有无 用户态缓存 可以将IO分为以下两大类 (注意都有内核态缓存) 文件IO 文件I/O 又称为低级磁盘I/O(是操作系统提供的基本IO服务), 遵循POSIX相关标准, 与os绑定; 任何兼容POSIX标准的操作系统上都支持文件I/O (特定于 Linux 或 unix平台) 文件I/O 称之为不带缓存的IO(unbuffered I/O), 像 open(), read() 这些 posix 标准的会进行系统调用的函数, 主要是指其在用户空间没有缓冲, 在内核空间还是进行了缓存的 数据流转过程为: 数据---&gt;内核缓存区--&gt;磁盘 假设 内核缓存区 长度为100字节, 你调用 write() 进行写操作时, 设每次写入 10 个字节, 那么你要调用10次这个函数才能把 内核缓存区 写满, 没写满时数据还是在内核缓冲区中, 并没有写入到磁盘中, 内核缓存区满了之后或者执行了 fsync(强制写入硬盘)之后, 才进行实际的IO操作, 把数据写入磁盘上 标准IO (缓存IO) 标准I/O是 ANSI C 建立的一个标准I/O模型, 是一个 标准函数包 和 stdio.h 头文件中的定义, 具有一定的可移植性 标准I/O 被称为 高级磁盘I/O, 又被称作 缓存 IO, 大多数文件系统的默认 IO 操作都是缓存 IO, 像 fopen(), fwrite(), fget() 等, 是c标准库中定义的 数据流转过程为: 数据--&gt;流缓存区--&gt;内核缓存区--&gt;磁盘 假设 流缓存区 长度为50字节, 内核缓存区100字节, 我们用标准c库函数 fwrite() 将数据写入到这个流缓存中, 每次写10字节, 需要写5次流缓存区满后才会调用 write() (或调用 fflush() ), 将数据写到内核缓存区, 直到内核缓存区满了之后或者执行了 fsync(强制写入硬盘) 之后, 才进行实际的IO操作, 把数据写入磁盘上标准IO操作 fwrite() 最后还是要调用 无缓存的IO操作 write()如 标准IO函数 fopen() 底层用的还是 低级的文件IO函数 read() (这两个函数都是面向用户的用户编程接口API, 是用户空间的调用, 而 read() 底层则是进行 系统调用 (调用内核函数 sys_read()) ) tip: fsync: 是把内核缓冲刷到磁盘上 fflush: 是把C库中的缓冲调用 write() 函数写到磁盘 (其实是写到内核的缓冲区, 内核缓冲区满了之后才会真正写入磁盘) 标准IO的三种缓冲类型 (隐藏) 文件IO 与 标准IO 区别 有隐藏部分 标准IO 在用用户态下就有了缓存, 如调用 fopen(), fwrite() 的时候就减少了 用户态 和 内核态 的切换;而 文件IO 如 open(), write() 每次调用都会进行 内核态 和 用户态 的切换 所以, 如果顺序访问文件, fopen() 系列的函数要比直接调用 open() 系列快, 如果随机访问文件 open() 要比 fopen() 快 缓存 IO 的缺点:当然, 标准I/O库的并非没有缺点, 这与他需要复制的数据有关, 通常需要复制两次数据: 一次是在 内核 与 标准I/O缓冲区 之间(当标准IO缓冲区满后, 数据会被拷贝到内核缓冲区), 第二次是在 标准I/O缓冲区 和 应用程序行缓存 之间 参考https://www.cnblogs.com/orlion/p/6258691.htmlhttps://blog.csdn.net/m0_37542524/article/details/83663124#1IOIO_2https://blog.csdn.net/tanqiuwei/article/details/20641965https://www.jianshu.com/p/a77613045601","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"41. 基础概念 -- 用户态, 内核态, 系统调用","slug":"JavaSE/2019-04-07-41","date":"2019-04-07T06:20:18.000Z","updated":"2019-09-06T10:35:27.000Z","comments":true,"path":"2019/04/07/JavaSE/2019-04-07-41/","link":"","permalink":"http://blog.renyimin.com/2019/04/07/JavaSE/2019-04-07-41/","excerpt":"","text":"在了解 IO 之前, 我们有必要先了解一下 系统调用 的概念而在了解 系统调用 的概念前, 又不得不先了解Linux操作系统中 用户态与内核态 的概念 用户态 与 内核态 为什么需要区分内核空间与用户空间?在 CPU 的所有指令中, 有些指令是非常危险的, 如果错用, 将导致系统崩溃, 比如清内存、设置时钟等。如果允许所有的程序都可以使用这些指令, 那么系统崩溃的概率将大大增加。所以, CPU 将指令分为特权指令和非特权指令, 对于那些危险的指令, 只允许操作系统及其相关模块使用, 普通应用程序只能使用那些不会造成灾难的指令。比如 Intel 的 CPU 将特权等级分为 4 个级别：Ring0~Ring3。其实 Linux 系统只使用了 Ring0 和 Ring3 两个运行级别(Windows 系统也是一样的)。 当进程运行在 Ring3 级别时被称为运行在用户态, 而运行在 Ring0 级别时被称为运行在内核态。 Linux 从整体上分为 内核态(内核空间) 与 用户态(用户空间) 比如一个32位的操作系统, 寻址地址(虚拟内存空间)是2的32次方, 也就是4G 操作系统将较高的1G字节作为内核空间 (内核空间具有用户空间所不具备的操作权限) 将较低的3G字节作为用户空间 内核态 与 用户态 内核态 就是内核所处的空间, 内核负责调用底层硬件资源, 并为上层应用程序提供运行环境 用户态 即 应用程序的活动空间 (应用程序通常运行在用户空间, 当某些操作需要内核权限时, 通过 系统调用(System calls), 进入内核态执行, 这也就是一次 用户态\\内核态 的转换) 系统调用(System calls) 应用程序的运行必须依托于内核提供的硬件资源(如cpu\\存储\\IO), 而内核是通过暴露外部接口供应用程序进行调用 — 内核提供的这些外部接口就称为 SystemCall, 系统调用 系统调用是操作系统的最小功能单位, 每次系统调用都会发生一次用户态与内核态的切换, 切换过程中涉及了各种函数的调用以及数据的复制 系统调用是每个操作系统必不可少的一部分, 操作系统上的每个应用程序都必须依靠 系统调用 来进入内核态,进而实现对硬件资源的操作 而应用程序的 read() 实现其实是来源于内核空间的内核函数 sys_read(), 这样, 通过 read() 就形成一个系统调用 (更多系统调用可以搜索: Linux系统调用表(system call table) 去查看) 系统调用是 用户态 进入 内核态 的唯一入口 用户空间 与 操作系统内核空间 的系统调用 架构简图1 如下: 用户空间 与 操作系统内核空间 的系统调用 架构简图2 如下:","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"40. IO 基础概念","slug":"JavaSE/2019-04-07-40","date":"2019-04-07T05:41:08.000Z","updated":"2019-09-06T10:14:42.000Z","comments":true,"path":"2019/04/07/JavaSE/2019-04-07-40/","link":"","permalink":"http://blog.renyimin.com/2019/04/07/JavaSE/2019-04-07-40/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://blog.renyimin.com/categories/Java/IO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://blog.renyimin.com/tags/IO/"}]},{"title":"Paxos","slug":"distributed-consensus/paxos","date":"2019-03-29T06:16:31.000Z","updated":"2019-07-30T10:42:33.000Z","comments":true,"path":"2019/03/29/distributed-consensus/paxos/","link":"","permalink":"http://blog.renyimin.com/2019/03/29/distributed-consensus/paxos/","excerpt":"","text":"参考https://github.com/Tencent/phxpaxos/blob/master/README.zh_CN.mdhttps://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&amp;mid=2247483695&amp;idx=1&amp;sn=91ea422913fc62579e020e941d1d059e#rdhttps://github.com/Tencent/phxpaxos/wiki 活锁 https://github.com/benbjohnson/thesecretlivesofdata https://www.jdon.com/artichect/paxos.html 两军问题: https://www.cnblogs.com/charlesblc/p/6037963.html mvcc: https://www.cnblogs.com/charlesblc/p/6037963.html 秘书竞标的例子 https://www.cnblogs.com/williamjie/p/10214133.html","categories":[{"name":"分布式一致性协议","slug":"分布式一致性协议","permalink":"http://blog.renyimin.com/categories/分布式一致性协议/"}],"tags":[{"name":"分布式一致性协议","slug":"分布式一致性协议","permalink":"http://blog.renyimin.com/tags/分布式一致性协议/"}]},{"title":"29. 无边界通配符 `?`","slug":"JavaSE/2019-03-21-35","date":"2019-03-21T08:21:13.000Z","updated":"2019-09-02T09:08:28.000Z","comments":true,"path":"2019/03/21/JavaSE/2019-03-21-35/","link":"","permalink":"http://blog.renyimin.com/2019/03/21/JavaSE/2019-03-21-35/","excerpt":"","text":"问题的出现使用泛型后, 多态的用法出现问题: 虽然 String 是 Object 的子类, 但 Demo&lt;String&gt; 并不是 Demo&lt;Object&gt; 的子类, 这就会造成无法使用诸如 Demo&lt;Object&gt; str = new Demo&lt;String&gt; 这类多态用法 &lt;?&gt; 与 &lt;Object&gt; 使用泛型出现多态的问题之后, 通配符? 就应运而生了 (可以把 ? 看成是泛型中所有参数类型的父类, ? 同 Sting、Integer、Double 等一样, 都是具体的类型实参) 示例: 小结: Student&lt;?&gt; 与 Student&lt;String&gt; 可以实现多态 原始类型 Student 与 Student&lt;String&gt; 也可以实现多态 但是 Student&lt;Object&gt; 与 Student&lt;String&gt; 不可以实现多态 通配符? VS 原始类型 都可以实现多态 尽管 Student&lt;?&gt; 和 原始类型 Student 都能与 Student&lt;String&gt; 实现多态, 但是仍然有区别: 使用了 &lt;?&gt; 的话, compiler 会进行类型检查所以不能够通过 List&lt;?&gt; 的实例, 往List实例中添加任何元素 (null除外, 因为null可以赋值给任何引用对象)因为根本无法确定 List&lt;?&gt; 到底指向了什么类型的List, 所以无法保证类型安全, 所以不能通过这种引用添加元素 而原始类型的话, 编译不会进行类型检查, 可以添加任意值 Demo 通配符? VS 泛型变量 (实参 VS 形参) ? 和 之前接触的 泛型变量(常用的如 E, T, K, V) 是完全不同的东西 泛型变量 (及其 绑定限制)的用法, 只能在类、接口、函数中声明, 即 只能用于类型的 形参 而 无边界通配符 ? 则是 类型实参, 表示通配任何类型, 它是用来填充 泛型变量 的, 常见用法如下12// Student&lt;?&gt; student2 = new Student&lt;&gt;(); 1234// 注意: 对下面代码, 虽然 `Student&lt;?&gt; student` 是作为方法show的形参, 但是类型 `Student&lt;?&gt;` 中的 `?` 是泛型类型的实参 (直白点说, ?和Integer, String 一样, 都是一种实际的类型) private static void show(Student&lt;?&gt; student) &#123; System.out.println(student.getName());&#125; 重要的事情说三遍: 无边界通配符 ? 用于实参! 无边界通配符 ?用于实参! 无边界通配符 ?用于实参! 通配符 ? 的 extends 绑定实参 设置上限 通配符 ? 可以代表任意实参类型, 但跟泛型变量一样, 如果不加以限定, 在后期的使用中编译器可能会报错 绑定的形式, 同样是通过 extends 关键字, 意义和使用方法都与 泛型变量的extends绑定 一致 示例 通配符? 的 super 绑定实参 设置下限 如果说 &lt;? extends XXX&gt; 指填充的泛型类型 为派生于XXX的任意子类及其自身, 那么 &lt;? super XXX&gt; 则表示填充为任意XXX的父类及其自身 extends 通配符, 能取不能存 super 能存不能取 总结? extends 和 ? super 通配符的特征, 我们可以得出以下结论： 如果你想从一个数据类型里获取数据, 使用 ? extends 通配符（能取不能存） 如果你想把对象写入一个数据结构里, 使用 ? super 通配符（能存不能取） 如果你既想存, 又想取, 那就别用通配符","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"28. 泛型变量的类型限定、自限定  (extends)","slug":"JavaSE/2019-03-21-34","date":"2019-03-21T08:04:52.000Z","updated":"2019-09-02T08:49:23.000Z","comments":true,"path":"2019/03/21/JavaSE/2019-03-21-34/","link":"","permalink":"http://blog.renyimin.com/2019/03/21/JavaSE/2019-03-21-34/","excerpt":"","text":"问题的出现 上述例子中, 在编译阶段, 由于编译器只能推断出你将来传递的具体类型是Object的子类(无法缩小到具体的范围), 所以认为泛型变量只能调用Object的方法 对于上述问题 首先, 我们不希望让编译器把泛型变量的范围定位在Object这个大范围下, 而是希望 泛型变量 的类型能尽可能精确到某个比较明确的范围 (从而尽可能精确地知道这个泛型变量能干点什么 (tip: 就好比仅仅告知你一个线索 “A是一个生物”, 你除了知道A有生物泛有的特性外, 根本无法具体知道它有哪些特性, 但其实A可能就是个人, 你知道他能做很多事情…)) 我们希望的是编译器能在编译阶段识别到我们将来传递的具体泛型类型的属性/方法 为了让我们 泛型变量 尽可能精确到我们已知的范围内, 就需要使用 泛型变量的类型绑定 机制了 通过 &lt;T extends BoundingType&gt; 可以表示 T 应该是 BoundingType 的子类型, 表示 T 是在 BoundingType 基础上创建的, 具有 BoundingType 的功能 (BoundingType 可以是类, 也可以是接口) tip: 此处的 extends 关键字表示 子类型, 而不等同于继承 （ extends 在这里不是类继承里的那个 extends , 两个根本没有任何关联） 泛型限定 泛型变量绑定接口 泛型变量绑定类 泛型变量 绑定多个限定: 泛型变量 还可以同时绑定多个 BoundingType , 用 &amp; 连接 (注意: 多个绑定中只能有一个类, 可以有多个接口)多个泛型分别绑定多个限定 泛型限定的两个作用 对泛型实参加以限定 编译阶段使用泛型变量T时, 可以识别到 BoundingType 内部的成员 泛型自限定类型 在《Java编程思想》中关于泛型的讲解中, 提到了 自限定类型, 型如: class SelfBounded&lt;T extends SelfBounded&lt;T&gt;&gt; 作者也提道: 这就像两面镜子彼此照向对方所引起的目眩效果一样, 是一种无限反射 SelfBounded 类接受泛型参数T, 而T由一个边界限定, 这个边界就是 拥有T作为其泛型参数的SelfBounded 直接先看Demo 自限定参数据有何意义呢? 它可以保证: 类型参数要与正在被定义的类相同 或者还可以使用另一个继承了 SelfBounded&lt;T&gt; 的子类作为参数 但是 让参数与正在被定义的类相同是主要用法 (即上例中的 class A extends SelfBounded&lt;A&gt; {}) 使用场景Enum&lt;E extends Enum&lt;E&gt;&gt; 可写成 Enum&lt;E extends Enum&gt; 泛型自限定 小结 面向对象的继承特性可以让多个类属同一类型 多个属于同一类型的类, 在对它们进行操作前, 最好尽可能地精确到我们所需的范围 (可以使用 泛型绑定 技术) 多个属于同一类型的类, 他们之间如果在进行某项操作时, 需要进行区别对待 (可以使用 泛型绑定 技术, 并且采用自绑定技术) 泛型变量的绑定只有 extends 关键字, 没有 super 关键字 参考https://blog.csdn.net/zunwangtianqian/article/details/82049753https://www.jianshu.com/p/2bf15c5265c5","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"27. Generics(泛型)","slug":"JavaSE/2019-03-20-33","date":"2019-03-20T11:42:19.000Z","updated":"2019-09-02T08:40:16.000Z","comments":true,"path":"2019/03/20/JavaSE/2019-03-20-33/","link":"","permalink":"http://blog.renyimin.com/2019/03/20/JavaSE/2019-03-20-33/","excerpt":"","text":"概述 泛型是 JDK5 中引入的一个新特性, 其本质是为了将 数据类型 参数化 在学习泛型以前, 比较常见的是在定义函数时, 将 数据 作为函数的 型式参数 而 泛型 的目的则是 是为了把 数据类型 当做 形式参数 数据类型 的参数化, 可以用在 类、接口 和 方法上, 分别被称为 泛型类、泛型接口、泛型方法 泛型变量命名规范: 为了简化命名, 通常只用一个字母来命名 泛型变量, 常用字母如下 E: 代表Element (在集合中使用, 因为集合中存放的是元素, ArrayList 用的就是 E) T: Type (表示派生自 Object 类的任何类) K V: 分别代表键值中的Key和Value N: Number(数值类型) ?: 表示不确定的类型 多泛型变量定义: 如果需要传进多个泛型, 可以如 class GenericTest&lt;T, U&gt;、class GenericTest&lt;T, U, A, B, C&gt; 即可 (多个类型参数之间用逗号分隔) 在对 泛型变量 这个形参命名时, 即便你使用的名字是java中已存在的类型名, 也会被看做像是 T, E, K, V 这样的普通的名字 泛型带来的几点好处 泛型提供了 编译时类型安全检测机制, 允许我们在编译时检测到非法的类型 如, 在 Java 语言中, 对于 ArrayList&lt;T&gt; 泛型类, 在实例化对象时, 可以通过指定 T 为 Integer/String ...., 以便对 对象 中的将来要存放的元素类型进行限制, 从而在编译阶段即可检测出异常类型元素的存入虽然也可以在实例化对象时, 直接 new ArrayList() 而不使用泛型, 但此时就无法在编译阶段对 对象 中的元素类型进行限制, 而造成运行时可能出现的异常 而由于PHP是弱类型, 所以对于PHP中的Array, 即便是不使用泛型, 由于在运行时也不会对上述基本的类型做强制校验, 所以也不会出现异常, 所以PHP没有引入泛型机制 当然, 泛型所带来的好处不仅仅是以上, 使用泛型类, 还可以减少对大量相同结构的类的创建; 同时, 在不使用泛型时, 默认又可以传递任意类型的实例, 非常方便 泛型类 定义格式: 修饰符 class 类名&lt;泛型变量&gt; {} 对于泛型类中的方法, 其 返回值, 形参 都可以使用 泛型类上的泛型变量 (但是, 需要注意的是: 由于泛型类的泛型实参是在实例化泛型类之后才确定的, 所以 在泛型类中, 只有成员方法可以使用泛型变量, 静态方法不可以使用泛型类的泛型变量) 注意: 泛型类中使用了泛型类的泛型变量的 成员方法并不一定就是 泛型方法 （泛型方法 和 泛型类 没有必然联系, 普通类中也常常会有泛型方法） 泛型方法 定义格式: 修饰符 &lt;泛型变量&gt; 返回值类型 方法名(参数) {} ( 泛型变量 要写在方法的修饰符 和 返回值之间 ) 注意: 泛型类中使用了泛型类的泛型变量的 成员方法并不一定就是 泛型方法 （泛型方法 和 泛型类 没有必然联系, 普通类中也常常会有泛型方法） 1234// 这不是泛型方法, 只是用到了 泛型类的 泛型变量public E test(E content) &#123; return content;&#125; Demo static 与 (泛型方法/泛型类的 泛型变量) 当 泛型方法 没有使用 泛型类的泛型变量时, 泛型方法本是其实是可以被 static 关键字修饰为静态方法的, 因为 泛型方法的 泛型变量 是局部变量, 并不是通过泛型类传递的 泛型类中 使用了泛型变量 || 返回值是泛型变量 的非泛型方法 不能被 static 关键字修饰, 因为泛型变量是需要类被实例化之后才能确定具体类型值 泛型接口 格式和 泛型类 类似 : 修饰符 interface 接口名称&lt;泛型变量&gt; {} 泛型接口有两种使用方式 实现类确定泛型类型 实现类继续使用泛型 注意事项 前后泛型的类型要一致 (即便前后的类型有继承关系也不可以) : 12// 即便Student类继承自Person类, 也不会通过编译List&lt;Person&gt; list = new ArrayList&lt;Student&gt;(); 在 jdk7 推出了一个新特性, 泛型的菱形语法, 即 后面的泛型类型可以省略, 如: List&lt;Person&gt; list = new ArrayList&lt;&gt;(); 在向集合中添加元素时, 可以添加泛型类型的子类型 12List&lt;Person&gt; list = new ArrayList&lt;&gt;();list.add(new Student()); 泛型只支持对象类型, 如下是错误示例: 1List&lt;int&gt; list = new ArrayList&lt;int&gt;(); 注意: 不能对确切的泛型类型使用 instanceof 操作, 如下面的操作是非法的, 编译时会出错 泛型数组 (暂时先了解)泛型数组的定义方法为 T[], 与 String[] 类似 参考https://blog.csdn.net/harvic880925/article/details/49872903#","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"","slug":"MyBatis/2019-03-09-02","date":"2019-03-09T08:45:11.000Z","updated":"2019-08-16T02:19:41.000Z","comments":true,"path":"2019/03/09/MyBatis/2019-03-09-02/","link":"","permalink":"http://blog.renyimin.com/2019/03/09/MyBatis/2019-03-09-02/","excerpt":"","text":"","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.renyimin.com/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.renyimin.com/tags/MyBatis/"}]},{"title":"概述","slug":"MyBatis/2019-03-09-01","date":"2019-03-09T08:23:07.000Z","updated":"2019-08-15T10:31:46.000Z","comments":true,"path":"2019/03/09/MyBatis/2019-03-09-01/","link":"","permalink":"http://blog.renyimin.com/2019/03/09/MyBatis/2019-03-09-01/","excerpt":"","text":"概述mybatis 是一个优秀的基于 java 的持久层框架, 它内部封装了 jdbc, 使开发者只需要关注 sql 语句本身, 而不需要花费精力去处理 加载驱动、创建连接、创建 statement 等繁杂的过程 mybatis 通过 xml 或 注解 的方式将要执行的各种 statement 配置起来, 并通过 java 对象和 statement 中 sql 的动态参数进行映射生成最终执行的 sql 语句, 最后由 mybatis 框架执行 sql 并将结果映射为 java 对象并 返回 采用 ORM(Object Relational Mapping) 思想解决了实体和数据库映射的问题, 对 jdbc 进行了封装, 屏蔽了 jdbc api 底层访问细节, 使我们不用与jdbc api打交道, 就可以完成对数据库的持久化操作ORM简单来说就是把 数据库表 和 实体类及类属性对应起来, 让我们可以操作实体类就实现操作数据库表 环境搭建 创建maven, 并在pom.xml中导入所需的各包 创建实体类和Dao接口 创建 MyBatis 主配置文件 mybatis.xml 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- 这里写配置内容 --&gt;&lt;/configuration&gt; 创建 MyBatis 映射文件 mapper.xml 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;命名空间&quot;&gt; &lt;!-- SQL语句 --&gt;&lt;/mapper&gt; 如下图 tips: mybatis的映射文件的包结构要和dao接口的包结构一样 映射配置文件的mapper标签的namespace属性的取值必须是对应的dao接口的全限定类名 映射配置文件的操作配置, id属性必须是dao接口的方法名 当我们遵从了以上三点之后, 在开发中我们是无需再写 Dao 的具体实现类的 实例分析 其实在读取配置文件时，一般不用绝对路径或者相对路径, 一般会采用 使用类加载器, 它只能读取类路径的配置文件 使用ServletContext对象的getRealPath() mybatis使用了构建者模式来创建工厂 123SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder();// builder 即为构建者SqlSessionFactory factory = builder.build(in); 使用了工厂模式 SqlSession session = factory.openSession(); 代理模式 IUsersDao userDao = session.getMapper(IUsersDao.class); mybatis注解开发 另外: 一般都是使用不写dao实现类的方式","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.renyimin.com/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://blog.renyimin.com/tags/MyBatis/"}]},{"title":"31","slug":"JavaSE/2019-03-02-31","date":"2019-03-02T08:21:17.000Z","updated":"2019-09-03T06:22:56.000Z","comments":true,"path":"2019/03/02/JavaSE/2019-03-02-31/","link":"","permalink":"http://blog.renyimin.com/2019/03/02/JavaSE/2019-03-02-31/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"29. Java中array、List、Set互相转换","slug":"JavaSE/2019-03-02-29","date":"2019-03-02T04:36:29.000Z","updated":"2019-09-03T06:23:53.000Z","comments":true,"path":"2019/03/02/JavaSE/2019-03-02-29/","link":"","permalink":"http://blog.renyimin.com/2019/03/02/JavaSE/2019-03-02-29/","excerpt":"","text":"数组转 ListArrays.asList() 数组转 Set12345String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;// 数组先转为List, 在初始化为SetSet&lt;String&gt; staffsSet = new HashSet&lt;&gt;(Arrays.asList(staffs));staffsSet.add(&quot;D&quot;); // okstaffsSet.remove(&quot;Tom&quot;); // ok 数组转 StringArrays.toString() List 转数组1234String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;List staffsList = Arrays.asList(staffs);// 上面就是个List, 要转数组, 直接 toArray()Object[] result = staffsList.toArray(); List 转 Set1234String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;List staffsList = Arrays.asList(staffs);// 上面是个ListSet result = new HashSet(staffsList); List 转 String12345String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;List staffsList = Arrays.asList(staffs);// 上面是个List// List 需要先转为 Array, 才可以换成字符串格式展示 System.out.println(Arrays.toString(staffsList.toArray())); // [A, B, C] Set 转数组toArray() Set 转 List12345String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;Set&lt;String&gt; staffsSet = new HashSet&lt;&gt;(Arrays.asList(staffs));// 上面就是个setList&lt;String&gt; result = new ArrayList&lt;&gt;(staffsSet); Set 转 String123456String[] staffs = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;&#125;;Set&lt;String&gt; staffsSet = new HashSet&lt;&gt;(Arrays.asList(staffs));// 上面就是个set// 需要先将 Set 转为 数组, 然后才能转为字符串进行展示System.out.println(Arrays.toString(staffsSet.toArray()));","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"27. 集合操作 API","slug":"JavaSE/2019-03-02-27","date":"2019-03-02T04:36:29.000Z","updated":"2019-09-03T06:23:03.000Z","comments":true,"path":"2019/03/02/JavaSE/2019-03-02-27/","link":"","permalink":"http://blog.renyimin.com/2019/03/02/JavaSE/2019-03-02-27/","excerpt":"","text":"List 基本操作java.util.Collectionsorg.springframework.util.CollectionUtils引入SpringFramework即可使用该工具 org.apache.commons.collections4.CollectionUtils12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt; &lt;version&gt;4.1&lt;/version&gt;&lt;/dependency&gt; 集合是否为空并集交集差集集合是否相等","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"25. 数组常用操作 (ArrayUtils)","slug":"JavaSE/2019-03-01-25","date":"2019-03-01T11:53:16.000Z","updated":"2019-09-03T06:49:44.000Z","comments":true,"path":"2019/03/01/JavaSE/2019-03-01-25/","link":"","permalink":"http://blog.renyimin.com/2019/03/01/JavaSE/2019-03-01-25/","excerpt":"","text":"java.util.Arrays提供了大量的静态方法用于对数组进行一些常见操作 (有空指针问题) public static &lt;T&gt; List&lt;T&gt; asList(T... a) : 返回由指定数组支持的固定大小的列表 (转换成的列表, 不支持 add、remove 操作) public static String toString(Object[] a) : 将数组用字符串形式展示 {1,2,3} 结果为 [1,2,3] sort(float[] a)/sort(float[] a, int fromIndex, int toIndex) : 将数组升序排序 (也可将数组中的指定范围进行排序) sort(T[] a, Comparator&lt;? super T&gt; c) 、sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) org.apache.commons.lang3.ArrayUtils12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt;&lt;/dependency&gt; 集合是否为空并集交集差集集合是否相等","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"22. String常用操作 (StringUtils)","slug":"JavaSE/2019-03-01-22","date":"2019-03-01T11:37:42.000Z","updated":"2019-09-03T06:50:31.000Z","comments":true,"path":"2019/03/01/JavaSE/2019-03-01-22/","link":"","permalink":"http://blog.renyimin.com/2019/03/01/JavaSE/2019-03-01-22/","excerpt":"","text":"java.lang.String使用时需要注意, 空指针问题 org.springframework.util.StringUtils首先, 这个工具类里面的所有方法, 都是针对字符串常见操作, 其次, 里面有不少方法, 既可以针对 String, 也可以使用 CharSequence 判断类属于该类别的方法都是在对字符串进行一些判定操作 字符串头尾操作属于该类别的方法, 都是对字符串前, 或者字符串后的内容进行 判定 或者 操作 字符串和子串的操作该组方法中主要是提供了 字符串 和 字符串子串 的操作, 比如 子串的匹配、子串的替换、子串的删除 等等操作 字符串数组 的一些基本操作该组方法主要是完成 字符串 和 字符串数组 之间的基本操作, 比如 追加、删除、排序等另一部分 文件路径名称相关操作本地化相关CharSequencejava.lang.StringBuilder","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"26. Java 迭代器 spliterator","slug":"JavaSE/2019-03-02-26","date":"2019-03-01T03:26:31.000Z","updated":"2019-09-03T06:26:32.000Z","comments":true,"path":"2019/03/01/JavaSE/2019-03-02-26/","link":"","permalink":"http://blog.renyimin.com/2019/03/01/JavaSE/2019-03-02-26/","excerpt":"","text":"spliterator 是 java1.8 引入的一种遍历的机制, 不同于 Iterator, 该遍历为 并行遍历","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"BeanUtils工具类","slug":"tomcat-servlet/2019-02-10-08","date":"2019-02-10T11:31:18.000Z","updated":"2019-06-27T06:50:21.000Z","comments":true,"path":"2019/02/10/tomcat-servlet/2019-02-10-08/","link":"","permalink":"http://blog.renyimin.com/2019/02/10/tomcat-servlet/2019-02-10-08/","excerpt":"","text":"概述 BeanUtils 工具类是用于封装JavaBean的, 可以用来简化数据的封装 JavaBean (标准的Java类) 的基本规范： 类必须被public修饰 必须提供空参的构造器 成员变量必须使用private修饰 需要提供公共 setter 和 getter 方法 成员变量 和 属性 成员变量: 类种定义的变量即成员变量 属性: setter, getter 方法 去掉 get, set 后, 剩余的名字首字母转小写后, 即是属性 一般成员变量名和属性是一样的, 但是也可以使用 setHaHa() 去给成员变量 name 赋值, 此时成员变量name和属性名haHa就不一样了 使用, 需要引入 commons-beanutils-1.9.3.jar, commons-logging-1.2.jar 包 (https://mvnrepository.com/artifact/commons-beanutils/commons-beanutils) 方法 setProperty() // 操作的是属性, 可不是成员变量名 getProperty() populate(Object obj , Map map): 将map集合的键值对信息, 封装到对应的 JavaBean 对象中 RRC:12AuthUserDTO subDTO = new AuthUserDTO();BeanUtils.copyProperties(vo, subDTO);","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"","slug":"tomcat-servlet/2019-02-09-05","date":"2019-02-09T12:13:29.000Z","updated":"2019-08-13T10:12:37.000Z","comments":true,"path":"2019/02/09/tomcat-servlet/2019-02-09-05/","link":"","permalink":"http://blog.renyimin.com/2019/02/09/tomcat-servlet/2019-02-09-05/","excerpt":"","text":"","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"ServletRequest","slug":"tomcat-servlet/2019-02-09-04","date":"2019-02-09T11:30:29.000Z","updated":"2019-08-30T09:17:02.000Z","comments":true,"path":"2019/02/09/tomcat-servlet/2019-02-09-04/","link":"","permalink":"http://blog.renyimin.com/2019/02/09/tomcat-servlet/2019-02-09-04/","excerpt":"","text":"概述 request 对象继承体系结构： ServletRequest 接口 |— HttpServletRequest 接口 request 对象是tomcat创建并传给service方法的 (tomcat 本身就是纯java写的 (可以在其源代码中看到下面的类)) request 功能获取请求消息数据 获取请求行数据 GET /day14/demo1?name=zhangsan HTTP/1.1 方法: 获取请求方式: String getMethod() 一般我们是继承 HttpServlet 类来写自己的 Servlet类, 并重写doGet和doPost, 既然 HttpServlet中已经判断了是什么方法, 所以 getMethod 方法一般就不咋用了 获取虚拟目录: String getContextPath() (url除了host和下面的路径, 剩下的就是虚拟目录了.. 哈哈) 获取Servlet路径: String getServletPath() (即 @WebServlet 指定的 urlPatterns 参数) 获取get方式请求参数: String getQueryString() (name=zhangsan) 获取请求URI: String getRequestURI() 获取请求URL: StringBuffer getRequestURL()URL: 统一资源定位符 ： http://localhost/day14/demo1 中华人民共和国URI：统一资源标识符 : /day14/demo1 共和国 获取协议及版本: String getProtocol() (HTTP/1.1) 获取客户机的IP地址: String getRemoteAddr() ( 如果用localhost可能会获取到ipv6的 0:0:0:0:0:0:0:1, 可以换成 127.0.0.1 ) 获取请求头数据 String getHeader(String name): 通过请求头的名称获取请求头的值 Enumeration&lt;String&gt; getHeaderNames(): 获取所有的请求头名称 获取请求体数据请求体：只有POST请求方式 才有请求体, 在请求体中封装了POST请求的请求参数步骤： 获取流对象: BufferedReader getReader(): 获取字符输入流，只能操作字符数据 ServletInputStream getInputStream(): 获取字节输入流, 可以操作所有类型数据 再从流对象中拿数据 获取请求参数通用方式 不论 get 还是 post 请求方式都可以使用下列方法来获取请求参数 String getParameter(String name): 根据参数名称获取参数值 String[] getParameterValues(String name): 根据参数名称获取参数值的数组 (针对form中的复选框) Enumeration&lt;String&gt; getParameterNames(): 获取所有请求的参数名称 Map&lt;String,String[]&gt; getParameterMap(): 获取所有参数的map集合 中文乱码问题： get方式：tomcat8 已经将get方式乱码问题解决了 post方式：会乱码解决: 在获取参数前, 设置request的编码 request.setCharacterEncoding(&quot;utf-8&quot;);另外还要注意tomcat在IDEA控制台中文出现 ???? 的问题: 需要在 “Edit Configuration” tomcat的配置项 VM options加上 -Dfile.encoding=UTF-8 请求转发 一种在服务器内部的资源跳转方式 步骤： 通过 request 对象获取请求转发器对象：RequestDispatcher getRequestDispatcher(String path) 使用 RequestDispatcher 对象来进行转发：forward(ServletRequest request, ServletResponse response) 特点: 浏览器地址栏路径不发生变化 只能转发到当前服务器内部资源中 转发是一次请求 (虽然服务器内部进行了一次转发请求, 但对客户端来说用的都是同一个请求) 共享数据： 域对象: 一个有作用范围的对象，可以在范围内共享数据 request域: 代表一次请求的范围，一般用于请求转发的多个资源中共享数据可以在A servlet中存储一些数据, 然后在其转发的 B servlet 中取到该数据 方法:void setAttribute(String name,Object obj): 存储数据Object getAttribute(String name): 通过键获取值void removeAttribute(String name): 通过键移除键值对 ServletContextServletContext 代表整个web应用, 可以和程序的容器(服务器)来通信 获取 ServletContext 可以通过request对象获取 System.out.println(req.getServletContext()); // org.apache.catalina.core.ApplicationContextFacade@6c263e08 也可以通过HttpServlet获取 this.getServletContext(); 功能 获取 MIME 类型 MIME类型: 在互联网通信过程中定义的一种文件数据类型格式: 大类型/小类型 text/html image/jpeg 获取：String getMimeType(String file) 域对象: 共享数据 setAttribute(String name,Object value) getAttribute(String name) removeAttribute(String name) 获取文件的真实(服务器)路径, 方法: String getRealPath(String path) 12345678String b = context.getRealPath(&quot;/b.txt&quot;);//web目录下资源访问System.out.println(b);String c = context.getRealPath(&quot;/WEB-INF/c.txt&quot;);//WEB-INF目录下的资源访问System.out.println(c);String a = context.getRealPath(&quot;/WEB-INF/classes/a.txt&quot;);//src目录下的资源访问System.out.println(a); 案例 文件下载需求： 页面显示超链接 点击超链接后弹出下载提示框 完成图片文件下载 分析： 超链接指向的资源如果能够被浏览器解析，则在浏览器中展示，如果不能解析，则弹出下载提示框。不满足需求 任何资源都必须弹出下载提示框 使用响应头设置资源的打开方式： content-disposition:attachment;filename=xxx 步骤： 定义页面，编辑超链接href属性，指向Servlet，传递资源名称filename 定义Servlet 获取文件名称 使用字节输入流加载文件进内存 指定response的响应头： content-disposition:attachment;filename=xxx 将数据写出到response输出流 问题： 中文文件问题 解决思路： 获取客户端使用的浏览器版本信息 根据不同的版本信息，设置filename的编码方式不同","categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/categories/JavaWeb/"}],"tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://blog.renyimin.com/tags/JavaWeb/"}]},{"title":"01. redis","slug":"redis/2019-01-26-01","date":"2019-01-16T02:29:26.000Z","updated":"2019-03-15T03:08:45.000Z","comments":true,"path":"2019/01/16/redis/2019-01-26-01/","link":"","permalink":"http://blog.renyimin.com/2019/01/16/redis/2019-01-26-01/","excerpt":"","text":"LRU和AOF持久化文件 SYNC 和 BGSAVEhttp://chenzhenianqing.com/articles/1069.html redis的持久化主要是为了做灾难恢复, 如果redis的持久化做好, 备份和恢复方案做到企业级, 即使redis故障, 也可以通过备份数据, 快速恢复并对外提供服务 危险命令的禁用Redis中线上使用keys *命令，也是非常危险的。因此线上的Redis必须考虑禁用一些危险的命令，或者尽量避免谁都可以使用这些命令，Redis没有完整的管理系统，但是也提供了一些方案https://blog.csdn.net/qq_36101933/article/details/82893540 redis库名每个数据库对外都是一个从0开始的递增数字命名，Redis默认支持16个数据库（可以通过配置文件支持更多，无上限），可以通过配置databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据库，不过可以随时使用SELECT命令更换数据库，如要选择1号数据库然而这些以数字命名的数据库又与我们理解的数据库有所区别。首先Redis不支持自定义数据库的名字，每个数据库都以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。另外Redis也不支持为每个数据库设置不同的访问密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库之间并不是完全隔离的，比如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。综上所述，这些数据库更像是一种命名空间，而不适宜存储不同应用程序的数据。比如可以使用0号数据库存储某个应用生产环境中的数据，使用1号数据库存储测试环境中的数据，但不适宜使用0号数据库存储A应用的数据而使用1号数据库B应用的数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用的内在只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。 Redis白名单设置 阿里云数据库 Redis 设置 ECS-IP白名单 redis的持久化 持久化的意义主要是在于 故障恢复 redis版本选择 redis 3.2 迁移到4.x注意事项 和 redis 4.0的主要特性 redis的阻塞?Redis 节点故障后，主备切换的数据一致性 主从, 集群, 分片 网络分区, 崩溃问题? redis 高并发+高性能+高可用(99.99% 4个9), qps极限, 压测? 如何用复杂的缓存架构去支撑高并发, 将缓存架构做成高可用机会，也可以学到高可用系统架构构建的技术 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 通过主从, 读写分离, 来实现redis支撑高并发 QPS10万+ 日志分析? redis事务问题? WATCH, 重试? redis锁问题? 云数据库Redis版（ApsaraDB for Redis） 架构? Redis LRU 算法? lua脚本? keylord : https://protonail.com/ 主从同步机制是? 同步时, 也额外需要大量内存? 123456789101112131415161718192021222324252627真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？（1）如何让redis集群支撑几十万QPS高并发+99.99%高可用+TB级海量数据+企业级数据备份与恢复？：redis企业级集群架构（2）如何支撑高性能以及高并发到极致？同时给缓存架构最后的安全保护层？：(nginx+lua)+redis+ehcache的三级缓存架构（3）高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？：企业级的完美的数据库+缓存双写一致性解决方案（4）如何解决大value缓存的全量更新效率低下问题？：缓存维度化拆分解决方案（5）如何将缓存命中率提升到极致？：双层nginx部署架构，以及lua脚本实现的一致性hash流量分发策略（6）如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？：基于zookeeper分布式锁的缓存并发重建解决方案（7）如何解决高并发场景下，缓存冷启动MySQL瞬间被打死的问题？：基于storm实时统计热数据的分布式快速缓存预热解决方案（8）如何解决热点缓存导致单机器负载瞬间超高？：基于storm的实时热点发现，以及毫秒级的实时热点缓存负载均衡降级（9）如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？：基于hystrix的高可用缓存服务，资源隔离+限流+降级+熔断+超时控制（10）如何应用分布式系统中的高可用服务的高阶技术？：基于hystrix的容错+多级降级+手动降级+生产环境参数优化经验+可视化运维与监控（11）如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？：独家的事前+事中+事后三层次完美解决方案（12）如何解决高并发场景下的缓存穿透问题？避免给MySQL带来过大的压力？：缓存穿透解决方案（13）如何解决高并发场景下的缓存失效问题？避免给redis集群带来过大的压力？：缓存失效解决方案","categories":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/tags/redis/"}]},{"title":"02. redis","slug":"redis/2019-01-26-02","date":"2019-01-16T02:29:26.000Z","updated":"2019-03-05T05:24:34.000Z","comments":true,"path":"2019/01/16/redis/2019-01-26-02/","link":"","permalink":"http://blog.renyimin.com/2019/01/16/redis/2019-01-26-02/","excerpt":"","text":"https://carlosfu.iteye.com/blog/2254154 如何发觉redis被阻塞","categories":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/tags/redis/"}]},{"title":"FastCGI 进程管理器 (FPM)","slug":"PHP/2019-01-02-FastCGI","date":"2019-01-02T06:16:31.000Z","updated":"2019-06-27T09:08:18.000Z","comments":true,"path":"2019/01/02/PHP/2019-01-02-FastCGI/","link":"","permalink":"http://blog.renyimin.com/2019/01/02/PHP/2019-01-02-FastCGI/","excerpt":"","text":"CGI, FastCGIPHP-FPM php-fpm.conf 和 php.iniPHP-FPM 调优","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"PHP7 新特性","slug":"PHP/2019-01-02-php7","date":"2019-01-02T06:16:31.000Z","updated":"2019-08-01T08:59:45.000Z","comments":true,"path":"2019/01/02/PHP/2019-01-02-php7/","link":"","permalink":"http://blog.renyimin.com/2019/01/02/PHP/2019-01-02-php7/","excerpt":"","text":"可变参数的函数(PHP5.6+)123456789function sum(...$numbers) &#123; $acc = 0; foreach ($numbers as $n) &#123; $acc += $n; &#125; return $acc;&#125;echo sum(1, 2, 3, 4); NULL 合并运算符 (isset()) PHP7 新增加的 NULL合并运算符 ?? 是用于执行 isset() 检测的三元运算的快捷方式 NULL合并运算符 会判断变量是否存在且值不为NULL, 如果是, 它就会返回自身的值, 否则返回它的第二个操作数 以前我们这样写三元运算符: $res = isset($site) ? $site : &#39;phper&#39;; 现在可以简化为: $res = $site ?? &#39;phper&#39;; 太空船操作符PHP7 引入的一个新功能, 在 PHP7 中, 它用来比较两个表达式: 当第一个表达式分别小于、等于或大于第二个表达式时, 它返回的值为: -1、0 或 1 PHP 标量类型与返回值类型声明默认情况下, 所有的PHP文件都处于弱类型校验模式 PHP7 增加了 标量类型声明 的特性, 标量类型声明有两种模式: 强制模式 (默认) 严格模式 标量类型声明语法格式为: declare(strict_types=1); declare(strict_types=0);: 表示 弱类型 校验模式(默认) declare(strict_types=1);: 表示 严格类型 校验模式, 作用于函数调用和返回语句 可以使用的类型参数有: int、float、bool、string、interfaces、array、callable 标量类型声明 的 强制模式校验 模式: 标量类型声明 的 严格类型校验 模式: 匿名类","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"PHP 基础","slug":"PHP/2018-04-06-basic-point","date":"2019-01-02T02:26:58.000Z","updated":"2019-07-29T11:11:29.000Z","comments":true,"path":"2019/01/02/PHP/2018-04-06-basic-point/","link":"","permalink":"http://blog.renyimin.com/2019/01/02/PHP/2018-04-06-basic-point/","excerpt":"","text":"语言结构和函数 什么是语言结构和函数 语言结构: 就是PHP语言的关键词, 语言语法的一部分 函数: 由代码块组成的,可以复用 语言结构为什么比函数快? 原因是在PHP中, 函数都要先被PHP解析器(Zend引擎)分解成语言结构, 所以有此可见, 函数比语言结构多了一层解析器解析这样就能比较好的理解, 为什么语言结构比函数快了 语言结构和函数的不同 语言结构比对应功能的函数快 语言结构在错误处理上比较鲁棒, 由于是语言关键词, 所以不具备再处理的环节 语言结构不能在配置项(php.ini)中禁用, 函数则可以 语言结构不能被用做回调函数 语言结构列表: echo()、print()、die()、isset()、unset()、include()、require()、array()、list()、empty() isset() VS empty()两个函数对比 代码风格指南 PSR 规范 PHP 社区百花齐放，拥有大量的函数库、框架和组件。PHP 开发者通常会在自己的项目中使用若干个外部库，因此 PHP 代码遵循（尽可能接近）同一个代码风格就非常重要，这让开发者可以轻松地将多个代码库整合到自己的项目中。 PHP标准组 提出并发布了一系列的风格建议。其中有部分是关于代码风格的，即 PSR-0, PSR-1, PSR-2 和 PSR-4。这些推荐只是一些被其他项目所遵循的规则，如 Drupal, Zend, Symfony, CakePHP, phpBB, AWS SDK, FuelPHP, Lithium 等。你可以把这些规则用在自己的项目中，或者继续使用自己的风格。 通常情况下，你应该遵循一个已知的标准来编写 PHP 代码。可能是 PSR 的组合或者是 PEAR 或 Zend 编码准则中的一个。这代表其他开发者能够方便的阅读和使用你的代码，并且使用这些组件的应用程序可以和其他第三方的组件保持一致。 阅读 PSR-0 阅读 PSR-1 阅读 PSR-2 阅读 PSR-4 阅读 PEAR 编码准则 阅读 Symfony 编码准则 你可以使用 PHP_CodeSniffer 来检查代码是否符合这些准则 Restful 和 Rpc 区别?https://help.aliyun.com/document_detail/73740.html?spm=a2c4g.11186623.6.545.5d7f2a41QT13zj 幂等https://help.aliyun.com/document_detail/104450.html?spm=a2c4g.11186623.6.553.bb4f4c07Lpyv10","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"Facade","slug":"PHP/2018-12-09-Laravel-03","date":"2018-12-09T08:31:07.000Z","updated":"2019-08-21T02:05:34.000Z","comments":true,"path":"2018/12/09/PHP/2018-12-09-Laravel-03/","link":"","permalink":"http://blog.renyimin.com/2018/12/09/PHP/2018-12-09-Laravel-03/","excerpt":"","text":"","categories":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/categories/Laravel/"}],"tags":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/tags/Laravel/"}]},{"title":"AOP 面向切面编程","slug":"PHP/2018-12-08-Laravel-02","date":"2018-12-08T05:21:32.000Z","updated":"2019-08-21T02:05:15.000Z","comments":true,"path":"2018/12/08/PHP/2018-12-08-Laravel-02/","link":"","permalink":"http://blog.renyimin.com/2018/12/08/PHP/2018-12-08-Laravel-02/","excerpt":"","text":"","categories":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/categories/Laravel/"}],"tags":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/tags/Laravel/"}]},{"title":"DI, Ioc","slug":"PHP/2018-12-08-Laravel-01","date":"2018-12-08T02:49:11.000Z","updated":"2019-08-21T02:04:55.000Z","comments":true,"path":"2018/12/08/PHP/2018-12-08-Laravel-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/08/PHP/2018-12-08-Laravel-01/","excerpt":"","text":"","categories":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/categories/Laravel/"}],"tags":[{"name":"Laravel","slug":"Laravel","permalink":"http://blog.renyimin.com/tags/Laravel/"}]},{"title":"bps, pps, Bps","slug":"tips/bps,pps,Bps","date":"2018-12-03T11:57:03.000Z","updated":"2019-07-03T04:01:46.000Z","comments":true,"path":"2018/12/03/tips/bps,pps,Bps/","link":"","permalink":"http://blog.renyimin.com/2018/12/03/tips/bps,pps,Bps/","excerpt":"","text":"bpsppsBps","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"流量","slug":"tips/流量","date":"2018-11-26T05:11:29.000Z","updated":"2019-07-04T03:40:21.000Z","comments":true,"path":"2018/11/26/tips/流量/","link":"","permalink":"http://blog.renyimin.com/2018/11/26/tips/流量/","excerpt":"","text":"虚拟主机流量如何计算? 如何估计站点流量流量的价格与价值视频网站流量250万PPS 上行流量下行流量","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"QPS, TPS","slug":"tips/qps,tps","date":"2018-11-26T05:11:29.000Z","updated":"2019-07-03T04:22:00.000Z","comments":true,"path":"2018/11/26/tips/qps,tps/","link":"","permalink":"http://blog.renyimin.com/2018/11/26/tips/qps,tps/","excerpt":"","text":"日常工作中经常会听到 QPS、TPS 这些名词, 也会经常被别人问起说你的系统吞吐量有多大, 这个问题从业务上来讲, 可以理解为应用系统每秒钟最大能接受的用户访问量或者每秒钟最大能处理的请求数 QPS每秒钟处理完请求的次数, 注意这里是处理完, 具体是指发出请求到服务器处理完成功返回结果, 可以理解在server中有个counter, 每处理一个请求加1, 1秒后counter=QPS TPS每秒钟处理完的事务次数, 一般TPS是对整个系统来讲的, 一个应用系统1s能完成多少事务处理; 一个事务在分布式处理中, 可能会对应多个请求, 对于衡量单个接口服务的处理能力, 用QPS比较多 并发量指系统能同时处理的请求数 RT响应时间, 处理一次请求所需要的平均处理时间 计算关系 QPS = 并发量 / 平均响应时间即可算出每秒能处理多少请求, 协成这样比较好理解: 并发量 * (1s / 平均响应时间) 即 并发量 * 1秒能响应多少次 并发量 = QPS * 平均响应时间","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"PV, UV, VV, IP","slug":"tips/pv,uv,vv,ip","date":"2018-11-26T02:45:17.000Z","updated":"2019-07-02T03:04:35.000Z","comments":true,"path":"2018/11/26/tips/pv,uv,vv,ip/","link":"","permalink":"http://blog.renyimin.com/2018/11/26/tips/pv,uv,vv,ip/","excerpt":"","text":"PVPV 即 PageView, 页面访问量, 每打开一次页面, PV计数+1，刷新页面也是 指页面的浏览次数, 用于衡量网站用户访问的网页数量, 用户每次打开一个页面便记录1次PV, 多次打开同一页面则浏览量累计。 一般来说，PV与来访者的数量成正比，但是PV并不直接决定页面的真实来访者数量，如同一个来访者通过不断的刷新页面，也可以制造出非常高的PV 具体的说，PV值就是所有访问者在24小时（0点到24点）内看了某个网站多少个页面或某个网页多少次。PV是指页面刷新的次数，每一次页面刷新，就算做一次PV流量。度量方法就是从浏览器发出一个对网络服务器的请求（Request），网络服务器接到这个请求后，会将该请求对应的一个网页（Page）发送给浏览器，从而产生了一个PV。那么在这里只要是这个请求发送给了浏览器，无论这个页面是否完全打开（下载完成），那么都是应当计为1个PV UVUV 即 Unique Visitor, 独立访客数 指一天内访问某站点的人数, 以cookie为依据, 1天内同一访客的多次访问只记录为一个访客, 通过 IP 和 cookie 是判断UV值的两种方式 当客户端第一次访问某个网站服务器的时候, 网站服务器会给这个客户端的电脑发出一个Cookie, 通常放在这个客户端电脑的C盘当中, 在这个Cookie中会分配一个独一无二的编号, 这其中会记录一些访问服务器的信息, 如访问时间, 访问了哪些页面等等; 当你下次再访问这个服务器的时候, 服务器就可以直接从你的电脑中找到上一次放进去的Cookie文件, 并且对其进行一些更新, 但那个独一无二的编号是不会变的。 VVVV 即 Visit View, 访客的访问次数 用以记录所有访客1天内访问了您的网站多少次, 当访客完成所有的浏览并最终关掉该网站的所有页面时便完成了一次访问, 同一访客1天内可能有多次访问行为, 访问次数累计 和 PV 不同, PV是访问每个页面都累计, VV是从进入网站到关闭网站, 算一次 IPIP 即 Internet Protocol, 独立IP数 指1天内使用不同IP地址的用户访问网站的数量, 同一IP无论访问了几个页面, 独立IP数均为1 IP和UV之间的数据不会有太大的差异，通常UV量和比IP量高出一点，每个UV相对于每个IP更准确地对应一个实际的浏览者 UV大于IP:这种情况就是在网吧、学校、公司等，公用相同IP的场所中不同的用户，或者多种不同浏览器访问您网站，那么UV数会大于IP数。 UV小于IP:在家庭中大多数电脑使用ADSL拨号上网，所以同一个用户在家里不同时间访问您网站时，IP可能会不同，因为它会根据时间变动IP，即动态的IP地址，但是实际访客数唯一，便会出现UV数小于IP数 (网站说日均IP/ PV 访问量约为600 / 2400的意思是，今天访问首页次数为2400次，访问IP为600个。也就是说这600个IP一共访问首页2400次。) 实例说明 小明在家用ADSL拨号上网，早上8点访问了www.a.com下的2个页面，下午2点又访问了www.a.com3个页面。那么，对于www.a.com来讲，今天的PV、UV、VV、IP各项指标该如何计算呢？ PV：5 PV指浏览量，因此PV指等于上午浏览的2个页面和下午浏览的3个页面之和； UV：1 UV指独立访客数，因此一天内同一访客的多次访问只计为1个UV； VV：1 VV指访客的访问次数，上午和下午分别有一次访问行为，因此VV为2 IP：2 IP为独立IP数，由于ADSL拨号上网每次都IP不同，因此独立IP数位2；","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"32. 泛型 与 JVM","slug":"JavaSE/2018-11-18-32","date":"2018-11-18T12:03:57.000Z","updated":"2019-07-17T07:43:32.000Z","comments":true,"path":"2018/11/18/JavaSE/2018-11-18-32/","link":"","permalink":"http://blog.renyimin.com/2018/11/18/JavaSE/2018-11-18-32/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"31. 泛型擦除(Type Erasure)","slug":"JavaSE/2018-11-18-31","date":"2018-11-18T07:51:17.000Z","updated":"2019-08-21T02:14:28.000Z","comments":true,"path":"2018/11/18/JavaSE/2018-11-18-31/","link":"","permalink":"http://blog.renyimin.com/2018/11/18/JavaSE/2018-11-18-31/","excerpt":"","text":"概述 泛型信息只存在于代码编译阶段, 在进入 JVM 之前, 与泛型相关的信息会被擦除掉, 专业术语叫做类型擦除 简单来说, 泛型擦除是指在编译后, 字节码文件中的泛型类型信息被擦除, 变为 原生类型(raw type), 因此在运行期, ArrayList&lt;Integer&gt; 与 ArrayList&lt;String&gt; 就是同一个类 实际上 Java 泛型的擦除并不是对所有使用泛型的地方都会擦除的, 部分地方会保留泛型信息 下面的例子在编译期无法通过: 因为编译器知道如下代码编译后泛型会被擦除掉 因为 ArrayList&lt;Integer&gt; 与 ArrayList&lt;String&gt; 在编译后都被擦除了, 变成了原生类型 ArrayList tips: 123List&lt;String&gt; stringList = new ArrayList&lt;&gt;();List&lt;Integer&gt; integerList = new ArrayList&lt;&gt;();System.out.println(stringList.getClass() == integerList.getClass()); //true 在泛型类被类型擦除的时候, 如果类型参数部分没有指定上限, 如 &lt;T&gt; 会被转译成普通的 Object 类型, 如果指定了上限, 则类型参数被替换成类型上限 我们可以借助 Java 的 Type 接口获取泛型(Java中的Type详解) ? https://blog.csdn.net/briblue/article/details/76736356绕过泛型擦除","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"30. Class<T>, Class<?>","slug":"JavaSE/2018-11-18-30","date":"2018-11-18T03:12:37.000Z","updated":"2019-08-19T11:01:29.000Z","comments":true,"path":"2018/11/18/JavaSE/2018-11-18-30/","link":"","permalink":"http://blog.renyimin.com/2018/11/18/JavaSE/2018-11-18-30/","excerpt":"","text":"Class&lt;T&gt; 类传递 使用 Class&lt;T&gt; 传递泛型类Class对象 比如, 我们在使用JSON解析字符串时, 代码可能如下 1234public static List&lt;PartnerCityDto&gt; parseArray(PushProductDto pushProductDto)&#123; List&lt;PartnerCityDto&gt; partnerCityDtos = JSONArray.parseArray(pushProductDto.getCityName(), PartnerCityDto.class); return partnerCityDtos;&#125; 把上面的语句组装成一个泛型函数要怎么来做? 首先, 我们应该把 PartnerCityDto 单独抽出来做为泛型变量, 但 parseArray() 中用到的 PartnerCityDto.class 要怎么弄 12345// 一般 Class&lt;T&gt; 常见会用于泛型方法public static &lt;T&gt; List&lt;T&gt; parseArray(PushProductDto pushProductDto, Class&lt;T&gt; object)&#123; List&lt;PartnerCityDto&gt; partnerCityDtos = JSONArray.parseArray(pushProductDto, object); return partnerCityDtos;&#125; 注意, 我们用的 Class&lt;T&gt; object 来传递类的class对象, 即上面提到的 PartnerCityDto.class Class&lt;T&gt; 也是泛型, 它是传来用来装载类的class对象的, 它的定义如下: 123public final class Class&lt;T&gt; implements Serializable &#123; // ...&#125; Class&lt;T&gt; 和 Class 的区别: 前者会进行类型检测, 后者不会","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.renyimin.com/tags/Java/"}]},{"title":"05. 版本部分回退","slug":"git/2018-11-16-05","date":"2018-11-16T09:10:06.000Z","updated":"2019-07-18T09:23:12.000Z","comments":true,"path":"2018/11/16/git/2018-11-16-05/","link":"","permalink":"http://blog.renyimin.com/2018/11/16/git/2018-11-16-05/","excerpt":"","text":"场景 假设你正在A分支上进行开发, 此时对 1,2,3 三份文件做了修改, 然后直接 git add ., git commit -m &quot;&quot; 进行了提交, 但是提交后发现, 文件3 是不需要修改的, 你需要将该文件恢复之前版本 如果直接进行版本回退, 1,2,3 这三个文件的改动都会被回退, 如果改动内容比较少, 你可以这样做; 但是如果这个功能对三个文件进行的改动非常大, 你总不能重来一遍吧, 即便是可以通过编辑器本身的history功能可以找到文件的修改版本, 但这种经方法还是比较low 所以你需要的是能够将某次commit中的部分文件进行版本回退的功能","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"04. git stash","slug":"git/2018-11-16-04","date":"2018-11-16T08:53:02.000Z","updated":"2019-07-18T09:23:12.000Z","comments":true,"path":"2018/11/16/git/2018-11-16-04/","link":"","permalink":"http://blog.renyimin.com/2018/11/16/git/2018-11-16-04/","excerpt":"","text":"场景 在使用git时, 经常会出现 你正在A分支进行开发, 但是突然有个小需求需要紧急切换到B分支进行开发, 遇到这种情况, 我以前的做法通常如下: 把A分支上上榜为完成开发的代码直接 commit 然后checkout到B分支进行开发 上面方式存在的问题是: 本来我每次commit, 都希望是一次比较完整的功能点提交, 而上述操作可能会出现, 某个功能块是尚未完成就直接进行了一次无意义的commit, 显得比较low了, 此时就应该使用 git stash 了 先将A分支目前的工作现场保暂存起来 然后checkout到B分支进行开发工作 B分支上的工作开发完成后, 在checkout到A分支, 将之前暂存的开发内容恢复到工作区即可 git rebase git stash 可以用来保存工作现场 git stash list 可以用来查看保存的工作现场列表 恢复工作现场: git stash apply : 恢复时不删除stash中的内容, 需要通过 git stash drop stash@{0} 手动删除例 git stash apply stash@{0} git stash pop : 恢复的同时也将stash内容删除掉","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"03. git push 时出现 \"Merge branch 'master' of ...\"","slug":"git/2018-11-14-03","date":"2018-11-14T06:16:39.000Z","updated":"2019-07-18T09:23:12.000Z","comments":true,"path":"2018/11/14/git/2018-11-14-03/","link":"","permalink":"http://blog.renyimin.com/2018/11/14/git/2018-11-14-03/","excerpt":"","text":"当我们在使用git进行协同开发时, 假设 develop 为大家的一条协同分支: 当你在本地进行开发完后, 会将自己的分支合并到develop并 git push 到远端 如果此时在你之前并没有其他人对develop进行过commit并push到远端, 你是可以直接push成功的, 但这种情况一般不多, 毕竟多人协同开发现在很普遍 与上面相对的是, 在你push之前, 很可能已经有其他人对develop进行过commit并push到远端, 所以你经常会出现 push 不成功的问题, 如下: 此时一般常见做法是 先 git pull, 然后再 git push, 但在 git pull 时, 其实还分两种情况 一种是远端和本地产生了冲突:此时你需要在本地解决冲突, 并重新进行 commit, push最终的分支结构如下: 另外一种是远端虽然有改动, 但是和本地没有冲突此时虽然没有提示冲突, 但依然push失败, 当你执行pull时, 会直接弹出如下编辑框如果你wq保存编辑页, 你会发现git pull自动进行了ff自动合并最终的分支结构如下: 可以发现, 当 git pull 时, 会将远端与本地进行一次 git merge, 此时 可能会无冲突自动完成 fast-farward 合并 (此时会出现一个 Merge branch &#39;master&#39; of ... commit点) 也可能需要解决冲突再手动提交 (此时是自己手动打上的 commit点) 如何在 git push 时避免出现 Merge branch &#39;master&#39; of ... ? 方案一: (比较low的做法)在 git pull 弹出编辑页面时, 对commit点的日志内容进行修改 ; 或者先 git fetch 然后 git merge, 在本地编写commit点日志信息 (简单来说 git pull = git fetch + git merge)上面的做法并没有解决实际问题, 还会有一个合并时的commit点 方案二: (比较巧) 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature 方案二测试: 两个分支的初始基点一致 两个分支分别进行两次提交, 构造冲突: 然后在dev分支rebase来合并master分支内容 (master可以先pull, 这是不会有冲突的) 查看两分支 最后再在master分支merge去合并dev分支的内容, 此时自然也不会有冲突了 最终的分支都是直线向上 出现类似: “Merge remote-tracking branch ‘remotes/origin/develop’” : ?? git pull 和 git pull origin master ?? 参考: https://www.cnblogs.com/Sinte-Beuve/p/9195018.html https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"02. 选择 merge 还是 rebase?","slug":"git/2018-11-12-02","date":"2018-11-12T09:01:23.000Z","updated":"2019-07-18T09:23:12.000Z","comments":true,"path":"2018/11/12/git/2018-11-12-02/","link":"","permalink":"http://blog.renyimin.com/2018/11/12/git/2018-11-12-02/","excerpt":"","text":"git rebase 介绍 rebase: 变基, 即改变分支的根基 从某种程度来说, git rebase 和 git merge 都可以完成类似的合并工作, 但实际上两者有着本质的不同 假设你的项目有 mywork, origin 两个分支, 如果你想让 mywork 的分支历史看起来像没有经过任何合并一样, 你可以用 git rebase (虽然 git merge 也可以实现, 但是一旦合并遇到冲突时, 还是会出现分叉) 如下命令: 会把你的 origin 分支从 mywork 切出来后的每个提交(commit)取消掉, 并且把它们临时保存为补丁(patch)(这些补丁放到”.git/rebase”目录中), 然后把 origin 分支更新到最新的 mywork 分支, 最后把保存的这些补丁应用到 mywork 分支上。 12$ git checkout mywork$ git rebase origin 具体如下图: 尝试主分支没有commit时 git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 (不修改同一行,先不构造冲突) git log --graph 查看dev日志 git rebase 合并 发现效果和之前的 git merge fast-forward无冲突时 貌似没什么区别 主分支有commit且出现冲突时 接着上面的例子, master 和 dev 分别修改同一行内容, 构造冲突 切换到 master 分支, git rebase 合并dev分支 如上 git rebase 在出现冲突时, Git会停止rebase并会让你去解决冲突, 在解决完冲突后, 用 git add 命令去更新这些内容的索引(index), 然后, 你无需执行 git-commit, 只要执行: git rebase --continue （当然, 无冲突时, 直接 git rebase 就直接完成了合并） 发现好像之前master上的那次提交的信息(时间, 日志等信息)都在, 但是取而代之的是一个新的commit_id, rebase会修改 根基之后 的提交历史 主分支有commit但无冲突 经过测试, 即便无冲突, master分支的新commit也会丢失 测试如下: master, dev两个分支基点相同 master, dev两个分支各自做改动, 但是改动并不不冲突 发现master进行 git rebase dev 直接就合并成功了, 虽然没有冲突, 但是master的commit历史确实还是被改掉了!! 小结 不要在master和其他协作分支上使用 git rebase, 它会修改提交历史; 当使用git做合并操作时, 如果没有冲突, 则 git merge(fast-forward) 和 git rebase 效果一样都是直线, 而git merge --no-ff会出现分叉和新提交点; 当使用git做合并操作时, 如果出现冲突, 则 git merge(fast-forward) 和 git merge --no-ff 一样都会出现分叉和新提交点, git rebase则不同, 它是直线, 但会修改历史提交点; rebase 使用技巧 git rebase 由于会改变历史提交点, 所以一定不能用在协作分支上 使用rebase既可以保证分支直线美观, 也可以保证在git push时不出现类似 “Merge remote-tracking branch…” 问题 通常如下来使用, 下文在解决 类似 “Merge remote-tracking branch…” 问题时, 会有详细过程: 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"01. git merge 对比 fast-forward 与 --no-ff","slug":"git/2018-11-12-01","date":"2018-11-12T05:32:54.000Z","updated":"2019-07-18T09:23:12.000Z","comments":true,"path":"2018/11/12/git/2018-11-12-01/","link":"","permalink":"http://blog.renyimin.com/2018/11/12/git/2018-11-12-01/","excerpt":"","text":"git merge 介绍 在使用 git merge 进行分支合并时, 可以直接使用如下两种方式 git merge (fast-forward) git merge --no-ff 这两者的区别如下: 但是需要注意的是 git merge 的一个特例: 当合并出现冲突时, 其实是无法执行快速合并的,需要解决冲突后再进行一次提交, 所以效果和 git merge --no-ff 是一样的, 也就是还会出现分叉 (可以参考此文) fast-forward git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 Fast-forward 合并 最终可以看到, master分支只是简单地将dev分支的那次提交合进自己的分支内 (不会在graph图中保留dev分支线) 即便是在dev分支做了多次提交, master分支也只是简单地将dev分支的多次提交合进自己的分支内 (不会保留dev分支) —no-ff git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 —no-ff 合并 (相比 FastForward, 这里还会弹出编辑界面, 允许你对本次合并进行说明) 最终可以看到: master分支并没有将dev上的两次提交合到自己的分支上 而是在graph图中保留了dev分支线 并且将这次合并当做一次dev向master的提交, 在master上生成一个新的commit 此时master的分支的commit点已经比dev上的commit要更新一步 此时如果不把master的提交分支合并到dev, 而是在dev上继续做两次提交, 然后再在master上进行 —no-ff 合并, 效果如下: 红色框表示—no-ff合并时所进行的新commit及备注日志 现在如果把master的提交合并到dev 如果在master切出新的分支, 然后再新分支上进行提交, 再回到master进行 —no-ff 合并, 效果依然如下图: ff 冲突导致的例外 重新创建本地仓库, 对master进行两次提交; 新建并切换到dev分支, 并修改文件: 切换回master分支, 修改dev上修改的同一行内容, 构造冲突: 此时, 即便是在master上使用 fast-forward 合并dev, 由于有冲突, 还是会进行一次提交:","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"2. 插件","slug":"IDEA/2018-11-11-02","date":"2018-11-11T09:08:13.000Z","updated":"2019-07-29T03:46:54.000Z","comments":true,"path":"2018/11/11/IDEA/2018-11-11-02/","link":"","permalink":"http://blog.renyimin.com/2018/11/11/IDEA/2018-11-11-02/","excerpt":"","text":"插件安装 可以在IDEA中直接下载安装 也可以下载jar包, 然后在IDEA插件安装的地方引入即可 statistic https://plugins.jetbrains.com/plugin/4509-statistic/versions Free Mybatis plugin 可以从 XXXMapper.java 跳转到 XXXMapper.xml 中 Alibaba Java Coding Guidelines 《阿里巴巴Java开发手册》IDEA插件 直接搜索 alibaba 即可查找到 主题http://color-themes.com/?view=theme&amp;id=5afc014550544f1700232c32","categories":[{"name":"IDEA","slug":"IDEA","permalink":"http://blog.renyimin.com/categories/IDEA/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://blog.renyimin.com/tags/IDEA/"}]},{"title":"1. IntelliJ IDEA","slug":"IDEA/2018-11-11-01","date":"2018-11-11T09:08:13.000Z","updated":"2019-07-18T07:54:25.000Z","comments":true,"path":"2018/11/11/IDEA/2018-11-11-01/","link":"","permalink":"http://blog.renyimin.com/2018/11/11/IDEA/2018-11-11-01/","excerpt":"","text":"基础快捷键 Ctrl+Alt+L : 代码格式化 Alt+Enter : 导入包 ctrl + 点击方法 从调用处, 跳到方法处, Ctrl + Alt + &lt;- 再调跳回 方法的调用处 Ctrl+d : 删除当前行 Alt+Shift+ 上,下键 : 移动本行代码 批量修改变量名: 选中变量名, shift+F6, 写新的变量名即可 跳转 最近打开的文件 Command + E项目之间的跳转当IDEA 项目窗口比较多的时候, 可以使用这种方式 上一个窗口：Command+shift+ ` 下一个窗口：Command+ ` 文件之间的跳转Ctrl + e 调出最近打开的文件(同时还可以进行搜索筛选) 最近修改文件切换Ctrl + Shift + E 代码便捷操作 psvm : 生成主函数 sout : 打印 5.fori : for (int i = 0; i &lt; 5; i++) { } Ctrl+Enter : 生成 getter、setter、构造方法、toString 等方法 常用设置 修改类头部的文档信息: Editor -&gt; File and Code Templates -&gt; File Header, 可以设置如下 12345/** * * @Author:YiminRen * @Date: $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;TIME&#125; */ 列编辑模式: Alt+鼠标左键 即可进入列编辑模式 忽略大小写的提示 (一般也没有人去设置) tips 初次运行程序, 如果报错 Error:java: 无效的源发行版: 9 (解决方案: https://www.cnblogs.com/dongling/p/8670852.html) IDEA 右键代码文件, 可以通过 “Show in Explorer” 查看字节码文件 (貌似在Mac上 没看到, 可以通过 “File”-&gt;”Project Structure” 找到 “out” 编译目录) 查看编译目录, 内部类的.class字节码文件, 命名格式为 外部类名$内部类名.class","categories":[{"name":"IDEA","slug":"IDEA","permalink":"http://blog.renyimin.com/categories/IDEA/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://blog.renyimin.com/tags/IDEA/"}]},{"title":"PHP7 vs HHVM","slug":"PHP/2018-07-10-php7-vs-hhvm-01","date":"2018-07-10T05:21:32.000Z","updated":"2019-07-31T03:30:13.000Z","comments":true,"path":"2018/07/10/PHP/2018-07-10-php7-vs-hhvm-01/","link":"","permalink":"http://blog.renyimin.com/2018/07/10/PHP/2018-07-10-php7-vs-hhvm-01/","excerpt":"","text":"HHVM 概述 HHVM (HipHop Virtual Machine) PHP为什么慢? https://cloud.tencent.com/developer/article/1181601","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"82. 集群","slug":"rabbitmq/2018-07-03-rabbitmq-82","date":"2018-07-03T11:40:30.000Z","updated":"2018-06-19T12:16:38.000Z","comments":true,"path":"2018/07/03/rabbitmq/2018-07-03-rabbitmq-82/","link":"","permalink":"http://blog.renyimin.com/2018/07/03/rabbitmq/2018-07-03-rabbitmq-82/","excerpt":"","text":"普通模式集群镜像模式集群","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"14. 建立双索引--- text分词 + 排序","slug":"elasticsearch/2018-06-21-14","date":"2018-06-21T11:30:52.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/21/elasticsearch/2018-06-21-14/","link":"","permalink":"http://blog.renyimin.com/2018/06/21/elasticsearch/2018-06-21-14/","excerpt":"","text":"如果对一个 text 类型的字段进行排序, 由于该字段会进行分词处理, 这样的话, 排序的结果就可能不是我们想要的结果; 通常的解决方案是在建立 mapping 时, 同时为该字段建立两个索引: 一个进行分词用来进行全文检索 一个不进行分词, 用来进行排序 注意使用 &quot;fielddata&quot;: true 练习: 创建mapping, 构造数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253DELETE /mywebsitePUT /mywebsite&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot; : &quot;keyword&quot; # &quot;index&quot;: false &#125; &#125;, &quot;fielddata&quot;: true &#125;, &quot;contennt&quot;: &#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125;PUT /mywebsite/article/1&#123; &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57&#125;PUT /mywebsite/article/2&#123; &quot;title&quot;: &quot;JAVA Language&quot;, &quot;content&quot;: &quot;Java LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-12&quot;, &quot;author_id&quot;: 32&#125;PUT /mywebsite/article/3&#123; &quot;title&quot;: &quot;C Language&quot;, &quot;content&quot;: &quot;c LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-09&quot;, &quot;author_id&quot;: 86&#125;GET /mywebsite/_mapping/article``` - 测试, 如果使用**title字段的不分词索引进行检索**, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title.raw”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容{“took”: 9,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;PHP Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;JAVA Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;C Language&quot; ] } ]}} 1- 测试, 如果使用title默认的分词索引进行检索, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容, 是title字段分词后的内容{“took”: 8,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;php&quot; ] }, { ...... &quot;sort&quot;: [ &quot;language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;language&quot; ] } ]}}```","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"13. mapping","slug":"elasticsearch/2018-06-19-13","date":"2018-06-19T11:21:52.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/19/elasticsearch/2018-06-19-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/elasticsearch/2018-06-19-13/","excerpt":"","text":"mapping核心数据类型 es的文档中, 每个字段都有一个数据类型, 可以是: 一个简单的类型, 如 text, keyword, date, long, double, boolean 或 ip-支持JSON的分层特性的类型,如对象或嵌套 或者像 geo_point, geo_shape 或 completion 这样的特殊类型 为不同目的以不同方式索引相同字段通常很有用, 例如, 字符串字段可以被索引为用于全文搜索的文本字段, 并且可以被索引为用于排序或聚合的关键字字段, 或者, 可以使用标准分析器, 英语分析器和法语分析器索引字符串字段; 之前已经了解过: 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping;为了更准确方便地让es理解我们的意图, 一般我们会对index的type手动来创建mapping mapping操作 GET /products/_mapping/computer 只能在创建index时手动创建mapping, 或者新增field mapping, 但是不能 update filed mapping; 手动创建index并设置mapping 1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot; : &quot;date&quot; &#125;, # 如果不想进行分词, 就设置为 keyword &quot;publisher_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 尝试修改某个字段的mapping, 会报错 1234567891011121314151617181920212223242526272829303132PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; &#125;&#125;# 结果报错&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 测试mapping 测试1 123456789101112131415161718192021222324GET website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 注意 只能在不同的索引中对相同的字段设定不同的datatype, 即便是在同一个index中的不同type, 也不能对相同的field设置不同的datatype;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"12. 分词器","slug":"elasticsearch/2018-06-17-12","date":"2018-06-17T09:05:52.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/17/elasticsearch/2018-06-17-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/17/elasticsearch/2018-06-17-12/","excerpt":"","text":"之前在介绍mapping时, 已经了解到, ES会根据文档的字段类型, 来决定该字段是否需要进行分词和倒排索引, 而分词器的主要工作就是对字段内容进行分词, 通过分词器处理好的结果才会拿去建立倒排索引; ES内置的分词器: standard analyzer simple analyzer whitespace analyzer language analyzer 测试分词器: 12345678910111213141516171819202122232425262728293031GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, # 切换进行测试 &quot;text&quot; : &quot;Test to analyze&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;test&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125; ]&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"11. 诡异的搜索结果 引出 mapping","slug":"elasticsearch/2018-06-16-11","date":"2018-06-16T11:23:16.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-11/","excerpt":"","text":"诡异的搜索结果 构造数据 1234567891011121314151617181920DELETE /websiteGET /website/_mappingPUT /website/article/1&#123; &quot;post_date&quot;: &quot;2018-06-20&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;php is the best language&quot;&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2018-06-21&quot;, &quot;title&quot;: &quot;java&quot;, &quot;content&quot;: &quot;java is the second&quot;&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2018-06-22&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;C++ is third&quot;&#125; 诡异的搜索结果 1234GET /website/article/_search?q=2018 # 3条查询结果GET /website/article/_search?q=2018-06-21 # 3条查询结果GET /website/article/_search?q=post_date:2018-06-22 # 1条查询结果GET /website/article/_search?q=post_date:2018 # 0条查询结果 结果分析 前两个查询之所以能匹配到所有文档, 是因为查询时并没有指定字段进行匹配, 所以默认查询的是_all字段, 而_all是经过分词的并且有倒排索引对于第一个查询来说, 2018 这个值进行分词后还是2018, 自然是可以匹配到所有文档的而对于第二个查询来说, q=2018-06-21 进行分词后也包含2018, 所以也可以匹配到所有文档 对于第三个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 可以匹配到是比较正常的情况 而对于第四个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 但却没有结果 这就要引出 ES 的mapping机制了 Mapping映射 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping; mapping就是index的type的元数据, 每个type都有一个自己的mapping, 决定了该type下文档中每个field的数据类型, 分词及建立倒排索引的行为 以及 进行搜索的行为; ES在自动创建mapping时, 会根据字段值去自行猜测字段的类型, 不同类型的field, 有的是full-text, 有的就是exact-value 对于 full-text型的field, es会对该filed内容进行分词, 时态转换, 大小写转换, 同义词转换等一系列操作后, 建立倒排索引; 对于 exact-value型的field, es则不会进行分词等处理工作 full-text型 和 exact-value型 的不同, 也决定了当你进行搜索时, 其处理行为也是不同的 如果指明要搜索的field, ES也会根据你要搜索的字段的类型, 来决定你发送的搜索内容是否先需要进行全文分析…等一些列操作 当然, 如果你搜索时不指定你具体字段, 则搜索的是 _all, 是会先对你的发送的搜索内容进行分词等操作的 之前诡异的例子中, 其实就是因为在创建文档时, 由于 post_date 字段的值被ES自认为是date类型(exact-value精确值), 所以es不会对这种类型做分词及倒排索引, 所以 GET /website/article/_search?q=post_date:2018 在搜索时候, 其实是去精准匹配post_date字段了, 所以匹配不到; 查看你索引type的默认mapping: 123456789101112131415161718192021222324252627282930313233GET /website/_mapping&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 引出手动创建mapping 除了让es自动为我们创建mapping, 一般我们都是在创建文档前, 先手动去创建index和type, 以及type对应的mapping 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, ES 需要知道每个域中数据的类型 而很显然我们比ES更了解我们的字段类型, ES根据值去判断的话, 很容易出现误判 比如你如果字段是个日期, 可能形式为 2018-06-20 12:13:15 但ES可能不会认为这是个date类型, 如果是 2018-06-20 它又认为是date类型, 所以还是自己手动设置比较好 虽然映射是index的type的, 但事实上, 如果在相同的index中, 即使在不同的type, 你也不能对相同字段做不同的类型指定, 可参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"57. MyISAM InnoDB 区别","slug":"MySQL 暂停/2018-06-16-mysql-57","date":"2018-06-16T08:30:56.000Z","updated":"2019-04-26T11:19:43.000Z","comments":true,"path":"2018/06/16/MySQL 暂停/2018-06-16-mysql-57/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/MySQL 暂停/2018-06-16-mysql-57/","excerpt":"","text":"InnoDB支持事务, MyISAM不支持, 对于InnoDB每一条SQL语言都默认封装成事务, 自动提交, 这样会影响速度, 所以最好把多条SQL语言放在begin和commit之间, 组成一个事务; InnoDB支持外键, 而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败; InnoDB是聚集索引, 数据文件是和索引绑在一起的, 必须要有主键, 通过主键索引效率很高。但是辅助索引需要两次查询, 先查询到主键, 然后再通过主键查询到数据。因此, 主键不应该过大, 因为主键太大, 其他索引也都会很大。而MyISAM是非聚集索引, 数据文件是分离的, 索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB不保存表的具体行数, 执行 select count(*) from table 时需要全表扫描。而MyISAM用一个变量保存了整个表的行数, 执行上述语句时只需要读出该变量即可, 速度很快; Innodb不支持全文索引, 而MyISAM支持全文索引, 查询效率上MyISAM要高;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"10. 了解 `_all` field","slug":"elasticsearch/2018-06-16-10","date":"2018-06-16T06:29:39.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-10/","excerpt":"","text":"_all 对于在学习 query-string 搜索时, 一般这样来用 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc 这种查指定字段进行筛选的方式; 其实ES还可以直接 GET /products/computer/_search?q=diaonao 来进行检索, 这种检索方式会对文档中的所有字段进行匹配; 之所以可以对文档中的所有字段进行匹配, 是 _all 元数据的作用 当你在ES中索引一个document时, es会自动将该文档的多个field的值全部用字符串的方式连接起来, 变成一个长的字符串, 作为 _all field值, 同时对_all分词并建立索引; 之后在搜索时, 如果没有指定对某个field进行搜索, 默认就会搜索 _all field, 而你传递的内容也会进行分词后去匹配 _all 的倒排索引 练习 1234567891011DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 下面的检索都可以搜索到上面的文档GET /products/computer/_search?q=4500GET /products/computer/_search?q=xuhang","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"09. 组合多条件搜索","slug":"elasticsearch/2018-06-16-09","date":"2018-06-16T02:07:26.000Z","updated":"2019-08-30T08:52:14.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-09/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-09/","excerpt":"","text":"查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all : 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match : 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 keyword 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 12345678910111213141516171819202122232425DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot; : &#123; &quot;query&quot;: &quot;language&quot;, &quot;fields&quot;:[&quot;content&quot;, &quot;title&quot;] &#125; &#125;&#125; range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者 keyword字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678910111213141516DELETE /testGET /test/_mapping/languagePUT /test/language/1&#123; &quot;first_name&quot;: &quot;jhon&quot;, &quot;last_name&quot;: &quot;ren&quot;&#125;GET /test/language/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;Jhon&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter, 这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果 (不过, 这只是当should不在must或should下的时候) 只有 must 和 must_not 中的子句是决定了是否能查询出数据 而 should 只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分 (如果should之外的其他语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, # 增加评分 &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 上面那种 should 用法的不同之处 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;, &quot;author_id&quot;: 71&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;, &quot;author_id&quot;: 32&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;, &quot;author_id&quot;: 56&#125;# 下面顺带演示了sort定制排序, 而不是使用默认的相关度排序GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125; a AND (b OR c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句此时的用法和一开始那种不同, 它不仅仅是提升结果评分, 而是直接决定了结果是否匹配 12345678910111213141516GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 123456789101112131415161718192021GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; # 过滤可以使用 constant_score &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; a OR (b AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 1234567891011121314151617181920GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 constant_score &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; 组合过滤 和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"08. 全文检索,结构化精确检索,短语检索,统计 预习","slug":"elasticsearch/2018-06-15-08","date":"2018-06-15T10:56:31.000Z","updated":"2019-08-30T08:52:14.000Z","comments":true,"path":"2018/06/15/elasticsearch/2018-06-15-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/elasticsearch/2018-06-15-08/","excerpt":"","text":"查询和过滤 在es中检索文档时候, 对文档的筛选分为 查询 和 过滤, 这两种方式是不太一样的 练习, 搜索商品desc字段中包含 ‘diannao’, 并且售价大于5000的商品 1234567891011121314151617GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot; : &#123;&quot;gt&quot;: 5000&#125; &#125; &#125; &#125; &#125;&#125; 注意: 结构化检索(精确类型字段的检索) 一般会被放到filter过滤语句中, 不会进行分词和相关度排名, 但会对过滤进行缓存 全文检索(全文类型字段的检索) 一般用查询语句进行筛选, 会进行分词和相关度排名 full-text 检索 ES可以进行全文检索并可以进行相关度排名 重新准备数据 1234567891011121314151617181920212223242526272829DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer&quot;, &quot;desc&quot; : &quot;gaoqing hongji diannao&quot;, &quot;price&quot; : 4870, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;chaobao&quot;, &quot;gaoqing&quot;] &#125;PUT /products/computer/3&#123; &quot;name&quot; : &quot;dell&quot;, &quot;desc&quot; : &quot;daier chaoji diannao&quot;, &quot;price&quot; : 5499, &quot;tag&quot; : [&quot;shishang&quot;, &quot;gaoqing&quot;, &quot;gaoxingneng&quot;] &#125;POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125; 练习, 全文检索 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;&#125; 练习 全文高亮检索 12345678910111213GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;desc&quot; : &#123;&#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWbE6HmlWC0s-aachNUv&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;huawei&quot;, &quot;desc&quot;: &quot;china best diannao gaoqing&quot;, &quot;price&quot;: 6080, &quot;tag&quot;: [ &quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;desc&quot;: [ &quot;china best &lt;em&gt;diannao&lt;/em&gt; &lt;em&gt;gaoqing&lt;/em&gt;&quot; ] &#125; &#125;, ...... 结构化精确检索phrase search(短语搜索) 与全文索引不同, 全文索引会对你发送的 查询串 进行拆分(做分词处理), 然后去倒排索引中与之前在存储文档时分好的词项进行匹配, 只要你发送的查询内容拆分后, 有一个词能匹配到倒排索引中的词项, 该词项所对应的文档就可以返回; phrase search(短语搜索)则不会对你发送的 查询串 进行分词, 而是要求在指定查询的字段中必须包含和你发送的查询串一模一样的内容 才算是匹配, 否则该文档不能作为结果返回; 短语搜索 和 结构化搜索还是不一样 结构化搜索是 你的查询串 和 指定的文档字段内容 是完全一致的, 查询串和字段本身都不会做分词, 一般该字段也是精确类型的字段类型; 而 短语搜索 则是, 你的 查询串 不会做分词, 但是你查询的字段可能会做分词, 你的查询串需要包含在 指定字段中; 下一篇可以查看一下terms的用法和效果 搜索商品desc字段中包含 ‘gaoqing diannao’短语 的文档 123456789# 短语检索GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;desc&quot;: &quot;diannao gaoqing&quot; &#125; &#125;&#125; 结果发现, 虽然还是查询的全文字段desc, 但是结果却只有一个 提前了解ES统计语法 统计商品 每个tag下的商品数量, 即, 根据商品的tag进行分组 12345678GET /products/computer/_search&#123; &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 初次运行报错 1234567891011121314151617181920212223242526&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;products&quot;, &quot;node&quot;: &quot;eCgKpl8JRbqwL3QY0Vuz3A&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; &#125; ] &#125;, &quot;status&quot;: 400&#125; 解决方案: 将文本field的 filedata 属性设置为true (现在不用知道这玩意儿, 先尽快解决, 看到聚合分析的预发和效果, 后面讲在详聊该问题) 123456789PUT /products/_mapping/computer&#123; &quot;properties&quot;: &#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 重新执行统计语句, 发现返回中除了分析的结果, 还包含了查询的文档内容; 如果只想显示聚合分析的结果, 可以如下设置size为0: 123456789GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 针对名称中包含”china”的商品, 计算每个tag下的商品数 12345678910111213GET /products/computer/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;gaoqing&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot; : &#123; &quot;terms&quot; : &#123;&quot;field&quot;: &quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格 (先分组, 再计算每组的平均值) 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot; : &#123; &quot;avg&quot; : &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格, 并且按照平均价格进行排序 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;, &quot;order&quot;: &#123;&quot;avg_by_price&quot;:&quot;desc&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 按照指定的价格范围区间进行分组, 然后再每个分组内再按照tag进行分组, 最后在计算每组的平均价格 1234567891011121314151617181920212223242526GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ &#123;&quot;from&quot;:4500, &quot;to&quot;:5000&#125;, &#123;&quot;from&quot;:5000, &quot;to&quot;:5500&#125;, &#123;&quot;from&quot;:5500, &quot;to&quot;:6100&#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;:&#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;4500.0-5000.0&quot;, &quot;from&quot;: 4500, &quot;to&quot;: 5000, &quot;doc_count&quot;: 2, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4870 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5000.0-5500.0&quot;, &quot;from&quot;: 5000, &quot;to&quot;: 5500, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5500.0-6100.0&quot;, &quot;from&quot;: 5500, &quot;to&quot;: 6100, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"07. 查询小优化","slug":"elasticsearch/2018-06-14-07","date":"2018-06-14T02:50:39.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/14/elasticsearch/2018-06-14-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/14/elasticsearch/2018-06-14-07/","excerpt":"","text":"由于你的每个查询操作都可能会被转发到不同node的shard去执行, 现在假设你的查询, 会打到不同的10个shard上, 每个shard上都要花费1秒钟才能出结果, 这样你总共10s后才会给用户响应, 如果是个商品列表, 用户体验就会非常差 假设本来需要在10秒钟拿到100条数据(每个shard上10条), 现在你可以设置让es在1秒钟就让请求返回, 只拿到部分数据即可 此时可以在查询请求时跟上 timeout 参数(10ms, 1s， 1m): GET /_search?timeout=1ms (可灌入大量数据做测试) 深度分页问题: 假设你的列表每页展示20条数据, 总共1万页, 当我们在使用ES进行分页搜索时, 你想查询第9900页的那20条数据当你的请求到达第一个协调节点后, 它会要求ES给你返回所有该索引对应的primary-shard上的前9900页的数据, 然后es在内存中排序后, 这样会大量占用当前协调节点的计算机资源, 所以尽量避免出现这种深度分页的查询;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"52. 索引 和 锁","slug":"MySQL 暂停/2018-06-13-mysql-52","date":"2018-06-13T11:20:53.000Z","updated":"2019-05-07T06:02:25.000Z","comments":true,"path":"2018/06/13/MySQL 暂停/2018-06-13-mysql-52/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/MySQL 暂停/2018-06-13-mysql-52/","excerpt":"","text":"https://segmentfault.com/a/1190000013669062 聚集索引就是按照每张表的主键构造一棵B+树，同时叶子节点存放的即为整张表的行记录数据，聚集索引的叶子节点称为数据页。每个数据页通过一个双向链表来进行连接。通过查看表空间文件发现非数据页节点存放的仅仅是键值及指向数据页的偏移量，而不是一个完整的行记录，而数据页上存放的是完整的每行的记录。聚集索引的存储其实并不是物理上连续的，而仅仅逻辑上连续。页之间通过双向链表进行维护，每个页中的记录之间也通过双向链表维护。聚集索引还有一个优点是它对于主键的排序查找和范围查找速度非常快。 问题 MySQL源码: 为什么INNODB数据页面中最少存储2条记录 页溢出 行溢出后?? https://blog.csdn.net/coolwriter/article/details/80348263 页是InnoDB存储引擎管理数据库的最小磁盘单位。页类型为B-Tree node的页, 存放的即是表中行的实际数据了InnoDB中的页大小为16KB, 且不可以更改InnoDB可以将一条记录中的某些数据存储在真正的数据页面之外, 即作为行溢出数据MySQL的varchar数据类型可以存放65535个字节, 但实际只能存储65532个同时InnoDB是B+树结构的, 因此每个页中至少应该有两个行记录, 否则失去了B+树的意义, 变成了链表, 所以一行记录最大长度的阈值是8098, 如果大于这个值就会将其存到溢出行中 锁的额外https://segmentfault.com/a/1190000015738121https://www.jianshu.com/p/8c71e12be8ac https://segmentfault.com/a/1190000013307132 http://www.php.cn/mysql-tutorials-388945.html 页https://blog.csdn.net/u013967628/article/details/84305511#%E7%B4%A2%E5%BC%95%E7%9A%84%E6%9C%AC%E8%B4%A8 如果单行数据超过每页16k?https://www.cnblogs.com/zhiqian-ali/p/5037317.html 数据库连接数满了?大表操作修改表结构, 加索引 长时间阻塞表?导致阻塞?主从延迟? B+Treehttp://hedengcheng.com/?p=525 为什么用 B+Tree, 磁盘, 页https://www.cnblogs.com/coshaho/p/7203186.html?utm_source=itdadao&amp;utm_medium=referral https://www.jianshu.com/p/000da6f11629 B树, B+Tree, B*, Rhttps://blog.csdn.net/v_JULY_v/article/details/6530142 http://hedengcheng.com/?p=525","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"12. 惰性队列","slug":"rabbitmq/2018-06-14-rabbitmq-12","date":"2018-06-13T11:14:30.000Z","updated":"2018-06-13T11:15:08.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-14-rabbitmq-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-14-rabbitmq-12/","excerpt":"","text":"https://www.rabbitmq.com/lazy-queues.html","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"10. 消息拉取和推送","slug":"rabbitmq/2018-06-11-rabbitmq-10","date":"2018-06-12T06:36:51.000Z","updated":"2018-06-13T09:24:58.000Z","comments":true,"path":"2018/06/12/rabbitmq/2018-06-11-rabbitmq-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/12/rabbitmq/2018-06-11-rabbitmq-10/","excerpt":"","text":"","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"","slug":"MySQL 暂停/2018-06-11-mysql-50","date":"2018-06-11T02:03:25.000Z","updated":"2019-04-26T11:20:46.000Z","comments":true,"path":"2018/06/11/MySQL 暂停/2018-06-11-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2018/06/11/MySQL 暂停/2018-06-11-mysql-50/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"51. 索引案例","slug":"MySQL 暂停/2018-06-11-mysql-51","date":"2018-06-10T11:20:53.000Z","updated":"2019-05-05T11:26:58.000Z","comments":true,"path":"2018/06/10/MySQL 暂停/2018-06-11-mysql-51/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/MySQL 暂停/2018-06-11-mysql-51/","excerpt":"","text":"选择性低 vs 高使用频率 假设要设计一个在线约会的网站, 用户信息表有很多列, 包括 国家, 地区, 城市, 性别, 年龄 等, 网站必须支持以上列的各种组合来搜索用户, 此时如何设计索引? 根据之前对 选择性 这一概念的了解, 你可能会想到我们应该在选择性高的列上创建索引 但在这里有两个特殊的列: 选择性很低的 sex 列, 和 选择性不怎么高的 country 列; 这两个列的选择性虽然很低, 但使用频率却很高, 因此往往建议在创建不同组合索引的时候, 将 (sex, country) 列作为前缀 这里之所以将两个选择性很低的字段作为索引的前缀列, 理由是: 在查询时 sex 列的使用频率非常高, 可能几乎每个查询都会用到sex列, 甚至会把整个网站设计成每次都只能按某一种性别搜索用户 而对于某个不限制性别的查询, 可以通过在查询条件中新增 SEX IN (&#39;m&#39;, &#39;f&#39;) AND 来让 MySQL 匹配索引的最左前缀不过需要注意的是, 如果列有太多不同的值, 就会让 IN() 列表太长, 就不适合这样做了 范围查询?? 对于上述表的 age 字段, 在创建组合索引时, 通常放在索引的最后, 因为查询只能使用索引的最左前缀, 直到遇到第一个范围条件列(&gt;,&lt;,BETWEEN,LIKE), 而age通常是范围查找 当然你他也可以使用 IN() 来代替范围查询, 但毕竟不是所有范围查询都可以转换 要避免多个范围查询 http://myrock.github.io/2014/09/24/in-and-range/ 优化排序","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"06. mget, bulk 批量操作","slug":"elasticsearch/2018-06-10-06","date":"2018-06-10T09:06:39.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-06/","excerpt":"","text":"mget 批量查询 批量查询可以只发送一次网络请求, 返回多条查询结果, 能大大缩减网络请求的性能开销 练习 : 12345678GET /_mget&#123; &quot;docs&quot; : [ &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:1&#125;, &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:2&#125;, &#123;&quot;_index&quot;:&quot;blogs&quot;,&quot;_type&quot;:&quot;php&quot;,&quot;_id&quot;:1&#125; ]&#125; bulk 语法: 每个操作要两个json串, 语法如下: 12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; 可以执行的操作类型如: delete: 删除一个文档, 只要一个json串就可以了 create: PUT /index/type/id/_create 创建, 存在会报错 index: 即普通的 put 操作, 可以是创建也可以是全量替换文档 update: 执行部分字段更新 练习: 12345678910111213141516171819DELETE /productsPUT /products/computer/1 # 先创建一个文档&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;GET /products/computer/_searchPOST /products/_bulk&#123;&quot;delete&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 1&#125;&#125; # 删除id为1的文档 (1行json即可)&#123;&quot;create&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 2&#125;&#125; # 创建id为2的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-create-test2&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;&#125;&#125; # 创建一个文档 (es生成id, 2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3&#125;&#125; # 创建一个id为3的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;, &quot;test_field2&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;update&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3, &quot;_retry_on_conflict&quot;: 3 &#125;&#125; # 更改id为3的文档中的test_field字段&#123;&quot;doc&quot; : &#123;&quot;test_field&quot; : &quot;_bulk-index-update-test3&quot;&#125;&#125; bulk操作中, 任何一个操作失败, 不会影响其他的操作, 但是在返回结果里会有异常日志 bulk的请求会被加载到内存中, 所以如果太大的话, 性能反而会下降, 因此需要通过反复测试来获取一个比较合理的bulk size, 一般从1000~5000条数据开始尝试增加数据; 如果看大小的话, 最好在5-15M之间;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"49. 未使用的索引","slug":"MySQL 暂停/2018-06-10-mysql-49","date":"2018-06-10T07:21:01.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/10/MySQL 暂停/2018-06-10-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/MySQL 暂停/2018-06-10-mysql-49/","excerpt":"","text":"有些索引可能是服务器永远不会用到的索引, 这些未使用的索引完全是累赘, 建议删除; 如何定位未使用的索引? … P188","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"05. ES的搜索方式 Query-string 与 query DSL, multi-index, multi-type搜索模式","slug":"elasticsearch/2018-06-10-05","date":"2018-06-10T06:33:46.000Z","updated":"2019-08-30T08:52:14.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-05/","excerpt":"","text":"Query-string 搜索 之所以叫 query-string, 是因为search的参数都是以http请求的 query-string 来传递的 练习, 搜索全部商品 GET /products/computer/_search 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc query-string这种搜索比较适合在命令行使用curl快速地发一个请求来检索信息, 如果查询比较复杂, 一般不太适用, 正式开发中比较少用; query DSL DSL(Domain Specified Language): 领域特定语言 (这里即 ES的领域特定语言), 是在HTTP的请求体中通过json构建查询语法, 比较方便, 可以构建各种复杂语法; 练习, 查询所有商品 123456GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 1234567891011GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &quot;desc&quot;&#125; ]&#125; 练习, 分页查询商品 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot; : 0, &quot;size&quot; : 2&#125; 练习, 指定需要返回的字段 (使用_source元数据: 可以指定返回哪些field) 1GET /products/computer/1?_source=name,price 1234567GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot; : [&quot;name&quot;, &quot;desc&quot;, &quot;tag&quot;]&#125; query DSL 可以在HTTP请求体中构建非常复杂的查询语句, 所以比较常用; 更多复杂用法后面会聊到; multi-index, multi-type搜索模式 GET /_search : 检索所有index, 所有type下的数据 GET /index/_search : 指定一个index, 搜索其下所有type的数据 GET /index1,index2/_search : 指定多个index, 搜索他们下面所有type的数据 GET /index1,index2/type1,type2/_search : 指定多个index, 搜索他们下面指定的多个type的数据 _all/type1,type2/_search : 搜索所有index下指定的多个type的数据","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"48. 重复索引 和 冗余索引","slug":"MySQL 暂停/2018-06-10-mysql-48","date":"2018-06-10T02:52:01.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/10/MySQL 暂停/2018-06-10-mysql-48/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/MySQL 暂停/2018-06-10-mysql-48/","excerpt":"","text":"重复索引 是指在 相同的列上 按照 相同的顺序 创建的 相同类型 的索引, 应该避免这样创建重复索引, 发现后也应该立即移除; 因为 MySQL 本身是允许你创建重复索引的, 但它却需要单独维护重复的索引, 并且优化器在优化查询的时候也需要逐个地进行考虑, 这会影响性能; 冗余索引 和 重复索引不同 如果创建了索引(A,B), 再创建索引(A)其实就是冗余索引, 但索引(B,A)、索引(B) 就不是 索引(A,B) 的冗余索引 大多数情况下其实都不需要冗余索引, 应该扩展已有的索引而不是创建新索引; 但也有时候处于性能方面的考虑需要冗余索引, 因为如果扩展已有的索引会导致其变太大, 那就会影响其他使用该索引的查询性能例如, 如果要在一个只包含一个整数列的索引中, 额外增加一个很长的 VARCHAR 列来扩展该索引, 那性能可能就会急剧下降 (特别是有的查询把这个索引当做覆盖索引) P179-181 …","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"04. 简单尝试 CURD","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T02:36:57.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"Cat Api ES提供的 Cat Api 可以用来查看 集群当前状态, 涉及到 shard/node/cluster 几个层次 尝试使用 GET /_cat/health?v 查看 时间戳、集群名称、集群状态、集群中节点的数量 等等 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1540815645 20:20:45 elasticsearch yellow 1 1 6 6 0 0 6 0 - 50.0% 返回信息 和 集群健康API(GET _cluster/health) 返回都一样 索引文档ES 中可以使用 POST 或 PUT 来索引一个新文档, 熟悉HTTP协议的话, 应该知道 PUT是幂等的, 而POST是非幂等的, ES也遵循了这一点 PUT PUT 创建文档的时候需要手动设定文档ID (类似已知id, 进行修改) 如果文档不存在, 则会创建新文档; 如果文档存在, 则会覆盖整个文档 (所以需要留意) 虽然使用PUT可以防止POST非幂等引起的多次创建, 但也要留意使用PUT带来的文档覆盖问题 练习: 12345678910111213141516171819202122# 此处创建一个 索引为 products , 类型为 computer, 文档ID为1的商品 PUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 返回&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, # 表示应该写入的有两个分片(1个主分片和1个副本分片, 但注意: 这里代表的可不是总分片数, 显然es的索引默认对应5个主分片, 每个主分片又对应一个副本分片, 总共会有10个分片) &quot;successful&quot;: 1, # 表示成功写入一个分片, 即写入了主分片, 但是副本分片并未写入, 因为目前只启了一个节点 &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 另外, 注意: 使用PUT创建文档时, 如果不指定ID, 则会报错 POST POST 创建文档时不需要手动传递文档ID, es会自动生成全局唯一的文档ID 练习 12345678910111213141516171819202122POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125;# 返回, 可以看到文档ID是自动生成的, 其他字段和使用`PUT`时返回的信息相同&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWa_MgAhWC0s-aachNUS&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 检索文档先尝试最简单的一种 query-string 查询方式: GET /products/computer/_search : 查询/products/computer/下的所有文档 更新文档 PUT、POST PUT 对整个文档进行覆盖更新 1234PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer-hongji&quot;&#125; partial update: 如果只是想更新文档的部分指定字段, 可以使用 POST 结合 _update : (partial update内置乐观锁并发控制) 123456POST /products/computer/2/_update?retry_on_conflict=5&#123; &quot;doc&quot;: &#123; &quot;name&quot; : &quot;acer-hongji-鸿基&quot; &#125;&#125; 这里注意一下_update的内部机制其实是: es先获取整个文档, 然后更新部分字段, 最后老文档标记为deleted, 然后创建新文档此时在标记老文档为deleted时就可能会出现并发问题, 如果线程1抢先一步将老文档标注为deleted, 那么线程2在将新文档标注为deleted时就会失败(version内部乐观锁机制)此时在es内部会做处理, 他内部完成了对乐观锁的实现, 如果失败后, 其实也是进行重试, 你可以手动传递 retry_on_conflict参数来决定其内部的重试次数 PUT如何只创建不替换: 由于创建文档与全量替换文档的语法是一样的, 都是 PUT, 而有时我们只是想新建文档, 不想替换文档 可以使用 op_type=create 来说明此命令只是用来执行创建操作的PUT /index/type/id?op_type=create 或 PUT /index/type/id/_create 可以看到, 此时, 如果文档已经存在, 会进行报错提示冲突, 而不会帮你直接替换1234567PUT /products/computer/1?op_type=create&#123; &quot;name&quot; : &quot;huawei create&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing create&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;, &quot;create&quot;] &#125; 删除文档 ES的文档替换: 上面已经了解过, 其实就是PUT创建文档, 如果传递的文档id不存在, 就是创建, 如果文档id已经存在, 则是替换操作; 注意: es在做文档的替换操作时, 会将老的document标记为deleted, 然后新增我们给定的那个document, 当后续创建越来越多的document时, es会在适当的时机在后台自动删除标记为delete的document; ES的删除: 不会直接进行物理删除, 而是在数据越来越多的时候, es在合适的时候在后台进行删除 练习: 123456789101112131415DELETE /products/computer/2# 返回&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"47. 使用索引扫描来做排序","slug":"MySQL 暂停/2018-06-09-mysql-47","date":"2018-06-09T10:25:03.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/09/MySQL 暂停/2018-06-09-mysql-47/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/MySQL 暂停/2018-06-09-mysql-47/","excerpt":"","text":"两种排序实现在MySQL中, 可以有两种方式来生成有序结果, 即 ORDER BY 有两种排序实现方式: 通过索引顺序扫描 和 文件排序(filesort) 通过索引顺序扫描之前已经提到过: 由于 B+Tree 索引树是顺序组织存储的, 所以除了按值查找之外, 索引还可以用于查询中的 ORDER BY 操作(按顺序查找), 一般来说, 如果 B-Tree 可以按照某种方式查询到值, 那么也可以按照这种方式用于排序; 当然, 也很适合查找范围数据;扫描索引本身是很快的, 只用从一条索引记录移动到紧接着的下一条记录; 但如果索引不能覆盖查询所需的全部列, 即如果做不到覆盖索引, 那就不得不每扫描一条索引记录就回表查询一次对应的行, 这基本上都是随机I/O, 所以这种情况下按索引顺序地读取数据的速度通常要比顺序地全表扫描还慢;注意: MySQL可以使用同一个索引既满足查找, 又满足排序; 如果可能, 设计索引时应该尽可能地同时满足这两种任务, 这样是最好的; ORDER BY 中所有的列必须包含在同一个索引内 索引列的先后次序 和 order by 子句中列的先后次序要一致, 这一点 和 查找型查询的限制是一样的, 需要满足索引的最左前缀的要求, 否则, MySQL都需要亲自去执行排序操作, 而无法利用索引排序最左前缀 有一种情况下, ORDER BY 子句可以不用满足最左前缀的要求, 那就是前导列为常量的时候 比如一张表的索引是 key(a,b,c) , 而 查询语句是 ... where a=100 order by b,c, 即使 order by 不满足最左前缀的要求, 也可以使用索引做排序 并且所有列的排序方向(升序或者降序) 必须一致, 混合使用ASC模式和DESC模式则不使用索引 如果查询联接了多个表, 只有在order by子句的所有列引用的是第一个表的列才可以 在其他的情况下, mysql使用文件排序 filesort 这个 filesort 并不是说通过磁盘文件进行排序, 而只是告诉我们进行了一个排序操作, 即在 MySQL Query Optimizer 所给出的执行计划(通过 EXPLAIN 命令查看)中被称为文件排序(filesort) 文件排序是通过相应的排序算法, 将取得的数据在内存中进行排序 (EXPLAIN 结果的Extra列值为 Using filesort ) 如果order by的子句只引用了联接中的第一个表，MySQL会先对第一个表进行排序，然后进行联接。也就是expain中的Extra的Using Filesort.否则MySQL先把结果保存到临时表(Temporary Table),然后再对临时表的数据进行排序.此时expain中的Extra的显示Using temporary Using Filesort. P177 列出了很多不可以使用索引做排序的查询;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"03. ES 一些基本概念","slug":"elasticsearch/2018-06-09-03","date":"2018-06-09T10:23:07.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-03/","excerpt":"","text":"近实时 从文档被索引到可以被检索会有轻微延时, 约1s Index(索引 n) 这里的Index是个名词, 类似于传统RDS的一个数据库, 是存储document的地方 一个Index可以包含多个 type (索引的复数词为 indices 或 indexes) index 名称必须是小写, 不能用下划线开头, 不能包含逗号 一般将不同的项目数据放到不同的index中 每个index会物理地对应多个分片, 这样, 每个项目都有自己的分片, 互相物理地独立开, 如果有项目是做复杂运算的, 也不会影响其他项目的分片 索引(v) : ES中的还会提到 索引一个文档, 这里的 索引 是动词, 存储文档并建立倒排索引的意思; Type(类型) 一个Index中可以有多个type 代表document属于index中的哪个类别(type 可以对同一个index中不同类型的document进行逻辑上的划分,可以粗略地理解成传统数据库中的数据表?) 名称可以是大小写, 不能用下划线开头, 不能包含逗号 注意: type是对index做的逻辑划分, 而shard是对index做的物理划分 Document(文档) ES中的最小数据单元, ES使用 JSON 作为文档的序列化格式 (ES中的文档可以通俗地理解成传统数据库表中的一条记录) _id: 文档id 可以手动指定, 也可以由es为我们生成; 手动指定id: 根据应用情况来判断是否符合手动指定 document id, 一般如果是从某些其他的系统中导入数据到es, 就会采用这种方式, 就是使用系统中已有的数据的唯一标识作为es中的document的id;比如从数据库中迁移数据到es中, 就比较适合采用数据在数据库中已有的primary key;put /index/type/id 自动生成id: 如果说我们目前要做的系统主要就是将数据存储到es中, 数据产生出来以后直接就会存放到es, 所以不需要手动指定document id的形式, 可以直接让es自动生成id即可;post /index/typees自动生成的id长度为20个字符, URL安全, base64编码, GUID, 分布式并行生成时, es会通过全局id来保证不会发生冲突; Cluster(集群) 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 一个ES节点属于哪个集群, 是由其配置中的 cluster.name 决定的; 节点启动后, 其默认name是elasticsearch, 因此如果在一个机器中启动一堆节点, 那它们会自动组成一个es集群(因为它们的cluster.name都是elasticsearch) 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; Shard(分片): type是对index做的逻辑划分, 而shard是对index做的物理划分 一个分片就是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 分片是数据的容器, 文档其实是保存在分片中的: 当我们将很多条document数据添加到索引中时, 索引实际上是指向一个或者多个物理分片; 因此, 你要存储到索引中的数据其实会被分发到不同的分片中, 而每个分片也仅保存了整个索引中的一部分文档; 当你的集群规模扩大或者缩小时(即增加或减少节点时), ES 会自动的在各节点中迁移分片, 而数据是存放在shard中的, 所以最终会使得数据仍然均匀分布在集群里 shard 可以分为 primary shard(主分片), replica shard(副本分片) replica shard 可以容灾, 水平扩容节点时, 还可以自动分配来提高系统负载 默认情况下, 每个index有5个parimary shard, 而每个parimary shard都有1个replica shard, 即每个index默认会对应10个shard 另外, ES规定了, 每个index的 parimary shard 和 replica shard 不能在全部都在同一个节点上, 相同内容的 replica shard 也不能在同一节点上, 不然起不到容灾作用; 集群状态 yellow 在ES中, 每个索引可能对应多个主分片, 每个主分片也都可能对应多个副本分片 对于每个索引, 要保证不会导致es集群为 yellow, 需要注意: es节点数 &gt;= number_of_replicas+1 当索引的 `number_of_replicas=1` 时, 无论 `number_of_shards` 为多少, 2个节点 (`es节点数 = number_of_replicas+1`) 就可以保证集群是 green; 当索引的 `number_of_replicas&gt;1` 时, 只有当 `es节点数 = number_of_replicas+1` 时, 集群才会变为green; 对于任何一个索引, 由于任何具有相同内容的分片(相同主分片的两个副本分片, 或者主分片和其某个副本分片)不会被放在同一个节点上, 所以如果节点数量不够的话, 有些replica-shard分片会处于未分配状态, 集群状态就不可能是green而是yellow; 比如索引 test 有 3个主分片, 每个主分片对应3个副本分片(该索引总共 3+3*3=12 个分片), 那么至少得4(number_of_replicas+1)个节点, 才能保证每个节点上都不会出现具有相同内容的分片, 即可以保证集群是green;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"02. ES 版本选择及简单安装","slug":"elasticsearch/2018-06-09-02","date":"2018-06-09T06:56:01.000Z","updated":"2019-08-30T08:52:13.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-02/","excerpt":"","text":"版本选择 ES 的版本迭代比较快, 目前(06/2018)为止, 已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X; 此处之所以选用5.5.3来学习调研, 主要是因为公司选用的阿里云服务提供的是 ES 5.5.3版本 (所以你在选择版本时, 也可以根据 自建、购买云服务 来决定) 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (此处使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错(调研使用的是 centos7-minimal 版) 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加如下两行配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 12345678910111213&#123; &quot;name&quot; : &quot;rjAFeY9&quot;, # node 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, # 节点默认的集群名称 (可以在es节点的配置文件elasticsearch.yml中进行配置) &quot;cluster_uuid&quot; : &quot;zaJApkNPRryFohhEMEVH5w&quot;, &quot;version&quot; : &#123; # es 版本号 &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 上面未解释的信息暂时先不用了解 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本和集群相关的信息, 但如果能有一个可视化的环境来操作它可能会更直观一些, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装Kibana 是一个开源的分析和可视化平台, 属于 Elastic stack 技术栈中的一部分, Kibana 主要提供搜索、查看和与存储在 Elasticsearch 索引中的数据进行交互的功能, 开发者或运维人员可以轻松地执行高级数据分析, 并在各种图表、表格和地图中可视化数据;接下来主要就是使用Kibana的DevTools提供的控制台进行ES的学习 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 (因为kibana有个副本分片并没有处于正常状态, 因为当前只有一个节点, 副本分片无法被分配到其他节点, 具体细节先不用着急, 后面会进行分析) 小结到此为止, 应该对ES有了最基础的了解, 且基本环境已经安装完毕, 对于后续的练习暂时就够了","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"01. 初识 Elasticsearch","slug":"elasticsearch/2018-06-09-01","date":"2018-06-09T06:24:25.000Z","updated":"2019-08-30T08:53:41.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-01/","excerpt":"","text":"开源 基于 Lucene, 提供比较简单的Restful API Lucene 可以说是当下最先进、高性能、全功能的搜索引擎库, 由Apache软件基金会支持和提供(更多细节自行了解) 但Lucene非常复杂, ES的目的是使全文检索变得简单, 通过隐藏 Lucene 的复杂性, 取而代之的提供一套简单一致的 RESTful API 高性能全文检索和分析引擎, 并可根据相关度对结果进行排序 可以快速且 近实时 地存储, 检索(从文档被索引到可以被检索只有轻微延时, 约1s)以及分析 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级 结构化 或 非结构化 数据 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引 应用场景 当你的应用数据量很大, 数据结构灵活多变, 数据之间的结构比较复杂, 如果用传统数据库, 可能不仅需要面对大量的表设计及数据库的性能问题, 此时可以考虑使用ES, 它不仅可以处理非结构化数据, 而且可以帮你快速进行扩容, 承载大量数据 具体比如多数据源聚合大列表页: 微服务架构是目前很多公司都采用的架构, 所以经常会面对 多数据源聚合的 大列表页, 一个列表中的筛选字段,展示字段可能会来自多个服务, 同时涉及到分页, 所以传统方案可能比较吃力, 而且也得不到比较好的效果; (RRC这边目前是使用 ES 做 数据视图服务, 对这种大列表页所用到的数据源字段做统一配置和聚合) 日志数据分析, RRC 使用 ElasticStack 技术栈来很方便地对各服务的日志进行查询,分析,统计 站内搜索(电商, 招聘, 门户 等等)都可以使用 ES 来做全文检索并根据相关性进行排名, 高亮展示关键词等","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"46. 高性能索引 -- 覆盖索引","slug":"MySQL 暂停/2018-06-09-mysql-46","date":"2018-06-09T03:12:09.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/09/MySQL 暂停/2018-06-09-mysql-46/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/MySQL 暂停/2018-06-09-mysql-46/","excerpt":"","text":"概述 如果 索引包含(或者说覆盖) select 查询所需的所有列时, 我们就称该索引为 “覆盖索引” 对于 MyISAM 来说, 直接从 主键/普通 索引 中就取到了列值, 不用回表; 尤其是对于 InnoDB 来说, 直接从 二级索引 中就取到了列值, 可以避免对主键索引的二次查询(即不用回表走 聚簇索引) 基于以上原因, 一般情况下, 我们不使用 select *, 为的就是能做到覆盖索引 疑问 主键索引 vs 聚簇索引 vs 覆盖索引 试想一下, 如果你使用的是 主键索引 时, select * 可以做到覆盖索引么? select name .... where id = 1 select * .... where id = 1 通过 explain 分析, 上面第一条是覆盖索引 （Extra: Using index）, 而第二条不是覆盖索引 （Extra: null） 分析: 首先, InnoDB 的数据默认就是用主键构造的 B+Tree, InnoDB的聚簇索引和数据是一体的 所以会想当然的认为 如果使用了主键索引, 那就是在走聚簇索引, 而聚簇索引的叶子节点包含了整列的内容, 那么你 select * 自然也可以做到覆盖索引 但结果却不是, 貌似是因为 主键索引 和 聚簇索引 仍然是分离的 ?? 这块有点小疑问","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"RabbitMQ内置集群","slug":"rabbitmq/2018-06-09-rabbitmq-15","date":"2018-06-09T02:00:35.000Z","updated":"2018-06-09T02:03:49.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-15/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-15/","excerpt":"","text":"集群中: 如果队列被持久化, 而该节点掉线, 如果消费者尝试创建该队列, 并且也是持久化的, 则会报404; 但是貌似在单节点环境不会报错; 而如果消费者尝试创建该队列时, 设置的是非持久化, 则貌似能创建成功, 此时就是一个新的队列了, 由于没有去恢复节点, 所以节点中的队列等内容还是没有恢复;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"45. 高性能索引 -- 聚簇索引","slug":"MySQL 暂停/2018-06-05-mysql-45","date":"2018-06-05T12:07:57.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/05/MySQL 暂停/2018-06-05-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/MySQL 暂停/2018-06-05-mysql-45/","excerpt":"","text":"“索引组织表” 第一篇 重点: InnoDB存储引擎的表是 索引组织表, 即 表中的数据本身就是按照主键顺序存放, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 同时叶子结点存放的即为表的行纪录数据(聚集索引的叶子结点也称为数据页); 聚集索引的这个特性也决定了索引组织表中数据也是索引的一部分, 和 B+Tree 数据结构一样, 每个数据页之间都通过一个双向链表来进行链接; 从 表存储文件 看 “索引组织表” 对于 InnoDB 表, 可以通过 innodb_file_per_table 选项 开启 独立表空间, 此时用户不用将所有数据都存放于默认的表空间(ibdataX文件)中, 而是会产生单独的 .ibd 独立表空间文件可以看到: 这些独立的表空间文件中存储了 innodb表的数据、索引 等信息 (而InnoDB的数据本身就是按照B+Tree组织的, 所以说该文件中其实包含了 数据+聚簇索引 和 二级索引) 但是对于 MyISAM 来说, 其数据表文件分为 .frm(存储表的结构), .MYD(存储数据), .MYI(存储索引)可以明显的看到, MyISAM 的数据和索引是分开存储的 (其 主键索引 和 普通索引 的索引策略没有本质区别) 小结: InnoDB是聚集索引, 数据文件是和索引绑在一起的, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 因此通过主键索引效率很高; 但是辅助索引需要两次查询, 先通过二级索引查询到主键, 然后再通过主键查询到数据; 而MyISAM是非聚集索引, 数据文件和索引是分离的; “索引组织表” 第二篇 InnoDB存储引擎表中, 每张表都有个主键(Primary Key), 因为 表中的数据本身就是按照主键顺序存放, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 所以如果在创建表时没有显示地定义主键, 则InnoDB存储引擎会按如下方式选择或创建主键: 首先判断表中是否有 非空的唯一索引(unique not null), 如果有, 则该列即为主键 如果不符合上述条件, InnoDB存储引擎会自动创建一个6字节大小的指针 (_rowid) 当表中有多个非空唯一索引时, InnoDB存储引擎会选择建表时第一个定义的非空索引为主键 (这里要注意的是, 主键的选择根据的是定义索引的顺序, 而不是建表时列的顺序) 例: 下面创建一张表t, 有a,b,c,d 4个列, b,c,d上都有唯一索引, 不同的是b列允许为NULL, 由于没有显示地定义主键, 因此会选择非空的唯一索引为主键, 并且顺序是第一个定义的索引, 即 字段 d 12345678910111213141516171819202122232425262728293031323334mysql&gt; CREATE TABLE t ( -&gt; a int not null, -&gt; b int null, -&gt; c int not null, -&gt; d int not null, -&gt; UNIQUE KEY(b), -&gt; UNIQUE KEY(d), -&gt; UNIQUE KEY(c) -&gt; );Query OK, 0 rows affected (0.02 sec)// 插入几条测试数据mysql&gt; insert into t select 1,2,3,4;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into t select 5,6,7,8;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into t select 9,10,11,12;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0// _rowid 可以显示表的主键, 因此通过下面的查询语句可以找到表t的主键 (可以看到是d列的值, d被选为主键了)mysql&gt; select a,b,c,d,_rowid FROM t;+---+------+----+----+--------+| a | b | c | d | _rowid |+---+------+----+----+--------+| 1 | 2 | 3 | 4 | 4 || 5 | 6 | 7 | 8 | 8 || 9 | 10 | 11 | 12 | 12 |+---+------+----+----+--------+3 rows in set (0.00 sec) _rowid 可以显示表的主键, 但是只能用于查看单个列为主键的情况, 对于多列组成的主键就无能为力了: 1234567891011121314151617mysql&gt; create table a ( -&gt; a INT, -&gt; b INT, -&gt; PRIMARY KEY(a,b) -&gt; );Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into a select 1,2;Query OK, 1 row affected (0.01 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into a select 3,4;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; select a,_rowid FROM a;ERROR 1054 (42S22): Unknown column &apos;_rowid&apos; in &apos;field list&apos; MyISAM 与 InnoDB 索引对比 主键索引+覆盖索引 MyISAM 从 主键索引 找到B+Tree中对应的叶子节点 找到数据的物理行指针, 然后寻址找到数据 InnoDB 从 聚簇索引 找到B+Tree中对应的叶子节点 直接就找到了数据 看上去 InnoDB是直接找到了数据, 而 MyISAM 还需要寻址, 但其实 MyISAM 还是要快一些 ??? 普通索引+覆盖索引 MyISAM 从普通索引中就可以取到具体数据 而InnoDB 从二级索引中就可以取到数据的 看起来是一样快的 如果索引生效, 但没做到覆盖索引: MyISAM 从普通索引中 就可以取到具体数据的物理行位置, 然后通过 物理行号 再回表, 到具体数据行 而InnoDB 需要两次索引扫描, 先从二级索引中可以取到数据的主键, 然后 通过 主键 在聚簇索引上找到具体数据行 如果索引未生效, 走全表扫描 MyISAM 的数据分布比较简单, 按照数据的插入顺序存储在磁盘上, 扫表比较快 InnoDB 表的数据本身就是聚集的, 也就是说, 表本身就是聚集索引, 全表扫描, 扫的就是当然就是聚集索引本身 正常情况下, 我们可以定义一个代理键作为主键, 这种主键的数据应该和应用无关, 最简单的方法是使用 AUTO_INCREMENT 自增列, 这样可以保证数据行是按顺序写入的最好避免随机的(不连续且值的分布范围非常大)的聚簇索引, 特别是对于I/O密集型的应用, 这样会导致聚簇索引的插入变得完全随机, 这是最坏情况, 数据没有任何聚集特性 做全表扫描时, InnoDB 会按主键顺序扫描页面和行, 如果主键页表没有碎片(存储主键和行的页表), 全表扫描是相当快, 因为读取顺序接近物理存储顺序, 但如果主键碎片化变大, 随机性高, 则性能会下降; 执行 select count(*) from table 时需要全表扫描 InnoDB不保存表的具体行数 而MyISAM用一个变量保存了整个表的行数, 执行上述语句时只需要读出该变量即可, 速度很快 聚簇索引 在MySQL中, InnoDB使用的是聚簇索引, 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上, 若使用 “where id = 14” 这样的条件查找主键, 即做到了 主键 覆盖索引, 直接会走 聚簇索引, 按照B+树的检索算法即可查找到对应的叶节点, 之后获得行数据; 若对Name普通索引列进行条件搜索, 则需要两个步骤: 第一步在辅助索引B+树中检索Name, 到达其叶子节点获取对应的主键; 第二步使用主键在主索引B+树种再执行一次B+树检索操作, 最终到达叶子节点即可获取整行数据 MyISM使用的是非聚簇索引, 非聚簇索引的两棵B+树看上去没什么不同, 节点的结构完全一致。表数据存储在独立的地方, 这两颗B+树的叶子节点都使用一个 地址 指向真正的表数据, 对于表数据来说, 这两个键没有任何差别, 由于索引树是独立的, 通过辅助键检索无需访问主键的索引树; 下图这张抽象图描述了两者保存数据和索引的区别 一些缺点: 更新聚簇索引列的代价很高, 因为会强制Innodb将每个被更新的行移动到新的位置 基于聚簇索引的表在插入新行, 或者逐渐被更新导致需要移动行时, 可能面临 “页分裂” 的问题, 页分裂会导致表占用更多的磁盘空间 … 参考 P163 小结: 所以, MyISAM 用索引检索数据时, 无论是使用的 主键索引 还是 普通索引, 只会访问一次索引即可拿到叶子节点的物理行指针; 而InnoDB在使用 二级(非聚簇)索引 时, 需要访问可能两次索引查找, 而不是一次, 除非你做到了覆盖索引 或者 你用到了主键索引直接走了聚簇索引; 另外需要知道的是: 在MySQL目前内建的存储引擎中, 不支持 手动选择一个列作为聚簇索引 (InnoDB 如果没有定义主键, MySQL 会隐式定义一个主键来作为聚簇索引) 覆盖索引可以模拟多个聚簇索引的情况?? 聚簇索引可能对性能有帮助, 但也可能导致严重的性能问题 (尤其是将表的存储引擎从InnoDB改成其他引擎, 或者反过来时, 需要仔细考虑聚簇索引) ?? 在InnoDB中按主键顺序插入行… 未完待续 参考:http://blog.haohtml.com/archives/17372","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"44. 高性能索引 -- 多列索引","slug":"MySQL 暂停/2018-06-05-mysql-44","date":"2018-06-05T06:12:31.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/05/MySQL 暂停/2018-06-05-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/MySQL 暂停/2018-06-05-mysql-44/","excerpt":"","text":"合并索引 技术 首先, 你需要了解的是 “为多个列各自创建独立的索引 在大部分情况下并不能提高MySQL的性能” 尽管MySQL5.0+引入了一种叫 “索引合并”(index merge) 的技术, 在一定程度上可以使用表上的多个单列索引来定位指定的行 示例: explain_goods 创建了 stock, goods_weight 两个独立索引 1234567891011121314151617// 当表中有数据时, 确实使用了 索引合并 技术mysql&gt; explain select id from explain_goods where goods_weight=10 or stock=11; +----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+| 1 | SIMPLE | explain_goods | index_merge | idx_stock,idx_goods_weight | idx_goods_weight,idx_stock | 4,4 | NULL | 2 | Using union(idx_goods_weight,idx_stock); Using where |+----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+1 row in set (0.00 sec) // 注意一个例外情况: 当表中没有数据时, 发现MySQL优化器走了全表扫描mysql&gt; explain select id from explain_goods_copy1 where goods_weight=10 or stock=11; +----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods_copy1 | ALL | idx_stock,idx_goods_weight | NULL | NULL | NULL | 1 | Using where |+----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 这种算法有三个变种: or条件的联合, and条件的相交, 组合前两种情况的联合及相交 多列索引的顺序 本节适用于 B-Tree 索引 多列 B-Tree 索引的顺序至关重要 当不需要考虑排序和分组时, 通常将选择性最高的列放在前面 (这样设计索引, 能够最快地过滤出处需要的行, 对于在where条件中只使用了索引部分前缀列的查询来说选择性也更好)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"43. 高性能索引 -- 前缀索引 & 索引选择性","slug":"MySQL 暂停/2018-06-03-mysql-43","date":"2018-06-03T12:07:57.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/03/MySQL 暂停/2018-06-03-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2018/06/03/MySQL 暂停/2018-06-03-mysql-43/","excerpt":"","text":"什么时候要用前缀索引? 有时候需要为某个很长的字符列创建索引, 这会让索引变得大且慢, 一个策略是前面提到过的模拟哈希索引, 但有时候这样却不那么合适 其实对于很长的字符列, 如果要创建索引, 我们可以尝试为该列开始的部分字符创建索引, 这样就可以大大节约索引空间, 从而提高索引效率, 但可能会降低索引的选择性; 什么是索引的选择性? 索引的选择性是指 不重复的索引值的数量 和 数据表的记录总数(n) 的比值 (范围从 1/n 到 1之间) 索引的选择性越高, 则查询效率越高; 因为选择性高的索引可以让MySQL在查找时过滤更多的行, 唯一索引的选择性是1, 这是最好的索引选择性, 性能也是最好的; 一般情况下, 某个列的前缀索引的选择性都是比较高的, 足以满足查询性能, 对于 BLOB, TEXT 或 很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 要注意的是: 我们要选择足够长的前缀字符, 以保证较高的选择性, 但同时又不能太长(以便于节约空间) 如何选择合适的前缀长度? 那么 当你为一个 较长的字符列 创建索引时, 如何决定前缀的合适长度呢? (我们的目的是选择足够长的前缀, 从而提高前缀索引的选择性, 提高查询性能) 准备一张简单的城市表, 里面存放了美国的一些地方名 前缀长度 — 统计观察 尝试 根据各城市名分组 根据各组统计的次数 倒序排名: 可以看到, 完整列的选择性也并非是1, 我们可以选择合适的前缀, 让选择性逐渐接近完整列的 12345678910111213141516mysql&gt; select count(*) as cnt, city_name from test_city group by city_name order by cnt desc limit 10;+-----+-----------+| cnt | city_name |+-----+-----------+| 4 | SantaRosa || 3 | Auburn || 3 | Norfolk || 3 | Roanoke || 3 | Richfield || 3 | Olympia || 3 | Danville || 3 | Arlington || 3 | Provo || 3 | Plano |+-----+-----------+10 rows in set (0.01 sec) 尝试取长度为3的前缀, 继续上述统计: 12345678910111213141516mysql&gt; select count(*) as cnt, left(city_name, 3) as pref from test_city group by pref order by cnt desc limit 10;+-----+------+| cnt | pref |+-----+------+| 19 | New || 17 | Nor || 16 | For || 16 | San || 13 | Cha || 11 | Sal || 10 | Roc || 9 | Gra || 9 | Ken || 9 | Bel |+-----+------+10 rows in set (0.01 sec) 尝试增加前缀长度, 让前缀选择性接近完整列的选择性 前缀长度 — 选择性计算 计算合适前缀长度的另外一个方法就是计算完整列的选择性, 并使前缀的选择性接近于完整列的选择性 下面显示了如何计算完整列的选择性: 1234567mysql&gt; SELECT COUNT(DISTINCT city_name ) / COUNT( * ) FROM test_city;+-----------------------------------------+| COUNT(DISTINCT city_name ) / COUNT( * ) |+-----------------------------------------+| 0.4821 |+-----------------------------------------+1 row in set (0.00 sec) 上面看来, 对 city_name 这列来说, 完整列的选择性是 0.4821, 不算高, 但如果要对该列建索引, 那么选择的前缀要保证选择性能接近 0.4821 即可 下面给出了如何在同一个查询中计算不同前缀长度的选择性: 12345678910111213141516171819mysql&gt; SELECT COUNT( DISTINCT city_name ) / COUNT( * ) as t0, COUNT( DISTINCT left(city_name, 4) ) / COUNT( * ) as t1, COUNT( DISTINCT left(city_name, 5) ) / COUNT( * ) as t2, COUNT( DISTINCT left(city_name, 6) ) / COUNT( * ) as t3, COUNT( DISTINCT left(city_name, 7) ) / COUNT( * ) as t4, COUNT( DISTINCT left(city_name, 8) ) / COUNT( * ) as t5, COUNT( DISTINCT left(city_name, 9) ) / COUNT( * ) as t6, COUNT( DISTINCT left(city_name, 10) ) / COUNT( * ) as t7, COUNT( DISTINCT left(city_name, 11) ) / COUNT( * ) as t8FROM test_city;+--------+--------+--------+--------+--------+--------+--------+--------+--------+| t0 | t1 | t2 | t3 | t4 | t5 | t6 | t7 | t8 |+--------+--------+--------+--------+--------+--------+--------+--------+--------+| 0.4821 | 0.4055 | 0.4351 | 0.4597 | 0.4729 | 0.4760 | 0.4791 | 0.4811 | 0.4811 |+--------+--------+--------+--------+--------+--------+--------+--------+--------+1 row in set (0.01 sec) 可以看到, 前缀长度到达 10 的时候, 选择性的提升已经很小了, 因此 10 可以算是一个差不多合理的长度了 注意: 前缀索引是一种能似索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 也无法使用前缀索引做覆盖索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"42. 高性能索引 -- 独立的列","slug":"MySQL 暂停/2018-06-03-mysql-42","date":"2018-06-03T11:05:16.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/03/MySQL 暂停/2018-06-03-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2018/06/03/MySQL 暂停/2018-06-03-mysql-42/","excerpt":"","text":"本篇非常简单 独立的列 是指索引列不能是表达式的一部分, 也不能是函数的参数; 示例: 很容易看出 where 中的表达式其实等价于 actor_id=4, 但MySQL却无法自动解析这个方程式 1mysql&gt; select actor_id FROM actor where actor_id+1=5; (就像 mysql的查询优化器不能将 id&gt;5 和 id&gt;6 这两个查询条件优化合并成一个 id&gt;6 一样, MySQL优化器有时候没有我们想的那么自然, 但的确非常有用)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"41. 高性能索引","slug":"MySQL 暂停/2018-06-02-mysql-41","date":"2018-06-02T13:51:21.000Z","updated":"2019-04-26T11:00:28.000Z","comments":true,"path":"2018/06/02/MySQL 暂停/2018-06-02-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2018/06/02/MySQL 暂停/2018-06-02-mysql-41/","excerpt":"","text":"独立的列前缀索引 &amp; 索引选择性多列索引聚簇索引覆盖索引使用索引扫描做排序压缩(前缀压缩)索引冗余和重复索引未使用的索引索引和锁","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"40. 索引基础","slug":"MySQL 暂停/2018-06-02-mysql-40","date":"2018-06-02T06:30:25.000Z","updated":"2019-04-26T11:06:05.000Z","comments":true,"path":"2018/06/02/MySQL 暂停/2018-06-02-mysql-40/","link":"","permalink":"http://blog.renyimin.com/2018/06/02/MySQL 暂停/2018-06-02-mysql-40/","excerpt":"","text":"概述 索引是存储引擎用于快速找到记录的一种数据结构, 这也是它最基本的功能 索引有很多种类型, 可以为不同的场景提供更好的性能, MySQL支持的常见索引类型有: B-Tree索引 (事实上其升级版B+Tree更常被采用) 哈希索引 全文索引 空间数据索引(R-Tree) … 其他类型的索引 MySQL中, 索引是在存储引擎层而不是服务器层实现的, 所以并没有统一的索引标准, 这就导致了 不同存储引擎的索引的工作方式可能不一样 也不是所有的存储引擎都支持所有类型的索引 即使多个存储引擎支持同一种类型的索引, 其底层实现也可能不同 索引对于良好的性能非常关键, 尤其是当表中的数据量越来越大时, 索引对性能的影响愈发重要; 它是对查询性能优化最有效的手段了, 能轻易将查询性能提高几个数量级; 索引可以让服务器快速地定位到表的指定位置, 但这并不是索引的唯一作用, 根据创建索引的数据结构的不同, 索引还有一些其他的附加作用, 如 最常见的B-Tree(实际应用比较多的是其升级版B+Tree), 是按照顺序存储数据结构的, 所以MySQL可以用来做 ORDER BY 和 GROUP BY 操作, 而且B-Tree会将相关的列值都存储在一起, 这样由于索引中存储了实际的列值, 所以某些查询只使用索引就能够完成全部查询 总结下来索引有以下三个优点: 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O编为顺序I/O 索引虽好, 但我们要学会创建最优的索引, 不恰当的索引反而会导致性能的下降 ? 索引并不总是最好的工具, 总的来说, 只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时, 索引才是有效的 对于非常小的表, 大部分情况下简单的全表扫描更高效 对于中到大型的表, 索引就非常有效 但是对于特大型的表, 建立和使用索引的代价将随之增长, 这种情况下需要一种技术可以直接区分出查询需要的一组数据, 而不是一条记录一条记录的匹配, 可以考虑分区技术; 不恰当的索引 数据量小且负载较低时, 不恰当的索引对性能的影响可能还不明显, 但当数据量逐渐增大时, 会导致性能急剧下降; 待补充 索引可以包含 一个 或 多个列: 如果如果索引包含多个列, 索引中列的顺序也非常重要, 因为MySQL只能高效地使用索引的 最左前缀列 创建一个包含两个列的索引 和 创建两个只包含一个列的索引 是大不相同的 B-Tree索引基础概述 当人们讨论索引时, 如果没有特别指明类型, 多半说的是B-Tree索引, 它使用B-Tree数据结构来存储数据, 但实际上很多存储引擎使用的是升级版的B+Tree (即每一个叶子节点都包含了指向下一个叶子节点的指针, 从而方便了叶子节点的范围遍历) 所以在MySQL中我们讨论B-Tree时, 其实说的就是B+Tree, 它是目前关系型数据库系统中查找最为常用和最为有效的索引, 很多地方使用术语 B-Tree, 是因为MySQL 官方很多地方都是用的 B-Tree/BTree(其实 B-Tree 中间的 - 不是 减, 只是个分隔词, 参考旧金山大学数据结构可视化平台) , 比如创建表索引时, 指定的索引类型就是BTREE, 而没有B+TREE B-Tree 索引列是顺序组织存储的, 所以很适合查找范围数据; 另外, 其每一个叶子页到根的距离都是相同的; 叶子页 比较特别, 他们的指针指向的是被索引的数据, 而不是其他的节点页(不同引擎的”指针”类型不同); 下图展示了 B-Tree 索引的抽象表示(从技术上来说是B+Tree), 大致反映了InnoDB索引是如何工作的, MyISAM使用的结构有所不同, 但基本思想类似: 可以看出B+Tree索引能够加快访问数据的速度, 是因为存储引擎不再需要进行全表扫描来获取需要的数据, 而是直接从索引树中进行搜索 最后, 你可以去旧金山大学数据结构可视化平台尝试构建一棵自己的B+TREE MySQL大多数存储引擎都支持这种索引(除了Archive), 不同的存储引擎以不同的方式使用B-Tree索引, 性能也各有不同, 各有优劣, 如: MyISAM 使用前缀压缩技术使得索引更小; 而 InnoDB 则按照原数据格式进行存储 MyISAM 索引通过数据的物理位置引用被索引的行; 而 InnoDB 则根据主键引用被索引的行 案例 假设有如下数据表: 1234567891011121314151617CREATE TABLE `People` ( `last_name` varchar(50) NOT NULL, `first_name` varchar(50) NOT NULL, `dob` date NOT NULL, `gender` enum(&apos;m&apos;,&apos;f&apos;) NOT NULL, KEY `idx_test` (`last_name`,`first_name`,`dob`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Christian&apos;, &apos;1958-12-07&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Debbie&apos;, &apos;1990-03-18&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Kirsten&apos;, &apos;1978-11-02&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Cuba&apos;, &apos;1960-01-01&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Kim&apos;, &apos;1930-07-12&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Meryl&apos;, &apos;1980-12-12&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Julia&apos;, &apos;2000-05-16&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Viven&apos;, &apos;1976-12-08&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Viven&apos;, &apos;1979-01-24&apos;, &apos;m&apos;); 如下显示了该索引是如何组织数据的存储的: 注意: 索引对多个列的值进行排序的依据是 定义索引时列的顺序 (如上图中, 最后两个条目, 两个人的姓和名都一样, 则根据他们的出生日期来进行排列顺序) B-Tree 使用限制 B-Tree 索引对如下类型的查询有效 全值匹配: 指的是where条件中有序地包含了索引中的所有列 (需要注意的是, 如果你的查询条件做到了全值匹配, 那么即使你查询条件的顺序不是依照左前缀原则, MySQL也会做优化) 匹配最左列: 可以只匹配 多列索引 的第一列, 比如查找 所有姓为 Allen 的人 匹配最左列的前缀: 可以匹配 索引 中第一列的值的开头部分, 比如, 查找 姓以’J’开头 的人 ( like ‘J%’) 匹配最左列范围值: 查找 ‘姓在Allen和Barrymore’ 之间的人 精确匹配第一列 并 范围匹配第二列: 查找 ‘姓为Allen 并且 名字以K开头’ 的人 只访问索引的查询: 这种查询只需要访问索引, 而无需访问数据行, 后面会单独讨论这种 覆盖索引B-Tree索引中存储了实际的列值, 所以某些查询(满足覆盖索引时)可能只使用索引就能够完成查询工作了 B-Tree 索引的有效排序: 由于索引树中的节点是有序的, 所以除了按值查找之外, 索引还可以用于查询中的 ORDER BY 操作(按顺序查找); 一般来说, 如果 B-Tree 可以按照某种方式查询到值, 那么也可以按照这种方式用于排序 B-Tree索引的限制: 如果查询条件不是从索引的最左列开始写, 则无法使用索引 如People表的索引, 无法用于 ‘查找名字为Bill的人’, 也无法用于 ‘查找生日为1960-01-01的人’, 因为这两列都不是最左数据列 (不满足匹配最左列)也无法用于查找 ‘姓以某个字母结尾的人’ (你建索引时指定的列顺序, 列的值内容 都要符合最左前缀才能利用到索引) (不满足匹配最左列的前缀) 查询条件不能跳过索引中的列: 比如, 查询 ‘姓为Smith 并且 生日为1960-01-01’ 只能只用到索引的第一列 如果查询条件中有某个列是范围查询, 则其右边的所有列都无法使用索引来优化查找 如 where last_name=&#39;Smith&#39; and first_name LIKE &#39;J%&#39; AND dob=&#39;1976-12-23&#39; 这个查询只能使用索引的前两列所以, 如果范围查询的列的值结果有限, 比如数据表中只有2个人是 ‘名字以J开头’ 的, 那你也别用范围查找了, 直接用多个等于条件来代替就行: where last_name=&#39;Smith&#39; and (first_name=&#39;Jack&#39; or first_name=&#39;Jieke&#39; ) AND dob=&#39;1976-12-23&#39; 哈希索引概述 hash index 是基于哈希表实现, 只有精确匹配索引所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code), 哈希码是一个较小的值, 并且不同键值的行计算出来的哈希码也不一样, 哈希索引将所有的哈希码存储在索引中, 同时在哈希表中保存指向每个数据行的指针; 哈希索引自身只需要存储对应的哈希值, 所以索引的结构之分紧促, 查找的速度非常快 哈希索引的缺陷: 由于哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快, 所以大部分情况下这一点对性能的影响并不明显; 哈希索引数据并不是按照索引值顺序存储的, 所以也就无法用于排序 哈希索引页不支持部分索引列匹配查找, 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 哈希索引只支持等值比较查询, 包括 =, in(), 不支持任何范围查询(如 &gt; 等) 访问哈希索引的数据非常快, 除非有很多哈希冲突 (当出现哈希冲突时, 存储引擎必须遍历链表中所有的行指针, 逐行进行比较, 直到找到所有符合条件的行) 如果哈希索引冲突很多的话, 一些索引维护操作的代价也会很高, 例如在某个选择性很低(哈希冲突很多)的列上创建哈希索引, 那么当从表中删除一行时, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的引用, 冲突越多, 代价越大; 因为这些限制, 哈希索引只适用于某些特定的场合。而一旦适合哈希索引, 则它带来的性能提升将非常显著。举个例子, 在数据仓库应用中有一种经典的 “星型” schema, 需要关联很多查询表, 哈希索引就非常适合查找表的需求。 InnoDB引擎有一个特殊的功能叫做 **自适应哈希索引(adaptive hash index), 当InnoDB注意到某些索引值被使用得非常频繁时, 它会在内存中基于B-Tree索引之上再创建一个哈希索引, 这样就让B-Tree索引也具有哈希索引的一些优点, 比如快速的哈希查找; 这是一个完全自动的、内部的行为, 用户无法控制或者配置, 不过如果有必要, 完全可以关闭该功能; InnoDB — HASH索引陷阱 首先, 在翻阅《高性能MySQL》一书时, P146页提到 “在MySQL中, 只有Memory存储引擎显示支持哈希索引”; 而 《MySQL技术内幕 InnoDB存储引擎》一书却在P183提到 InnoDB是支持哈希索引的; 那么 InnoDB 到底支不支持Hash索引呢? 参考官方手册 发现手册中说的是InnoDB不支持Hash索引, 但是同时给一段特殊的提示 ‘InnoDB utilizes hash indexes internally for its Adaptive Hash Index feature’ 其实官方的描述和《MySQL技术内幕 InnoDB存储引擎》中表述的类似 InnoDB存储引擎 是支持hash索引的, 不过, 我们必须启用; 如下可以查看hash的开启状态 (默认就是开启的) 1234567mysql&gt; show variables like &apos;%ap%hash_index&apos;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| innodb_adaptive_hash_index | ON |+----------------------------+-------+1 row in set (0.00 sec) hash索引的创建由InnoDB存储引擎根据表的使用情况自动优化为表生成哈希索引, 我们干预不了 平时遇到的假象 既然我们无法干预是否在一张表中生成哈希索引, 为什么在 Navicat 中为InnoDB表创建索引时, 可以指定索引类型为 HASH索引, 而且MySql 并不会报错? 而且你通过 SHOW CREATE TABLE 查看该索引也是 Hash123456CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) USING HASH) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 真相: 事实上并非如此, 我们都被 MySql 给骗了, 我们使用 SHOW INDEXES FROM 语句对该表索引进行检索, 发现其实用的是BTREE 12345678mysql&gt; SHOW INDEXES FROM t2;+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| t2 | 0 | PRIMARY | 1 | id | A | 0 | NULL | NULL | | BTREE | | || t2 | 1 | idx_name | 1 | name | A | 0 | NULL | NULL | | BTREE | | |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+2 rows in set (0.00 sec) 虽然InnoDB并不支持 Hash 索引, 但 InnoDB 有另一种实现方法: 自适应哈希索引 InnoDB 存储引擎会监控对表上索引的查找, 如果观察到建立哈希索引可以带来速度的提升, 则建立哈希索引 可以通过 SHOW ENGINE INNODB STATUS 来查看当前自适应哈希索引的使用状况 创建自定义hash索引 如果存储引擎不支持哈希索引, 你可以模拟像InnoDB一样创建哈希索引, 这可以享受一些哈希索引的便利, 例如只需要很小的索引就可以为超长的键创建索引, 思路很简单: 在B-Tree基础上创建一个伪哈希索引, 这和真正的哈希索引不是一回事, 因为还是使用B-Tree进行查找, 但是它使用哈希值而不是键本身进行索引查找; 你需要做的就是在查询的WHERE子句中手动指定使用哈希函数; 下面是一个实例, 例如需要存储大量的URL, 并需要根据URL进行搜索查找如果使用B-Tree来存储URL, 存储的内容就会很大, 因为URL本身都很长, 正常情况下会有如下查询: 1mysql&gt; select id from url where url=&quot;http://www.mysql.com&quot;; 若删除原来的URL列上的索引, 而新增一个被索引的url_crc列, 使用CRC32做哈希, 就可以使用下面的方式查询: 1mysql&gt; select id from url where url=&quot;http://www.mysql.com&quot; and url_crc=CRC32(&quot;http://www.mysql.com&quot;); 这样做的性能会非常高, 因为MySQL优化器会使用这个选择性很高而体积很小的基于url_crc列的索引来完成查找, 即使有多个记录有相同的索引值, 查找仍然很快, 只需要根据哈希值做快速的整数比较就能找到索引条目, 然后一一比较返回对应的行; 这样实现的缺陷是需要维护哈希值, 可以手动维护, 也可以使用触发器实现 下面的案例演示了触发器如何在插入和更新时维护url_crc列, 首先创建如下表: ?? 数据空间索引全文索引其他索引类别","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"05. 一条消息的一生","slug":"rabbitmq/2018-05-28-rabbitmq-05","date":"2018-05-28T09:16:23.000Z","updated":"2018-06-12T08:06:13.000Z","comments":true,"path":"2018/05/28/rabbitmq/2018-05-28-rabbitmq-05/","link":"","permalink":"http://blog.renyimin.com/2018/05/28/rabbitmq/2018-05-28-rabbitmq-05/","excerpt":"","text":"到现在, 已经成功安装运行了Rabbit, 并且简单了解了RabbitMQ的消息通信机制及Rabbit内部的一些组件; 接下来就需要通过编码来学习如何使用RabbitMQ简单发布并消费一条消息;本系列文章代码Demo使用 Laravel5.5 + php-amqplib来做演示; 生产者应用程序 先尝试创建一个生产者, 其基本步骤可以分为如下几步 123456789创建与Rabbit Server的TCP连接创建信道通过信道--创建exchange通过信道--创建queue通过信道--对exchange与queue进行绑定创建消息通过信道--发布消息关闭信道关闭连接 代码已上传至Github中的 firstProducer() 方法 消费者应用程序 再尝试创建一个消费者, 其基本步骤可以分为如下几步 1234567创建与Rabbit Server的TCP连接创建信道通过信道--创建exchange通过信道--创建queue通过信道--对exchange与queue进行绑定创建消息回调函数消费消息 代码已上传Github 参数讲解在上面的生产者的小实例中, 对 声明交换器、声明队列、绑定交换机与队列、声明消息、消息发布 的一些基础参数属性进行了注释讲解; 声明交换器 passive 参数 默认为false: rabbit-server 会查看有没有已存在的同名exchange, 没有则直接创建, 有则不会进行创建; 结果总是返回 null; 如果你只是希望查询一下交换机是否存在, 而不是创建这个交换机, 设置为true即可; 如果存在则返回NULL, 如果交换机不存在, 则会抛出如下异常: 12PhpAmqpLib \\ Exception \\ AMQPProtocolChannelException (404)NOT_FOUND - no exchange &apos;ex1&apos; in vhost &apos;/&apos; 所以一般这个选项比较少用; 另外需要了解的是, passive设置为true时, 如果exchange已存在, 你当前创建的exchange即使是诸如type之类的参数进行了变更, 也不会报错, 因为压根不会尝试创建, 只是返回null; 但是如果passive设置为false, 则当前会尝试创建exchange, 此时, 如果exchange的参数有变更,比如已存在的exchange的type为direct, 而当前创建的同名exchange的type为fanout, 就会报错: 1PRECONDITION_FAILED - inequivalent arg &apos;type&apos; for exchange &apos;ex1&apos; in vhost &apos;/&apos;: received &apos;fanout&apos; but current is &apos;direct&apos; - 可参考[此文](https://www.kancloud.cn/xsnet/xinshangjingyan/297803) druable(消息持久化的条件之一) : true为持久化 auto_delete: 自动删除(默认是启用的, 交换器将会在所有与其绑定的队列被删除后自动删除; (所以如果做持久化的话, 需要设置为false) arguments : 声明exchange时, 可以使用AMQPTable对象来创建一些额外的说明参数, 比如: 12345678$arguments = new AMQPTable([ &apos;arguments1&apos; =&gt; &apos;想写什么信息都行&apos;, &apos;arguments2&apos; =&gt; [ &apos;想写什么信息都行, 比如声明是那条业务线的&apos;, &apos;想写什么信息都行, 比如连接信息....&apos;, ] ]); internal : 稍后实例解释 nowait : 稍后实例解释 声明队列 passive 参数 (貌似和exchange的passive稍有不同) 默认为false: rabbit-server 会查看有没有已存在的同名queue, 没有则直接创建, 有则不进行创建; 无论创建与否, 结果都返回 队列基础信息 1234array (size=3) 0 =&gt; string &apos;queue1&apos; (length=6) 1 =&gt; int 5 2 =&gt; int 0 如果你希望查询队列是否存在, 而又不想在查询时创建这个队列, 设置此为true即可; 如果存在则返回 队列基础信息, 如果队列不存在, 则会抛出一个错误的异常; 另外需要了解的是, passive设置为true时, 如果queue已存在, 你当前创建的queue即使是诸如type之类的参数进行了变更, 也不会报错, 因为压根不会尝试创建, 只是返回已存在的queue信息; 但是如果passive设置为false, 则当前会尝试创建queue, 此时, 如果queue的参数有变更,比如已存在的queue的为持久型, 而当前创建的同名queue的为非持久的, 就会报错: 1PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;queue1&apos; in vhost &apos;/&apos;: received &apos;false&apos; but current is &apos;true&apos; 和 exchange 的 passive 参数不同的是, 此处队列声明的结果会返回 队列基础信息, 但是这是依赖于 nowait 参数, 如果nowait参数为默认值false, 则会返回, 如果为true, 则就返回null; druable(消息持久化的条件之一) : true为持久化 exclusive : 如果设置为true, 则创建的为 排他队列 如果一个队列被声明为排他队列, 该队列仅对首次声明它的连接可见, 并在连接断开时自动删除。也就是说, 如果你在生产者中创建排他队列, 则连接结束, 队列就没了, 所以你可能一直看不到创建的队列; 另外需要注意三点: 1.排他队列是基于连接可见的, 同一连接的不同信道是可以同时访问同一个连接创建的排他队列的 2.如果一个连接已经声明了一个排他队列, 其他连接是不允许建立同名的排他队列的, 这个与普通队列不同 3.即使该队列是持久化的,一旦连接关闭或者客户端退出,该排他队列都会被自动删除的 所以, 貌似排他队列只能由消费者创建, 而且这种队列适用于只有一个消费者消费消息的场景 暂时没有找到特别适合的场景~~ auto_delete: 自动删除(默认是启用的, 队列将会在所有的消费者停止使用之后自动删除掉自身, 注意: 没有消费者不算, 只有在有了消费之后, 所有的消费者又断开后, 就会自动删除自己) arguments : 声明queue时, 可以使用AMQPTable对象来创建一些额外的说明参数, 同exchange的arguments参数效果一样; 创建消息AMQPMessage 类的第二个参数properties可以设置很多属性, 目前需要熟悉的是 delivery_mode : 消息持久化的条件之一, 值为2时表示持久化 content_type : 比如: ‘text/plain’消息发布消息暂时接触到的属性都比较熟悉, 稍后会接触一些特殊作用的属性 mandatory 当mandatory标志位设置为true时, 如果exchange根据自身类型和消息routeKey无法找到一个符合条件的queue, 那么rabbit server会自动去调用 basic.return 方法将消息返回给生产者(Basic.Return + Content-Header + Content-Body);生产者可以使用channel的set_return_listener()绑定一个回调函数来进行监听, 但是注意, 此时生产者貌似需要为阻塞状态在命令行启动, 所以, 我是不会这么写一个生产者的… 当mandatory设置为false时, 出现上述情形broker会直接将消息扔掉; 代码已上传github, php-amqplib文档Demo php-amqplib并未实现的immediate 当immediate标志位设置为true时, 如果exchange在将消息路由到queue(s)时发现对应的queue上没有消费者, 那么这条消息不会放入队列中, 当与消息routeKey关联的所有queue(一个或者多个)都没有消费者时, 该消息会通过 basic.return 方法返还给生产者; 但是可惜的是, 这只是AMQP的规定, 客户端不一定会严格实现, 如 php-amqplib 包就没有实现, 如果设置了immediate为true, 运行会报错: PhpAmqpLib \\ Exception \\ AMQPProtocolConnectionException (540) NOT_IMPLEMENTED - immediate=true 概括来说, mandatory标志告诉服务器至少将该消息route到一个队列中, 否则将消息返还给生产者; immediate标志告诉服务器如果该消息关联的queue上有消费者, 则马上将消息投递给它, 如果所有queue都没有消费者, 直接把消息返还给生产者, 不用将消息入队列等待消费者了; 注意AMQP协议规定了不同客户端(比如php-amqplib)需要实现的内容, 但是这些客户端不一定会完全并严格地实现;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"04. 浅析RabbitMQ消息通信模式 (内部各组件介绍)","slug":"rabbitmq/2018-05-27-rabbitmq-04","date":"2018-05-27T10:30:26.000Z","updated":"2019-04-17T14:23:18.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-04/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-04/","excerpt":"","text":"前言RabbitMQ专注于应用程序之间的消息通信, 在尝试使用Rabbit进行消息通信前, 清楚地理解通信模式的概念是非常重要的 Rabbit消息通信架构图大致如下: Rabbit Server(Broker Server)RabbitMQ是作为一个消息投递的中间代理服务器存在的 (broker有 ‘中间人’, ‘代理人’的意思), 可以认为它在应用程序之间扮演着路由器的角色; 所以在使用Rabbit进行消息通信时, 当应用程序连接到RabbitMQ时, 你就必须做个决定, 你的应用程序是准备作为生产者发送消息呢, 还是作为消费者来接收消息; ConnectionConnection其实就是一个TCP的连接, 无论你的应用程序是作为 Producer 还是 Consumer, 都是通过TCP连接到RabbitMQ Server的, 任何与Rabbit Server内部交互的应用程序都需要通过TCP连接到RabbitMQ Server, 后面会看到, 程序的起始就是建立这个TCP连接; Connection是RabbitMQ的socket链接, 它封装了socket协议相关部分逻辑; ChannelChannel 是我们与RabbitMQ打交道的最重要的一个接口, 我们大部分的业务操作是在channel这个接口中完成的 (包括 定义exchange, 定义queue, 绑定exchange与queue、发布消息 等); Channel 是虚拟连接, 它建立在上述的TCP连接 connection 中, 数据流动都是在channel中进行的, 程序起始第一步就是建立TCP连接 Connection, 第二步就是建立信道 Channel; 为什么使用Channel, 而不是直接使用TCP连接?对于OS来说, 建立和关闭TCP连接是有代价的, 频繁的建立关闭TCP连接对于系统的性能有很大的影响, 而且TCP的连接数也有限制, 这也限制了系统处理高并发的能力;但是, 在TCP连接中建立channel是没有上述代价的, 对于 producer 或者 consumer 来说, 可以并发的使用多个channel进行 ‘publish’或者’receive’; Exchange在你的应用程序连接到Rabbit Server后, 如果应用程序是作为Producer, 其发布的消息, 并不会直接被Rabbit Server投递到queue中, 这是务必要注意的; (消息永远不可能被直接发送到Rabbit Server的Queue中, 它总是会经过Rabbit Server的内部组件 — exchange交换机); 为什么Rabbit Server不是将生产者投递的消息直接发送到队列, 而必须要经过内部的exchange组件? 毕竟对于生产者来说, 只是希望消息能够到队列而已, Rabbit Server中为啥还多了一步? 如果没有exchange内部组件, 对于生产者投递来的消息, broker其实貌似也可以根据消息内容中的某个属性来判断消息该投递给哪个队列, 或者也可以指定投递给全部队列, 或者根据正则来投递给某些队列; 但这样的话, 相当于生产者和队列直接面对面做了强绑定; 而如果有exchange的话, 对于绑定和解析绑定, 直接交由exchange来进行即可, 这样在内部也做到了解耦和单一职责的划分, 可以将一些业务逻辑独立出来(比方你是要群发, 还是指定队列发送, 或者匹配队列发送, 这些绑定和解析工作都由exchange在生产者和队列中间进行处理, 生产者和队列不需要直接面对面); 在RabbitMQ中, exchange组件有四种类型 ： direct :使用这种类型, 会将生产者发送消息时设置的 routingkey 与 queue和exchange绑定时的 bindingkey 进行精确匹配, 如果 routingkey 和 bindingkey 完全一样, 那么Message就会被传递到当前exchange的对应queue中;如果使用了direct类型Exchange, 绑定队列和exchange时, 可以不必指定 routingkey 的名字, 在此类型下创建的queue有一个默认的 routingkey, 这个 routingkey 一般和 queue 同名;注意: 虽然不可以创建两个相同名字的队列, 但是多个不同名的队列与exchange进行绑定时, bindingkey 可以相同, 所以可能会有多个队列的bindingkey都 与 同一条消息发布时的routingkey可以匹配, 从而导致同一条消息被exchange路由到多个队列中! fanout:使用这种类型, 会将生产者发送消息时设置的 routingkey 与 queue和exchange绑定时的 bindingkey 进行模式匹配(正则匹配)比如 生产者发送消息时设置的routing key为 ab*, 则可以与 bindingkey 为 ab 开头的所有的queue匹配; topic ：使用这种类型, 会忽略routingkey的存在, 直接将消息广播到所有与当前exchange绑定的queue中; headers : 不太实用, 而且性能比较差, 几乎再也用不到了 BindingKey队列在声明后, 需要与exchange进行绑定, 绑定的时候可以显示地区指定一个 routingkey, 这里我们叫 bindingkey;注意: 队列在与exchange绑定时, 如果没有显示地去设置 bindingkey, 此时 bindingkey 默认为队列名字; RoutingKey生产者在消息发布时, 需要指明消息的 routingkey, exchange则会依据此routingkey去匹配 与 自身绑定的那些队列在绑定时 所显示或非显示设置的 bindingkey, 从而决定消息将会被路由到与自身绑定的哪些队列中;当然, 如果exchange设置的type是topic类型, 则会忽略 routingkey 和 bindingkey 的存在; Queue生产者投递给broker server的消息, 最终会被内部组件exchange路由到匹配的队列中, 等待consumers处理; 队列在声明后, 需要与exchange进行绑定 (绑定时会使用一个叫 routingkey 的东西, 这里我一般会叫他 bindingkey); 注意: 消息被exchange接收以后, 如果没有匹配的queue, 则消息会被丢弃, 即, 发送出去的消息如果路由到了不存在的队列的话, Rabbit 会忽略它们, 因此如果你不能承担消息丢入 黑洞 而丢失的话, 你的应用无论是生产者还是消费者, 都应该尝试去创建队列; 队列有很多参数属性可以设置, 后面在代码中经会一一展示如何使用它的这些属性; Producer图中左侧的客户端程序’A,B’, 其实就是生产者: 连接到 Rabbit Server , 并将消息投递到 Rabbit Server, 然后由exchange交换机决定该如何转发消息到队列中;Producer永远不可能直接将消息丢到queue中;Producer可以通过bindingkey与exchange进行绑定; Consumer图中右侧的客户端程序’1,2,3’, 其实就是消费者: 连接到 Rabbit Server , 并从 Rabbit Server 中接收消息; 发送消息时需要指定 routingkey, 如果没有指定routingkey 或者 指定的routingkey匹配不到对应的bindingkey, 消息会被 Rabbit 服务丢弃掉, 掉入”黑洞”; 另外, consumer在处理完消息之后, 消息的投递标识当一个消费者向RabbitMQ注册后, RabbitMQ会用 basic.deliver 方法向消费者推送消息, 这个方法携带了一个 delivery tag, 它在一个channel中唯一代表了一次投递。delivery tag的唯一标识范围限于channel;delivery-tag是单调递增的正整数, 可以在消费者中进行获取;(customer之间会不会累加? delivery-tag是生产者没法送一次就自增?) QoS 通道预取设置通过 basicQos 这个方法可以设置预取个数, 这个数值定义了一个 channel通道最多有多少个未确认的消息;值得重申的是, 投递流程和手动客户端确认是完全异步的, 因此, 如果在投递中已经有消息的情况下改变预取值, 则会出现自然竞争条件, 并且在信道上可能暂时存在多于预取未确认消息数量;https://blog.csdn.net/KuaiLeShiFu/article/details/77746431RabbitMQ提供了一种qos(服务质量保证)功能, 即在非自动确认消息的前提下, 如果一定数目的消息未被确认前, 消费者不消费新的消息;这种机制一方面可以实现限速(将消息暂存到RabbitMQ内存中)的作用, 一方面可以保证消息确认质量(比如确认了但是处理有异常的情况); 消费确认模式必须是非自动ACK机制(这个是使用baseQos的前提条件, 否则会Qos不生效), 然后设置basicQos的值;另外, 还可以基于consume和channel的粒度进行设置(global) Consumer Prefetchhttps://www.jianshu.com/p/4d043d3045ca 官方文档消费预取一节可以看到, 在RabbitMQ中, 对prefetch_count的定义与AMQP0-9-1貌似不太一样; prefetch允许为每个consumer指定最大的unacked messages数目, 简单来说就是用来指定一个consumer一次可以从Rabbit中获取多少条message并缓存在client中(RabbitMQ提供的各种语言的client library), 一旦缓冲区满了, Rabbit将会停止投递新的message到该consumer中直到它发出ack; 假设prefetch值设为10, 共有两个consumer, 意味着每个consumer每次会从queue中预抓取10条消息到本地缓存着等待消费, 同时该channel的unacked数变为20, 而Rabbit投递的顺序是，先为consumer1投递满10个message，再往consumer2投递10个message。如果这时有新message需要投递，先判断channel的unacked数是否等于20，如果是则不会将消息投递到consumer中，message继续呆在queue中。之后其中consumer对一条消息进行ack，unacked此时等于19，Rabbit就判断哪个consumer的unacked少于10，就投递到哪个consumer中。 总的来说，consumer负责不断处理消息，不断ack，然后只要unacked数少于prefetch * consumer数目，broker就不断将消息投递过去 正常情况下, 如果有多个消费者订阅了同一个Queue, Queue中的消息会被平摊给多个消费者;但是, 如果每个消息的处理时间不同, 就有可能会导致某些消费者一直在忙, 而另外一些消费者很快就处理完手头工作并处于空闲的情况;此时, 可以通过设置 prefetchCount 来限制queue每次发送给每个消费者的消息数, 比如我们设置prefetchCount=1, 则queue每次给每个消费者发送一条消息, 消费者处理完这条消息后Queue会再给该消费者发送一条消息; delivery_tagvhost最后, 还有一个在图中并未体现出来的概念 vhost 每一个RabbitMQ服务器都能创建虚拟消息服务器, 我们称之为虚拟主机(vhost), 每一个 vhost 本质上都是一个 mini 版的 RabbitMQ Server, 拥有自己的 exchagne, queue, 和 bindings rule 等等, 更重要的是, 它还拥有自己的 权限机制, 这使得你能够安全地使用一个RabbitMQ服务器来服务众多应用程序, 而不用担心A应用可能会删除B应用正在使用的队列; vhost之于Rabbit就像虚拟机之于物理服务器一样: 通过在各个实例间提供逻辑上隔离, 允许你为不同应用程序安全保密地运行数据, 这很有用, 它既能将同一Rabbit的众多客户端区分开来, 又可以避免队列和交换器的命名冲突; 否则, 你可能不得不去运行多个Rabbit, 并承担随之而来的管理问题, 相反, 你可以只运行一个Rabbit, 然后按需启动或者关闭vhost;当你有多个不同的应用要使用RabbitMQ时, 你可以为每个应用定义一个vhost来将应用从逻辑上隔离开; vhost是AMQP概念的基础, 你必须在连接时进行指定, 由于RabbitMQ包含了开箱即用的默认vhost: / , 因此时候用起来非常简单, 如果你不需要多个vhost的话, 就使用默认的即可;通过使用默认的guest用户名和密码guest, 就可以访问默认的 vhost, 为了安全起见, 你应该更改它; AMQP 并没有指定权限控制是在vhost级别还是在服务器端级别实现, 这留给了消息服务器的开发者去决定, 而在RabbitMQ中, 权限控制是以vhost为单位的, 当你在Rabbit里创建一个用户时, 用户通常会被指派给至少一个vhost, 并且只能访问被指派的vhost内的队列, 交换机和绑定;当你在设计消息通信架构时, 记住vhost之间是绝对隔离的, 你无法将 vhost ‘a’ 中的交换机绑定到 vhost ‘b’ 中的队列去; 另外需要注意: 当你在RabbitMQ集群上创建vhost时, 整个集群上都会创建该vhost; 如何创建vhost? vhost和权限控制非常独特, 它们是AMQP中唯一无法通过AMQP协议创建的基元(不同于队列, 交换机和绑定), 对于RabbitMQ来说, 你需要通过 RabbitMQ 安装路径下 ./sbin/ 目录中的 rabbitmqctl 工具来创建 如果想知道特定rabbitmq服务器上运行着哪些vhost时, 可以执行 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_vhostsListing vhosts/ 创建一个vhost 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl add_vhost testCreating vhost &quot;test&quot; 删除一个vhost 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl delete_vhost testDeleting vhost &quot;test&quot; 小结这一部分只是大概了解一下RabbitMQ服务一些概念性知识, 当然, 关于这些知识点在后面进行编码的时候会进行进一步扩展, 比如 队列, 消息, 交换机的诸多属性都会涉及到;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"03. 服务器管理","slug":"rabbitmq/2018-05-27-rabbitmq-03","date":"2018-05-27T10:20:09.000Z","updated":"2018-05-28T09:07:06.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-03/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-03/","excerpt":"","text":"前言到目前为止我们已经 下载安装并成功启动了Rabbitmq服务, 接下来, 可以简单了解一些与服务器管理相关的知识点; Erlang 和 Erlang Cookie命令行的基本使用RabbitMQ的启动可以在安装目录下的 ‘sbin/‘ 目录下, 运行 rabbitmq-server -detached 来启动后来运行;如果启动中遇到了任何错误, 可以检查RabbitMQ的日志文件; 如果不知道日志文件的位置, 可以查看: 2. 配置相关一般情况下, RabbitMQ的默认配置就足够了, 通过在Web管理平台可以看到, 默认是没有配置文件的; 如果希望特殊设置的话, 有几个位置可以进行配置;注意, 文件默认是没有的, 如果需要必须自己创建, 至于创建配置文件的位置在哪里, 可以通过日志文件来查看: 12345678910111213141516171819enyimindembp:rabbitmq renyimin$ pwd/usr/local/var/log/rabbitmqrenyimindembp:rabbitmq renyimin$ lslog rabbit@localhost-sasl.log rabbit@localhost.log rabbit@localhost_upgrade.logenyimindembp:rabbitmq renyimin$ head -20 rabbit\\@localhost.log=INFO REPORT==== 11-Dec-2017::15:37:02 ===Starting RabbitMQ 3.6.14 on Erlang 20.1.7Copyright (C) 2007-2017 Pivotal Software, Inc.Licensed under the MPL. See http://www.rabbitmq.com/=INFO REPORT==== 11-Dec-2017::15:37:02 ===node : rabbit@localhosthome dir : /Users/renyiminconfig file(s) : /usr/local/etc/rabbitmq/rabbitmq.config (not found)cookie hash : qNSOVbC4c3Bg7punCVVdaQ==log : /usr/local/var/log/rabbitmq/rabbit@localhost.logsasl log : /usr/local/var/log/rabbitmq/rabbit@localhost-sasl.logdatabase dir : /usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost rabbitmq-env.conf环境变量的配置文件 rabbitmq-env.conf : 可以用来配置rabbitmq的一些基本配置文件的路径; 这些文件的位置是特定于分发的, 默认情况下, 它们可能不会创建, 但希望位于每个平台的以下位置： 通用UNIX $ RABBITMQ_HOME/etc/rabbitmq / Debian /etc/rabbitmq/ RPM /etc/rabbitmq/ Mac OSX(Homebrew) $ {install_prefix}/etc/rabbitmq/，Homebrew前缀通常是/usr/local Windows ％APPDATA％\\RabbitMQ\\ 现在修改 rabbitmq-env.conf 中指定的 rabbitmq.conf 位置为 ‘/usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq.conf’1234567891011121314renyimindembp:rabbitmq renyimin$ pwd/usr/local/etc/rabbitmqrenyimindembp:rabbitmq renyimin$ cat rabbitmq-env.confCONFIG_FILE=/usr/local/etc/rabbitmq/rabbitmqNODE_IP_ADDRESS=127.0.0.1NODENAME=rabbit@localhostrenyimindembp:rabbitmq renyimin$ vim rabbitmq-env.conf....renyimindembp:rabbitmq renyimin$ cat rabbitmq-env.conf#CONFIG_FILE=/usr/local/etc/rabbitmq/rabbitmqCONFIG_FILE=/usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmqNODE_IP_ADDRESS=127.0.0.1NODENAME=rabbit@localhostrenyimindembp:rabbitmq renyimin$ 改完重启之后, 在Web管理界面会发现也有了配置文件的路径: rabbitmq.conf如果 rabbitmq.conf 不存在, 可以手动创建;如果更改位置, 请设置 RABBITMQ_CONFIG_FILE 环境变量, RabbitMQ会自动将.conf扩展名附加到此变量的值; 配置对于rabbitmq.config文件, rabbit官网提供了默认的rabbitmq.conf.example, 可以从此处获取;12 主要参考官方文档：http://www.rabbitmq.com/configure.html RabbitMQ权限控制解读Rabbit日志文件 在成功启动了RabbitMQ服务之后, 命令行查看虚拟机, 队列, 交换器和绑定状态; 通常情况下, 你是在服务器上直接运行 rabbitmqctl 来管理自己的rabbitmq节点, 不过你也可以通过 -n rabbit@[server_name]选项来管理远程rabbitmq节点, @符号将节点标识符(rabbit@[server_name])分成两部分: 左边的是Erlang应用程序名称, 在这里永远都是 rabbit; 右边的是服务器主机名或者IP地址; 你要确保运行Rabbit节点的服务器和运行 rabbitmqctl 的工作站点安装了相同的 Erlang Cookie;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"02. Win,MacOS,Linux下载安装RabbitMQ","slug":"rabbitmq/2018-05-27-rabbitmq-02","date":"2018-05-27T04:20:21.000Z","updated":"2018-05-27T10:19:44.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-02/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-02/","excerpt":"","text":"Windows 下载windows版RabbitMQ: 到官网下载windows installer即可; 直接傻瓜式双击安装 (可能会提示: Erlang could be detected. You must install Erlang before install RabbitMQ...., 点击是, 会跳转到 Erlang 的下载地址) 然后下载并安装对应版本的 Erlang (你的RabbitMQ对应的Erlang版本该如何选择, 可以参考Windows: With installer (recommended) | Manual) 然后傻瓜式下载安装 Erlang 即可 安装完Erlang之后, 双击安装RabbitMQ 在安装完RabbitMQ之后, 在Windows的开始菜单中会有 RabbitMQ 服务相关的一些选项可以使用, 如 RabbitMQ service - start 启动RabbitMQ服务 RabbitMQ service - stop 关闭RabbitMQ服务 RabbitMQ Command Prompt 连接RabbitMQ服务 (命令行连接工具) …… 当然, 你可以配置rabbitmq安装目录下的sbin目录到环境变量中, 这样就可以直接cmd使用相关的rabbitmq相关命令了; 注意windows下的问题: 如果启动命令行连接后, 执行 rabbitmqctl status 报错为 erlang cookie 问题, 目前找到的解决方案是: 将C:\\Windows\\System32\\config\\systemprofile\\.erlang.cookie 替换到 C:\\Users\\Administrator\\.erlang.cookie 之后, 可以开启rabbitmq-management插件, 来通过Web UI的形式来进行RabbitMQ的管理 执行如下操作打开rabbitmq-management插件 1234567891011121314151617181920E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt;rabbitmqctl list_usersListing users ...guest [administrator]E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt;rabbitmq-plugins enable rabbitmq_managementEnabling plugins on node rabbit@DHCCOEPA4DNL18R:rabbitmq_managementThe following plugins have been configured: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatchApplying plugin configuration to rabbit@DHCCOEPA4DNL18R...The following plugins have been enabled: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatchstarted 3 plugins.E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt; 访问http://127.0.0.1:15672即可打开Web管理界面, 默认账号和密码为guest; Mac Mac下载安装RabbitMQ比较简单, 直接 home install rabbitmq 安装完成即可 1234567891011121314151617181920renyimin$ brew info rabbitmqrabbitmq: stable 3.7.5Messaging brokerhttps://www.rabbitmq.com/usr/local/Cellar/rabbitmq/3.7.5 (232 files, 10.1MB) * // 可以看到, hombrew安装的软件基本上会被放置到 `cd /usr/local/Cellar/` 下 Built from source on 2018-05-27 at 11:43:30From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/rabbitmq.rb==&gt; DependenciesRequired: erlang ✔ // 可以看到, 已经自动帮你安装了Erlang依赖==&gt; CaveatsManagement Plugin enabled by default at http://localhost:15672 // 并且自动开启了插件管理Bash completion has been installed to: /usr/local/etc/bash_completion.dTo have launchd start rabbitmq now and restart at login: brew services start rabbitmqOr, if you don&apos;t want/need a background service you can just run: rabbitmq-serverrenyimindembp:Cellar renyimin$ 按照指示, 启动rabbitmq后台运行 12renyimin$ brew services start rabbitmq==&gt; Successfully started `rabbitmq` (label: homebrew.mxcl.rabbitmq) 访问 http://127.0.0.1:15672, 默认账号密码为 guest Linux.. 后续完善 Docker.. 后续完善","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"01. 认识 RabbitMQ","slug":"rabbitmq/2018-05-27-rabbitmq-01","date":"2018-05-27T02:35:21.000Z","updated":"2019-04-17T14:03:36.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-01/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-01/","excerpt":"","text":"RabbitMQ是一个由 Erlang 开发的 AMQP 协议的开源消息队列软件, 遵循 Mozilla Public License开源协议; (rabbit : 兔子, 行动迅速, 繁殖疯狂) AMQP 即 Advanced Message Queuing Protocol(高级消息队列协议), 一个提供统一消息服务的 应用层 标准 高级消息队列协议, 是应用层协议的一个开放标准, 为面向消息的中间件设计 基于此协议的客户端与消息中间件可传递消息, 并不受客户端/中间件不同产品, 不同的开发语言等条件的限制 在《RabbitMQ实战 高效部署分布式消息队列》1.1节 消息队列软件 的相关历史可以了解到, 很早就有很多消息队列软件, 但是由于 供应商壁垒, 导致中小型公司对高价格MQ供应商不满, 而大型公司不可避免地使用来自众多供应商的MQ产品来服务企业内部不同的应用, 这些产品使用不同的API, 不同的协议, 因而无法联合起来组成单一总线; 虽然在2001年诞生的JMS通过提供公共Java API的方式, 隐藏了单独MQ产品供应商提供的实际接口, 从而跨越了壁垒和解决了互通问题。但其实我们需要的是新的消息通信标准化方案, 所以就有了 AMQP 协议的诞生; 消息队列解决的问题 应用解耦, 比如你可能想着：如何将一个耗时的任务从触发它的应用程序中移出? 如何整合用不同语言编写的应用程序, 使得他们运行起来像单个系统?(虽然看起来是两个不同的问题, 但却有着共同的本质: 解耦 请求 和 处理, 这两个问题均需要从同步编程模型转向异步编程模型) 缓解流量高峰,提高系统吞吐量: 当系统中的同步处理方式在面对高并发涌入的大量操作时, 系统可能会无法立即做出响应, 从而严重影响系统的吞吐量时, 消息队列就可以用来缓解流量高峰, 提高系统吞吐量; 从上面的 应用解耦 可以看到, 消息队列提供了一个 异步通信协议, 消息的发送者 不用一直等待直到消息被成功处理, 而是消息被成功发送后就立即返回, 而消息则被暂存于队列当中, 暂时缓解系统压力, 系统后续再进行逐一处理; 这样自然会缓解流量高峰, 提高系统吞吐量; 系统调用中断时可以重试: 如果系统 B, C 出现中断, 系统A中的操作已经完成(无法回滚), 那么整个流程就会不完整; 此时就需要系统A的后续流程能够被保留重试; … 学习内容 安装 浅析RabbitMQ消息通信模式 (内部各组件介绍) 服务器运行及管理 日志分析 消息通信及各种细节问题 消息通信各种实际案例分析 高性能, 高可用RabbitMQ 技术点顺序: Qos 消费者预取 消费者优先级","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"23. 高并发事务问题 (二)","slug":"MySQL 暂停/2018-05-04-mysql-23","date":"2018-05-04T09:04:19.000Z","updated":"2019-04-26T11:03:00.000Z","comments":true,"path":"2018/05/04/MySQL 暂停/2018-05-04-mysql-23/","link":"","permalink":"http://blog.renyimin.com/2018/05/04/MySQL 暂停/2018-05-04-mysql-23/","excerpt":"","text":"更新丢失 丢失更新问题 需要和之前几个问题需要区分开, 因为该问题需要我们自己来解决; 更新丢失问题分为两类 第一类丢失更新(回滚覆盖) 事务A 回滚时, 将 事务B 已经提交的数据覆盖了 需要注意的是: 这种情况在Mysql中不会出现; 测试环境准备 MySQL 5.7.25 1234567mysql&gt; show variables like &quot;%innodb_version%&quot;;+----------------+--------+| Variable_name | Value |+----------------+--------+| innodb_version | 5.7.25 |+----------------+--------+1 row in set (0.00 sec) 测试表: 123456CREATE TABLE `goods` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键ID&apos;, `goods_title` varchar(30) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;商品title&apos;, `stock` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;库存&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 测试数据 123INSERT INTO `goods` VALUES (1, &apos;华为荣耀6plus&apos;, 100);INSERT INTO `goods` VALUES (2, &apos;iphone6s&apos;, 200);INSERT INTO `goods` VALUES (3, &apos;锤子手机&apos;, 300); READ UNCOMMITED？？？ 测试出问题12345678910111213mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from goods where id=1;+----+-------------------+-------+| id | goods_title | stock |+----+-------------------+-------+| 1 | 华为荣耀6plus | 100 |+----+-------------------+-------+1 row in set (0.00 sec)mysql&gt; update goods set stock=stock-15 where id=1;ERROR 1665 (HY000): Cannot execute statement: impossible to write to binary log since BINLOG_FORMAT = STATEMENT and at least one table uses a storage engine limited to row-based logging. InnoDB is limited to row-logging when transaction isolation level is READ COMMITTED or READ UNCOMMITTED. 在InnoDB事务的最低隔离级别 READ UNCOMMITED 下, 对于当前事务来说, 其他并行事务的未提交数据都可以读到, 更别说已提交数据了 (所以当前事务在回滚时也会回滚到事务B提交的最新数据) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from goods where id=1;select * from goods where id=1;update goods set stock=stock-10 where id=1;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from goods where id=1;update goods set stock=stock-15 where id=1;commit; RC 级别演示 对于 READ COMMITTED: 在事务B提交之后, 事务A在T3阶段是可以select(快照读)到事务B最终提交的数据的, 更别说update(当前读)到了, 所以事务A最终的Rollback其实也是基于事务B提交后的数据的 (关于这里提到的快照读和当前读, 下一篇会介绍) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RR 级别演示 对于 REPEATABLE READ 可重复读, 事务A在T3阶段虽然select不到事务B最终提交的数据(快照读), 但是可以update(当前读)到事务B最终提交的数据的 (注意: RR与RC虽然都会有快照读, 但是快照读的结果却不一致, 其实是因为两者的MVCC机制快找时机不同导致的, 后面会讲解) 语句如下: 1234567SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age+10 where id=2;rollback; 12345SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age-15 where id=2;commit; SERIALIZABLE 演示 SERIALIZABLE 串行化: 读写都加锁, 最容易出现死锁, 所以也不会出现第一类丢失更新的问题, 直接就死锁了 语句如下: 123456SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;SELECT @@SESSION.tx_isolation;begin;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age -15 where id=2;commit; 第二类丢失更新(提交覆盖) 直接上图 另外, 这里可以解释一下为什么 SERIALIZABLE级别 通常不会不被采用 其实 SERIALIZABLE 虽然做了串行化, 其实也就是对读写都加了锁, 但一旦事务并行, 如果将判断库存的读操作放在事务内就很容易会死锁而放在事务外, 由于更新操作仍然会依据上一个查询的结果, 所以仍然是避免不了第二类丢失更新问题的, 会造成超卖等问题; SERIALIZABLE 的串行化本身也太低效 另外, 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 解决第二类丢失更新的方案: 乐观锁 (在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制) 悲观锁 参考资料: 《高性能MySQL》 淘宝数据库内核6月报 美团技术博客 MySQL官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"22. 高并发事务问题 (一)","slug":"MySQL 暂停/2018-05-04-mysql-22","date":"2018-05-04T03:56:32.000Z","updated":"2019-04-26T11:28:59.000Z","comments":true,"path":"2018/05/04/MySQL 暂停/2018-05-04-mysql-22/","link":"","permalink":"http://blog.renyimin.com/2018/05/04/MySQL 暂停/2018-05-04-mysql-22/","excerpt":"","text":"上一篇MySQL事务简介对MySQL事务的 基本概念 及 特性 做了简单介绍; 接下来会分析在实际生产环境中面对高并发场景时, 事务会出现的一些常见问题 事务并行在并发量比较大的应用中, 很容易出现 多个事务并行 的情况; 现在我们假设有两个事务正在同时进行, 需要注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境 过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 最终就会导致一些问题的出现 问题出现脏读 如果 事务A 读取了另一个并行 事务B 未最终提交数据, 那事务A的这次读取操作就叫 脏读 因为 事务A 此时读取到的是 并行事务B 尚未最终持久化的数据 事务B 最终可能会因为其事务单元内部其他后续操作的失败 或者 系统后续突然崩溃等原因, 导致事务B最终整体提交失败而回滚, 那么最终 事务A 之前拿到就是 脏的数据 了 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身隔离性解决了 脏读 问题 : READ COMMITED 或 以上隔离级别 (RC+) READ COMMITED 隔离级别保证了: 只有已经被其他事务提交的持久性落地数据, 才对当前事务中的 select 可见 不可重复读 之前的 脏读问题 解决了, 仅仅只意味着事务单元中的每条语句读取到的数据都是 具备持久性的落地数据而已, 但还有个问题并未得到解决, 那就是一个事务单元中 不可重复读 的问题 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了 解决方案 : RR+ 在MySQL中, 事务已经用自身隔离性解决了 不可重复读 问题 : REPEATABLE READ 或 以上隔离级别(RR+) REPEATABLE READ 级别保证了:在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的, 否则对该语句不可见 ( READ COMMITED )在事务中, 如果需要多次读取同一个数据, 在两次读取操作之间, 无论数据被 提交 多少次(即无论落地过多少遍), 每次读取的结果都应该是和事务中第一次读取的结果一样 ( REPEATABLE READ ) 幻读 可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 不可重复读 和 幻读 这两个概念容易搞混 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录); 解决方案: RR + MVCC 其实对于 幻读 问题, 在Mysql的InnoDB存储引擎中, 是通过事务的 RR + MVCC机制 进行解决的; 参考《高性能MySQL》对 RR 隔离级别的描述: 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 之所以 不可重复读 和 幻读 容易搞混, 可能是因为: 在mysql中, 由于默认就是RR隔离级别下, 该隔离级别已经解决了幻读, 所以无法模拟出幻读的场景; 而 退回到 RC隔离级别 的话, 虽然 幻读 和 不可重复读 都会出现, 但由于现象都是两次读取结果不一样, 容易分辨不出! 幻读的延伸… 参考资料: 淘宝数据库内核6月报 美团技术博客 MySQL官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"21. MySQL 事务简介","slug":"MySQL 暂停/2018-05-03-mysql-21","date":"2018-05-03T11:31:07.000Z","updated":"2019-04-26T11:03:10.000Z","comments":true,"path":"2018/05/03/MySQL 暂停/2018-05-03-mysql-21/","link":"","permalink":"http://blog.renyimin.com/2018/05/03/MySQL 暂停/2018-05-03-mysql-21/","excerpt":"","text":"概述 事务 可以理解为一个 独立的工作单元, 在这个独立的工作单元中, 可以有一组操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败; 这个独立工作单元中的操作, 只要有任何一个执行失败, 则整体就应该是失败的, 那就必须回滚所有已经执行了的步骤; ACID一个运行良好的事务处理系统必须具备下面这些标准特性, 高并发离不开事务的这几个标准特性: Atomicity 原子性 一个事务必须被视为一个不可分割的最小工作单元 对于一个事务来说, 不能只成功执行其中的一部分操作, 整个事务中的所有操作要么全部成功提交, 要么有操作失败导致所有操作全部回滚, 这就是事务的原子性 Consistency 一致性 你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性保证的是 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态, 而不是分布式中提到的数据一致性; 比如一个最简单的A转账给B的示例, 初始状态为 A(余额500), B(余额15) :转账前的数据一致性状态是: A(余额500), B(余额15)转账成功后的数据一致性状态是: A(余额400), B(余额115)转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: A(余额500), B(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的在上面的A转账给B的示例中, 假设A的账户已完成扣款, 但是B的账户还未增加, 那么在事务最终提交之前, 如果有另一个程序去读取 A 账户 的余额, 这个程序读到的应该是最初500才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务 隔离性 的四个隔离级别, 到时候就知道这里为什么说 通常来说 ; (确实有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) Durability 持久性: 是指 一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 落地到磁盘中 事务的四种隔离级别(从低到高)READ UNCOMMITTED(未提交读)READ COMMITTED(提交读)注意: 和RR一样都采用了MVCC机制, 但与RR级别主要区别是快照时机不同 REPEATABLE READ(可重复读)SERIALIZABLE(可串行化)注意: 只有该隔离级别才会读写都加锁","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"18. 事务, redolog, undolog","slug":"MySQL 暂停/2018-04-26-mysql-18","date":"2018-04-26T14:48:49.000Z","updated":"2019-04-26T11:03:19.000Z","comments":true,"path":"2018/04/26/MySQL 暂停/2018-04-26-mysql-18/","link":"","permalink":"http://blog.renyimin.com/2018/04/26/MySQL 暂停/2018-04-26-mysql-18/","excerpt":"","text":"https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html redo 概述 redo log 也叫重做日志, 用来实现事务的持久性, 即 ACID 中的 D, 其由两部分组成: 一是 内存中的 重做日志缓冲(redo log buffer), 其是易失的 二是 重做日志文件(redo log file), 其是持久的 InnoDB存储引擎通过 Force Log at Commit 机制实现事务的持久性, 即当事务提交(COMMIT)时, 必须先将该事务的所有日志写入到磁盘上的 重做日志文件 进行持久化 注意: 这里的重做日志, 不是特指 redo log, 这里说的重做日志是包含了 redo log, undo log 两部分 为了确保事务在提交时, 其所有日志都能写入到 重做日志文件 中, 在每次将 log buffer 中的日志写入日志文件的过程中都会调用一次操作系统的 fsync 操作(即 fsync系统调用) 在 InnoDB存储引擎的配置中参数 innodb_flush_method 通常设置为 O_DIRECT, 这也是官方文档所推荐的设置值。DBA或开发人员 知道该参数是文件打开的一个标识，启用后文件的写入将绕过操作系统缓存, 直接写文件。其在InnoDB存储引擎中的表现为对于写入到数据表空间将绕过操作 系统缓存。这样设置通常不会有更好的性能，但是数据库已经有自己的缓存系统，这样的设置可以确定数据库系统对于内存的使用。 123456789mysql&gt; show global variables like &quot;%innodb_flush_method%&quot;;+---------------------+-------+| Variable_name | Value |+---------------------+-------+| innodb_flush_method | |+---------------------+-------+1 row in set (0.00 sec)mysql&gt; 也就是说, 从 redo log buffer 将日志刷到磁盘的 redo log file 的过程大致如下: MySQL 支持用户自定义在commit事务时, 如何将 log buffer 中的日志刷到 磁盘的 log file 中, 参数 innodb_flush_log_at_trx_commit 就是用来控制重做日志刷新到磁盘的策略 innodb_flush_log_at_trx_commit 有3种值: 0, 1, 2 (默认为1) 1234567mysql&gt; show variables like &quot;%innodb_flush_log_at_trx_commit%&quot;;+--------------------------------+-------+| Variable_name | Value |+--------------------------------+-------+| innodb_flush_log_at_trx_commit | 1 |+--------------------------------+-------+1 row in set (0.01 sec) 当设置为1时, 事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file on disk中, 这种方式即使系统崩溃也不会丢失任何数据, 但是因为每次提交都写入磁盘, IO的性能较差 当设置为0时, 事务提交时不会将log buffer中日志写入到os buffer, 而是每秒写入os buffer并调用fsync()写入到log file on disk中, 也就是说设置为0时, 是(大约)每秒刷新写入到磁盘中的, 当系统崩溃, 会丢失1秒钟的数据 当设置为2时, 每次提交都仅写入到os buffer, 然后是每秒调用fsync()将os buffer中的日志写入到log file on disk, 在这个设置下, 当 MySQL数据库发生宕机而操作系统不发生宕机时, 并不会导致事务的丢失而当操作系统宕机时, 重启数据库后会丢失未从文件系统缓存刷新到重做日志文件那部分事务 innodb_flush_log_at_trx_commit 策略大致如下: 注意, 有一个变量 innodb_flush_log_at_timeout 的值为1秒, 该变量表示的是刷日志的频率 注意: 在主从复制结构中, 要保证事务的持久性和一致性, 需要对日志相关变量设置为如下: 如果启用了二进制日志, 则设置 sync_binlog=1, 即每提交一次事务同步写到磁盘中 总是设置 innodb_flush_log_at_trx_commit=1, 即每提交一次事务都写到磁盘中 上述两项变量的设置保证了 每次提交事务都写入 二进制日志 和 事务日志, 并在提交时将它们刷新到磁盘中 log block Innodb存储引擎中, redo log 是以 块 为单位进行存储的, 每个块占512字节, 这称为 redo log block, 所以不管是log buffer中还是os buffer中以及redo log file on disk中, 都是这样以512字节的块存储的; 每个redo log block由3部分组成：日志块头、日志块尾和日志主体。其中日志块头占用12字节，日志块尾占用8字节，所以每个redo log block的日志主体部分只有512-12-8=492字节 redo 与 binlog undolog redolog 记录了事务的行为, 可以很好地通过其对页进行 “重做” 操作, 但是事务有时还需要进行回滚操作, 这时就需要 undo; 因此在对数据库进行修改时, InnoDB存储引擎不但会产生redo, 还会产生一定量的 undo 这样, 如果用户执行的事务由于某种原因失败了, 或者用户执行了 ROLLBACK 语句请求回滚, 就可以利用这些 undo 信息将数据回滚到修改之前的样子; redo存放在重做日志文件中, 与redo不同, undo存放在数据库内部的一个特殊段(segment)中, 这个段称为 undo 段 (undo segment) undo段位于共享表空间内 用户通常对undo有这样的误解:undo用于将数据库物理地恢复到执行语句或事务之前的样子, 但事实并非如此undo是逻辑日志, 因此只是将数据库逻辑地恢复到原来的样子, 所有修改都被逻辑地取消了, 但是数据结构和页本身在回滚之后可能大不相同; 这是因为在多用户并发系统中, 可能会有数十、数百甚至数千个并发事务。数据库的主要任务就是协调对数据记录的并发访问。比如, 一个事务在修改当前一个页中某几条记录, 同时还有别的事务在对同一个页中另几条记录进行修改, 因此,不能将一个页回滚到事务开始的样子, 因为这样会影响其他事务正在进行的工作;例如, 用户执行了一个INSERT10W条记录的事务, 这个事务会导致分配一个新的段, 即表空间会增大, 在用户执行 ROLLBACK时, 会将插入的事务进行回滚, 但是表空间的大小并不会因此而收缩, 因此, 当 InnoDB存储引擎回滚时, 它实际上做的是与先前相反的工作, 对于每个INSERT, InnoDB存储引擎会完成一个DELETE; 对于每个DELETE, InnoDB存储引擎会执行一个 INSERT; 对于每个UPDATE, InnoDB存储引擎会执行一个相反的UPDATE, 将修改前的行放回去;除了回滚操作, undo的另一个作用是MVCC, 即在 InnoDB存储引擎中MVCC的实现是通过undo来完成, 当用户读取一行记录时, 若该记录已经被其他事务占用, 当前事务可以通过undo读取之前的行版本信息, 以此实现非锁定读取最后也是最为重要的一点是, undo log会产生redo log, 也就是undo log的产生会伴随着 redo log的产生, 这是因为undo log也需要持久性的保护","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"11. char(M) 和 varchar(M)","slug":"MySQL 暂停/2018-04-21-mysql-10","date":"2018-04-21T08:31:17.000Z","updated":"2019-05-06T11:54:06.000Z","comments":true,"path":"2018/04/21/MySQL 暂停/2018-04-21-mysql-10/","link":"","permalink":"http://blog.renyimin.com/2018/04/21/MySQL 暂停/2018-04-21-mysql-10/","excerpt":"","text":"MySQL技术内幕 InnoDB存储引擎行溢出数据 char(M) char(M) 中的 M 用于指定该字段可存储的 字符(注意不是字节) 的最大个数 M 最大可以设置到 255 , 即 char(M) 最大可以存储 255个字符 注意: 在任何字符集下, M的最大值都是 255, why? 因为不管在什么字符集下, 对于char(M), MySQL都会首先给该字段的分配 M*3(utf8) 个字节, 那这个字段的所有数据占用的空间都是定长的字节数了 所以char(M)的存储空间就是这么定长的, 并且你永远不可能超过MySQL分配给 char(M) 的字节数, 因为在这之前你已经被M所限制的字符个数限制住了 varchar(M) varchar(M) 中的 M 也是用于指定该字段可存储的 字符(注意不是字节) 的最大个数 不过 varchar(M) 中的 M 和 字符编码有关, 由于 varchar最大长度是65535个字节, 所以: 在utf-8编码下, M值不应该超过 21845 (因为M超过255后, 会有两个字节用于记录M的大小, 所以虽然M的最大值为 21845, 但实际能存储的最大字符数为: 65535-2 / 3 = 21844 ) 在GBK编码下, M值也不应该超过 32767 (最大有效字符能存储, …. 65535-2 / 2 = 32766) 在utfbbm4编码下, M值也不应该超过 16383 (最大有效字符能存储, … 65535-2 / 4 = 16383) 另外, 由于varchar(M)是不定长的, 因此, 每次在读取数据的时候需要知道这个数据有多少个字符, 这样才能算出总字节, 从而完整地把varchar(M)字段的数据读出来, 所以varchar事实上还会提供一个值来记录每个数据的字符长度, 这个值会占用 1-2 个字节的空间 为什么该值是 1-2 个字节, 不是固定的字节? 因为1个字节可以表示的最大数值255, 而如果varchar(M)的M指定的字符数超过了255, 一个字节就不够了, 那MySQL就需要两个字节来记录字符的长度 varchar 坑来了 表中只要有字段没有设置为 not null 属性, 每行记录就会浪费一个字节来记录null字段(具体也不知道为什么要记录), 只有所有的字段都为not null的时候, 才不会浪费一个字节来记录null 表中一旦有了varchar(M)字段之后, 表中的每行记录都自动有了每行记录的总长度, 也是65535个字节, 所以, varchar(M)的M一般不可能达到最大值 验证 如下, 你觉着如下的varchar(M), M最大可设置为多少? 推算: 1234表中一旦有了varchar(M)字段之后, 表中的每行记录都自动有了每行记录的总长度, 也是65535个字节65535(行记录的总字节数) - 4*3(3个int字段) - 10*3+1(varchar(10)字段, 1个字节记录长度) = 6549265492/3 = 21830.67所以最大可以设置为21830 设置为 varchar(21831) 报错, 设置 varchar(21830) 就ok了 由于表中只要有字段没有设置为 not null 属性, 每行记录就会浪费一个字节来记录null字段, 此时如果设置一个字段为null, 会发现又不够了 text (可忽略) 每个BLOB和TEXT列, 账户只占其中的5至9个字节, BLOB和TEXT类型需要 1、2、3或者4个字节来记录列值的长度，取决于该类型的最大可能的长度 varchar(21830) —&gt; varchar(21826) 少了4个utf8字符, 即12字节： 再增一个text字段, varchar(21826) —&gt; varchar(21823) 少了3个utf8字符, 即9字节: 小结在建表选择字段时, 当使用varchar(M)的M值较大时, 需要注意以上内容","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"closure 闭包","slug":"PHP/2018-04-18-closure","date":"2018-04-18T08:40:32.000Z","updated":"2019-07-29T10:52:09.000Z","comments":true,"path":"2018/04/18/PHP/2018-04-18-closure/","link":"","permalink":"http://blog.renyimin.com/2018/04/18/PHP/2018-04-18-closure/","excerpt":"","text":"概述 闭包 和 匿名函数 在PHP 5.3.0中引入, 这两个特性非常有用, 每个PHP开发者都应该掌握 (其实感觉闭包就是在匿名函数的基础上, 使用 use, bindTo(), bind 等方式对上下文环境做了关联, 将上下文状态闭合到了匿名函数中, 并且即便离开了上下文环境, 闭包中闭合的状态依然存在) 闭包很有用, 因为它允许将 函数 与 其所操作的某些数据(环境)关联起来; 这显然类似于面向对象编程, 在面向对象编程中, 对象允许我们将某些数据(对象的属性)与一个或者多个方法相关联; 通常你使用 只有一个方法的对象 的地方, 都可以使用闭包; PHP中的闭包是一个 Closure 对象(代表匿名函数类) PHP的闭包不会像JS中那样自动封装应用的状态, 在PHP中, 你必须手动调用闭包对象的 bindTo() 方法, 或者使用 use 关键字, 来手动把应用的状态附加到PHP闭包上; 闭包是指 在创建时会封装周围状态的函数, 即使闭包所在的环境的不存在了, 闭包中封装的状态依然存在, 这句话如何理解? use关键字use 的用法比较常见, 经常被用于从 父作用域 继承变量 bindTo() bindTo() 方法为闭包增加了一些有趣的东西 可以使用这个方法把 Closure 对象的内部状态绑定到其他对象上 bindTo() 方法的第二个参数可以指定绑定闭包的那个对象所属的 PHP 类, 这样我们就可以访问这个类的受保护和私有的成员变量 tip: bindTo() 的第二个参数尤为重要, 其作用是 指定绑定闭包的那个对象所属的PHP类, 这样, 闭包就可以在其他地方(比如类外部), 在闭包内部通过 $this 访问绑定闭包的对象中受保护和私有的成员变量 主要用于将 匿名(回调)函数本身 绑定到类中; (这样, 就可以在类外部, 在这个匿名函数中, 使用类中的 public, protected, private 成员属性 及 静态属性) Laravel中路由绑定的实现其实使用了该技术 注意 bindTo(), 的第一个参数 $this, 并不是指当前类的对象, 而是第二个参数对应的类的对象 bind() 和 bindTo() 作用相似, 只不过 bind() 是个静态方法 (直接使用 Closure 类来调用即可) 还是上面的例子, 只用做很小的改动即可 123456789101112131415161718192021222324252627282930313233343536373839&lt;?phpclass App &#123; protected $routes = []; protected $responseStatus = &apos;200 OK&apos;; protected $responseContentType = &apos;text/html&apos;; public $responseBody1 = &apos;name&apos;; protected $responseBody2 = &apos;age&apos;; private $responseBody3 = &apos;gender&apos;; private static $responseBody4 = &apos;address&apos;; public function addRoute($routePath, $routeCallback) &#123; // 注意此处改动 (仍然是将闭包绑定到对应的类（此处仍是当前类）, 并指明当前类的 $this 绑定到 类名 __CLASS__) $this-&gt;routes[$routePath] = Closure::bind($routeCallback, $this, __CLASS__); &#125; public function dispatch($currentPath) &#123; foreach ($this-&gt;routes as $routePath =&gt; $callback) &#123; if( $routePath === $currentPath) &#123; $callback(); &#125; &#125; header(&apos;HTTP/1.1 &apos; . $this-&gt;responseStatus); header(&apos;Content-Type: &apos; . $this-&gt;responseContentType); header(&apos;Content-Length: &apos; . strlen($this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4)); echo $this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4; &#125;&#125;$app = new App();$app-&gt;addRoute(&apos;user/nonfu&apos;, function()&#123; $this-&gt;responseContentType = &apos;application/json;charset=utf8&apos;; $this-&gt;responseBody1 = &apos;&#123;&quot;name&quot;:&quot;LaravelAcademy&quot;&#125;&apos;; $this-&gt;responseBody2 = &apos;&#123;&quot;age&quot;:&quot;100&quot;&#125;&apos;; $this-&gt;responseBody3 = &apos;&#123;&quot;gender&quot;:&quot;male&quot;&#125;&apos;; self::$responseBody4 = &apos;&#123;&quot;address&quot;:&quot;beijing&quot;&#125;&apos;;&#125;);$app-&gt;dispatch(&apos;user/nonfu&apos;); 再看一个例子 () tip: __invoke 还有一个 __invoke() 方法, 这是为了与其他实现了 __invoke() 魔术方法的对象保持一致性, 但调用匿名函数的过程与它无关, 如下: 123456&lt;?php$greet = function ($name) &#123; return sprintf(&quot;Hello %s\\r\\n&quot;, $name);&#125;;echo $greet(&apos;LaravelAcademy.org&apos;); 之所以能调用 $greet(), 是因为这个变量的值其实是一个闭包, 而 闭包对象 实现了 __invoke() 魔术方法, 只要变量名后有(), PHP就会查找并调用__invoke方法; ~~未完待续 参考https://learnku.com/articles/5388/closures-and-anonymous-functions-of-php-new-featureshttps://www.cnblogs.com/one-villager/p/8423097.htmlhttps://laravelacademy.org/post/4341.html","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"","slug":"PHP/2018-04-14-03","date":"2018-04-14T08:21:37.000Z","updated":"2019-06-27T09:10:56.000Z","comments":true,"path":"2018/04/14/PHP/2018-04-14-03/","link":"","permalink":"http://blog.renyimin.com/2018/04/14/PHP/2018-04-14-03/","excerpt":"","text":"","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"05. MySQL5.7 源码编译安装","slug":"MySQL 暂停/2018-04-14-mysql-05","date":"2018-04-14T06:20:27.000Z","updated":"2019-04-26T11:01:27.000Z","comments":true,"path":"2018/04/14/MySQL 暂停/2018-04-14-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2018/04/14/MySQL 暂停/2018-04-14-mysql-05/","excerpt":"","text":"源码下载 mysql5.7.25 源码下载 因为 mysql5.7系列要求安装boost_1_59_0, 这里我们选择包含boost的版本 (相对于前者, 它内部已经准备好了boost, 不用你再去下载对应的boost版本) 依赖安装 参考 cmake: mysql使用cmake跨平台工具预编译源码, 用于设置mysql的编译参数 (如: 安装目录、数据存放目录、字符编码、排序规则等, 安装最新版本即可) make3.75 : mysql源代码是由C和C++语言编写, 在linux下使用make对源码进行编译和构建, 要求必须安装 make 3.75或以上版本 (yum默认都是安装最新版的软件) gcc4.4.6 : GCC是Linux下的C语言编译工具, mysql源码编译完全由C和C++编写, 要求必须安装GCC4.4.6或以上版本 Boost1.59.0 : mysql源码中用到了C++的Boost库, 要求必须安装boost1.59.0或以上版本 bison2.1 : Linux下C/C++语法分析器 ncurses : 字符终端处理库 所以在安装前, 需先安装相关的依赖库 1yum -y install gcc gcc-c++ ncurses ncurses-devel bison libgcrypt perl make cmake 编译、安装 相比之前的Mysql编译选项, CMake编译选项现在都是大写的, 参考 在编译时通常会设置的编译选项如下: (可参考 lnmp1.6 中的MySQL5.7编译脚本) 123456789101112131415161718192021222324cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql // 指定MySQL程序的安装目录, 默认 /usr/local/mysql-DSYSCONFDIR=/usr/local/mysql/etc // 初始化参数文件my.cnf的目录, 貌似没有默认值, 需要指定-DWITH_MYISAM_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_INNOBASE_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_PARTITION_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_FEDERATED_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DEXTRA_CHARSETS=all // 默认就是all-DDEFAULT_CHARSET=utf8mb4 // 指定服务器默认字符集, 默认 latin1-DDEFAULT_COLLATION=utf8mb4_general_ci // 指定服务器默认的校对规则, 默认 latin1_general_ci-DWITH_EMBEDDED_SERVER=1 -DENABLED_LOCAL_INFILE=1 // 指定是否允许本地执行 LOAD DATA INFILE, 默认OFF-DWITH_BOOST=/usr/local/src/mysql-5.7.25/boost // 不用指定 -DENABLE_DOWNLOADS=1# 以下选项, lnmp1.6 并没有指定-DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_BLACKHOLE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DMYSQL_TCP_PORT // 指定服务器默认的端口号, 默认 3306-DMYSQL_DATADIR=/usr/local/mysql/data // 数据文件目录 (这个没有默认值)-MYSQL_UNIX_ADDR // socket文件路径, 默认 /tmp/mysql.sock-DENABLE_DOWNLOADS=1 // 是否要下载可选的文件。例如, 启用此选项（设置为1）, cmake将下载谷歌所使用的测试套件运行单元测试。-DMYSQL_USER=mysql // 官网没找到该选项-DDOWNLOAD_BOOST // 不用指定, 因为本次选择的是 mysql-boost 版本的, 所以不用下载boost, 而且下载过程很容易被墙而下载失败 编译安装 123cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DSYSCONFDIR=/usr/local/mysql/etc -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DWITH_FEDERATED_STORAGE_ENGINE=1 -DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8mb4 -DDEFAULT_COLLATION=utf8mb4_general_ci -DWITH_EMBEDDED_SERVER=1 -DENABLED_LOCAL_INFILE=1 -DWITH_BOOST=/usr/local/src/mysql-5.7.25/boost -DMYSQL_DATADIR=/usr/local/mysql/datamake // 花费时间可能会比较长make install 已经生成 /usr/local/mysql my.cnf 首先, 在编译时, 我们通过 -DSYSCONFDIR 指定的 my.cnf 的路径是在 /usr/local/mysql/etc 下, 但是编译完成后, /usr/local/mysql/ 下并没有 etc 目录 这个选项只是指定将来 my.cnf 的位置, 不会帮我们创建目录和文件 (包括之前在编译时通过 -DMYSQL_DATADIR=/usr/local/mysql/data 指定的mysql的数据目录, 都是需要自己创建) 另外, mysql5.7.18之后, 貌似已经不在解压包的support-files目录中提供my-default.cnf文件, 参考 参考, 在Unix和类Unix系统上, MySQL程序按照指定的顺序从下表中显示的文件中读取启动选项（首先列出的文件首先读取, 后面读取的文件优先） (你会发现在centos7-minimal系统的/etc下默认就有my.cnf文件) 自己准备 my.cnf 文件 Mysql参数优化对于新手来讲, 是比较难懂的东西, 其实这个参数优化, 是个很复杂的东西, 对于不同的网站, 及其在线量, 访问量, 帖子数量, 网络情况, 以及机器硬件配置都有关系, 优化不可能一次性完成, 需要不断的观察以及调试, 才有可能得到最佳效果 下面是个my.cnf示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185[client] # 客户端设置，即客户端默认的连接参数default-character-set = utf8mb4 port = 3306 # 默认连接端口socket = /usr/local/mysql/mysql.sock # 用于本地连接的socket套接字 (注意client的socket要和mysqld是一样的，因为客户端和服务端通信靠的就是这个文件，肯定要一致)[mysql] # 服务端基本设置#auto-rehash # MySQL开启命令自动补全功能, mysql命令行工具自带这个功能, 但是默认是禁用的, 想启用其实很简单, 打开配置文件找到 no-auto-rehash, 用符号 # 将其注释, 另外增加auto-rehash即可 # 亲测, 感觉不咋实用[mysqld]user=mysql # ?? mysqld程序在启动后将在给定UNIX/Linux账户下执行 mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序socket = /usr/local/mysql/mysql.sock # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件 (编译时未指定socket位置, 就用默认的位置)port=3306 # MySQL监听端口basedir=/usr/local/mysql # 指定了安装 MySQL 的安装路径, 填写全路径可以解决相对路径所造成的问题datadir=/usr/local/mysql/data # 数据文件存放的目录tmpdir = /usr/local/mysql/tmp # ?? MySQL存放临时文件的目录symbolic-links=0 # ?? 是否支持符号链接, 即数据库或表可以存储在my.cnf中指定datadir之外的分区或目录, 为0不开启log-error=/usr/local/mysql/logs/mysql.log # 错误日志位置pid-file=/usr/local/mysql/mysqld.pidskip-name-resolve # ?? 禁用主机名解析 (待答疑)default-storage-engine = InnoDB # 默认的数据库引擎innodb-file-per-table=1 # ??innodb_force_recovery = 1 # ??group_concat_max_len = 10240 # ??#sql_mode = &apos;PIPES_AS_CONCAT,ANSI_QUOTES,IGNORE_SPACE,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_FIELD_OPTIONS,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;sql_mode = NO_ENGINE_SUBSTITUTION,NO_AUTO_CREATE_USER # sql_mode, 定义了mysql应该支持的sql语法，数据校验等 # NO_AUTO_CREATE_USER: 禁止GRANT创建密码为空的用户 # NO_ENGINE_SUBSTITUTION: 如果需要的存储引擎被禁用或未编译, 可以防止自动替换存储引擎expire_logs_days = 7 # ??memlock # ??### 字符集配置character-set-client-handshake = FALSEcharacter-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect=&apos;SET NAMES utf8mb4&apos; # ??### GTIDserver_id = 330759 # ?? 服务端ID，用来高可用时做区分binlog_format = row # ?? 为保证 GTID 复制的稳定, 行级日志binlog_rows_query_log_events=1 # 在row模式下, 开启该参数, 将把sql语句打印到binlog日志里面, 默认是0(off), 虽然将语句放入了binlog,但不会执行这个sql,就相当于注释一样.但对于dba来说,在查看binlog的时候,很有用处.binlog_row_image=&apos;minimal&apos; # 默认为full, 在binlog为row格式下, full将记录update前后所有字段的值, minimal时, 只记录更改字段的值和where字段的值, noblob时, 记录除了blob和text的所有字段的值, 如果update的blob或text字段,也只记录该字段更改后的值,更改前的不记录; # 大家都知道row格式下的binlog增长速度太快, 对存储空间, 主从传输都是一个不小的压力.因为每个event记录了所有更改数据前后的值,不管数据是否有改动都会记录.binlog_row_image的引入就大大减少了binlog记录的数据.在结合binlog_rows_query_log_events,对于dba日常维护binlog是完全没有压力的,而且节省了硬盘空间开销,减小I/O,减少了主从传输压力;gtid_mode = on # ?? 开启 gtid 功能# 保障 GTID 事务安全# 当启用 enforce_gtid_consistency 功能时# MySQL只允许能够保障事务安全, 并且能够被日志记录的SQL语句被执行# 像create table ... select 和 create temporarytable语句# 以及同时更新事务表和非事务表的SQL语句或事务都不允许执行enforce-gtid-consistency = true # 以下两条配置为主从切换, 数据库高可用的必须配置log_bin = mysql57-bin # 开启 binlog 日志功能log-slave-updates = on # ?? 开启从库更新 binlog 日志skip_slave_start=1 # ?? slave复制进程不随mysql启动而启动### 慢查询日志slow_query_log = 1 # 打开慢查询日志功能long_query_time = 2 # 超过2秒的查询记录下来log_queries_not_using_indexes = 0 # 记录下没有使用索引的查询slow_query_log_file =/usr/local/mysql/logs/slow.log # 慢查询日志文件#log=/data/logs/mysql57/all.log### 自动修复relay_log_info_repository = TABLE # 记录 relay.info 到数据表中master_info_repository = TABLE # 记录 master.info 到数据表中 relay_log_recovery = on # 启用 relaylog 的自动修复功能relay_log_purge = 1 # 在 SQL 线程执行完一个 relaylog 后自动删除### 数据安全性配置log_bin_trust_function_creators = on # wei关闭 master 创建 function 的功能sync_binlog = 1 # 每执行一个事务都强制写入磁盘explicit_defaults_for_timestamp=true # 设置 timestamp 的列值为 null, 不会被设置为 current timestamp # timestamp 列如果没有显式定义为 not null, 则支持null属性### 优化配置ft_min_word_len = 1 # 优化中文全文模糊索引lower_case_table_names = 1 # 默认库名表名保存为小写, 不区分大小写max_allowed_packet = 256M # 单条记录写入最大的大小限制, 过小可能会导致写入(导入)数据失败#rpl_semi_sync_master_enabled = 1 # ?? 半同步复制开启#rpl_semi_sync_slave_enabled = 1 #rpl_semi_sync_master_timeout = 1000 # ?? 半同步复制超时时间设置#rpl_semi_sync_master_wait_point = AFTER_SYNC # ?? 复制模式(保持系统默认)#rpl_semi_sync_master_wait_slave_count = 1 # 多线程复制 # ?? 后端只要有一台收到日志并写入 relaylog 就算成功slave_parallel_type = logical_clock # ?? 在MySQL5.7版本后可以利用逻辑时钟方式分配SQL多线程slave_parallel_workers = 4 # ?? 并行的SQL线程数量, 此参数只有设置 1&lt;N 的情况下才会才起N个线程进行SQL重做 # ?? 经过测试对比发现, 如果主库的连接线程为M, 只有M &lt; N的情况下, 备库的延迟才可以完全避免### 连接数限制max_connections = 1500 max_connect_errors = 200 # 验证密码超过20次拒绝连接# back_log值指出在mysql暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中# 也就是说, 如果MySql的连接数达到max_connections时, 新来的请求将会被存在堆栈中# 以等待某一连接释放资源, 该堆栈的数量即back_log, 如果等待连接的数量超过back_log# 将不被授予连接资源back_log = 500open_files_limit = 65535 # 服务器关闭交互式连接前等待活动的秒数interactive_timeout = 3600 # 服务器关闭非交互连接之前等待活动的秒数wait_timeout = 3600### 内存分配# 指定表高速缓存的大小。每当MySQL访问一个表时, 如果在表缓冲区中还有空间# 该表就被打开并放入其中, 这样可以更快地访问表内容table_open_cache = 1024 binlog_cache_size = 4M # 为每个session 分配的内存, 在事务过程中用来存储二进制日志的缓存tmp_table_size = 128M # 在内存的临时表最大大小# 创建内存表的最大大小(保持系统默认, 不允许创建过大的内存表)# 如果有需求当做缓存来用, 可以适当调大此值max_heap_table_size = 16M# 顺序读, 读入缓冲区大小设置# 全表扫描次数多的话, 可以调大此值read_buffer_size = 1M# 随机读, 读入缓冲区大小设置read_rnd_buffer_size = 8M# 高并发的情况下, 需要减小此值到64K-128Ksort_buffer_size = 1M# 每个查询最大的缓存大小是1M, 最大缓存64M 数据query_cache_size = 64Mquery_cache_limit = 1M# 提到 join 的效率join_buffer_size = 16M# 线程连接重复利用thread_cache_size = 64### InnoDB 优化## 内存利用方面的设置# 数据缓冲区innodb_buffer_pool_size=2G## 日志方面设置# 事务日志大小innodb_log_file_size = 256M# 日志缓冲区大小innodb_log_buffer_size = 4M# 事务在内存中的缓冲innodb_log_buffer_size = 3M# 主库保持系统默认, 事务立即写入磁盘, 不会丢失任何一个事务innodb_flush_log_at_trx_commit = 1# mysql 的数据文件设置, 初始100, 以10M 自动扩展#innodb_data_file_path = ibdata1:100M:autoextend# 为提高性能, MySQL可以以循环方式将日志文件写到多个文件innodb_log_files_in_group = 3##其他设置# 如果库里的表特别多的情况, 请增加此值#innodb_open_files = 800# 为每个 InnoDB 表分配单独的表空间innodb_file_per_table = 1# InnoDB 使用后台线程处理数据页上写 I/O（输入）请求的数量innodb_write_io_threads = 8# InnoDB 使用后台线程处理数据页上读 I/O（输出）请求的数量innodb_read_io_threads = 8# 启用单独的线程来回收无用的数据innodb_purge_threads = 1# 脏数据刷入磁盘(先保持系统默认, swap 过多使用时, 调小此值, 调小后, 与磁盘交互增多, 性能降低)innodb_max_dirty_pages_pct = 90 # 事务等待获取资源等待的最长时间innodb_lock_wait_timeout = 120 # 开启 InnoDB 严格检查模式, 不警告, 直接报错 1开启 0关闭innodb_strict_mode=1 # 允许列索引最大达到3072innodb_large_prefix = on[mysqldump]# 开启快速导出quickdefault-character-set = utf8mb4max_allowed_packet = 256M## include all files from the config directory#!includedir /etc/my.cnf.d https://www.cnblogs.com/pengineer/p/4845218.html 问题 貌似开启 skip-name-resolve模式 后, 启动mysqld的话, 会报警告 (貌似是因为初始数据库的时候, 会默认有一个 root | localhost 用户生成, 有该模式不支持反向dns, 但是默认却使用了localhost，所以会有警告) 123456789mysql&gt; select user,host from user;+---------------+-----------+| user | host |+---------------+-----------+| mysql.session | localhost || mysql.sys | localhost || root | localhost |+---------------+-----------+3 rows in set (0.00 sec) 参考: http://blog.itpub.net/14184018/viewspace-1061224/ 1234567891011121314152019-03-29T09:21:02.228170Z 0 [Note] Server hostname (bind-address): &apos;*&apos;; port: 33062019-03-29T09:21:02.228201Z 0 [Note] IPv6 is available.2019-03-29T09:21:02.228211Z 0 [Note] - &apos;::&apos; resolves to &apos;::&apos;;2019-03-29T09:21:02.228223Z 0 [Note] Server socket created on IP: &apos;::&apos;.2019-03-29T09:21:02.231618Z 0 [Warning] &apos;user&apos; entry &apos;mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231654Z 0 [Warning] &apos;user&apos; entry &apos;mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231665Z 0 [Warning] &apos;user&apos; entry &apos;lant@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231671Z 0 [Warning] &apos;user&apos; entry &apos;lant1@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231695Z 0 [Warning] &apos;db&apos; entry &apos;performance_schema mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231699Z 0 [Warning] &apos;db&apos; entry &apos;sys mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231717Z 0 [Warning] &apos;proxies_priv&apos; entry &apos;@ root@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.232858Z 0 [Warning] &apos;tables_priv&apos; entry &apos;user mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.232897Z 0 [Warning] &apos;tables_priv&apos; entry &apos;sys_config mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.242378Z 0 [Note] Event Scheduler: Loaded 0 events2019-03-29T09:21:02.242750Z 0 [Note] /usr/local/mysql/bin/mysqld: ready for connections. 编码相关https://blog.csdn.net/javandroid/article/details/81235387 运行用户 (-M: 不要自动建立用户的登入目录; -s: 指定用户登入后所使用的shell; -g: 指定用户所属的起始群组 ) 12groupadd mysqluseradd -s /sbin/nologin -M -g mysql mysql 创建所需目录 12cd /usr/local/mysql/mkdir logs tmp data 设置目录相关权限 1chown -R mysql:mysql /usr/local/mysql/ root密码忘记 mysqld --skip-grant-tables &amp; mysql -uroot -p 直接回车即可设置环境变量 环境变量设置 123echo &apos;PATH=/usr/local/mysql/bin:/usr/local/mysql/lib:$PATH&apos; &gt;&gt; /etc/profile // 设置环境变量, 并开机运行echo &apos;export PATH&apos; &gt;&gt; /etc/profile //把PATH设为全局变量source /etc/profile 此时就可以全局使用/usr/local/mysql/bin下的命令 1234567891011121314151617[root@lant mysql]# mysqld --help2019-03-28T04:11:28.230733Z 0 [ERROR] mysqld: option &apos;--init_connect&apos; requires an argumentmysqld Ver 5.7.25 for Linux on x86_64 (Source distribution)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Starts the MySQL database server.Usage: mysqld [OPTIONS]For more help options (several pages), use mysqld --verbose --help.2019-03-28T04:11:28.231753Z 0 [ERROR] Aborting[root@lant mysql]# 初始化数据库 会自动在/usr/local/mysql下生成data目录, 并且是mysql用户身份 1mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 开机启动 将mysql添加到systemctl系统服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445cp /usr/local/src/mysql-5.7.23/scripts/systemd/mysqld.service.in /usr/lib/systemd/system/mysqld.servicecd /usr/lib/systemd/system/vi mysqld.service[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/usr/local/mysql/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables // 这里没找到这个脚本, 就先注释掉了#ExecStartPre=/usr/local/mysql/bin/mysqld_pre_systemd# Start main serviceExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysql# Sets open_files_limitLimitNOFILE = 5000Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false 然后设置开机自动启动: 123456789systemctl daemon-reloadsystemctl start mysqldnetstat -anpt | grep 3306systemctl enable mysqld[root@lant mysql]# ps aux |grep mysqlmysql 2833 0.1 9.8 1768144 179708 ? Sl 02:40 0:00 /usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pidroot 2867 0.0 0.0 112704 972 pts/0 S+ 02:43 0:00 grep --color=auto mysql[root@lant mysql]# 设置密码 默认没有密码, 所以直接设置新密码后即可登录 12mysqladmin -u root -p password &quot;renyimin&quot;;mysql -uroot -p 外部客户端连接? 最后更新时间 2018/08/16 安装存储引擎 : https://www.cnblogs.com/wt645631686/p/8086682.html 编译过程中的问题 CMAKE 编译参数指定后, 是否在启动 mysql 时, 还可以动态修改配置? CMAKE 编译参数未指定的话, 是否在启动mysql时, 还可以动态修改配置? 存储引擎是以插件形式存在的, 如果编译时没有将某个存储引擎编译进来, 后续应该就不能用了https://www.cnblogs.com/hllnj2008/p/4043778.html 如果忘记编译进某个存储引擎 https://help.aliyun.com/knowledge_detail/41107.html?spm=5176.11065259.1996646101.searchclickresult.35c13f8dXRN0Rx 3","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"PhpStorm","slug":"PHP/2018-04-13-phpstorm","date":"2018-04-13T11:46:23.000Z","updated":"2019-07-19T08:03:58.000Z","comments":true,"path":"2018/04/13/PHP/2018-04-13-phpstorm/","link":"","permalink":"http://blog.renyimin.com/2018/04/13/PHP/2018-04-13-phpstorm/","excerpt":"","text":"PhpStorm + PSR 插件 Background Image Plus 想想看, 别人看到你的IDE有个美女或者是二次元的背景 安装之后, 在打开View选项, 就可以看到 Set Background Image 选项了 .ignore 可以友好地查看 .ignore 文件","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"04. MySQL(5.7, 5.5) - YUM 安装","slug":"MySQL 暂停/2018-04-13-mysql-04","date":"2018-04-13T11:43:26.000Z","updated":"2019-04-26T11:01:31.000Z","comments":true,"path":"2018/04/13/MySQL 暂停/2018-04-13-mysql-04/","link":"","permalink":"http://blog.renyimin.com/2018/04/13/MySQL 暂停/2018-04-13-mysql-04/","excerpt":"","text":"YUM 库配置 要在Linux上使用YUM安装MySQL, 可以使用官网提供的 YUM 库 MySQL Yum库提供用于安装 MySQL server, client, MySQL Workbench, MySQL Utilities, MySQL Router, MySQL Shell, Connector/ODBC, Connector/Python 等的RPM软件包 可以通过安装MySQL提供的RPM包, 将MySQL Yum存储库添加到系统的存储库列表中 该RPM包的全称会遵循以下格式包名-MySQL版本号-Linux版本号-处理器(x64还是x86).rpm 官网提供了该RPM包的下载页但页面直接提供的 RPM Package 对应的平台版本可能不是你想要的, 可以尝试点击下载该版本, 通过下载链接找到mysql的yum库地址 Linux不同版本的标记 1234标记 对应Linux版本el6, el7 Red Hat Enterprise Linux/Oracle Linux/CentOS 6/7 |fc26, fc27, fc28 Fedora 26/27/28|sles12 SUSE Linux Enterprise Server 12| 从yum库中选择并下载适用于自己平台的发行包 (本次使用的是centos7) 找到 https://repo.mysql.com//yum/mysql-5.7-community/el/7/x86_64/ 页 (el) 最后找到 mysql57-community-release-el7-10.noarch.rpm 包wget https://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/mysql57-community-release-el7-10.noarch.rpm 安装 RPM 包: yum localinstall mysql57-community-release-el7-10.noarch.rpm (安装命令将MySQL Yum存储库添加到系统的存储库列表中, 并下载GnuPG密钥以检查软件包的完整性) 检查MySQL Yum仓库是否添加成功: yum repolist enabled | grep &quot;mysql.*-community.*&quot; 12345[root@lant ~]# yum repolist enabled | grep &quot;mysql.*-community.*&quot;mysql-connectors-community/x86_64 MySQL Connectors Community 95mysql-tools-community/x86_64 MySQL Tools Community 84mysql57-community/x86_64 MySQL 5.7 Community Server 327[root@lant ~]# 选择你要安装的版本 默认情况下启用最新GA系列(当前为MySQL 5.7)的子存储库, 默认情况下禁用所有其他系列(例如, MySQL 5.6系列)的子存储库; 使用如下命令查看MySQL Yum存储库中的所有子存储库, 并查看哪些子存储库已启用或禁用 123456789101112131415161718[root@lant ~]# yum repolist all | grep mysqlmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community disabledmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - disabledmysql-connectors-community/x86_64 MySQL Connectors Community enabled: 95mysql-connectors-community-source MySQL Connectors Community - disabledmysql-tools-community/x86_64 MySQL Tools Community enabled: 84mysql-tools-community-source MySQL Tools Community - Sourc disabledmysql-tools-preview/x86_64 MySQL Tools Preview disabledmysql-tools-preview-source MySQL Tools Preview - Source disabledmysql55-community/x86_64 MySQL 5.5 Community Server disabledmysql55-community-source MySQL 5.5 Community Server - disabledmysql56-community/x86_64 MySQL 5.6 Community Server disabledmysql56-community-source MySQL 5.6 Community Server - disabledmysql57-community/x86_64 MySQL 5.7 Community Server enabled: 327mysql57-community-source MySQL 5.7 Community Server - disabledmysql80-community/x86_64 MySQL 8.0 Community Server disabledmysql80-community-source MySQL 8.0 Community Server - disabled[root@lant ~]# 可以看到 mysql57 是开启状态, 正是我们要安装的版本 如果需要安装低版本, 如 5.5.62, 此时需要 下载 yum-config-manager: yum -y install yum-utils 关闭5.7的安装 yum-config-manager --disable mysql57-community 开启5.5的安装 yum-config-manager --enable mysql55-community yum repolist all | grep mysql12345678910111213141516mysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community disabledmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - disabledmysql-connectors-community/x86_64 MySQL Connectors Community enabled: 95mysql-connectors-community-source MySQL Connectors Community - disabledmysql-tools-community/x86_64 MySQL Tools Community enabled: 84mysql-tools-community-source MySQL Tools Community - Sourc disabledmysql-tools-preview/x86_64 MySQL Tools Preview disabledmysql-tools-preview-source MySQL Tools Preview - Source disabledmysql55-community/x86_64 MySQL 5.5 Community Server enabled: 427mysql55-community-source MySQL 5.5 Community Server - disabledmysql56-community/x86_64 MySQL 5.6 Community Server disabledmysql56-community-source MySQL 5.6 Community Server - disabledmysql57-community/x86_64 MySQL 5.7 Community Server disabledmysql57-community-source MySQL 5.7 Community Server - disabledmysql80-community/x86_64 MySQL 8.0 Community Server disabledmysql80-community-source MySQL 8.0 Community Server - disabled 安装 如下将安装MySQL服务器(mysql-community-server)的软件包以及运行服务器所需组件的软件包, 包括客户端软件包(mysql-community-client), 客户端和服务器的常见错误消息和字符集(mysql-community-common)和共享客户端库(mysql-community-libs) 1234567891011121314151617181920212223242526yum install mysql-community-server......(1/9): mysql-community-common-5.7.25-1.el7.x86_64.rpm | 274 kB 00:00:00(2/9): mysql-community-libs-5.7.25-1.el7.x86_64.rpm | 2.2 MB 00:00:03(3/9): mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm | 2.0 MB 00:00:04(4/9): net-tools-2.0-0.24.20131004git.el7.x86_64.rpm | 306 kB 00:00:02(5/9): openssl-1.0.2k-16.el7_6.1.x86_64.rpm | 493 kB 00:00:05(6/9): openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm | 1.2 MB 00:00:06(7/9): postfix-2.10.1-7.el7.x86_64.rpm | 2.4 MB 00:00:08(9/9): mysql-community-server-5.7.25-1.el7.x86_64.rpm 37% [=================================- ] 480 kB/s | 75 MB 00:04:23 ETA......Installed: mysql-community-libs.x86_64 0:5.7.25-1.el7 mysql-community-libs-compat.x86_64 0:5.7.25-1.el7 mysql-community-server.x86_64 0:5.7.25-1.el7Dependency Installed: mysql-community-client.x86_64 0:5.7.25-1.el7 mysql-community-common.x86_64 0:5.7.25-1.el7 net-tools.x86_64 0:2.0-0.24.20131004git.el7Dependency Updated: openssl.x86_64 1:1.0.2k-16.el7_6.1 openssl-libs.x86_64 1:1.0.2k-16.el7_6.1 postfix.x86_64 2:2.10.1-7.el7Replaced: mariadb-libs.x86_64 1:5.5.44-2.el7.centosComplete! 大概熟悉一下MySQL各文件的安装位置 (比如 my.cnf 位置) 123456789[root@lant ~]# rpm -qa | grep &apos;mysql&apos;mysql-community-common-5.7.25-1.el7.x86_64mysql-community-client-5.7.25-1.el7.x86_64mysql57-community-release-el7-10.noarchmysql-community-libs-5.7.25-1.el7.x86_64mysql-community-libs-compat-5.7.25-1.el7.x86_64mysql-community-server-5.7.25-1.el7.x86_64[root@lant ~]# rpm -ql mysql-community-server-5.7.25-1.el7.x86_64.... 启动 成功启动 12[root@lant ~]# sudo service mysqld startRedirecting to /bin/systemctl start mysqld.service 检查MySQL服务状态 12345678910111213141516[root@lant ~]# service mysqld statusRedirecting to /bin/systemctl status mysqld.service● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-03-30 03:04:18 GMT; 1min 13s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 5731 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS) Process: 5657 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 5734 (mysqld) CGroup: /system.slice/mysqld.service └─5734 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pidMar 30 03:04:15 lant systemd[1]: Starting MySQL Server...Mar 30 03:04:18 lant systemd[1]: Started MySQL Server.[root@lant ~]# 初始化 创建超级用户帐户 “root”@”localhost”, 超级用户的密码并将其存储在错误日志文件中, 要显示它, 请使用以下命令: 12[root@lant ~]# grep &apos;temporary password&apos; /var/log/mysqld.log2019-03-30T03:04:16.141197Z 1 [Note] A temporary password is generated for root@localhost: sho&gt;kLGZf63# 连接 : mysql -uroot -p (使用上述密码), 即可成功连接 重置密码, 否则不能继续操作 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Lant123456,&#39;; (密码不能过于简单) 熟悉各文件目录 数据库文件: show global variables like &quot;%datadir%&quot;; 日志文件: show variables like &#39;%log_error%&#39;; 慢日志文件: show variables like &#39;%slow_query_log%&#39;;… vagrant 打包(可忽略) 打包环境: vagrant package --output centos7.2_mysql57_yum.box 将包导入(添加)到box列表: vagrant box add mysql57_yum centos7.2_mysql57_yum.box 参考: 官网安装步骤 已经写的比较清楚了","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"23. Redis Replication","slug":"redis/2018-04-06-redis-23","date":"2018-04-06T08:05:39.000Z","updated":"2019-02-27T06:06:53.000Z","comments":true,"path":"2018/04/06/redis/2018-04-06-redis-23/","link":"","permalink":"http://blog.renyimin.com/2018/04/06/redis/2018-04-06-redis-23/","excerpt":"","text":"概述 Redis replication 主从复制, 是指将一台Redis服务器的数据, 复制到其他的Redis服务器, 前者称为主节点(master), 后者称为从节点(slave) 数据的复制是单向的, 只能由主节点到从节点 默认情况下, 每台Redis服务器都是master(主节点) 一个master可以有多个slave, 而一个slave只能有一个master 主从结构可以采用 一主多从 或者 级联结构 (即从服务器可以级联从服务器, M-&gt;S-&gt;S) 异步复制: M : replication时是非阻塞的(在replication期间, M依然能够处理客户端的请求) S : replication时也是非阻塞的 slave 在数据复制期间, 响应给客户端的数据是之前的旧数据 slave在replication期间, 也可以接受来自客户端的请求, 但是它用的是之前的旧数据 注意: 可以通过配置来决slave在replication时是否用旧数据响应客户端的请求, 如果配置为否, 那么slave将会返回一个错误消息给客户端 slave在新的数据接收完全后, 必须将新数据与旧数据替换, 即删除旧数据在替换数据的这个时间窗口内, slave出现阻塞, 会拒绝客户端的请求和连接那当slave拒绝请求时, 客户端会打到其他slave? 还是需要客户端自己重试进行请求? 主从复制的作用主要包括: 数据冗余: 主从复制实现了数据的热备份, 是持久化之外的一种数据冗余方式 (但不建议使用slave作为master的数据热备, 不能完全依赖slave的热备份, master仍然需要配置持久化) 故障恢复: 当主节点出现问题时, 可以由从节点提供服务, 实现快速的故障恢复, 实际上是一种服务的冗余 提高系统负载: 在主从复制的基础上, 配合读写分离, 可以由主节点提供写服务, 由从节点提供读服务, 分担服务器负载尤其是在写少读多的场景下, 通过多个从节点分担读负载, 可以大大提高Redis服务器的并发量, 提高系统QPS 高可用基石: 除了上述作用以外, 主从复制还是哨兵和集群能够实施的基础, 因此说主从复制是Redis高可用的基础 master持久化的意义 如果采用主从架构, 建议必须开启master的持久化! 不建议使用slave作为master的数据热备, 因为如果master的持久化被关闭, 一旦master意外宕机, 重启master后, 由于其数据是空的, 一经复制, salve的数据也就丢失了 master -&gt; RDB和AOF都关闭了 -&gt; master数据全部在内存中 master宕机, 重启, 是没有本地数据可以恢复的 master就会将空的数据集同步到slave上去, 所有slave的数据全部清空 最终造成100%的数据丢失 所以: master节点, 必须要使用持久化机制 redis replication 基本原理 当启动一个slave时, 它会发送一个PSYNC命令给master 如果slave是第一次连接master node, 那么会触发一次 full resynchronization 如果slave是重新连接master node, 那么master node仅仅会复制给slave部分缺少的数据; 开始full resynchronization时 master会fork一个子进程, 开始生成一份RDB快照文件 (所以此操作不会影响master继续接收客户端的请求), 因此, 同时master还会将从客户端收到的所有写命令缓存在内存中 RDB文件生成完毕之后, master会将这个RDB发送给slave, slave会先写入本地磁盘, 然后再从本地磁盘加载到内存中 然后master会将内存中缓存的写命令发送给slave, slave也会同步这些数据 slave 如果跟master 有网络故障, 断开了连接, 会自动重连; master如果发现有多个slave都来重新连接, 仅仅会启动一个rdb save操作, 用一份数据服务所有slave 主从复制的断点续传 从redis 2.8开始, 就支持主从复制的断点续传, 如果主从复制过程中, 网络连接断掉了, 那么可以接着上次复制的地方, 继续复制下去, 而不是从头开始复制一份 master会在内存中常见一个backlog, master和slave都会保存一个replica offset还有一个master id, offset就是保存在backlog中的如果master和slave网络连接断掉了, slave会让master从上次的replica offset开始继续复制, 但是如果没有找到对应的offset, 那么就会执行一次resynchronization 无磁盘化复制: 如果master配置了该项, 则master在内存中直接创建rdb, 然后发送给slave, 不会在自己本地落地磁盘了 12repl-diskless-syncrepl-diskless-sync-delay // 等待一定时长再开始复制, 因为要等更多slave重新连接过来 https://blog.csdn.net/mishifangxiangdefeng/article/details/50032357 过期key处理: slave不会过期key, 只会等待master过期key; 如果master过期了一个key, 或者通过LRU淘汰了一个key, 那么会模拟一条del命令发送给slave 延迟与不一致 主从复制完成之后, master上的写操作会异步地发送给slave, 而不会等待从节点的回复, 因此主从节点之间很难保持实时的一致性, 延迟在所难免 数据不一致的程度, 与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的 repl-disable-tcp-nodelay 配置等有关 repl-disable-tcp-nodelay no: 该配置作用于命令传播阶段当设置为yes时, TCP会对包进行合并从而减少带宽, 但是发送的频率会降低, 从节点数据延迟增加, 一致性变差 (具体发送频率与Linux内核的配置有关, 默认配置为40ms, 当设置为no时, TCP会立马将主节点的数据发送给从节点, 带宽增加但延迟变小) 一般来说, 只有当应用对Redis数据不一致的容忍度较高, 且主从节点之间网络状况不好时, 才会设置为yes, 多数情况使用默认值no 全量、部分复制 全量复制: 用于初次复制或其他无法进行部分复制的情况, 将主节点中的所有数据都发送给从节点, 是一个非常重型的操作 部分/增量复制: 用于网络中断等情况后的复制, 只将中断期间主节点执行的写命令发送给从节点, 与全量复制相比更加高效 (需要注意的是, 如果网络中断时间过长, 导致主节点没有能够完整地保存中断期间执行的写命令, 则无法进行部分复制, 仍使用全量复制) Redis通过psync命令进行全量复制的过程如下: 从节点判断无法进行部分复制, 向主节点发送全量复制的请求; 或从节点发送部分复制的请求, 但主节点判断无法进行全量复制 主节点收到全量复制的命令后, 执行 bgsave, 在后台生成RDB文件, 并使用一个缓冲区(称为复制缓冲区)记录从现在开始执行的所有写命令 主节点的bgsave执行完成后, 将RDB文件发送给从节点, 从节点首先清除自己的旧数据, 然后载入接收的RDB文件, 将数据库状态更新至主节点执行bgsave时的数据库状态 主节点将前述复制缓冲区中的所有写命令发送给从节点, 从节点执行这些写命令, 将数据库状态更新至主节点的最新状态如果从节点开启了AOF, 则会触发 bgrewriteaof 的执行, 从而保证AOF文件更新至主节点的最新状态 通过全量复制的过程可以看出, 全量复制是非常重型的操作: 主节点通过 bgsave 命令fork子进程进行RDB持久化, 该过程是非常消耗CPU、内存(页表复制)、硬盘IO的 主节点通过网络将RDB文件发送给从节点, 对主从节点的带宽都会带来很大的消耗 从节点清空老数据、载入新RDB文件的过程是阻塞的, 无法响应客户端的命令, 如果从节点执行bgrewriteaof, 也会带来额外的消耗 复制的细节: master和slave都会维护一个offsetmaster会在自身不断累加offset, slave也会在自身不断累加offset; slave每秒都会上报自己的offset给master, 同时master也会保存每个slave的offsetmaster和slave都要知道各自的数据的offset, 才能知道互相之间的数据不一致的情况 backlogmaster node有一个backlog, 默认是1MB大小master node给slave node复制数据时, 也会将数据在backlog中同步写一份backlog主要是用来做全量复制中断候的增量复制的 master run idinfo server, 可以看到master run id如果根据host+ip定位master node, 是不靠谱的, 如果master node重启或者数据出现了变化, 那么slave node应该根据不同的run id区分, run id不同就做全量复制如果需要不更改run id重启redis, 可以使用redis-cli debug reload命令 psync从节点使用psync从master node进行复制, psync runid offsetmaster node会根据自身的情况返回响应信息, 可能是FULLRESYNC runid offset触发全量复制, 可能是CONTINUE触发增量复制 全量复制小结 master执行bgsave, 在本地生成一份rdb快照文件 master node将rdb快照文件发送给salve node, 如果rdb复制时间超过60秒(repl-timeout), 那么slave node就会认为复制失败, 可以适当调节大这个参数 对于千兆网卡的机器, 一般每秒传输100MB, 6G文件, 很可能超过60s master node在生成rdb时, 会将所有新的写命令缓存在内存中, 在salve node保存了rdb之后, 再将新的写命令复制给salve node client-output-buffer-limit slave 256MB 64MB 60, 如果在复制期间, 内存缓冲区持续消耗超过64MB, 或者一次性超过256MB, 那么停止复制, 复制失败 slave node接收到rdb之后, 清空自己的旧数据, 然后重新加载rdb到自己的内存中, 同时基于旧的数据版本对外提供服务 如果slave node开启了AOF, 那么会立即执行BGREWRITEAOF, 重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite, 很耗费时间 主从复制的阻塞 slave node在做复制的时候, 也不会block对自己的查询操作, 它会用旧的数据集来提供服务; 但是复制完成的时候, 需要删除旧数据集, 加载新数据集, 这个时候就会暂停对外服务了 当出现这种问题时, slave的这次请求会失败? 需要客户端重试? 还是会自动切换到其他slave? 在 深入学习Redis（2）：持久化 一文中，讲到了fork操作对Redis单机内存大小的限制。实际上在Redis的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响： （1）切主：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果Redis单机内存达到10GB，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。 （2）从库扩容：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。 （3）缓冲区溢出：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-&gt;复制缓冲区溢出导致复制中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致复制中断……的循环。 （4）超时：如果数据量过大，全量复制阶段主节点fork+保存RDB文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-&gt;超时导致复制中断-&gt;重连-&gt;全量复制-&gt;超时导致复制中断……的循环。 此外，主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大：最好只使用50%-65%的内存，留下30%-45%的内存用于执行bgsave命令和创建复制缓冲区等。 https://www.cnblogs.com/kismetv/p/9236731.html#t1","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"21. Redis Persistence","slug":"redis/2018-04-05-redis-21","date":"2018-04-05T05:36:52.000Z","updated":"2019-03-11T02:05:28.000Z","comments":true,"path":"2018/04/05/redis/2018-04-05-redis-21/","link":"","permalink":"http://blog.renyimin.com/2018/04/05/redis/2018-04-05-redis-21/","excerpt":"","text":"Redis 在运行时, 是以数据结构的形式将数据维持在内存中的, 所以为了将数据从掉电易失的内存存放到能够永久存储的设备上(让这些数据在 Redis 重启之后仍然可用), Redis 分别提供了 RDB 和 AOF 两种持久化模式 RDB(Redis DataBase) RDB持久化默认是开启的, 关闭方式: 注释掉配置文件中默认的几行 save策略 或者 在save策略下添加一行 save &quot;&quot; 策略 RDB模式可以将redis服务器包含的所有数据库数据以 二进制 .rdb 文件的形式保存到硬盘 默认文件名为 dump.rdb (文件名配置: dbfilename dump.rdb) .rdb文件存放路径配置: dir /usr/local/redis-4.0.12/persistence-db-dir 创建 .rdb 文件常见的三种方式: 手动在客户端向redis服务器发送 SAVE 命令 手动在客户端向redis服务器发送 BGSAVE 命令 在配置文件中配置rdb持久化的 save策略, redis服务运行期间, 如果配置选项被满足, 则服务器会自动执行 BGSAVE 这三种创建RDB的方式, 前两种需要手动去执行, 而第三种是服务器自动执行的 创建RDB文件三种方式的区别SAVE命令手动在客户端向redis服务器发送 SAVE命令 速度优点: 相对于下面介绍的 BGSAVE 命令, SAVE命令执行时, redis主进程不会fork新的进程, 所以可以集中资源来创建.rdb文件, 速度相对比 BGSAVE 命令 要快 缺点: 阻塞: 在redis服务器执行SAVE命令的过程中(即创建RDB文件的过程中), redis服务器将被阻塞, 服务器端无法再去处理客户端发送的命令, 只有在SAVE命令执行完毕之后, 服务器才会重新开始处理客户端发送的命令请求 安全性: .rdb 文件覆盖创建的安全性问题 BGSAVE 命令bgsave命令的执行可以分为手动或者自动 手动在客户端向redis服务器发送 BGSAVE 命令 自动创建.rdb文件 通过在redis配置文件中配置redis服务器生成.rdb文件的条件, 就可以让服务器在满足save策略时, 自动去创建.rdb文件 redis默认情况下就是开启REDB持久化的, 并且默认设置了几个save的策略: 12345save 900 1 #900秒 (15分钟) 内至少1个key值改变 (则进行数据库保存--持久化) save 300 10 #300秒 (5分钟) 内至少10个key值改变 (则进行数据库保存--持久化) save 60 10000 #60秒 (1分钟) 内至少10000个key值改变 (则进行数据库保存--持久化)// 只要三个条件中的任意一个条件被满足时, 服务器就会自动执行 BGSAVE 命令来创建新的.rdb文件// 每次创建完.rdb文件之后, 服务器为实现自动持久化, 会将&apos;为实现持久化而设置的时间计数器和次数计数器清零并重新开始计数&apos;, 所以多个保存条件的效果是不会叠加的 非阻塞 优点: BGSAVE 命令不会造成服务器阻塞, redis服务器在执行BGSAVE命令的过程中, 仍然可以正常的处理其他客户端发送的命令 BGSAVE命令不会造成服务器阻塞的原因在于: 当redis服务器接收到BGSAVE命令后, 它不会亲自去创建rdb持久化文件, 而是通过 fork 一个子进程, 然后由子进程去生成.rdb文件, 主进程则继续处理客户端的命令请求, 当子进程创建好.rdb文件并退出后, 它会向父进程发送一个信号, 告知redis服务器.rdb文件已经创建完毕 缺点: 性能的额外消耗: fork子进程会消耗额外的内存 RDB经常需要 fork() 才能使用子进程将数据持久存储在磁盘上,如果数据集很大, fork() 可能会非常耗时,如果数据集非常大且CPU性能不佳, 可能会导致Redis停止服务客户端几毫秒甚至一秒钟AOF重写也需要fork(), 但你可以调整你想要重写日志的频率而不需要对耐久性进行任何权衡 速度的降低: 相比 save 命令, 由于 fork子进程会消耗额外性能, 所以 bgsave创建.rdb文件的速度其实会比save慢 可能产生的阻塞:父进程 fork 子进程, 这个过程中父进程是阻塞的, Redis 不能执行来自客户端的任何命令 RDB经常需要 fork() 才能使用子进程将数据持久存储在磁盘上,如果数据集很大, fork() 可能会非常耗时,如果数据集非常大且CPU性能不佳, 可能会导致Redis停止服务客户端几毫秒甚至一秒钟AOF重写也需要fork(), 但你可以调整你想要重写日志的频率而不需要对耐久性进行任何权衡 .rdb文件覆盖创建的安全性问题 save 对比 bgsavesave 和 bgsave 这两个命令没有孰好孰坏, 你要考虑哪个更适合你 如果你的数据库正在上线当中, 自然使用 bgsave (让服务器以非阻塞方式进行最好); 相反, 如果你需要在凌晨3点钟维护你的redis, 比如维护需要停机一小时, 这时系统被阻塞了也是没关系的, 这时候你使用save命令就会好一点 都会阻塞: save会阻塞redis响应客户端请求 bgsave在 fork() 时, 如果数据集很大, 也可能造成几毫秒甚至一秒钟的阻塞 .rdb文件覆盖创建的安全性问题: 无论以上哪种方式去生成.rdb文件, 由于每次都是覆盖创建 所以之前的.rdb文件你可以用定时脚本定时地拷走(可以发送到自己的云服务器进行备份), 防止外一客户端执行了 flush db 并且服务器进行了 save, 此时旧的rdb会被新的rdb文件覆盖掉, 那就完蛋了!!! 事实上在生产环境中 FLUSHDB、FLUSHALL 等命令都属于必禁命令(如果开启AOF的话, 如果.aof文件尚未被重写, 你还可以停止服务, 打开.aof文件, 删除执行的flush命令, 然后重启, 不过线上可能会涌入大量的命令, 导致你找flush命令都是个问题) BGSAVE的执行流程浅析 Redis 父进程首先判断: 当前是否存在正在执行 bgsave/bgrewriteaof 子进程, 如果有子进程正在执行, 则 bgsave 命令直接返回 bgsave/bgrewriteaof 的子进程不能同时执行, 主要是基于性能方面的考虑: 两个并发的子进程同时执行大量的磁盘写操作, 可能引起严重的性能问题 父进程 fork 子进程, 这个过程中父进程是阻塞的, Redis 不能执行来自客户端的任何命令 父进程 fork 后, bgsave 命令返回 “Background saving started” 信息并不再阻塞父进程 子进程创建 .rdb 文件, 根据父进程内存快照生成临时快照文件, 完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成, 父进程更新统计信息 RDB持久化的优点 RDB对于灾难恢复非常好，因为一个紧凑的文件可以传输到远程数据中心 (你可能想要归档最近的24小时内每个小时的RDB文件, 并且每个归档保存30天, 这允许你再灾难发生的时候开业轻松地恢复数据集到不同版本) 重建快: (重建数据库是指将数据从硬盘移到内存, 并建立起数据库的过程) 因为对于RDB模式来说, 重建就是把 dump.rdb 文件加载到内存, 并解压字符串, 就建立起了数据库 而对于AOF模式来说, 则是在启动Redis服务器的时候, 运行 appendonly.aof 日志文件, 在内存中重新建立数据库 RDB持久化的缺点 意外宕机的数据丢失问题: 当你正常关闭redis的时候, redis服务器不会参考配置中的save策略, 而是会直接先调用save命令, 将redis所有数据持久化到磁盘之后才会真正进行退出 但是当redis出现意外断电宕机时, 你会发现从上一次快照之后的数据将全部丢失, 这是因为 RDB持久化 无法频繁执行导致的 由于创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作, 所以服务器需要隔一段时间再来创建一个新的rdb文件, 也就是说rdb文件的创建操作不能执行的过于频繁, 否则将会严重影响服务器的性能由于save策略设置的是每隔一段时间再去创建.rdb文件, 所以如果在间隔的这段时间中服务器宕机, 那这段间隔中的数据就丢失了 为了解决这个问题, rdb可以结合aof来一起进行持久化, AOF持久化模式就解决了服务器不能频繁执行rdb持久化的问题 如果你非常关心你的数据，但是在发生灾难时仍然可以忍受几分钟的数据丢失，那么你可以只使用RD .rdb文件每次都是覆盖创建, 所以为了安全起见, 需要不时地去备份已经生成的rdb文件 AOF (Append Only File)由于RDB的持久化策略的执行时间间隔不能设置的特别频繁, 所以在服务器意外宕机的情况下造成的数据丢失问题可能就会比较严重, 为此redis提供了另外一种持久化方案 AOF持久化 AOF模式是将 操作日志 记在 appendonly.aof 文件里, 每次启动服务器就会运行 appendonly.aof 里的 命令 重新建立数据库 默认AOF模式是关闭的, 可以在redis配置文件(redis.conf)中, 配置 appendonly yes 来打开AOF模式 AOF仍然会丢数据 在AOF持久化模式下, 虽然redis服务器在执行修改数据的命令后, 会把执行的命令写入到aof文件中, 但这并不意味着aof文件持久化不会丢失任何数据 在常见的操作系统中, 执行系统调用write函数, 将一些内容写到某个文件中时, 为了提高效率, 系统通常不会直接将内容写入到磁盘里面, 而是先将内容放入一个内存缓冲区(buffer)里面, 等到缓冲区被填满, 或者用户执行 fsync 调用和 fdatasync 调用时, 系统才会将存储在缓冲区里面的内容真正写入到硬盘里所以对AOF持久化来说, 只有当一条命令真正的被写入到硬盘里面时, 这条命令才不会因停机而意外丢失因此, 相对于rdb模式的save策略来看, rdb模式的策略由于可能会出现的较长时间间隔, 所以在redis意外宕机时, aof丢失的命令显然可能会少很多, 但仍然可能丢失命令 aof持久化在遭遇意外停机时所丢失的命令数量, 取决于命令被写入到硬盘的时间越早将命令写入到硬盘, 发生意外停机时丢失的数据就越少, 而越迟将命令写入硬盘,发生意外停机时丢失的数据就越多为此, AOF为我们提供了几个尽快将数据写入磁盘的追加策略 由于AOF仍然面临丢失命令的风险, AOF模式提供了三种 追加 策略, 这三种追加策略主要就是用来指定什么时机将操作日志真正追加到 appendonly.aof 文件里 always : 服务器每写入一个命令, 就调用一次fdatasync, 将缓冲区里面的命令写入到磁盘文件中, 在这种模式下, 服务器即使遭遇意外停机, 也不会丢失任何自己已经成功执行的命令数据比较安全, 但比较慢; (类似mysql了) everysec: 服务器每一秒重新调用一次fdatasync, 将缓冲区里面的操作日志写入到磁盘文件中, 这是系统默认的方式, 是一种权衡折衷通常这种方式会比较好, 在这种模式下, 服务器即使遭遇意外停机时, 最多只丢失一秒钟的内执行的命令相比RDB由于不能频繁执行而设置的save策略可能会间隔较长的时间, 在出现redis意外宕机时, 这种方式只会丢失1秒的数据显然要好多了 no: 服务器不主动调用fdatasync, 由操作系统去决定什么时候将缓冲区里面的命令写入到硬盘里面在这种模式下,服务器遭遇意外停机时, 丢失的命令数量是不确定的 可以在redis.conf中配置AOF的追加策略, 可以看到默认使用的是everysec这种折中策略 123# appendfsync alwaysappendfsync everysec# appendfsync no BGREWRITEAOF 由于redis只会写一个aof文件, 随着AOF文件越来越大, 里面会有大部分是重复命令或者可以合并的命令(100次incr = set key 100) 为了让aof文件的大小控制在合理的范围, 避免它疯狂增长, redis提供了AOF重写功能, 通过这个功能, 服务器可以产生一个新的aof文件 重写的好处: 减少AOF日志尺寸, 减少内存占用, 加快数据库恢复时间 有两种方法可以触发aof文件重写 客户端手动向服务器发送 BGREWRITEAOF 命令 通过设置配置文件选项来让服务器自动执行BGWRITEAOF 命令 12345auto-aof-rewrite-min-size &lt;size&gt;触发aof重写所需的最小体积: 只要aof文件的体积大于等于size时,服务器才会考虑是否需要进行aof重写, 这个选项用于避免对体积过小的aof文件进行重写auto-aof-rewrite-percentage 100指定触发重写所需的aof文件增长体积的百分比, 当aof文件增长的体积大于auto-aof-rewrite-min-size指定的体积, 并且超过上一次重写之后的aof文件体积的percent%时, 就会触发aof重写, (如果服务器启动刚刚不就,还没有进行过aof重写,那么使用服务器启动时载入的aof文件体积来作为基准值)将这个值设置为0表示关闭自动aof重写 例子: 1234//只有当aof文件的增量大于100%的时候才进行重写auto-aof-rewrite-percentage 100// 当aof文件大于64mb之后才考虑进行aof重写, 还需要看上一条的百分比增量够不够auto-aof-rewrite-min-size 64mb AOF重写命令是redis通过fork子进程在后台执行的 AOF重写并不需要对原有AOF文件进行任何的读取, 写入, 分析等操作, 这个功能是通过读取服务器当前的数据库状态来实现的 新的aof文件会使用尽可能少的命令来记录数据库数据, 因此新的aof文件的体积通常会比原有aof文件的体积要小得多 aof重写期间, 服务器不会被阻塞, 可以正常处理客户端发送的命令请求 新的aof文件创建完成后, 旧的aof文件会被删除 aof文件的重写机制: 整个过程还是比较谨慎的, 即使redis出现意外宕机, 老的.aof文件还在 redis fork一个子进程 子进程基于当前内存中的数据, 往一个新的临时的AOF文件中写入日志 同时, redis主进程仍然继续处理新的请求, 在接收到client新的写操作之后, redis会在内存中写入日志, 同时新的日志也继续写入旧的AOF文件 子进程写完新的日志文件之后, redis主进程将内存中的新日志再次追加到新的AOF文件中 用新的日志文件替换掉旧的日志文件 BGREWRITEAOF 导致的主线程阻塞 bgrewriteaof 是主进程fork的子进程来执行的, 按照正常逻辑, 不应该影响 Redis 主进程的正常服务 但其实问题是出在硬盘上: Redis 服务设置了 appendfsync everysec, 主进程每秒钟便会调用 fsync(), 要求内核将数据写到存储硬件里 但由于此时 bgrewriteaof子进程 同时也在写硬盘, 从而导致主进程 fsync()/write() 操作被阻塞, 最终导致 Redis 主进程阻塞了 AOF文件出错 服务器可能在程序正在对AOF文件进行写入时发生意外宕机, 如果造成了AOF文件出错(corrupt), 那么 Redis 在重启时会拒绝载入这个 AOF 文件, 从而确保数据的一致性不会被破坏 当发生这种情况时, 可以用以下方法来修复出错的 AOF 文件: 首先必须先为现有的 AOF 文件创建一个备份文件 然后使用 Redis 附带的 redis-check-aof 程序, 对原来的 AOF 文件进行修复: $ redis-check-aof --fix (可选)使用 diff -u 对比修复后的 AOF 文件和原始 AOF文件的备份, 查看两个文件之间的不同之处 重启 Redis 服务器, 等待服务器载入修复后的 AOF 文件, 并进行数据恢复 模拟让aof破损 打开文件, 在末尾随意删除两行 (造成aof出错一般是redis意外宕机导致的文件末尾出错, 你不能在aof文件中间随意删除几行然后指望fix, 显然不太现实) 然后fix (会有一条数据会被fix删除) 用fix的aof文件去重启redis, 发现数据确实只剩下一条了 RDB与AOF对比 Redis的配置文件中, 两者有各自的配置方式, 默认是开启了RDB Redis的写操作在RDB模式下比AOF模式下要快: 因为RDB模式下, redis的每次写操作都是直接写redis内存, 只有当满足save策略时, 会持久化一份.rdb文件 而AOF模式下, redis的每次写操作都会触发一次os cache的写入, 然后根据追加策略的不同, 写入.aof文件 当然, .rdb文件每次都是新生成(比较耗内存), 而.aof文件只有一份,是一直追加的 RDB意外宕机丢失数据的量看上去比AOF要大 RDB模式的策略不能太频繁(创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作), 如果redis出现意外宕机, 那从上次宕机到当前时刻, redis的数据就会都丢失 而AOF模式可以做到在redis出现意外宕机时, 最多丢失1秒的数据 .rdb文件是二进制文件, 而.aof文件对我们可读 由于AOF以易于理解和解析的格式, 因此你可以轻松导出AOF文件 假设你意外执行了 FLUSHALL 命令刷新了所有数据, 如果开启了AOF, 并且在此期间未执行重写日志 你仍然可以恢复尽可能新的数据, 只需停止服务器, 删除AOF上的最新命令, 然后重新启动Redis .rdb文件恢复数据时, 重建比较快, 而 .aof文件恢复相对比较慢 aof文件和rdb的数据是不一样的: aof文件中的数据显然要比rdb文件中的数据要新一些, 所以rdb和aof都开启的情况下, redis会优先使用aof日志进行恢复 RDB + AOF在redis使用中, 可以组合AOF和RDB, 组合使用导致的阻塞问题: 子进程互相阻塞问题 如果RDB在执行snapshotting操作, 那么redis不会执行AOF bgrewrite; 如果redis在执行AOF bgrewrite, 那么就不会执行 RDB snapshotting 如果RDB在执行snapshotting, 此时用户执行 BGREWRITEAOF 命令, 那么等RDB快照生成之后, 才会去执行AOF rewrite 子进程阻塞导致主进程阻塞: 在BGREWRITEAOF写硬盘期间, 也会阻塞AOF每秒对.aof文件的写入 另外注意: 那么redis重启的时候, 会优先使用AOF进行数据恢复, 因为aof的日志相比rdb会更完整 在有rdb的dump和aof的appendonly的同时, rdb里有数据, aof里也有数据, 但是由于aof的日志是每个命令每个一秒都会写入, 所以也更新, 如果redis意外宕机, 也不可能去优先拿rdb中的旧数据 内存的飙升 AOF重写 SAVE, BGSAVE 上述两种情况都会出现在内存中持有的 .aof/.rdb文件的情况, 内存自然会飙升, 比如可能会出现如下错误: 12345625018:C 15 May 06:12:46.416 # Write error writing append only file on disk: No space left on device1548:M 15 May 06:12:48.146 # Short write while writing to the AOF file: (nwritten=309, expected=37399)1548:M 15 May 06:12:48.526 # AOF write error looks solved, Redis can write again.1548:M 15 May 06:12:48.928 # Background AOF rewrite terminated with error1548:M 15 May 06:12:49.029 * Starting automatic rewriting of AOF on 108% growth1548:M 15 May 06:12:49.368 * Background append only file rewriting started by pid 25502 持久化恢复步骤 在redis出现意外宕机后, 如果要进行数据恢复, 注意: 如果你直接将云服务器上定期备份的 .rdb 文件直接拷贝到redis指定的生成rdb文件的目录下, 然后试图重启redis重启后你可能会发现redis中并没有.rdb文件中的数据原因是线上redis一般使用 appendonly.aof + dump.rdb 的方式进行持久化, 而redis重启后会优先用 appendonly.aof 去恢复数据, 而没有用.rdb文件的数据并且redis启动的时候, 自动重新基于内存的数据, 生成了一份最新的rdb快照, 即直接用空的数据, 覆盖掉了你刚刚拷贝过去的那份dump.rdb文件 所以, 你停止redis之后, 其实应该先删除 appendonly.aof, 然后将我们的dump.rdb拷贝过去, 然后再重启redis, 但是重启后发现仍然没有 aof 文件中的数据原因很简单, 虽然你删除了 appendonly.aof, 但是因为aof持久化是打开的, redis就一定会优先基于aof去恢复, 即使文件不在, 它也会在启动后立刻创建一个新的空的aof文件 正确操作: 停止redis, 暂时在配置中关闭aof, 然后拷贝一份rdb过来, 再重启redis, 数据能不能恢复过来, 可以恢复过来 之后, 如果你再关掉redis, 手动修改配置文件, 打开aof, 再重启redis, 数据又没了, 因为之前redis内存中的数据并不会生成aof文件, 而打开aof重启redis后, 才会又创建新的空的aof文件, 然后以新的aof文件进行恢复并且会覆盖之前的rdb文件 在数据安全丢失的情况下, 基于rdb冷备, 如何完美的恢复数据, 同时还保持aof和rdb的双开 停止redis, 关闭aof, 拷贝rdb备份, 重启redis, 确认数据恢复, 直接在命令行 config set 热修改配置参数, 打开aof, 这样redis就会将内存中的数据, 写入aof文件中 此时aof和rdb两份数据文件的数据就同步了 由于配置文件中的实际的参数没有被持久化的修改, 再次停止redis, 手动修改配置文件, 打开aof的命令, 再次重启redis 参考https://www.cnblogs.com/kismetv/p/9137897.html#t53","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"03. 杂项","slug":"redis/2018-03-10-redis-03","date":"2018-03-10T02:17:23.000Z","updated":"2019-02-27T06:02:45.000Z","comments":true,"path":"2018/03/10/redis/2018-03-10-redis-03/","link":"","permalink":"http://blog.renyimin.com/2018/03/10/redis/2018-03-10-redis-03/","excerpt":"","text":"redis访问控制 redis默认会启动受保护模型, 客户端要连接redis 要么在配置文件中设置 protected-mode no 或者在配置文件中为redis设置密码 redis密码设置: redis没有实现访问控制这个功能, 但是它提供了一个轻量级的认证方式, 可以编辑 redis.conf 配置来启用认证 可以在配置文件redis.conf中, 为redis配置连接密码 requirepass 你的密码 然后重启: service redisd stop, service redisd start 设置了密码后, 客户端或slave如何连接redis服务? 客户端连接redis : redis-cli -h 127.0.0.1 -p 6379 -a 你的密码 slave连接master时, 也需要为slave配置 masterauth 主的密码 连接redis不需要账号, 只需要密码, 但是要想连接redis, 还需要配置ip白名单: #bind 127.0.0.1 默认情况会开启, 只允许本地访问redis服务, 实际情况我们生产环境下基本都是远程访问, 所以可以注释掉, 即允许本机以外的所有ip访问它 也可以配置你的客户端ip, 如: bind 192.168.1.100 10.0.0.1 危险命令的禁用 Redis的危险命令主要有 flushdb // 清空数据库 flushall // 清空所有记录, 数据库 config // 客户端连接后不能配置服务器 keys // 客户端连接后可查看所有存在的键 (keys这个命令性能真的很差, keys模糊匹配 会引发Redis锁, 并且增加Redis的CPU占用, 情况会非常恶劣)不要使用keys正则匹配操作, 包括但不限于各种形式的模糊匹配操作因为Redis是单线程处理, 在线上KEY数量较多时, 操作效率极低(时间复杂度为O(N)), 该命令一旦执行会严重阻塞线上其它命令的正常请求, 而且在高QPS情况下会直接造成Redis服务崩溃 作为服务端的redis-server, 我们常常需要禁用以上命令来使服务器更加安全 禁用的具体做法是, 修改服务器的配置文件redis.conf, 找到 Command renaming, 新增以下命令: 1234rename-command FLUSHALL &quot;&quot;rename-command FLUSHDB &quot;&quot;rename-command CONFIG &quot;&quot;rename-command KEYS &quot;&quot; ---暂时未使用 而如果想要保留命令, 但是不能轻易使用, 可以重命名命令来设定: 1234rename-command FLUSHALL joYAPNXRPmcarcR4ZDgC81TbdkSmLAzRPmcarcRrename-command FLUSHDB qf69aZbLAX3cf3ednHM3SOlbpH71yEXLAX3cf3erename-command CONFIG FRaqbC8wSA1XvpFVjCRGryWtIIZS2TRvpFVjCRGrename-command KEYS eIiGXix4A2DreBBsQwY6YHkidcDjoYA2DreBBsQ redis库名 Redis默认支持16个数据库(可以通过配置文件支持更多, 无上限), 可以通过配置 databases 来修改这一数字 1234# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1databases 16 redis的每个数据库对外都是一个从0开始的递增数字命名客户端与Redis建立连接后会自动选择0号数据库, 不过可以随时使用 SELECT 命令更换数据库 然而这些以数字命名的数据库又与我们理解的数据库有所区别 首先Redis不支持自定义数据库的名字, 每个数据库都以编号命名, 开发者必须自己记录哪些数据库存储了哪些数据 另外Redis也不支持为每个数据库设置不同的访问密码, 所以一个客户端要么可以访问全部数据库, 要么连一个数据库也没有权限访问 最重要的一点是多个数据库之间并不是完全隔离的, 比如 FLUSHALL 命令可以清空一个Redis实例中所有数据库中的数据 redis 的 key设计技巧 把表名转换为key前缀 如, user: 第2段放置用于区分key的字段(对应mysql中的主键的列名,如 userid) 第3段放置主键值, 如2,3,4…., a , b ,c 第4段, 写要存储的列名 如下, 用户表 user, 转换为key-value存储 userid | username | password | email :-: | :-: | :-: | :-: | :-: | -: 9 | Lisi | 1111111 | lisi@163.com 123set user:userid:9:username lisiset user:userid:9:password 111111set user:userid:9:email lisi@163.com redis 日志配置 redis在默认情况下, 是不会生成日志文件的, 所以需要配置 打开配置文件 1234567891011# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)# loglevel是用来设置日志等级的，具体可以看配置文件中上面的注释loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;/var/log/redis/redis.log&quot; redis必须带配置文件启动, 如果直接启动的话, 它会使用默认配置(而且并不存在这个默认配置文件, 所以不要想着你能去改它) 让消息队列更可靠 Redis List经常被用于消息队列服务, 假设消费者程序在从队列中取出消息后立刻崩溃, 但由于该消息已经被取出但却没有被正常处理, 那么该消息就丢失了 为了避免这种情况, Redis提供了 RPOPLPUSH 命令, 消费者程序 会 原子性 的 从主消息队列中取出消息并将其插入到备份队列中, 并且会返回弹出队列的数据然后等到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除同时还可以提供一个守护进程, 当发现备份队列中的消息过期时, 可以重新将其再放回到主消息队列中, 以便其它的消费者程序继续处理 未完待续~~ 未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"03. HTTP状态码详解","slug":"http/2017-11-30-HTTP-03-httpStatusCode","date":"2017-11-30T06:30:12.000Z","updated":"2019-10-21T08:24:22.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03-httpStatusCode/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03-httpStatusCode/","excerpt":"","text":"1xx 101: 参考博文WebSocket简单示例分析 (做协议升级, 还会响应: Connection: Upgrade) 2xx Web API的设计与开发 P109 200 OK : 200码非常出名, 似乎没有对它进一步说明的必要 201 Created : 当在服务器端创建数据成功时, 会返回201状态码 (比如使用 POST 请求方法, 如 用户登录后添加了新用户, 上传了图片等新创建数据的场景) 202 Accepted : 在异步处理客户端请求时, 它用来表示服务器端已经接受了来自客户端的请求, 但处理尚未结束 在文件格式转换, 处理远程通知(Apple Push Notification等)这类很耗时的场景中, 如果等到所有处理都结束后才向客户端返回响应消息, 就会花费相当长的时间, 造成应用可用性不高; 这时采用的方法是服务器向客户端返回一次响应消息, 然后立刻开始异步处理 202状态码就被用于告知客户端服务器端已经开始处理请求, 但整个处理过程尚未结束 比如: 以LinkedIn的参与讨论的API为例如果成功参与讨论并发表意见, 服务器端通常会返回201状态码但如果需要得到群主的确认, 那么所发表的意见就无法立即在页面显示出来, 这时服务器端就需要返回202状态码; 从广义上来看, 该场景也属于异步处理, 但和程序设计里的异步执行当然不同 204 No Content : 正如其字面意思, 当响应消息为空时会返回该状态码 其实就是告诉浏览器, 服务端执行成功了, 但是没什么数据返回给你, 所以你不用刷新页面, 也不用导向新的页面 在用 DELETE 方法删除数据时, 服务器端通常会返回204状态码(阮一峰博文也提到过, 对DELETE适用) 除此之外, 也有人认为在使用 PUT或PATCH 方法更新数据时, 因为只是更新已有数据, 所以返回204状态码更加自然 书中建议 DELETE 返回204; PUT或PATCH返回200并返回该方法所操作的数据; 关于204状态码的讨论可以参考 p111; 205 Reset Content : 告诉浏览器, 页面表单需要被重置 205的意思是服务端在接收了浏览器POST请求以后, 处理成功以后, 告诉浏览器, 执行成功了, 请清空用户填写的Form表单, 方便用户再次填写; 206 Partial Content : 成功执行了一个部分或Range(范围)的请求 206响应中, 必须包含 Content-Range, Date 以及 ETag或Content-Location首部; 3xx300 Multiple Choices : 客户端驱动方式进行内容协商时, 服务器可能返回多个连接供客户端进行选择 (比如多语言网站可能会出现) 301 Moved Permanently : 俗称 永久重定向, 意思 是原 URI 已经 “永久” 性地不存在了, 今后的所有请求都必 须改用新的 URI浏览器看到 301，就知道原来的 URI “过时” 了, 就会做适当的优化 (比如历史记录、更新书签), 下次可能就会直接用新的 URI 访问, 省去了再次跳转的成本 (需要清除浏览器缓存才行)搜索引擎的爬虫看 到 301, 也会更新索引库, 不再使用老的 URI重定向后 请求里的方法 会转成 GET 302 Found: 俗称 临时重定向, 意思是原 URI 处于 “临时维护” 状态, 新的 URI 是起 “顶包” 作用的 “临时工”浏览器或者爬虫看到 302, 会认为原来的 URI 仍然有效, 但暂时不可用, 所以只会执行简单的跳转页面, 不记录新的 URI, 也不会有其他的多余动作, 下次访问还是用原 URI重定向后 请求里的方法 会转成 GET 303 See Other : HTTP/1.1使用303来实现和302一样的临时重定向重定向后 请求里的方法 会转成 GET 307 Temporary Redirect: 类似 302 (HTTP/1.1规范要求用307来取代302进行临时重定向; 302临时重定向留给HTTP/1.0)但重定向后请求 里的方法不允许变动 308 Permanent Redirect貌似不是rfc2616的标准具备和301永久重定向的特点 (需要清除浏览器缓存才行)但重定向后请求 里的方法不允许变动 304 Not Modified : 参考博文缓存相关 4xx Web API的设计与开发 P1134字头状态码主要用于描述因客户端请求的问题而引发的错误。也就是说, 服务器端不存在问题, 但服务器端无法理解客户端发送的请求, 或虽然服务器端能够理解但请求却没有被执行, 当遇到这些情况引发的错误时, 服务器端便会向客户端返回这一类别的状态码。因此, 当服务器端返回4字头的状态码时, 就表示客户端的访问方式发生了问题, 用户需要检查一下客户端的访问方式或访问的目标资源等。 400 Bad Request : 表示其他错误的意思, 即其他4字头状态码都无法描述的错误类型; 401 Unauthorized : 表示认证(Authentication)类型的错误 比如当需要先进行登录操作, 而却没有告诉服务器端所需的会话信息(比如token..), 服务器端就会返回401状态码, 告知客户端出错的大致原因; 403 Forbidden : 和401状态码比较相似, 所以也经常被混淆; 其实403表示的是 授权(Authotization) 类型的错误, 授权和认证的不同之处是: 认证表示”识别前来访问的是谁”, 而授权则表示”赋予特定用户执行特定操作的权限” 通俗地说: 401状态码表示”我不知道你是谁”, 403状态码表示”虽然知道你是谁, 但你没有执行该操作的权限” 404 Not Found : 表示访问的数据不存在, 但是 例如当客户端湿度获取不存在的用户信息时, 或者试图访问原本就不存在的端点时, 服务器就会返回404状态码; 所以, 如果客户端想要获取用户信息, 却得到服务器端返回的404状态码, 客户端仅凭”404 Not Found”将难以区分究竟是用户不存在, 还是端点URI错误导致访问了原本不存在的URI; 405 Method Not Allowed : 表示虽然访问的端点存在, 但客户端使用的HTTP方法不被服务器端允许; 比如客户端使用了POST方法来访问只支持GET方法的信息检索专用的API; 又比如客户端用了GET方法来访问更新数据专用的API等; 406 Not Acceptable : 服务器端API不支持客户端指定的数据格式时, 服务器端所返回的状态码; 比如, 服务器端只支持JSON和XML输出的API被客户端指定返回YAML的数据格式时, 服务器端就会返回406状态码; 408 Request Timeout : 当客户端发送请求至服务器端所需的时间过长时, 就会触发服务器端的超时处理, 从而使服务器端返回该状态码; 409 Conflict: 用于表示资源发生冲突时的错误 (est中就会有该错误码) 比如通过指定ID等唯一键值信息来调用注册功能的API时, 倘若已有相同ID的数据存在, 就会导致服务器端返回409状态码; 在使用邮箱地址及Facebook ID等信息进行新用户注册时, 如果该邮箱地址或者ID已经被其他用户注册, 就会引起冲突, 这时服务器端就会返回409状态码告知客户端该邮箱地址或ID已被使用; 410 Gone : 和 404状态码 相同, 都表示访问资源不存在, 只是410状态码不单表示资源不存在, 还进一步告知资源曾经存在, 只是目前已经消失了; 因此服务器端常在访问被删除的数据时返回该状态码, 但是为了返回该状态码, 服务器必须保存该数据已被删除的信息, 而且客户端也应该知晓服务器端保存了这样的信息; 但是在通过邮箱地址搜索用户信息的API中, 从保护个人信息的角度来说, 返回410状态码的做法也会受到质疑; (所以在此种资源不存在的情况下, 为了稍微安全一些, 返回410状态码需要慎重) 413 Request Entity Too Large : 413也是比较容易出现的一种状态码, 表示请求实体过大而引发的错误 请求消息体过长是指, 比如在上传文件这样的API中, 如果发送的数据超过了所允许的最大值, 就会引发这样的错误; 414 Request-URI Too Large : 414是表示请求首部过长而引发的错误 如果在进行GET请求时, 查询参数被指定了过长的数据, 就会导致服务器端返回414状态码 415 Unsupported Media Type : 和406比较相似 406我们知道是表示服务器端不支持客户端想要接收的数据格式 而415表示的是服务器端不支持客户端请求首部 Content-Type 里指定的数据格式, 也就是说, 当客户端通过POST,PUT,PATCH等方法发送的请求消息体的数据格式不被服务器支持时, 服务器端就会返回415状态码; 例如在只接收JSON格式的API里, 如果客户端请求时发送的是XML格式的数据去请求服务器端, 或者在 Content-Type 首部指定 application/xml, 都会导致该类型错误; 429 Too Many Requests : 是2012年RFC6585文档中新定义的状态码, 表示访问次数超过了所允许的范围; 例如某API存在一小时内只允许访问100次的访问限制, 这种情况下入股哦客户端视图进行第101次访问, 服务器便会返回该状态码; 表示在一定的时间内用户发送了太多的请求, 即超出了”频次限制”, 在响应中，可以提供一个 Retry-After 首部来提示用户需要等待多长时间之后再发送新的请求; 5xx 5字头状态码表示错误不发生在客户端, 而是由服务器自身问题引发的。 500 Internal Server Error : 是web应用程序开发里非常常见的错误, 当服务器代码里存在bug, 输出错误信息并停止运行等情况下, 就会返回该类型的错误; 因此, 不仅限于API, 对于5字头状态码的错误, 都要认真监视错误日志, 使系统在出错时及时告知管理员, 以便在错误发生时做好应对措施, 防止再次发生。 501 Not Implemented : ??? 502 Bad GateWay : ??? 503 Service Unavaliable : 用来表示服务器当前处于暂不可用状态 可以回送:响应首部 Retry-After 表示多久恢复; 不同的客户端与服务器端应用对于 Retry-After 首部的支持依然不太一致; 不过，一些爬虫程序，比如谷歌的爬虫程序Googlebot, 会遵循Retry-After响应首部的规则, 将其与503(Service Unavailable,当前服务不存在)响应一起发送有助于互联网引擎做出判断,在宕机结束之后继续对网站构建索引。 参考:https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After 504 Gateway Time-out: 复现这个错误码比较简单, 让你的php程序模拟耗时请求 如下代码 123&lt;?phpsleep(70);//模拟耗时，睡70秒echo &quot;睡醒了&quot;; 返回 12504 Gateway Time-outnginx/1.11.4 505 HTTP Version Not Supported: 服务器收到的请求, 使用的是它无法支持的HTTP协议版本; 参考: 《HTTP权威指南》、《Web API的设计与开发》 http://yongxiong.leanote.com/post/Nginx状态码含义及chong-xuan#title-14","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. HTTP Methods","slug":"http/2017-11-30-HTTP-02-httpMethods","date":"2017-11-30T02:29:12.000Z","updated":"2019-11-05T07:51:54.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-02-httpMethods/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-02-httpMethods/","excerpt":"","text":"前言 HTTP/1.1 常见的 Method 有: OPTIONS, HEAD, GET, POST, PUT, DELETE, TRACE, CONNECT (参考RFC2616) RFC2616中提到: PATCH, LINK, UNLINK 方法被定义, 但并不常见; (《图解http协议》中也提到 LINK, UNLINK 已经被http1.1废弃); 不同应用各自的实现不同, 有些应用会完整实现, 有些还会扩展, 有些可能只会实现一部分; PUT PUT: 替换资源 PUT 和 POST的区别: 在HTTP中, PUT被定义为 idempotent(幂等性) 的方法, POST则不是, 这是一个很重要的区别 应该用 PUT 还是 POST? 取决于这个REST服务的行为是否是 idempotent(幂等) 的 假如发送两个请求, 希望服务器端是产生两个新数据，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用 POST 方法 但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的, 那就应该使用 PUT 方法 虽然 POST 和 PUT 差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦; POST POST: 上面已经提过了, POST是非幂等的 POST 和 PUT 都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的 PATCHPATCH不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题 对已有资源的操作: 用于对资源的 部分内容 进行更新 (例如更新某一个字段, 具体比如说只更新用户信息的电话号码字段) 而 PUT 则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变 HEAD HEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回, 而仅仅返回HTTP头信息 比如: 欲判断某个资源是否存在, 我们通常使用GET, 但其实用HEAD, 意义更加明确 GET比较简单, 直接获取资源; OPTIONS这个方法使用比较少, 它用于获取当前URL所支持的方法若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST之前跨域相关博文 CORS方案 not-so-simple request 中的”预检”请求用的请求方法就是 OPTIONS CONNECT要求用隧道协议连接代理, 如使用SSL TRACE~~未完待续 DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"70. MySQL5.6 GTID 新特性","slug":"MySQL 暂停/2017-10-04-mysql-80","date":"2017-10-04T07:21:36.000Z","updated":"2019-03-27T06:22:37.000Z","comments":true,"path":"2017/10/04/MySQL 暂停/2017-10-04-mysql-80/","link":"","permalink":"http://blog.renyimin.com/2017/10/04/MySQL 暂停/2017-10-04-mysql-80/","excerpt":"","text":"http://cenalulu.github.io/mysql/mysql-5-6-gtid-basic/MMM, MHA","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"49. 索引和锁","slug":"MySQL 暂停/2017-09-25-mysql-49","date":"2017-09-25T13:10:40.000Z","updated":"2018-03-08T02:57:03.000Z","comments":true,"path":"2017/09/25/MySQL 暂停/2017-09-25-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/MySQL 暂停/2017-09-25-mysql-49/","excerpt":"","text":"索引可以让查询锁定更少的行 因为InnoDB只有在访问行的时候才会对其加锁, 而索引能够减少InnoDB访问的行数, 从而减少锁的数量;但这只有当InnoDB在存储引擎层就能过滤掉所有不需要的行时才行, 如果索引(处在存储引擎层)无法过滤掉无效的行, 那么在InnoDB检索到数据并发送给服务器层以后, 服务器层才能应用where子句, 这时已经无法避免锁定行了;虽然InnoDB的行锁效率很高, 内存使用也很少, 但是锁定行的时候仍然会带来额外开销;锁定超过需要的行会增加锁争用并减少并发性;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"27. REPEATABLE READ 可重复读","slug":"MySQL 暂停/2017-09-17-mysql-27","date":"2017-09-17T14:10:52.000Z","updated":"2019-04-26T11:10:37.000Z","comments":true,"path":"2017/09/17/MySQL 暂停/2017-09-17-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/MySQL 暂停/2017-09-17-mysql-27/","excerpt":"","text":"前言 该隔离级别可以解决不可重复读问题, 脏读问题; 也就是它既可以让事务只能读其他事务已提交的的记录, 又能在同一事务中保证多次读取的数据即使被其他事务修改,读到的数据也是一致的。 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免 脏读, 不可重复读 这两个问题的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"40. 阿里云redis","slug":"redis/2017-09-07-redis-40","date":"2017-09-07T03:21:13.000Z","updated":"2019-02-22T13:33:43.000Z","comments":true,"path":"2017/09/07/redis/2017-09-07-redis-40/","link":"","permalink":"http://blog.renyimin.com/2017/09/07/redis/2017-09-07-redis-40/","excerpt":"","text":"https://help.aliyun.com/document_detail/62870.html?spm=5176.7897645.101.3.35f74e7cWoih5m","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"28. 隔离级别 与 锁","slug":"MySQL 暂停/2017-09-03-mysql-28","date":"2017-09-03T06:20:52.000Z","updated":"2019-04-26T11:28:59.000Z","comments":true,"path":"2017/09/03/MySQL 暂停/2017-09-03-mysql-28/","link":"","permalink":"http://blog.renyimin.com/2017/09/03/MySQL 暂停/2017-09-03-mysql-28/","excerpt":"","text":"前言 之前几篇博文已经介绍了Mysql事务, 高并发下事务将会面对的问题 及 MySQL的解决方案; MySQL主要采用 事务隔离性中的4种隔离级别 结合 MVCC机制 来进行解决; 而事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略; 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的; READ UNCOMMITTED 未提交读READ COMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料 MySQL官方文档 美团技术博客 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"27. 幻读, 快照读(snapshot read), 当前读 (current read)","slug":"MySQL 暂停/2017-09-02-mysql-27","date":"2017-09-02T11:25:07.000Z","updated":"2019-04-26T11:10:29.000Z","comments":true,"path":"2017/09/02/MySQL 暂停/2017-09-02-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/MySQL 暂停/2017-09-02-mysql-27/","excerpt":"","text":"RR + MVCC 虽然解决了 幻读 问题, 但要注意, 幻读针对的是读操作(对于其他操作就不一样了); 演示 打开 两个客户端 1,2 确保隔离级别为默认级别RR, 提供语句: 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 123456789101112mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) 回到 客户端2 的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了 select 幻读)!! 12345678910111213141516171819202122mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 但如果尝试在客户端2的事务中执行 insert/delete/update , 却会发现此类操作都可以感知到客户端1提交的新数据 123mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; 小结 虽然发现已经克服了幻读问题; 但当 在客户端2事务中 insert 插入一条id为4的新数据, 却发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 — 一致性非阻塞读中的一段介绍 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。如果一个事务去更新或删除其他事务提交的行, 则那些更改对当前事务就变得可见;但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 也就是RR隔离级别, 在同一事务中多次读取的话, 对 select 克服了 幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念: 当前读 和 快照读 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的 select 都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 快照读： 是通过MVVC(多版本控制)和 undo log 来实现的, 常见语句如下(貌似就是常见的悲观锁么): 1简单的select操作 (不包括: `select ... lock in share mode`, `select ... for update`) 而 当前读 根本不会创建任何快照, insert, update, delete都是当前读, 所以这几个操作会察觉到其他事务对数据做的更改(而普通select是察觉不到的): 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"09. utf8和utf8mb4区别","slug":"MySQL 暂停/2017-08-23-mysql-09","date":"2017-08-23T10:51:28.000Z","updated":"2019-03-30T06:12:28.000Z","comments":true,"path":"2017/08/23/MySQL 暂停/2017-08-23-mysql-09/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/MySQL 暂停/2017-08-23-mysql-09/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"12. MySQL备份与恢复","slug":"MySQL 暂停/2017-08-23-mysql-12","date":"2017-08-23T10:51:28.000Z","updated":"2018-03-18T07:29:49.000Z","comments":true,"path":"2017/08/23/MySQL 暂停/2017-08-23-mysql-12/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/MySQL 暂停/2017-08-23-mysql-12/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"06. MySQL 设置白名单","slug":"MySQL 暂停/2017-08-19-mysql-07","date":"2017-08-19T09:03:52.000Z","updated":"2019-03-28T02:25:48.000Z","comments":true,"path":"2017/08/19/MySQL 暂停/2017-08-19-mysql-07/","link":"","permalink":"http://blog.renyimin.com/2017/08/19/MySQL 暂停/2017-08-19-mysql-07/","excerpt":"","text":"自建设置白名单https://www.cnblogs.com/lsdb/p/6795053.html 阿里云rds白名单 ip白名单 还可以配置ecs安全组https://help.aliyun.com/document_detail/43185.html?spm=5176.11065259.1996646101.searchclickresult.433a2e6cK2So7k","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"06. MySQL DNS解析","slug":"MySQL 暂停/2017-08-19-mysql-06","date":"2017-08-19T03:10:09.000Z","updated":"2019-03-28T02:26:36.000Z","comments":true,"path":"2017/08/19/MySQL 暂停/2017-08-19-mysql-06/","link":"","permalink":"http://blog.renyimin.com/2017/08/19/MySQL 暂停/2017-08-19-mysql-06/","excerpt":"","text":"https://cloud.tencent.com/developer/article/1039828 rrc的测试库中, skip_name_resolve 是开启状态 (不能通过主机域名进行dns解析)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"05. MySQL Replication","slug":"MySQL 暂停/2017-08-18-mysql-05","date":"2017-08-18T10:51:28.000Z","updated":"2019-03-13T08:26:53.000Z","comments":true,"path":"2017/08/18/MySQL 暂停/2017-08-18-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2017/08/18/MySQL 暂停/2017-08-18-mysql-05/","excerpt":"","text":"概述 MySQL内建的复制功能是构建基于MySQL的大规模、高性能应用的基础, 这类应用使用所谓的水平扩展的架构。 可以通过为服务器配置一个或多个备库的方式来进行数据同步, 复制功能不仅有利于构建高性能的应用, 同时也是高可用、可扩展性、灾难恢复、备份以及数据仓库等工作的基础。 复制解决的基本问题是让一台服务器的数据与其他服务器的数据保持同步。一台主库的数据可以同步到多台备库上, 备库本身也可以被配置成另外一台服务器的主库。主库和备库之间可以有多种不同的组合方式。 MySQL版本对主从复制的影响: 新版本服务器可以作为老版本服务器的备库, 但是老版本服务器作为新版本服务器的备库通常是不可行的, 因为老版本可能无法解析新版本所采用的新特性或语法; 另外, 所使用的二进制文件格式也可能不相同; 小版本升级通常是兼容的; 开销 复制通常不会增加主库的开销, 主要是启用二进制带来的开销, 但出于对备份或及时从崩溃中恢复的目的, 这点开销也是必要的; 每个备库也会对主库增加一些负载(例如网络I/O开销), 尤其当备库请求从主库读取旧的二进制日志文件时, 可能会造成更高的I/O开销; 另外, 锁竞争也可能阻碍事务的提交; 最后, 如果是从一个高吞吐量的主库上复制到多个备库, 唤醒多个复制线程发送事件的开销将会累加; 复制解决的问题 数据分布: 可以在不同的地理位置来分布数据备份; 负载均衡: 通过MySQL复制可以将读操作分布到多个服务器上, 实现对读密集型应用的优化, 并且实现很方便, 通过简单的代码修改就能实现基本的负载均衡, 备份: 对于备份来说, 复制是一项很有意义的技术补充; 高可用性和故障切换: 复制能够帮助应用程序避免MySQL单点失败, 一个包含复制的设计良好的故障切换系统能够显著地缩短宕机时间; MySQL升级测试: 这种做法比较普遍, 使用一个更高版本的MySQL作为备库, 保证在升级全部实例前, 查询能够在备库按照预期执行; 复制原理概述 简单来说, MySQL的复制有如下三个步骤 在主库上把数据更改记录到二进制日志(Binary Log)中(这些记录被称为二进制日志事件) 备库将主库上的日志复制到自己的中继日志(Relay Log)中 备库读取中继日志中的事件, 将其重放到备库数据之上 高性能MySQL中用下图描述了上面三步 二进制日志记录格式 事实上, MySQL支持两种复制方式: 基于行的复制 和 基于语句的复制; 这两种复制方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制(这也就意味着, 在同一时间点, 备库上的数据可能与主库存在不一致, 并且无法保证主备之间的延迟, 一些大的语句可能导致备库产生几秒,几分钟甚至几个小时的延迟) 这两种方式主要是指在主库在记录二进制日志时所采用的日志格式(Binary Logging Formats), 其实有 STATEMENT, ROW, MIXED 三种配置; 基于语句的日志记录: 早在MySQL3.23版本中就存在; 可以通过使用 --binlog-format = STATEMENT 启动服务器来使用此格式; 基于行的日志记录: 在5.1版本中才被加进来(在5.0之前的版本中是只支持基于语句的复制); 可以通过以 --binlog-format = ROW 启动它来使服务器使用基于行的日志记录; 基于混合日志记录: 对于混合日志记录, 默认情况下使用基于语句的日志记录, 但在某些情况下, 日志记录模式会自动切换为基于行的; 当然, 您可以通过使用--binlog-format = MIXED选项启动mysqld来显式使用混合日志记录; 优缺点未完待续~~ 三个线程 MySQL使用3个线程来执行复制功能(其中1个在主服务器上, 另外两个在从服务器上); 当从服务器发出START SLAVE时, 从服务器创建一个I/O线程, 以连接主服务器并让它发送记录在其二进制日志中的语句; 主服务器创建一个binlog dump线程, 将二进制日志中的内容发送到从服务器; 从服务器I/O线程读取主服务器Binlog dump线程发送的内容, 并将该数据拷贝到从服务器的中继日志中; 第3个线程是从服务器的SQL线程, 是从服务器创建用于读取中继日志并执行日志中包含的更新; 问题?未完待续~~ 配置复制 master服务器上进行sql写操作的时候, 是会引起磁盘变化的; 所以slave服务器要想和master上的数据保持一致, 可以有两种办法: slave按照master服务器上每次的sql写语句来执行一遍; slave按照master服务器磁盘上的变化来做一次变化 ; 主服务器master上的写操作都会被记录到binlog二进制日志中, 从服务器slave去读主服务器的binlog二进制日志, 形成自己的relay中继日志, 然后执行一遍 ; 所以主从配置需要做到 主服务器要配置binlog二进制 从服务器要配置relaylog(中继日志) master要授予slave账号: 从服务器如何有权读取主服务器的 binlog (binlog非常敏感, 不可能让谁去随便读) 从服务器用账号连接master 从服务器一声令下开启同步功能 start slave 注意: 一般会在集群中的每个sql服务器中加一个server-id来做唯一标识 ; 配置启动主从 主服务器配置 1234#主从复制配置server-id=4 #服务器起一个唯一的id作为标识log-bin=mysql-bin #声明二进制日志文件名binlog-format= #二进制日志格式 mixed,row,statement 主服务器的 binlog二进制日志 有三种记录方式 mixed, row, statement ; statement: 二进制记录执行语句, 如 update….. row: 记录的是磁盘的变化 如何选择 binlog二进制日志 记录方式? update salary=salary+100; // 语句短, 但影响上万行, 磁盘变化大, 宜用statement update age=age+1 where id=3; // 语句长而磁盘变化小, 宜用row 你要是拿不准用哪个? 那就设置为mixed, 由系统根据语句来决定; 从服务器配置 首先从服务器肯定要开启relaylog日志功能 ; 从服务器一般也会开启binlog, 一方面为了备份, 一方面可能还有别的服务器作为这台从服务器的slave ; 主从之间建立关系 : 主服务器上建立一个用户: grant replication client,replication slave on *.* to repl@&#39;192.168.56.%&#39; identified by &#39;repl&#39; 告诉从服务器要连接哪个主服务器: 在从服务器上进入mysql执行如下语句 1234567reset slave #可以把之前的从服务器同步机制重置一下change master to master_host=&apos;192.168.56.4&apos;,master_user=&apos;repl&apos;,master_password=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,#当前主服务器产生的binlog走到哪儿了(需要在主服务器上`show maste status`来查看file名和position指针位置)master_log_pos=349 然后查看从服务器的slave状态 show slave status 发现已经连上主服务器了 从服务器中启动slave: start slave 注意: 可以一主多从, 但一个从有多个主就会傻逼了 ; 此时我们做的只是mysql主从复制, 但不是读写分离, 距离读写分离还差一小步; 因为读写分离还需要对sql语句进行判断(可以在php层面判断sql语句进行路由, 决定哪种sql去哪个服务器) 可以参考本人有道笔记上的记录 https://www.cnblogs.com/clsn/p/8150036.html","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"31. Lua脚本","slug":"redis/2017-06-29-redis-31","date":"2017-06-29T10:40:07.000Z","updated":"2018-03-10T12:12:08.000Z","comments":true,"path":"2017/06/29/redis/2017-06-29-redis-31/","link":"","permalink":"http://blog.renyimin.com/2017/06/29/redis/2017-06-29-redis-31/","excerpt":"","text":"前言 之前已经了解到, 在redis事物中, 为了检测事务即将操作的keys, 是否有多个clients同时改变而引起冲突, 这些keys将会在事务开始前使用 watch 被监控; 如果至少有一个被监控的key在执行exec命令前被修改, 则事务会被打断, 不执行任何动作, 从而保证原子性操作。 之前的这种方案是在解决一种叫做 CAS(check-and-set) (检查后再设置) 的问题, 这种问题也可以使用 Lua 脚本来进行解决; Lua 脚本功能是Reids 2.6版本的最大亮点, 通过内嵌对Lua环境的支持, Redis解决了长久以来不能高效地处理CAS(check-and-set检查后再设置) 命令的缺点, 并且可以通过组合使用多个命令, 轻松实现以前很难实现或者不能高效实现的模式。 脚本的原子性Redis 使用单个 Lua 解释器去运行所有脚本,并且Redis 也保证脚本会以原子性(atomic)的方式执行;(当某个脚本正在运行的时候,会有其他脚本或 Redis 命令被执行, 这和使用 MULTI / EXEC 包围的事务很类似, 在其他别的客户端看来，脚本的效果要么是不可见的，要么就是已完成的;另一方面其实也意味着, 执行一个运行缓慢的脚本并不是一个好主意, 写一个跑得很快很顺溜的脚本并不难, 因为脚本的运行开销(overhead)非常少, 但是当你不得不使用一些跑得比较慢的脚本时, 请小心, 因为当这些蜗牛脚本在慢吞吞地运行的时候, 其他客户端会因为服务器正忙而无法执行命令; Redis中执行Lua脚本的好处 Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令; (一个Lua脚本中的命令, 就相当于类似Redis中GETSET这种原生命令, 是具备原子性的) Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这些命令常驻在Redis内存中，实现复用的效果; Lua脚本可以将多条命令一次性打包，有效地减少网络开销; 不应该使用Lua什么时候不应该把脚本嵌入到Redis里面?因为Redis的实现是堵塞的, 即, 一个Redis server, 在同一时候, 只能执行一个脚本;因此, 如果你写了一个逻辑非常复杂的脚本, 这个脚本的执行时间非常长, 这样, 这个时候如果有别的请求进来, 就只能排队, 等待这个脚本结束了, 这个Redis server才能处理下一个请求。在这个时候, 就连仅仅是获取数据的命令都会被堵住。这个Redis-Server的效率就会被大大的拖慢, 因此, 如果你的脚本执行非常复杂耗时, 那么这个时候你是不应该把它放在Redis里面执行的。 Lua 基础hello.lua hello.lua: 这个Lua脚本比较简单, 仅仅返回一个字符串, 没有与redis-Server进行比较有意义的操作(比如获取或设置数据) 12local msg = &quot;Hello, world!&quot; --定义了一个本地变量msg存储我们的信息return msg 保存这个文件到hello.lua, 运行: 123456 renyimindembp:test renyimin$ redis-cli eval \"$(cat /Users/renyimin/Desktop/test/test.lua)\" 0 \"Hello, world!\"-- 下面这种方法也行 renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0 \"Hello, world!\" renyimindembp:test renyimin$ 运行这段代码会打印”Hello,world!” EVAL 指令 语法: VAL script numkeys key [key ...] arg [arg ...] script 参数: 是一段lua脚本程序, 它会被运行在Redis服务器上下文中, 可以直接写在命令行, 也可以引入.lua文件; numkeys 参数: 指定即将传入lua脚本中的key的个数 (如果没有参数的话, 也需要指明参数个数为0, 否则会报错) 从第三个参数开始的numkerys个参数, 就是你要传入脚本的键名参数, 这些键名参数可以在Lua脚本中通过全局变量KEYS数组(索引从1开始)的形式访问(如:KEYS[1], KEYS[2]); 剩下的在命令最后的附加参数 arg [arg ...], 可以在Lua中通过全局变量 ARGV 数组访问, 访问的形式和KEYS变量类似; 例子: (..是lua中的字符串拼接语法) 123127.0.0.1:6379&gt; EVAL &apos;return &quot;K1: &quot;..KEYS[1]..&quot; K2: &quot;..KEYS[2]..&quot; A1: &quot;..ARGV[1]..&quot; A2: &quot;..ARGV[2]&apos; 3 k1 k2 k3 a1 a2 a3 a4&quot;K1: k1 K2: k2 A1: a1 A2: a2&quot;127.0.0.1:6379&gt; 带宽和 EVALSHA EVAL 命令要求你在每次执行脚本的时候都发送一次脚本, 但是Redis有一个内部的缓存机制, 因此它不会每次都重新编译脚本, 这样, eval发送脚本主体就是在浪费带宽了; 为了减少带宽的消耗, Redis实现了 EVALSHA 命令, 它的作用和EVAL一样, 都用于对脚本求值但它接受的第一个参数不是脚本, 而是脚本的SHA1校验和(sum)(SHA1校验和 的生成看下一节SCRIPT LOAD); EVALSHA执行后, 如果服务器还记得给定的 SHA1校验和 所代表的脚本, 那么执行这个脚本; 如果服务器不记得给定的 SHA1校验和 所代表的脚本, 那么它返回一个特殊的错误, 提醒用户使用EVAL代替EVALSHA以下是示例： 123456789101112// 现在是有a这个key的127.0.0.1:6379&gt; get a&quot;haha&quot;// 开始测试127.0.0.1:6379&gt; SCRIPT LOAD &quot;return redis.call(&apos;GET&apos;,&apos;a&apos;)&quot;&quot;8a2f221803757b26fc7d283bec7ba834d91202c9&quot;127.0.0.1:6379&gt; evalsha 8a2f221803757b26fc7d283bec7ba834d91202c9 0&quot;haha&quot;// 如下瞎写的 sha1校验和, 服务器就不认识了, 要你使用 eval 命令来直接使用脚本127.0.0.1:6379&gt; evalsha lalalala 0(error) NOSCRIPT No matching script. Please use EVAL.127.0.0.1:6379&gt; SCRIPT LOAD如果使用 EVALSHA 发送校验和给服务器, 从而调用正确脚本的话, 这个 校验和 如何生成? 这就要用到 SCRIPT LOAD 命令了;SCRIPT LOAD: 将脚本加载到脚本缓存, 而不执行它 (在将指定的命令加载到脚本缓存中之后, 之后你将需要使用EVALSHA命令和脚本的正确SHA1摘要来调用它) 该脚本被保证永远留在脚本缓存中(除非调用SCRIPT FLUSH) lua调用redis命令 Lua脚本中可以使用两个不同的Lua函数来调用Redis的命令 redis.call() redis.pcall() redis.call() 与 redis.pcall()的区别 这两个命令很类似, 他们唯一的区别是当redis命令执行结果返回错误时, redis.call()将返回给调用者一个错误; 而redis.pcall()会将捕获的错误以Lua表的形式返回; 测试 (redis.call() 和 redis.pcall() 两个函数的参数可以是任意的 Redis 命令) 12345127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;set&apos;,&apos;foo&apos;,&apos;bar&apos;)&quot; 0OK127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;get&apos;,&apos;foo&apos;)&quot; 0&quot;bar&quot;127.0.0.1:6379&gt; .lua 脚本文件 其实写lua脚本还是有很多注意点的, 可以参考中纯函数脚本这一小节; 本篇学习主要是将来能写一些简单的原子命令, 并不会涉及一些复杂的逻辑操作, 所以只是简单了解了一下上面那些注意点; 演示在redis设置一个num为10的库存 set num 10; 下面通过lua脚本写一个原子操作, 将检测库存和最终减少库存放在一起 123456local good = redis.call(&apos;get&apos;, &apos;num&apos;);if tonumber(good) &gt; 0then redis.call(&apos;decr&apos;, &apos;num&apos;)endreturn &apos;ok&apos; 调用该脚本 12renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0&quot;ok&quot; 会正常减少数字 laravel中实现如下 出现超卖的代码 123456789101112131415/** * 使用redis模拟并发超卖 * 由于check和set是分两步执行的 */public function redisOversell()&#123; $good = Redis::get(&apos;num&apos;); if ($good &gt; 0) &#123; Redis::multi(); usleep(500000); //预先已经设置好库存为10个了 Redis::decr(&apos;num&apos;); Redis::exec(); &#125;&#125; 定义脚本: 将check 和 set放到一个脚本里(但这个脚本由于没有设置sleep, 所以只能得出理论上是没有问题的) 123456protected $lua = &quot; local good = redis.call(&apos;get&apos;, &apos;num&apos;);&quot; . &quot; if tonumber(good) &gt; 0 then&quot; . &quot; redis.call(&apos;decr&apos;, &apos;num&apos;)&quot; . &quot; end&quot; . &quot; return &apos;ok&apos;&quot;; 执行 1234567/** * lua脚本解决超卖 */public function redisLua()&#123; Redis::eval($this-&gt;lua, 0);&#125; 参考https://segmentfault.com/a/1190000009811453#articleHeader0https://www.cnblogs.com/huangxincheng/p/6230129.htmlhttps://segmentfault.com/a/1190000007615411","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"24. Redis Replication 搭建","slug":"redis/2018-04-06-redis-24","date":"2017-06-27T11:43:11.000Z","updated":"2019-02-27T06:19:05.000Z","comments":true,"path":"2017/06/27/redis/2018-04-06-redis-24/","link":"","permalink":"http://blog.renyimin.com/2017/06/27/redis/2018-04-06-redis-24/","excerpt":"","text":"动态配置 动态配置有两种方式: 在启动新的redis实例的时候, 设置这个新的实例为某个redis主服务器的Slave: redis-server --port 6380 --slaveof 127.0.0.1 6379 对一个已经启动的redis服务发送命令: slaveof ip port, 则会将当前服务器从 Master 修改为别的服务器的 slave 尝试向从服务器发送写命令, 发现不会成功 在master上添加一个键值对, 发现从服务器上立马就会同步到这条数据 配置文件配置 为每个准备启动的redis从服务, 创建各自的配置文件, 然后在启动redis新实例的时候, 为新实例指定各自的配置文件即可 比如一个6380的从实例, 其配置文件为: redis-server /usr/local/redis/etc/redis-6380.conf, 配置文件内容如下: 12slaveof 127.0.0.1 6379port 6380 QPS 10万+ 比如一个电商平台的详情页, 要做到超高并发, QPS上十万, 甚至是百万, 一秒钟百万的请求量, 不可避免地就要把底层的缓存搞得很好 首先, 光是redis自然是不够的, 但是redis是整个大型的缓存架构中, 支撑高并发的架构里面, 非常重要的一个环节, 如果redis要支撑超过10万+的并发, 那应该怎么做? 单机的redis几乎不太可能说QPS超过10万+, 一般在1万+, 除非一些特殊情况, 比如你的机器性能特别好, 配置特别高, 物理机, 维护做的特别好, 而且你的整体的操作不是太复杂 要做到高并发, 提高QPS, 可以将redis做主从架构(读写分离) 一般来说, 对缓存, 都是用来支撑读高并发的, 写的请求是比较少的, 可能写请求也就一秒钟几千, 一两千, 大量的请求都是读, 一秒钟二十万次读 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 redis主从架构 -&gt; 读写分离架构 -&gt; 可支持水平扩展的读高并发架构 sentinel哨兵引出sentinel 之前的主从配置可以为我们解决一个redis实例读压力大的问题, 可以在一台服务器上进行主从多实例的配置, 也可以在不同机器之间进行主从配置; 如果slave服务器挂掉, 只是读性能会下降, 现在的问题是, 如果一旦主redis服务器(也就是master实例)挂了, 你目前貌似只能手动去把这台master实例启动起来; 如果确实这台机器/实例就是启动不起来的话, 那你可能就需要手动去将当前的某一个slave实例切换为master:slaveof no one //先把这个slave实例变成一个master实例slaveof ip port //在之前的从实例中执行, 将他们的master重新切换到这个新的master上 这种手动操作明显不可能被接收, 我们需要当master主挂了之后, 自动有一个slave能够勇于承担地顶上来(因为slave相对于之前的master除了分担读压力外, 还是之前master的备份), 这就引出了redis的sentinel哨兵; 简介 sentinel哨兵 是redis官方提供的高可用解决方案, 可以用它来管理多个redis服务的实例; 之前在编译安装好redis之后, 就可以在 /usr/local/redis/src/ 目录下看到 redis-sentinel 等很多命令; sentinel的监控: 它会不断地检查master和slaves是否正常; 一个sentinel可以监控任意多个master及其下的slaves; 当然, sentinel也可能挂掉, 也有单点问题 (还好Sentinel是一个分布式系统, 可以在一个架构中运行多个Sentinel进程, 他们可以组成一个sentinel网络, 之间是可以互相通信的, 可以通过”投票”来决定master是否挂了) 当一个sentinel认为被监控的服务已经下线时, 它会向网络中的其他sentinel进行确认, 判断该服务器是否真的下线; 如果下线的是一个主服务器, 那么sentinel将会对下线的主服务器进行 自动故障转移通过将下线主服务器的某个从服务器提升为新的主服务器;并将下线主服务器下的从服务器重新指向新的主服务器;来让系统从新回到正常; 之前的master下线后, 如果重新上线了, sentinel会让它作为一个salve, 去新的master中同步数据 实战 启动sentinel: 将 /usr/local/src/ 目录下的redis-sentinel程序文件复制到 /usr/local/redis/bin 目录下; 启动一个运行在sentinel模式下的redis服务实例(两种方式): redis-sentinel 或者 redis-server /usr/local/redis/sentinel.conf --sentinel sentinel配置 Sentinel之间的自动发现机制虽然sentinel集群中各个sentinel都互相连接彼此来检查对方的可用性以及互相发送消息, 但其实你是不用在任何一个sentinel中配置任何其它的sentinel的节点的, 因为sentinel利用了master的发布/订阅机制去自动发现其它也监控了同一master的sentinel节点。 同样, 你也不需要在sentinel中配置某个master的所有slave的地址, sentinel会通过询问master来得到这些slave的地址的。 https://www.cnblogs.com/leeSmall/p/8398401.htmlhttps://www.cnblogs.com/kismetv/p/9236731.html#t1","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"22","slug":"redis/2017-06-25-redis-22","date":"2017-06-25T08:35:21.000Z","updated":"2019-02-22T05:30:14.000Z","comments":true,"path":"2017/06/25/redis/2017-06-25-redis-22/","link":"","permalink":"http://blog.renyimin.com/2017/06/25/redis/2017-06-25-redis-22/","excerpt":"","text":"Redis内存不足时 LRU和AOF持久化文件","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"12. Redis -- 发布订阅","slug":"redis/2017-06-18-redis-12","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-09T09:55:58.000Z","comments":true,"path":"2017/06/17/redis/2017-06-18-redis-12/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/redis/2017-06-18-redis-12/","excerpt":"","text":"发布订阅其实 和 队列 非常相似: 这里的 频道, 就类似于任务队列中所讨论的 队列; 而订阅者就类似于 任务队列中所讨论的 消费者; 但他们还是有区别的: 对于队列来说, 即使消费者不在线, 消息也一直在队列中存放着, 等待消费者上线后进行处理; 而对于 发布订阅 中的 订阅者来说, 如果消息发布到频道中时, 订阅者不在线(没有一直阻塞等待), 那么这条消息也不会为它保存着; 另外对于队列来说, 其中的消息只要被一个消费者处理了, 其他消费者就不用处理了; 而对于发布订阅来说, 向频道中发布一条消息, 则所有在线的订阅者都可以收到这个消息; (貌似是比较是个做聊天室之类的东西) 实例可以参考有道笔记","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"11. Redis -- task queue","slug":"redis/2017-06-17-redis-11","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-16T06:08:00.000Z","comments":true,"path":"2017/06/17/redis/2017-06-17-redis-11/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/redis/2017-06-17-redis-11/","excerpt":"","text":"前言现在有很多专门的队列软件(如ActiveMQ, RabbitMQ等)，在缺少专门的任务队列可用的情况下, 也可以使用Redis的队列机制; FIFO 先进先出队列 Redis的列表结构, 允许通过 RPUSH和LPUSH以及RPOP和LPOP, 从列表的两端推入和弹出元素。 假设一个操作中, 有向用户发送电子邮件这一功能, 由于这一功能可能会有非常高的延迟,甚至可能会出现发送失败, 所以这里就不能采用平时常见的代码流方式来执行这个邮件发送操作; 为此, 可以使用 任务队列 来记录 “邮件的收信人,邮件内容,发送邮件的原因”, 以 先到先服务 的方式发送邮件(此处采用的是 从右推入队列, 从左弹出元素) 队列: redis服务中的列表类型就充当了队列服务 消费者: 阻塞等待 (在Laravel将其做成命令进行启动, 该消费者就会阻塞等待) 12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email&apos;], 0); return $res;&#125; 生产者: 直接访问如下代码推送多条消息 1234567891011121314151617181920public function rPush()&#123; $data1 = json_encode([ &apos;user_id&apos; =&gt; 11, &apos;content&apos; =&gt; &apos;最近推出新款汽车--宝马, 售价:$12W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data2 = json_encode([ &apos;user_id&apos; =&gt; 12, &apos;content&apos; =&gt; &apos;最近推出新款汽车--奔驰, 售价:$15.8W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data3 = json_encode([ &apos;user_id&apos; =&gt; 13, &apos;content&apos; =&gt; &apos;最近推出新款汽车--大众, 售价:$20W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); //一旦运行就会阻塞起来 Redis::rpush(&apos;queue:email&apos;, [$data1, $data2, $data3]);&#125; 最终消费者正常拿到消息, 并且顺序也是正确的; 多种任务的队列 一般情况下, 我们为每种任务单独使用一个队列的; 但如果有一个队列处理多种任务的场景, 实现起来也很方便; 只用在消息中指明消息所需要调用的回调函数即可; priority 优先级队列优先级队列其实在redis中可以依靠 BRPOP和BLPOP的特性; 当 BLPOP 被调用时, 如果给定key(队列)列表中, 至少有一个非空列表, 那么弹出遇到的第一个非空列表的头元素, 并弹出元素所属的列表名字一起, 组成结果返回给调用者; 也就是说, BLPOP 给定的队列列表中, 靠前的就是优先级高的;假设你有 ‘重置密码的邮件’, ‘提醒邮件’, ‘发广告的邮件’, 三种队列, 如果你期望他们按照优先级依次排列, 那么只用设置为:12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email:resetpwd&apos;, &apos;queue:email:warning&apos;, &apos;queue:email:advertisement&apos;], 0); return $res;&#125; 想优先推送的, 你就放入第一个队列’queue:email:resetpwd’中(BRPOP)也类似 也可参考: http://blog.csdn.net/woshiaotian/article/details/44757621 延迟队列未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"15. redis集群","slug":"redis/2017-06-18-redis-15","date":"2017-06-15T14:37:42.000Z","updated":"2018-03-08T07:54:10.000Z","comments":true,"path":"2017/06/15/redis/2017-06-18-redis-15/","link":"","permalink":"http://blog.renyimin.com/2017/06/15/redis/2017-06-18-redis-15/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"08. redis锁 -- 不太安全?","slug":"redis/2017-06-12-redis-08","date":"2017-06-12T08:35:08.000Z","updated":"2018-10-12T11:22:39.000Z","comments":true,"path":"2017/06/12/redis/2017-06-12-redis-08/","link":"","permalink":"http://blog.renyimin.com/2017/06/12/redis/2017-06-12-redis-08/","excerpt":"","text":"前言一般来说, 在对数据进行”加锁”时, 程序首先需要获取锁来得到对数据进行排他性访问的能力, 然后才能对数据执行一系列操作, 最后还要将锁释放给其他程序;之前已经了解过, Redis使用WATCH命令来代替对数据进行加锁, 因为WATCH只会在数据被其他客户端抢先修改了的情况下通知执行了这个命令的客户端, 所以这个命令被称为乐观锁; SETEX 实现锁介绍为了对数据进行排他性访问, 程序首先要做的就是获取锁; 而 Redis 的 SETEX 命令天生就适合用来实现锁的获取功能; 这个命令只会在键不存在的情况下为键设置值, 而其他进程一旦发现键存在, 那就只能等待之前锁的释放; 准备环境 数据表准备 123456789DROP TABLE IF EXISTS `goods`;CREATE TABLE `goods` ( `id` int(10) NOT NULL AUTO_INCREMENT, `goods_name` varchar(100) NOT NULL, `num` int(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `goods` VALUES (1, &apos;iphone 6 plus&apos;, 10); 超卖代码: 12345678public function mysqlOverSell()&#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(500000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125;&#125; Jmeter压测配置: 结果发现超卖: (后来设置压测为每秒3个线程, 也超卖了1件) 使用redis的setex加锁 redis 123127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; 代码 123456789101112131415public function setnx()&#123; $lock = Redis::setnx(&apos;tag&apos;, 1); // 如果加锁成功, 则可以进行如下操作(其他客户端只能等待锁释放) if ($lock) &#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(200000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125; // 注意: 执行完成之后, 必须释放锁 Redis::del(&apos;tag&apos;); &#125;&#125; 修改jmeter访问路由, 重新测试, 发现确实不会出现超卖现象了 SETEX锁死锁 客户端的突然下线导致锁不被释放: 由于客户端即使在使用锁的过程中也可能会因为这样或那样的原因而下线, 所以为了防止客户端在取得锁之后崩溃, 并导致锁一直处于 已获取 状态; 所以网上有不少做法是 为锁加上 超时限制, 这样 如果获得锁的进程未能在指定时限内完成操作, 那么可能会认为客户端已经crash掉线, 所以锁将需要被自动释放; 但锁的这个超时时间又会带来新的问题 设置时间过长, 会导致吞吐量的严重下降 设置时间过短, 又会导致锁自动释放导致的问题 SETEX锁设超时限制 为锁加超时限制的普通方法如下 12345678$lock = Redis::setnx(&quot;tag&quot;, 1)if ($lock) &#123; // 如果在此处突然崩溃... Redis::expire(&quot;my:lock&quot;, 10); // ... do something Redis::del(&quot;my:lock&quot;)&#125; 如果客户端是在 Redis::expire(&quot;my:lock&quot;, 10); 之前就崩溃, 锁不被释放的问题还是存在; 从 setnx 到 set 所以, 从redis2.6.12开始(set新增了可选选项), 官方建议使用set命令替代setnx来实现锁, 如下 12345if (Redis::set(&quot;tag&quot;, 1, &quot;nx&quot;, &quot;ex&quot;, 10)) &#123; ... do something Redis::del(&quot;tag&quot;)&#125; 网上有说 需要评估业务的复杂度, 来设置超时时间 - 如果设置过短会导致 释放了其他进程的锁如果持有锁的进程A因为操作时间过长, 而导致锁超时被自动释放, 这样的话, 又会导致其他进程在进程A尚未结束时获取锁, 这样还是会导致并发的出现;另外, 由于锁是自动被释放的, 进程A并不知道, 这样就会导致, 进程A在后续执行完成任务之后, 在做 释放锁 的操作时, 如果只是简单的 Redis::del(&quot;tag&quot;&quot;), 如果正好有其他进程获取了锁, 这就会导致进程A释放的并不是自己的锁, 而是释放掉其他进程持有的锁; 因此, 在获取锁的时候, 需要设置一个 token, 放入自己的锁中, 在释放锁的时候, 用来保证释放的是自己的锁; 但纵使是这样, 还是导致了并发的出现, 并未解决最根本的问题! 锁释放注意 如果客户端A是因为执行超时, 而导致锁被自动释放, 那么当客户端A最后在释放锁时, 可能此时客户端B已经加上了自己的锁, 所以在锁释放时需要做两个操作 检查token是否一致 释放锁 注意: 在上面两步之间, 比如说刚检查完token, 确认了token是当前进程的锁之后, 也还是有可能发生超时而自动释放锁, 导致锁token被换上别的客户端的, 所以 释放锁 这一步应该放在事务中, 并提前用watch监控代表锁的那个key, 伪代码如下: 123456Redis::watch(&apos;tag&apos;);if (Redis::get(&apos;tag&apos;) == $token) &#123; Redis::multi(); Redis::del(&apos;tag&apos;); Redis::exec();&#125; 多个客户端同时获取锁 假设客户端A超时后, 锁被自动释放, 此时客户端B拿到了锁, 如果客户端B也因为超时导致锁被释放(此时客户端A还没执行完, B也没执行完), 那么客户端C也能拿到锁; 可以看到甚至同时存在3个(还可能n个)客户端拿到了锁; 所以这个问题貌似就是一直没解决的问题!! 终极办法参考:基于Redis的分布式锁到底安全吗(上)https://segmentfault.com/q/1010000013626041?_ea=3427544~~未完待续","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"07. redis事务与WATCH乐观锁","slug":"redis/2017-06-11-redis-07","date":"2017-06-11T13:14:46.000Z","updated":"2018-03-16T05:41:21.000Z","comments":true,"path":"2017/06/11/redis/2017-06-11-redis-07/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/redis/2017-06-11-redis-07/","excerpt":"","text":"Redis事务与单线程 因为Redis是单线程的, 所以即使多个客户端同时来对同一数据发来很多命令, 也会被串行挨个执行; 但是需要注意的是, 这和mysql的srialize隔离级别一样, 即使是串行执行命令, redis也逃不过高并发事务时的 更新丢失 问题; mysql是使用乐观锁, 悲观锁来解决的 参考MySQL高并发事务问题 及 解决方案 而redis的事务也是通过与WATCH(乐观锁)的结合才得以解决这个问题 Redis事务与WATCH 演示Redis事务在客户端高并发时出现的 丢失更新 问题 redis 为了解决高并发事务时这种 丢失更新 的问题, 提供了 WATCH WATCH介绍 redis 并没有实现典型的加锁功能(比如MySQL中, 在访问以写入为目的数据时 SELECT FOR UPDATE), 因为加这种悲观锁可能会造成长时间的等待; 所以redis为了尽可能地减少客户端的等待时间, 采用了WATCH监控机制, 如果某个客户端A在事务开始之前 WATCH 了一个key, 那么其实就相当于对该key加了乐观锁 事务中正常执行你要执行的操作 (乐观地认为不会有其他客户端抢在你前面去改动那个key) 直到当客户端A真正exec的时候, 才会验证客户端A之前WATCH(监控)的key是否被变动过, 如果变动过, 则客户端A可以进行重试; 测试:","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"06. Redis 事务","slug":"redis/2017-06-11-redis-06","date":"2017-06-11T11:10:03.000Z","updated":"2019-03-25T07:43:32.000Z","comments":true,"path":"2017/06/11/redis/2017-06-11-redis-06/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/redis/2017-06-11-redis-06/","excerpt":"","text":"Redis事务介绍 redis事务是使用队列以先进先出(FIFO)的方法保存命令, 较先入队的命令会被放到数组的前面, 而较后入队的命令则会被放到数组的后面 redis事务从开始到结束通常会通过三个阶段: 事务开0始 命令入队 事务执行 Redis事务通常会使用 MULTI, EXEC, WATCH等命令来完成 redis事务的ACID特性在redis中, 事务具有 原子性(Atomicity) , 一致性(Consistency) 和 隔离性(Isolation);并且当redis运行在某种特定的持久化模式下,事务也具有 持久性(Durability); (弱)原子性 对于redis的事务来说, 事务队列中的命令也是要么就全部执行, 要么就一个都不执行, 因此redis的事务是具有原子性的; 不过需要注意的是: redis事务的原子性有两种情况需要区分 一种是 语法错误导致redis事务执行出错, 比如, 你事务中的某条命令语法错误, 那么你在exec的时候, 所有的命令都不会执行: 1234567891011127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age(error) ERR wrong number of arguments for &apos;set&apos; command127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; 另一种是 无语法错误,可以运行成功, 但是运行完之后会返回运行错误, 这种情况下, 错误之前的命令不会回滚; 123456789101112131415161718192021222324//比如命令或命令的参数格式错误,那么事务就会出现有可能部分命令成功,而部分命令失败的情况127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age 10QUEUED//这里就会出现运行错误127.0.0.1:6379&gt; incr nameQUEUED127.0.0.1:6379&gt; set addr yunchengQUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) (error) ERR value is not an integer or out of range4) OK127.0.0.1:6379&gt; keys *1) &quot;addr&quot;2) &quot;age&quot;3) &quot;name&quot;127.0.0.1:6379&gt; 需要说明的是: 上述的第二种情况, 从表面上看, 不符合”原子性”, 因为你所执行的命令中有一条出错后, redis并没有进行回滚; 其实是由于你的此种错误命令不属于语法错误, 对redis来说, 不会导致执行出错,所以你的这条错误命令是可以执行成功的, 不过成功后会返回错误提示, 所以redis并不会进行回滚; redis的作者在事务相关的文档中解释说:1234不支持事务回滚是因为这种复杂的功能和redis追求的简单高效的设计主旨不符合,并且他认为, redis事务的执行时错误通常都是编程错误造成的,这种错误通常只会出现在开发环境中, 而很少会在实际的生产环境中出现,所以他认为没有必要为redis开发事务回滚功能。 所以, 在事务中执行redis命令时, 最好确保所有命令都能执行成功; 一致性redis通过谨慎的错误检测和简单的设计来保证事务一致性。 隔离性 事务的隔离性指的是, 即使数据库中有多个事务并发在执行, 各个事务之间也不会互相影响, 并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同; 因为redis使用单线程的方式来执行事务(以及事务队列中的命令), 并且服务器保证, 在执行事务期间不会对事务进行中断, 因此, redis的事务总是以串行的方式运行的, 并且事务也总是具有隔离性的; Redis为单进程单线程模式, 采用队列模式将并发访问变为串行访问(Redis本身没有锁的概念, Redis对于多个客户端连接并不存在竞争) 持久性因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能, 所以redis事务的耐久性由redis使用的持久化模式(rdb/aof)来决定: 关于持久化, 可以参考博文:了解redis持久化","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"01. MySQL 知识点","slug":"MySQL 暂停/2017-06-10-mysql-01","date":"2017-06-10T03:02:51.000Z","updated":"2019-08-27T11:04:20.000Z","comments":true,"path":"2017/06/10/MySQL 暂停/2017-06-10-mysql-01/","link":"","permalink":"http://blog.renyimin.com/2017/06/10/MySQL 暂停/2017-06-10-mysql-01/","excerpt":"","text":"安装权限https://www.cnblogs.com/jackruicao/p/6068821.html?utm_source=itdadao&amp;utm_medium=referral 几个重要文件数据库层面 my.cnf (MySQL参数文件) error Log (MySQL 错误日志) slow Query Log (慢查询日志) general Query Log (全量日志) binary Log (二进制日志) relay Log (中继日志) audit Log (审计日志) 存储引擎层面 redo log undo log MySQL Storage Engines MyISAM InnoDB https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html MySQL 事务, 隔离级别, MVCC相关MySQL Optimization- [参考 手册](https://dev.mysql.com/doc/refman/5.6/en/optimization.html) - 索引优化 - 查询优化 MySQL ReplicationMySQL 高性能, 高可用阿里云 RDS 版本 https://help.aliyun.com/knowledge_detail/49059.html https://dev.mysql.com/doc/refman/8.0/en/storage-engines.htmlhttps://dev.mysql.com/doc/refman/5.6/en/optimization.html 一些资料https://www.aliyun.com/ss/bXlzcWzlronoo4XmlZnnqIs/1_h 云服务器 ECS MySQL 编译安装支持 Innodb 引擎https://help.aliyun.com/knowledge_detail/41107.html?spm=5176.11065259.1996646101.searchclickresult.35c13f8dXRN0Rx 问题 自增主键的优缺点https://blog.csdn.net/yixuandong9010/article/details/72286029自增主键这种方式是使用数据库提供的自增数值型字段作为自增主键，它的优点是：（1）数据库自动编号，速度快，而且是增量增长，按顺序存放，对于检索非常有利；（2）数字型，占用空间小，易排序，在程序中传递也方便；（3）如果通过非系统增加记录时，可以不用指定该字段，不用担心主键重复问题。其实它的缺点也就是来自其优点，缺点如下：（1）因为自动增长，在手动要插入指定ID的记录时会显得麻烦，尤其是当系统与其它系统集成时，需要数据导入时，很难保证原系统的ID不发生主键冲突（前提是老系统也是数字型的）。特别是在新系统上线时，新旧系统并行存在，并且是异库异构的数据库的情况下，需要双向同步时，自增主键将是你的噩梦；（2）在系统集成或割接时，如果新旧系统主键不同是数字型就会导致修改主键数据类型，这也会导致其它有外键关联的表的修改，后果同样很严重；（3）若系统也是数字型的，在导入时，为了区分新老数据，可能想在老数据主键前统一加一个字符标识（例如“o”，old）来表示这是老数据，那么自动增长的数字型又面临一个挑战。 select * 的问题? 废话, 纵使InnoDB有聚簇索引, 但是如果二级索引能够做到索引覆盖, 那岂不是更快! MySQL 基础知识点数据类型各种语句","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]}]}