{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"33. 为什么需要泛型类","slug":"JAVA/2018-12-22-34","date":"2018-12-22T05:43:51.000Z","updated":"2018-12-22T14:30:20.000Z","comments":true,"path":"2018/12/22/JAVA/2018-12-22-34/","link":"","permalink":"http://blog.renyimin.com/2018/12/22/JAVA/2018-12-22-34/","excerpt":"","text":"安全隐患 在没有使用泛型的情况下, 如果要实现参数 任意化, 通常会定义成Object类型来接受, 然后在使用时再进行强制类型转换使用; 而强制类型转换有明显的缺点, 就是必须要知道实际参数的具体类型的情况才可以进行转换, 同时在强制转换的过程中, 编译器不会报错提示的, 只有在运行阶段才会出现异常, 一定程度上存在安全隐患; 示例, 使用数组模拟一个简单的 ArrayList 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package generic;/** * 使用数组模拟一个ArrayList * 功能非常简单, 只是为学习泛型相关知识 */class ArrayListObject &#123; private int size; private int index; private Object[] array; public ArrayListObject(int size) &#123; this.size = size; array = new Object[size]; &#125; public boolean add(Object element) &#123; if (index &lt; size) &#123; array[index++] = element; return false; &#125; else &#123; return true; &#125; &#125; public Object get(int index) &#123; return array[index]; &#125; public Integer size() &#123; return size; &#125;&#125;public class ObjectTest&#123; public static void main(String[] args) &#123; ArrayListObject arrayListObject = new ArrayListObject(2); arrayListObject.add(&quot;renyimin&quot;); arrayListObject.add(100); for(int i = 0; i&lt; arrayListObject.size();i++)&#123; String item = (String)arrayListObject.get(i); System.out.println(&quot;泛型测试, item = &quot; + item); &#125; &#125;&#125; 上述代码运行后会报错: 由于ArrayList的底层使用的是数组, 所以要能够存放不同类型的对象, 只能将元素类型设定为Object; 这样在使用元素时, 需要进行强制类型转换, 并且安全隐患也是在运行时才会暴露出来, 在编译时并不会报错; 类膨胀问题 除了将元素类型定义为Object来实现参数 任意化, 来实现ArrayList可以存储不同类型的对象; 还可以为每种不同类型的对象各自实现一个ArrayList 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package generic;/** * 简单模拟一个存放整数的 ArrayList */class ArrayListInteger &#123; private int size = 0; private int index = 0; private Integer[] array; public ArrayListInteger(int size) &#123; this.size = size; array = new Integer[size]; &#125; public boolean add(Integer element) &#123; if (index &lt; size) &#123; array[index++] = element; &#125; return true; &#125; public Integer get(int index) &#123; return array[index]; &#125;&#125;/** * 简单模拟一个存放字符串的 ArrayList */class ArrayListString &#123; private int size = 0; private int index = 0; private String[] array; public ArrayListString(int size) &#123; this.size = size; array = new String[size]; &#125; public boolean add(String element) &#123; if (index &lt; size) &#123; array[index++] = element; &#125; return true; &#125; public String get(int index) &#123; return array[index]; &#125; public Integer size() &#123; return size; &#125;&#125;public class ClassBloat&#123; public static void main(String[] args) &#123; ArrayListInteger arrayListInteger = new ArrayListInteger(10); arrayListInteger.add(10); arrayListInteger.add(11); arrayListInteger.add(12); System.out.println(arrayListInteger.get(0)); ArrayListString arrayListString = new ArrayListString(10); arrayListString.add(&quot;renyimin&quot;); arrayListString.add(&quot;lant&quot;); arrayListString.add(&quot;rymuscle&quot;); System.out.println(arrayListString.get(0)); &#125;&#125; 很明显, 这样带来的严重问题是, 如果你的业务场景中出现类似问题, 那么你项目中的类文件会急剧增多, 出现类膨胀问题; 泛型类 对于上述问题, 可以引入泛型来进行解决, 代码如下: 1234567891011121314151617181920212223242526272829303132333435package generic;class ArrayListGeneric&lt;T&gt; &#123; private int size = 0; private int index = 0; private Object[] array; public ArrayListGeneric(int size) &#123; this.size = size; array = new Object[size]; &#125; public boolean add(T element) &#123; if (index &lt; size) &#123; array[index++] = element; &#125; return true; &#125; public T get(int index) &#123; return (T) array[index]; &#125;&#125;public class GenericTest&#123; public static void main(String[] args) &#123; ArrayListGeneric&lt;String&gt; arrayListGeneric = new ArrayListGeneric&lt;String&gt;(2); arrayListGeneric.add(&quot;renyimin&quot;); arrayListGeneric.add(100); &#125;&#125; 可以看到, IDE直接进行了错误提示, 并且运行代码会发现, 在编译阶段就报错了 泛型带来的好处 类型安全: 泛型主要是提供了一种类型检测的机制, 只有相匹配的数据才能正常的赋值, 否则编译器就不通过; 所以说, 它是一种类型安全检测机制, 一定程度上提高了软件的安全性, 防止出现低级的失误; 消除强制类型转换: 泛型的一个附带好处是, 它消除了代码中的许多强制类型转换, 提高了代码可读性, 减少了出错机会; 同时, 泛型也帮助普通类成为一个模板类, 然后当实例化该类时, 可以根据传入的类型实参, 创建出多个不同类型的对象; ArrayList回顾 之前已经学习过了ArrayList, 了解到其底层使用 数组 实现, 不过由于ArrayList中对数组元素类型使用的是Object, 所以虽然它底层采用的是数组, 你却可以往ArrayList中add多个不同类型的对象; 虽然ArrayList可以存放不同类型的对象元素, 但这样可能会导致当你对元素使用时, 需要进行强制类型转换, 所以代码可读性和安全性都会降低; 幸运的是, ArrayList 中使用了泛型, 如果你要往ArrayList中add数据元素, 最好在实例化时, 使用泛型类型; 一方面如果元素类型不一致的话, IDE会进行提示, 并且在编译阶段会将安全隐患暴露出来; 同时, 你在编译阶段就将ArrayList中的数据进行了类型统一, 所以不用在使用元素时做大量的类型检测和转换; 所以貌似在使用ArrayList, 我们仍然都会使用泛型来保证其存储的是同一类型的对象元素; 不能创建泛型数组 对于上面的适应泛型类来自己实现 ArrayList 的例子, 可能比较有争议的地方就是, 为什么没把数组的类型定义成泛型, 而是使用了Object ? (Java中对ArrayList的实现貌似也是如此) 其实这涉及到泛型的一个注意点: 在Java中, 你是不能直接创建泛型数组的 泛型解决方案出错, 这是因为在Java中, 不能直接创建泛型数组 (稍后再进行介绍)","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"30. Generics(泛型)","slug":"JAVA/2018-12-22-30","date":"2018-12-22T05:43:51.000Z","updated":"2018-12-22T14:11:51.000Z","comments":true,"path":"2018/12/22/JAVA/2018-12-22-30/","link":"","permalink":"http://blog.renyimin.com/2018/12/22/JAVA/2018-12-22-30/","excerpt":"","text":"概述 Java 泛型(generics) 是 JDK 5 中引入的一个新特性, 在java中有很重要的地位, 在面向对象编程及各种设计模式中有非常广泛的应用 泛型(Generics): 参数化类型, 将 类型 定义成 参数形式, 然后在调用时传入具体的类型(类型实参) 泛型提供了编译时类型安全检测机制, 该机制允许程序员在编译时检测到非法的类型 泛型的本质是为了参数化类型, 在泛型使用过程中, 数据类型被指定为一个参数, 这种参数类型可以用在 类、接口 和 方法中, 分别被称为 泛型类、泛型接口、泛型方法 为什么需要使用泛型?下面通过泛型类, 泛型接口, 泛型方法的学习来了解是使用泛型程序设计的好处 泛型类泛型接口泛型方法泛型写法注意 前后泛型的类型要一致, 即便前后的类型有继承关系也不可以: 12// 即便Student类继承自Person类List&lt;Person&gt; list = new ArrayList&lt;Student&gt;(); 虽然前后泛型的类型要一致, 但是在向集合中添加元素时, 可以添加泛型类型的子类型,: 12List&lt;Person&gt; list = new ArrayList&lt;Person&gt;();list.add(new Student()); 在jdk7推出了一个新特性, 泛型的菱形语法, 即 后面的泛型类型可以省略 (因为编译器可以从前面(List)推断出推断出类型参数, 所以后面的ArrayList之后可以不用写泛型参数, 只用一对空着的尖括号就可以): 12345// 在Java SE 7之前, 声明泛型对象的代码如下:List&lt;Person&gt; list = new ArrayList&lt;Person&gt;();// 而在Java 7中, 可以使用如下代码：List&lt;Person&gt; list = new ArrayList&lt;&gt;();list.add(new Student()); 泛型只支持对象类型, 如下是错误示例: 1List&lt;int&gt; list = new ArrayList&lt;int&gt;(); 无意义的泛型 1List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); 泛型类 泛型类型用于类的定义中, 被称为泛型类 一个简单示例: 123456789101112131415161718192021222324public class GenericTest3&lt;T&gt; &#123; private T key; public T getKey() &#123; return key; &#125; public GenericTest3(T key) &#123; this.key = key; &#125; public static void main(String[] args) &#123; //泛型的类型参数只能是类类型（包括自定义类）, 不能是简单类型 //传入的实参类型需与泛型的类型参数类型相同, 即为Integer. GenericTest3&lt;Integer&gt; genericInteger = new GenericTest3&lt;Integer&gt;(123456); //传入的实参类型需与泛型的类型参数类型相同, 即为String. GenericTest3&lt;String&gt; genericString = new GenericTest3&lt;String&gt;(&quot;key_vlaue&quot;); System.out.println(&quot;key is &quot; + genericInteger.getKey()); System.out.println(&quot;key is &quot; + genericString.getKey()); &#125;&#125; 定义的泛型类, 就一定要传入泛型类型实参么? 并不是这样, 在使用泛型的时候如果传入泛型实参, 则会根据传入的泛型实参做相应的限制, 此时泛型才会起到本应起到的限制作用; 如果不传入泛型类型实参的话, 在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型 1234567891011121314151617181920212223242526package jcf;public class GenericTest3&lt;T&gt; &#123; private T key; public T getKey() &#123; return key; &#125; public GenericTest3(T key) &#123; this.key = key; &#125; public static void main(String[] args) &#123; GenericTest3 generic = new GenericTest3(&quot;111111&quot;); GenericTest3 generic1 = new GenericTest3(4444); GenericTest3 generic2 = new GenericTest3(55.55); GenericTest3 generic3 = new GenericTest3(false); System.out.println(&quot;key is &quot; + generic.getKey()); // key is 111111 System.out.println(&quot;key is &quot; + generic1.getKey()); // key is 4444 System.out.println(&quot;key is &quot; + generic2.getKey()); // key is 55.55 System.out.println(&quot;key is &quot; + generic3.getKey()); // key is false &#125;&#125; 注意： 泛型的类型参数只能是类类型, 不能是简单类型; 不能对确切的泛型类型使用 instanceof 操作, 如下面的操作是非法的, 编译时会出错1if(ex_num instanceof Generic&lt;Number&gt;)&#123; &#125; 类型擦除 泛型是 Java 1.5 版本才引进的概念, 在这之前是没有泛型的概念的, 但显然, 泛型代码能够很好地和之前版本的代码很好地兼容。 这是因为, 泛型信息只存在于代码编译阶段, 在进入 JVM 之前, 与泛型相关的信息会被擦除掉, 专业术语叫做类型擦除; 通俗地讲, 泛型类和普通类在 java 虚拟机内是没有什么特别的地方, 对于下面的代码, 打印的结果为 true, 是因为 List 和 List 在 jvm 中的 Class 都是 List.class, 泛型信息被擦除了 1234List&lt;String&gt; l1 = new ArrayList&lt;String&gt;();List&lt;Integer&gt; l2 = new ArrayList&lt;Integer&gt;();System.out.println(l1.getClass() == l2.getClass()); 如果使用泛型, 编译器会在编译阶段就能够帮我们发现类似这样的问题 类型的参数化 (涉及到了自定义泛型类) 123456789101112131415161718192021222324252627package jcf;class A&lt;T&gt;&#123; private T name; public T getName() &#123; return name; &#125; public void setName(T name) &#123; this.name = name; &#125;&#125;public class GenericTest &#123; public static void main(String[] args) &#123; A&lt;String&gt; a = new A&lt;&gt;(); a.setName(&quot;renyimin&quot;); System.out.println(a.getName()); A&lt;Integer&gt; aa = new A&lt;&gt;(); aa.setName(100); System.out.println(aa.getName()); &#125;&#125; 泛型 使用泛型的好处 提高安全性: 将运行期的错误转换到编译期 省去强制转换的麻烦 减少类的数量, 将类做的更加通用一些 可以在定义类的同时, 预设一个占位符作为某种特定类型, 在类的内部方法和属性上就可以使用这个占位符来表示某种类型, 最后在构建new这个类对象的时候, 需要指明具体是哪个类型(本质上就是参数化类型, 把类型作为参数传递) 123456789101112131415161718192021222324252627package jcf; class A&lt;T&gt; &#123; private T name; public T getName() &#123; return name; &#125; public void setName(T name) &#123; this.name = name; &#125; &#125; public class GenericTest &#123; public static void main(String[] args) &#123; A&lt;String&gt; a = new A&lt;&gt;(); a.setName(&quot;renyimin&quot;); System.out.println(a.getName()); A&lt;Integer&gt; aa = new A&lt;&gt;(); aa.setName(100); System.out.println(aa.getName()); &#125; &#125; 泛型类 - 预设类型的命名 为了简化命名, 通常只用一个字母来命名类型参数, 常用字母如下: E: 代表Element (在集合中使用, 因为集合中存放的是元素)ArrayList用的就是E T: Type (Java 类) K V: 分别代表键值中的Key和Value N: Number(数值类型) ?: 表示不确定的类型 静态泛型方法不可以用类的泛型类型, 类的泛型类型是 new 的时候传入的但是静态方法可以用泛型 类型边界 extends 可以实现接口 既继承父类又实现接口 注意 &amp; 符号, 类在前, 接口在后 泛型接口泛型通配符 &lt;?&gt; 无边界? 和 Object 不一样么? Object 不接受子类 固定上边界通配符","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"26. Set的contains()方法","slug":"JAVA/2018-12-20-26","date":"2018-12-20T11:25:03.000Z","updated":"2018-12-23T09:24:47.000Z","comments":true,"path":"2018/12/20/JAVA/2018-12-20-26/","link":"","permalink":"http://blog.renyimin.com/2018/12/20/JAVA/2018-12-20-26/","excerpt":"","text":"https://blog.csdn.net/violet_echo_0908/article/details/50152915","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"23. HashMap 和 Hashtable 的区别","slug":"JAVA/2018-12-19-23","date":"2018-12-19T06:05:10.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/19/JAVA/2018-12-19-23/","link":"","permalink":"http://blog.renyimin.com/2018/12/19/JAVA/2018-12-19-23/","excerpt":"","text":"Hashtable 是个过时的集合类, 存在于Java API中很久了, 在Java 4中被重写了, 实现了Map接口, 所以自此以后也成了Java集合框架中的一部分 HashMap 和 Hashtable 都实现了Map接口, 但决定用哪一个之前先要弄清楚它们之间的分别, 主要的区别有: 线程安全性, 同步(synchronization), 以及速度 HashMap 几乎可以等价于 Hashtable, 除了HashMap是非synchronized的, 并可以接受null(HashMap可以接受为null的键值(key)和值(value), 而Hashtable则不行) HashMap是非synchronized, 而Hashtable是synchronized这意味着Hashtable是线程安全的, 多个线程可以共享一个Hashtable, 而如果没有正确的同步的话, 多个线程是不能共享HashMap的Java 5提供了ConcurrentHashMap, 它是HashTable的替代, 比HashTable的扩展性更好 另一个区别是 HashMap 的迭代器(Iterator)是 fail-fast 迭代器, 而 Hashtable 的 enumerator迭代器不是 fail-fast 的, 所以当有其它线程改变了HashMap的结构(增加或者移除元素), 将会抛出 ConcurrentModificationException, 但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常, 但这并不是一个一定发生的行为, 要看JVM, 这条同样也是Enumeration和Iterator的区别由于Hashtable是线程安全的也是synchronized, 所以在单线程环境下它比HashMap要慢, 如果你不需要同步, 只需要单一线程, 那么使用HashMap性能要好过HashtableHashMap不能保证随着时间的推移Map中的元素次序是不变的 要注意的一些重要术语：1) sychronized意味着在一次仅有一个线程能够更改Hashtable。就是说任何线程要更新Hashtable时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新Hashtable。 2) Fail-safe和iterator迭代器相关。如果某个集合对象创建了Iterator或者ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出ConcurrentModificationException异常。但其它线程可以通过set()方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用set()方法，将会抛出IllegalArgumentException异常。 3) 结构上的更改指的是删除或者插入一个元素，这样会影响到map的结构。 我们能否让HashMap同步？HashMap可以通过下面的语句进行同步：Map m = Collections.synchronizeMap(hashMap); 结论Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"22. JCF - Java集合框架","slug":"JAVA/2018-12-19-22","date":"2018-12-19T02:17:23.000Z","updated":"2018-12-29T06:30:20.000Z","comments":true,"path":"2018/12/19/JAVA/2018-12-19-22/","link":"","permalink":"http://blog.renyimin.com/2018/12/19/JAVA/2018-12-19-22/","excerpt":"","text":"JCF简介 JCF(Java Collection Framework) 主要是由一组精心设计的接口、类和隐含在其中的数据结构及算法所组成, 通过它们可以对Java对象进行存储、获取、操作和转换等功能; 虽然称为框架, 但这些接口和类是以库的方式工作: 所有的集合类都位于 java.util 包下, 是用来存放对象的容器, 后来为了处理多线程环境下的并发安全问题, java5还在 java.util.concurrent 包下提供了一些多线程支持的集合类; Java集合就像一种容器, 可以动态第把多个对象的引用放入容器中, 所以也叫Java容器, 这些容器可以用于存储数量不等的多个对象, 还可以用于保存具有映射关系的关联数组; 总之, JCF提供了各种集合的接口和实现它们的类; 用来存放数据对象并实现对其数据对象的各种操作; JCF框架结构 下面是一比较常见的Java集合框架图 从上图来看, 其实Java集合框架主要包括两种类型的容器: Collection和Map, 它们是Java集合框架的根接口, 这两个接口又包含了一些子接口或实现类 Collection: 一个独立元素的序列, 这些元素会遵循一定的规则 Map: 一组成对的 “键值对” 对象, 允许你使用键来查找值 Java的集合类也主要由 Collection和Map 这两个根接口派生而出, 不过Java不提供直接继承自Collection的集合类, Java提供的集合类都是实现自Collection的 子接口 如 List, Set, Queue Collection接口 Collection是最基本的集合接口, 一个Collection代表一组Object(即Collection的元素Elements); 一些Collection允许相同的元素而另一些不行, 一些能排序而另一些不行; Collection 接口下又有 3 种子接口 List、Set 和 Queue, 再下面是一些抽象类, 最后是具体实现类, 常用的有 ArrayList、LinkedList、HashSet、LinkedHashSet、HashMap、LinkedHashMap 等等 Collection中的几个主要实现类 (中间省略一些实现和继承关系) 12345678910111213141516|Collection | ├List | │--├LinkedList | │--├ArrayList | │--└Vector | │ └Stack | ├Set| │--├HashSet | │--├TreeSet | │--└LinkedSet | │| ├Queue| │--├PriorityQueue| │--├Deque| │ ├ArrayDeque| │ └LinkedList List, Set,Map三者的区别及总结: List:对付顺序的好帮手List接口存储一组不唯一(可以有多个元素引用相同的对象), 有序的对象 Set:注重独一无二的性质不允许重复的集合, 不会有多个元素引用相同的对象 Map: 用Key来搜索的专家使用键值对存储;Map会维护与Key有关联的值, 两个Key可以引用相同的对象, 但Key不能重复, 典型的Key是String类型, 但也可以是任何对象; List接口 List 接口存储一组不唯一, 有序(插入顺序)的对象 该接口是一个 有序的 Collection 使用此接口能够精确的控制每个元素插入的位置 能够通过索引(元素在List中位置, 类似于数组的下标)来访问List中的元素, 第一个元素的索引为0 允许有相同的元素 List接口下我们通常使用的实现类有: ArrayList, LinkedList, Vector及其子类Stack 12345├List │--├LinkedList │--├ArrayList │--└Vector (已过时)│ └Stack List子接口的操作有 List子接口是有序集合, 所以与Set相比, 增加了与索引位置相关的操作: 12345678add(int index, Object o): 在指定位置插入元素addAll(int index, Collection c): ...get(int index): 取得指定位置元素indexOf(Obejct o): 返回对象o在集合中第一次出现的位置lastIndexOf(Object o): ...remove(int index): 删除并返回指定位置的元素set(int index, Object o): 替换指定位置元素subList(int fromIndex, int endIndex): 返回子集合 ArrayList ArrayList是List接口的可变数组的实现, 它封装了一个动态增长的、允许再分配的Object[]数组; 并允许包括 null 在内的所有元素; 由于 Arraylist底层使用的是数组, 所以其 读取数据效率高,插入删除特定位置效率低 ArrayList不是线程同步的, 即线程不安全; Vector类 Vector是一种老的动态数组, 是线程同步的, 效率很低, 一般不赞成使用; 其实 ArrayList 和 Vector 在用法上完全相同, 但由于Vector是一个古老的集合(从jdk1.0就有了), 那时候java还没有提供系统的集合框架, 所以在Vector里提供了一些方法名很长的方法(例如:addElement(Object obj)), 实际上这个方法和add(Object obj)没什么区别; 从jdk1.2以后, Java提供了系统的集合框架, 就将Vector改为实现List接口, 作为List的实现之一, 从而导致Vector里有一些重复的方法; Vector里有一些功能重复的方法, 这些方法中方法名更短的是属于后来新增的方法, 更长的是原先vector的方法, 而后来ArrayList是作为List的主要实现类, 看过的Java思想编程中也提到了Vector有很多缺点, 尽量少用Vector实现类 Vector的子类Stack: Stack类表示后进先出(LIFO)的对象堆栈 由于 Vector是通过数组实现的, 这就意味着, Stack也是通过数组实现的, 而非链表 Deque 接口及其实现提供了 LIFO 堆栈操作 更完整和更一致的集合, 应该优先使用此集合, 而非Stack类。例如: Deque&lt;Integer&gt; stack = new ArrayDeque&lt;Integer&gt;(); Vector类 和 Stack类 这两个都是jdk1.0的过时API, 应该避免使用 jdk1.5新增了很多多线程情况下使用的集合类, 位于java.util.concurrent 如果你说, Vector是同步的, 你要在多线程使用, 那你应该使用 java.util.concurrent.CopyOnWriteArrayList 等, 而不是Vector 如果你要使用Stack做类似的业务, 那么非线程的你可以选择linkedList, 多线程情况你可以选择 java.util.concurrent.ConcurrentLinkedDeque 或者java.util.concurrent.ConcurrentLinkedQueue 多线程情况下, 应尽量使用java.util.concurrent包下的类; 摘自: https://www.cnblogs.com/devin-ou/p/7989451.html Vector类 &amp; ArrayList类 它们都是基于数组实现 Vector可以设置增长因子, 而ArrayList不可以 (类允许设置默认的增长长度, 默认扩容方式为原来的2倍) Vector的方法都是同步的(Synchronized), 是线程安全的(thread-safe), 而ArrayList的方法不是, 由于线程的同步必然要影响性能, 因此, ArrayList的性能比Vector好 详细: https://blog.csdn.net/weixin_37766296/article/details/80315375 LinkedList类 LinkedList底层使用的是双向循环链表数据结构(插入, 删除效率特别高); LinkedList实现了List接口, 允许 null 元素, 此外LinkedList提供额外的get, remove, insert方法在LinkedList的首部或尾部, 这些操作使LinkedList可被用作堆栈(stack), 队列(queue)或双向队列(deque) 当数据特别多, 而且经常需要插入删除元素时建议选用 LinkedList; 一般程序只用Arraylist就够用了, 因为一般数据量都不会蛮大, Arraylist是使用最多的集合类; 注意LinkedList没有同步方法, 如果多个线程同时访问一个List, 则必须自己实现访问同步, 一种解决方法是在创建List时构造一个同步的List: List list = Collections.synchronizedList(new LinkedList(…)); ArrayList类 &amp; LinkedList类 Arraylist 底层使用的是数组(存读数据效率高, 插入删除特定位置效率低), LinkedList底层使用的是双向循环链表数据结构(插入, 删除效率特别高) 在各种Lists中, 最好的做法是以ArrayList作为缺省选择; 当插入、删除频繁时, 使用LinkedList(); Set接口 Set 接口存储一组唯一, 无序的对象 (判断两个对象是否相同则是根据equals方法) Set接口下我们通常使用的实现类有: ArrayList, LinkedList, Vector及其子类Stack 1234├Set│--├HashSet │--├TreeSet │--└LinkedSet HashSet 类 HashSet是Set接口的典型实现, HashSet使用HASH算法来存储集合中的元素, 因此具有良好的存取和查找性能; 当向HashSet集合中存入一个元素时, HashSet会调用该对象的hashCode()方法来得到该对象的hashCode值, 然后根据该HashCode值决定该对象在HashSet中的存储位置; 该类不允许出现重复元素, 不保证集合中元素的顺序, 允许包含值为null的元素, 但最多只能一个(当然了, 元素不能重复); HashSet集合判断两个元素相等的标准是两个对象通过 equals() 方法比较相等, 并且两个对象的hashCode()方法的返回值相等; HashSet不是同步的, 多线程访问同一步HashSet对象时, 需要手工同步 内部使用 HashMap 来存储数据, 数据存储在HashMap的key中; HashSet几个重要的方法 add(E e) : HashSet的确定性(也可以理解为唯一性), 是通过 HashMap 的 put方法 来保证的, 往HashMap中put数据时, 如果key是一样的, 只会替换key对应的value, 不会新插入一条数据; 所以往HashSet中add相同的元素没有什么用, 这里的相同是通过equals方法保证的, 具体的在HashMap中细说。 123public boolean add(E var1) &#123; return this.map.put(var1, PRESENT) == null;&#125; remove(Object o) : 简单粗暴, 从HashMap中移除一条数据 123public boolean remove(Object var1) &#123; return this.map.remove(var1) == PRESENT;&#125; contains(Object o) iterator() 其他的方法诸如: size()、isEmpty()、contains()、clear()等都完全委托给了HashMap。需要注意的是: HashSet没有提供set、get等方法; 摘自: https://www.cnblogs.com/wlrhnh/p/7256969.html LinkedHashSet类 LinkedHashSet继承自HashSet, 内部使用的是LinkHashMap; 这样做的意义或者好处是, LinkedHashSet中的元素顺序是可以保证的, 也就是说遍历序和插入序是一致的; https://www.cnblogs.com/wlrhnh/p/7256969.html TreeSet类 可以实现排序等功能的集合, 对象元素添加到集合中时会自动按照某种比较规则将其插入到有序的对象序列中, 并保证该集合元素组成按照 升序 排列; TreeSet的内部操作的底层数据是TreeMap, 只是我们操作的是TreeMap的key; 在对大量信息进行检索的时候, TreeSet 比 ArrayList 更有效率, 能保证在log(n)的时间内完成 ( ?? 不对啊, arraylist的随机读取不是O(1)么?? 其实这句话的意思是, 当你不知道索引时, 查找一个元素, TreeSet自然是O(log(n)), 而ArrayList在不指定索引时, 如果没有搜索算法, 自然是log(n)) TreeSet 是用树形结构来存储信息的, 每个节点都会保存一下指针对象, 分别指向父节点, 左分支, 右分支, 相比较而言, ArrayList就是一个含有元素的简单数组了, 正因为如此, 它占的内存也要比ArrayList多一些; 向 TreeSet 插入元素也比 ArrayList 要快一些, 因为当元素插入到ArrayList的任意位置时, 平均每次要移动一半的列表, 需要O(n)的时间, 而TreeSet深度遍历查询花费的实施只需要O(log(n))普遍的都是, set查询慢, 插入快, list查询快, 插入慢 …… Queue接口 用于模拟 队列 这种数据结构, 实现 FIFO 等数据结构; 通常, 队列不允许随机访问队列中的元素; Queue 接口并未定义阻塞队列的方法, 但这在方法并发编程中是很常见的, 因此Queue的子接口 BlockingQueue接口 定义了那些等待元素出现或等待队列中有可用空间的方法, 这些方法扩展了此接口; Queue 实现通常不允许插入 null 元素, 尽管某些实现(如 LinkedList)并不禁止插入 null, 即使在允许 null 的实现中, 也不应该将 null 插入到 Queue 中, 因为 null 也用作 poll 方法的一个特殊返回值, 表明队列不包含元素; Map 接口 |Collection | ├List | │—├LinkedList | │—├ArrayList | │—└Vector | │ └Stack | ├Set | │—├HashSet | │—├TreeSet | │—└LinkedSet | |Map ├Hashtable ├HashMap └WeakHashMap Java容器注意事项 Java容器里只能放对象, 对于基本类型(int, long, float, double等), 需要将其包装成对象类型后(Integer, Long, Float, Double等)才能放到容器里, 很多时候拆包装和解包装能够自动完成, 这虽然会导致额外的性能和空间开销, 但简化了设计和编程; 集合与数组 数组: 大小固定, 只能存储相同数据类型的数据; 数组存储的元素可以是同一类基本类型, 也可以是同一类对象; 集合: 大小可动态扩展, 可以存储不同类型的数据; 集合里只能保存对象;在编程中, 常常需要集中存放多个数据, 虽然数组是个不错的选择, 但数组需要你事先明确知道你将要保存的对象的数量, 数组在初始化时就会指定长度, 并且之后这个数组长度是不可变的;而如果我们需要保存动态增长的数据, 就无法再使用数组了, 此时, java的集合类就是一个很好的设计方案了 可点击查看Java 集合框架图 Java 集合框架主要包括两种类型的容器: 一种是集合(Collection), 存储元素集合; 另一种是图(Map), 存储键/值对映射; Collection接口Collection接口定义了存取一组对象的方法, 其子接口Set和List分别定义了存储方式 List接口: 有序可重复, 长度可变 Set接口: 无序, 不可重复 ArrayList 底层是数组, 查询快, 修改,插入删除慢 LinkedList 底层是链表, 查询慢, 修改,插入,删除快 Vector: 线程安全的, 效率低 Map接口 无序键值对, 键唯一 Collection接口体系中一些常用的实现类有: ArrayList: 实现了List接口, 它是以数组的方式来实现的, 数组的特性是可以使用索引的方式来快速定位对象的位置, 因此对于快速的随机访问, 使用ArrayList实现执行效率上会比较好; LinkedList: 是采用链表的方式来实现List接口的, 由于是采用链表实现的, 因此在进行 insert 和 remove 动作时在效率上要比 ArrayList 要好得多! 适合用来实现 Stack(堆栈) 与 Queue(队列), 前者先进后出, 后者是先进先出若要从数组中删除或插入某一个对象, 需要移动后面的数组元素, 从而会重新调整索引顺序, 调整索引顺序会消耗一定的时间, 所以速度上就会比LinkedList要慢许多; 相反, 若要从链表中删除或插入某一个对象, 只需要改变前后对象的引用即可! Arraylist底层使用的是数组(存读数据效率高, 插入删除特定位置效率低), LinkedList底层使用的是双向循环链表数据结构(插入, 删除效率特别高)。学过数据结构这门课后我们就知道采用链表存储, 插入, 删除元素时间复杂度不受元素位置的影响, 都是近似O(1)而数组为近似O(n), 因此当数据特别多, 而且经常需要插入删除元素时建议选用LinkedList.一般程序只用Arraylist就够用了, 因为一般数据量都不会蛮大, Arraylist是使用最多的集合类。 Arraylist 与 LinkedList 异同 相同点: ArrayList 和 LinkedList 都是不同步的, 也就是不保证线程安全; 不同: Arraylist 底层使用的是Object数组; LinkedList 底层使用的是双向链表数据结构(JDK1.6之前为循环链表, JDK1.7取消了循环。注意双向链表和双向循环链表的区别) Map Map是java集合的另一个根接口, Map体系中一些常用的实现类如下:","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"20. 数组","slug":"JAVA/2018-12-18-20","date":"2018-12-18T14:07:26.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/18/JAVA/2018-12-18-20/","link":"","permalink":"http://blog.renyimin.com/2018/12/18/JAVA/2018-12-18-20/","excerpt":"","text":"数组的长度是固定的, 当添加的元素超过了数组的长度时, 需要对数组重新定义, 比较麻烦Java内部就提供了集合类, 能存储任意对象, 长度是可以改变的, 会随着元素的增加而增加, 减少而减少 数组既可以存储基本类型数据, 有可以存储引用类型数据, 基本数据类型存储的是值, 引用数据类型存储的是地址值集合中只能存储引用类型数据(对象) 当然, 也可以存储基本数据类型, 但是在存储的时候会自动装箱变对象 集合的底层其实还是用数组做的 List有序, 有索引, 可以存储重复值 ArrayList数组实现 LinkedList链表实现 Vector数组实现 Set无序, 无索引, 不可以存储重复值 HashSet哈希算法 TreeSet二叉树算法","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"15. 时间处理相关类","slug":"JAVA/2018-12-18-16","date":"2018-12-18T04:17:13.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/18/JAVA/2018-12-18-16/","link":"","permalink":"http://blog.renyimin.com/2018/12/18/JAVA/2018-12-18-16/","excerpt":"","text":"java.util.Date","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"15. 字符串相关类","slug":"JAVA/2018-12-17-15","date":"2018-12-17T05:21:26.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/17/JAVA/2018-12-17-15/","link":"","permalink":"http://blog.renyimin.com/2018/12/17/JAVA/2018-12-17-15/","excerpt":"","text":"java.lang.String String 类是不可改变的, 所以你一旦创建了 String 对象, 它的值就无法改变了, 当你需要改变时, 其实会创建出新的 String 对象 如下, String 对象经过循环后, 会创建多个不同的对象; (而可变字符串 StringBuilder对象 则始终都是一个) 123456789101112131415161718192021public class StringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;String:&quot;); String s1 = new String(&quot;a&quot;); for (int i = 0; i &lt; 3; i++) &#123; System.out.println(s1); System.out.println(System.identityHashCode(s1)); s1 = s1 + i; System.out.println(System.identityHashCode(s1)); &#125; System.out.println(); System.out.println(&quot;StringBuilder:&quot;); StringBuilder s2 = new StringBuilder(&quot;a&quot;); for (int i = 0; i &lt; 3; i++) &#123; System.out.println(s2); System.out.println(System.identityHashCode(s2)); s2.append(i); System.out.println(System.identityHashCode(s2)); &#125; &#125;&#125; 字符串字面量 和 new String 的区别: 1234567String s0 = new String(&quot;a&quot;);String s01 = &quot;a&quot;;System.out.println(System.identityHashCode(s0)); // 1627674070System.out.println(System.identityHashCode(s01)); // 1360875712System.out.println(System.identityHashCode(&quot;a&quot;)); // 1360875712System.out.println(s0 == s01); // falseSystem.out.println(s01 == &quot;a&quot;); // true StringBuffer, StringBuilder","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"05. 版本部分回退","slug":"git/2018-12-16-05","date":"2018-12-16T09:10:06.000Z","updated":"2018-12-16T09:17:36.000Z","comments":true,"path":"2018/12/16/git/2018-12-16-05/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/git/2018-12-16-05/","excerpt":"","text":"场景 假设你正在A分支上进行开发, 此时对 1,2,3 三份文件做了修改, 然后直接 git add ., git commit -m &quot;&quot; 进行了提交, 但是提交后发现, 文件3 是不需要修改的, 你需要将该文件恢复之前版本 如果直接进行版本回退, 1,2,3 这三个文件的改动都会被回退, 如果改动内容比较少, 你可以这样做; 但是如果这个功能对三个文件进行的改动非常大, 你总不能重来一遍吧, 即便是可以通过编辑器本身的history功能可以找到文件的修改版本, 但这种经方法还是比较low 所以你需要的是能够将某次commit中的部分文件进行版本回退的功能","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"04. git stash","slug":"git/2018-12-16-04","date":"2018-12-16T08:53:02.000Z","updated":"2018-12-16T09:09:54.000Z","comments":true,"path":"2018/12/16/git/2018-12-16-04/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/git/2018-12-16-04/","excerpt":"","text":"场景 在使用git时, 经常会出现 你正在A分支进行开发, 但是突然有个小需求需要紧急切换到B分支进行开发, 遇到这种情况, 我以前的做法通常如下: 把A分支上上榜为完成开发的代码直接 commit 然后checkout到B分支进行开发 上面方式存在的问题是: 本来我每次commit, 都希望是一次比较完整的功能点提交, 而上述操作可能会出现, 某个功能块是尚未完成就直接进行了一次无意义的commit, 显得比较low了, 此时就应该使用 git stash 了 先将A分支目前的工作现场保暂存起来 然后checkout到B分支进行开发工作 B分支上的工作开发完成后, 在checkout到A分支, 将之前暂存的开发内容恢复到工作区即可 git rebase git stash 可以用来保存工作现场 git stash list 可以用来查看保存的工作现场列表 恢复工作现场: git stash apply : 恢复时不删除stash中的内容, 需要通过 git stash drop stash@{0} 手动删除例 git stash apply stash@{0} git stash pop : 恢复的同时也将stash内容删除掉","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"10. Intellij IDE快捷键","slug":"JAVA/2018-12-16-10","date":"2018-12-16T03:00:26.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/16/JAVA/2018-12-16-10/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/JAVA/2018-12-16-10/","excerpt":"","text":"main 函数快捷键: 先敲出 main, 然后 command+j, 回车 System.out.println 快捷键: 先敲出 sout, 会自动弹出一个匹配列表, 选择第一项即可 get, set 方法生成: 光标移动到属性名上, 按 Command + n 查看某个类的继承结构: Ctrl+h annotate 注释: 在显示行号的位置右击, 选中 Annotate, 即可查看当前行代码的commit、author和date 查看类的继承结构: Navigate =&gt; Type Hierarchy","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"03. git push 时出现 \"Merge branch 'master' of ...\"","slug":"git/2018-12-14-03","date":"2018-12-14T06:16:39.000Z","updated":"2018-12-19T02:16:32.000Z","comments":true,"path":"2018/12/14/git/2018-12-14-03/","link":"","permalink":"http://blog.renyimin.com/2018/12/14/git/2018-12-14-03/","excerpt":"","text":"当我们在使用git进行协同开发时, 假设 develop 为大家的一条协同分支: 当你在本地进行开发完后, 会将自己的分支合并到develop并 git push 到远端 如果此时在你之前并没有其他人对develop进行过commit并push到远端, 你是可以直接push成功的, 但这种情况一般不多, 毕竟多人协同开发现在很普遍 与上面相对的是, 在你push之前, 很可能已经有其他人对develop进行过commit并push到远端, 所以你经常会出现 push 不成功的问题, 如下: 此时一般常见做法是 先 git pull, 然后再 git push, 但在 git pull 时, 其实还分两种情况 一种是远端和本地产生了冲突:此时你需要在本地解决冲突, 并重新进行 commit, push最终的分支结构如下: 另外一种是远端虽然有改动, 但是和本地没有冲突此时虽然没有提示冲突, 但依然push失败, 当你执行pull时, 会直接弹出如下编辑框如果你wq保存编辑页, 你会发现git pull自动进行了ff自动合并最终的分支结构如下: 可以发现, 当 git pull 时, 会将远端与本地进行一次 git merge, 此时 可能会无冲突自动完成 fast-farward 合并 (此时会出现一个 Merge branch &#39;master&#39; of ... commit点) 也可能需要解决冲突再手动提交 (此时是自己手动打上的 commit点) 如何在 git push 时避免出现 Merge branch &#39;master&#39; of ... ? 方案一: (比较low的做法)在 git pull 弹出编辑页面时, 对commit点的日志内容进行修改 ; 或者先 git fetch 然后 git merge, 在本地编写commit点日志信息 (简单来说 git pull = git fetch + git merge)上面的做法并没有解决实际问题, 还会有一个合并时的commit点 方案二: (比较巧) 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature 方案二测试: 两个分支的初始基点一致 两个分支分别进行两次提交, 构造冲突: 然后在dev分支rebase来合并master分支内容 (master可以先pull, 这是不会有冲突的) 查看两分支 最后再在master分支merge去合并dev分支的内容, 此时自然也不会有冲突了 最终的分支都是直线向上 出现类似: “Merge remote-tracking branch ‘remotes/origin/develop’” : ?? git pull 和 git pull origin master ?? 参考: https://www.cnblogs.com/Sinte-Beuve/p/9195018.html https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"02. 选择 merge 还是 rebase?","slug":"git/2018-12-12-02","date":"2018-12-12T09:01:23.000Z","updated":"2018-12-14T06:27:33.000Z","comments":true,"path":"2018/12/12/git/2018-12-12-02/","link":"","permalink":"http://blog.renyimin.com/2018/12/12/git/2018-12-12-02/","excerpt":"","text":"git rebase 介绍 rebase: 变基, 即改变分支的根基; 从某种程度来说, git rebase 和 git merge 都可以完成类似的合并工作, 但实际上两者有着本质的不同; 假设你的项目有 mywork, origin 两个分支, 如果你想让 mywork 的分支历史看起来像没有经过任何合并一样, 你可以用 git rebase (虽然 git merge 也可以实现, 但是一旦合并遇到冲突时, 还是会出现分叉) 如下命令: 会把你的 origin 分支从 mywork 切出来后的每个提交(commit)取消掉, 并且把它们临时保存为补丁(patch)(这些补丁放到”.git/rebase”目录中), 然后把 origin 分支更新到最新的 mywork 分支, 最后把保存的这些补丁应用到 mywork 分支上。 12$ git checkout mywork$ git rebase origin 具体如下图: 尝试主分支没有commit时 git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 (不修改同一行,先不构造冲突) git log --graph 查看dev日志 git rebase 合并 发现效果和之前的 git merge fast-forward无冲突时 貌似没什么区别 主分支有commit且出现冲突时 接着上面的例子, master 和 dev 分别修改同一行内容, 构造冲突 切换到 master 分支, git rebase 合并dev分支 如上 git rebase 在出现冲突时, Git会停止rebase并会让你去解决冲突, 在解决完冲突后, 用 git add 命令去更新这些内容的索引(index), 然后, 你无需执行 git-commit, 只要执行: git rebase --continue （当然, 无冲突时, 直接 git rebase 就直接完成了合并） 发现好像之前master上的那次提交的信息(时间, 日志等信息)都在, 但是取而代之的是一个新的commit_id, rebase会修改 根基之后 的提交历史 主分支有commit但无冲突 经过测试, 即便无冲突, master分支的新commit也会丢失 测试如下: master, dev两个分支基点相同 master, dev两个分支各自做改动, 但是改动并不不冲突 发现master进行 git rebase dev 直接就合并成功了, 虽然没有冲突, 但是master的commit历史确实还是被改掉了!! 小结 不要在master和其他协作分支上使用 git rebase, 它会修改提交历史; 当使用git做合并操作时, 如果没有冲突, 则 git merge(fast-forward) 和 git rebase 效果一样都是直线, 而git merge --no-ff会出现分叉和新提交点; 当使用git做合并操作时, 如果出现冲突, 则 git merge(fast-forward) 和 git merge --no-ff 一样都会出现分叉和新提交点, git rebase则不同, 它是直线, 但会修改历史提交点; rebase 使用技巧 git rebase 由于会改变历史提交点, 所以一定不能用在协作分支上 使用rebase既可以保证分支直线美观, 也可以保证在git push时不出现类似 “Merge remote-tracking branch…” 问题 通常如下来使用, 下文在解决 类似 “Merge remote-tracking branch…” 问题时, 会有详细过程: 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"01. git merge 对比 fast-forward 与 --no-ff","slug":"git/2018-12-12-01","date":"2018-12-12T05:32:54.000Z","updated":"2018-12-14T06:17:17.000Z","comments":true,"path":"2018/12/12/git/2018-12-12-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/12/git/2018-12-12-01/","excerpt":"","text":"git merge 介绍 在使用 git merge 进行分支合并时, 可以直接使用如下两种方式 git merge (fast-forward) git merge --no-ff 这两者的区别如下: 但是需要注意的是 git merge 的一个特例: 当合并出现冲突时, 其实是无法执行快速合并的,需要解决冲突后再进行一次提交, 所以效果和 git merge --no-ff 是一样的, 也就是还会出现分叉 (可以参考此文) fast-forward git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 Fast-forward 合并 最终可以看到, master分支只是简单地将dev分支的那次提交合进自己的分支内 (不会在graph图中保留dev分支线) 即便是在dev分支做了多次提交, master分支也只是简单地将dev分支的多次提交合进自己的分支内 (不会保留dev分支) —no-ff git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 —no-ff 合并 (相比 FastForward, 这里还会弹出编辑界面, 允许你对本次合并进行说明) 最终可以看到: master分支并没有将dev上的两次提交合到自己的分支上 而是在graph图中保留了dev分支线 并且将这次合并当做一次dev向master的提交, 在master上生成一个新的commit 此时master的分支的commit点已经比dev上的commit要更新一步 此时如果不把master的提交分支合并到dev, 而是在dev上继续做两次提交, 然后再在master上进行 —no-ff 合并, 效果如下: 红色框表示—no-ff合并时所进行的新commit及备注日志 现在如果把master的提交合并到dev 如果在master切出新的分支, 然后再新分支上进行提交, 再回到master进行 —no-ff 合并, 效果依然如下图: ff 冲突导致的例外 重新创建本地仓库, 对master进行两次提交; 新建并切换到dev分支, 并修改文件: 切换回master分支, 修改dev上修改的同一行内容, 构造冲突: 此时, 即便是在master上使用 fast-forward 合并dev, 由于有冲突, 还是会进行一次提交:","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"04. 基础面向对象知识","slug":"JAVA/2018-12-10-04","date":"2018-12-10T14:33:32.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/10/JAVA/2018-12-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/12/10/JAVA/2018-12-10-04/","excerpt":"","text":"类和对象可以理解类为模板, 对象为实物 匿名对象 没有名字的对象 Car c = new Car(); 就是有名字的对象; 匿名对象其实就是定义对象的简写格式: new Car(); 当对象对方法仅进行一次调用的时候, 就可以简化成使用匿名对象调用 new Car().run(); 构造方法 函数名与类名相同: 在Java中, 由于构造方法和类名同名, 所以子类无法进行重写, 但是可以重载 (php由于构造方法名为与类名无关的 __construct, 所以可以重写) 不用定义返回值类型 (这个函数是没有返回值的, 甚至连 void都不要写) 在创建对象时就会自动调用构造函数 ( 因为创建对象的时候一定会调用构造函数, 所以可以利用这个特性给对象进行初始化 ) 一个类中如果没有定义过构造函数, 那么该类中会有一个默认的空参数构造函数(在你编译过的.class文件中就有一个默认的空参数构造函数的) 12345678class Demo&#123;&#125;就这么个简单的类, 其实在编译完之后, 生成的.class文件中其实是 :class Demo&#123; Demo()&#123;&#125;&#125; 如果一个类中指定了构造函数, 那么类中的默认构造函数就没有了, 此时.class文件中保存的是你自己写的构造函数 构造函数和一般函数的区别 : 在new创建对象时, 构造函数会被调用并且只被调用一次(可以对对象进行初始化); 而一般函数在对象创建之后, 在需要函数功能时才调用; 在new创建对象时, 构造函数会被调用并且只被调用一次; 而一般函数在对象创建后, 可以被调用多次; 普通函数不能调用构造函数 ; 构造函数是用来给对象初始化的 ; 普通的函数也可以用类名作为自己的函数名123456void Person() // 加了void这个返回类型后, 就成了一般函数了&#123; name = &quot;baby&quot;; age = 1; System.out.println(&quot;person run&quot;);&#125; 构造函数的继承 如下, 子类对象被创建时, 会访问子类的构造函数, 但是下面会发现父类的构造函数也运行了 1234567891011121314151617181920212223class Fu&#123; Fu() &#123; System.out.println(&quot;fu run&quot;); &#125;&#125;class Zi extends Fu&#123; Zi() &#123; System.out.println(&quot;zi run&quot;); &#125;&#125;public class ExtendsDemo&#123; public static void main(String[] args) &#123; new Zi(); &#125;&#125; 其原因是在子类的构造函数中第一行有一个默认的隐式语句: super() (这点和php不同, php只有显示指定了 parent::__construct() 才会调用父类的构造函数) 其实子类中所有的构造函数默认都会访问父类中的空参数的构造函数 123456789101112131415161718192021222324252627282930313233343536class Fu&#123; Fu() &#123; System.out.println(&quot;fu run&quot;); &#125; Fu(int age) &#123; System.out.println(&quot;fu age:&quot; + age + &quot;run&quot;); &#125;&#125;class Zi extends Fu&#123; Zi() &#123; // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi run&quot;); &#125; Zi(int age) &#123; // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi age:&quot; + age + &quot; run&quot;); &#125;&#125;public class ExtendsDemo&#123; public static void main(String[] args) &#123; new Zi(); new Zi(100); &#125;&#125; 结果: 1234fu run # 父类无参构造函数zi runfu run # 父类无参构造函数zi age:100 run 如果父类中没有定义空参数构造函数, 则子类的构造函数必须用 super 明确要调用父类中的哪个构造函数 如果子类构造函数中还使用了 this() 调用了子类自己的构造函数, 那么子类的这个构造函数中的 super() 就没有了, 因为 super() 和 this() 只能有一个 123456789101112131415161718192021222324class Zi extends Fu&#123; Zi() &#123; // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi run&quot;); &#125; Zi(int age) &#123; this(); // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi age:&quot; + age + &quot; run&quot;); &#125;&#125;public class ExtendsDemo&#123; public static void main(String[] args) &#123; new Zi(); new Zi(100); &#125;&#125; 结果: 123456fu runzi runfu run // 并没有出现两次zi runzi age:100 run 注意: super() 语句必须要定义在子类构造函数的第一行, 因为父类的初始化动作要先完成 (不然会提示错误) super关键字 当子类和父类中的成员(变量或方法)重名的时候, 用 super 区分父类 this 和 super 的用法很相似 super 的用法跟 this 类似, this 代表对本类对象的引用, 指向本类已经创建的对象; 而 super 代表对父类对象的引用, 指向父类对象 静态方法中不可以出现 super, this 关键字 在子类的构造函数中, 第一行会隐式地执行一个: super() this 关键字 this 是在类的成员函数中使用, 代表的是当前对象; 成员变量和局部变量重名的话, 必须用 this 来给成员变量做引用 1234public Person(name)&#123; this.name = name&#125; 注意: 在一般情况下, 如果在类方法中的成员属性和局部变量不重名的情况下, 其实成员属性前是不用写this.的, 因为Java默认就是用的this. ( 这点和PHP不同 ) this() 用法 ?? static 关键字 static是一个修饰符, 用于修饰成员 (成员变量和成员函数) 被static修饰后的成员具备以下特点 : 随着类的加载而加载 ; 优先于对象存在 ; (因为它是随着类的加载而加载的, 而类加载的时候还没有对象的) 被该类的所有对象所共享 ; 多了一种调用方式, 可以直接被类名调用 ; (因为static成员存在的时候只有类存在, 对象尚未存在) static修饰的数据是共享数据, 对象中存储的是对象自己特有数据 使用注意: 静态方法只能访问静态成员(成员变量和成员函数) 静态方法中不可以写 this, super关键字; 之前我们一开始写的main主函数就是静态的; 成员变量与静态变量的区别 生命周期的不同: 成员变量: 随着对象的创建而存在, 随着对象的被回收而释放; 静态变量: 随着类的加载而加载, 随着类的消失而消失; (那类什么时候消失呢 ? 一般情况下, 虚拟机结束了, 类就消失了) 调用方式不同: 成员变量: 只能被对象调用; 静态变量: 可以被对象调用, 也可以被类名调用; (一般就用类名调用) 别名不同: 成员变量: 也称为实例变量; 静态变量: 称为类变量; 数据的存储位置不一样: 成员变量: 存储在堆内存的对象中, 所以是对象的特有数据; 静态变量: 存储在于方法区(也叫共享数据区)的静态区中, 所以也叫对象的共享数据; 静态方法只能调用静态变量, 不能调用成员变量 (但是, 非静态方法是可以访问静态成员) 因为静态方法在类被加载的时候就被加载了,可能还没有对象的时候, 就已经可以用类名来访问了 (Person.show();) 如果它里面有成员变量的话, 由于此时还没有对象, 而成员变量是每个对象所特有的, 如果对象没有创建, 自然无法使用成员变量 静态方法和实例方法 静态成员函数 提供给外部调用的函数应该是非静态的还是静态的呢?函数是否需要设置为静态函数, 只用参考该函数是否有访问到对象中的特有数据, 如果访问到了对象的特有数据的话, 那就不能用static来进行修饰了 (因为静态方法只能调用静态成员) 简单来说就是 : 该函数是否需要访问非静态的成员, 如果需要访问非静态成员, 那么该功能就不能是静态的(因为静态方法只能调用静态成员) 如果创建的对象仅仅是为了去调用没有访问特有数据的非静态方法, 那么这个对象的创建除了浪费空间这个坏处之外, 没有别的任何好处了; 此时这个方法完全可以设置为static, 然后使用类名来调用, 而不用创建对象, 不浪费空间 (对象的创建是为了访问特有数据的, 没有访问特有数据, 干嘛要创建对象呢) 静态代码块 静态代码块随着类的加载而执行, 而且只执行一次 ; 静态代码块的作用 : 用于给类进行一些初始化工作 ; 一般如果一个类里面都是静态成员, 这个类是不需要创建对象的, 直接用类名调用成员即可, 但此时因为没有创建对象, 所以不会执行构造函数, 那么如何进行一些初始化工作呢?如果此时还要进行一些初始化工作, 那就要靠静态代码块了!!!! 示例: 静态代码块只被调用了一次, 只是第一次创建对象的时候被调用了, 后面就不会再被执行了 1234567891011121314151617181920212223class StaticCode&#123; static int num; static &#123; num = 10; System.out.println(num); &#125; void show() &#123; System.out.println(&quot;test&quot;); &#125;&#125;class StaticCodeDemo&#123; public static void main(String[] args) &#123; new StaticCode(); new StaticCode().show(); &#125;&#125; 构造代码块 没有名字的普通代码块, 如下的代码块就没有名字, 可以看到在创建对象的时候, 无名代码块就被加载到了, 并且创建几次对象就加载几次 123456789101112131415161718192021222324public class ConstructBlock &#123; // 构造代码块 &#123; System.out.println(&quot;构造代码块被执行&quot;); &#125; public ConstructBlock() &#123; System.out.println(&quot;构造方法被执行&quot;); &#125;&#125;class ConstructBlockDemo&#123; public static void main(String[] args) &#123; new ConstructBlock(); &#125;&#125;// 结果:构造代码块被执行构造方法被执行 无名的普通代码块, 是随着对象的创建而加载的 所以构造代码块是可以给对象初始化的, 貌似和构造函数一样了? 其实构造函数是给对应的对象进行针对性的初始化, 而构造代码块是具备着对象初始化的通用性 ; 因为初始化的时候, 每个对象传递的值可能不同, 这样的话, 由于类中可能有构造函数的重载, 那么每个对象调用的构造函数可能就不同;但是可能对象在创建的时候都需要初始化一个相同的数据值, 这样的话, 除非给重载的每个构造函数中都初始化上该数据的值, 要不然的话, 创建对象的时候可能就会因为调用的构造函数不同而导致有些进行这个数据的初始化了, 有些对象的创建又没有进行 ; 所以可以把创建对象的时候, 通用的初始化数据写到构造代码块中 ; 注意: 静态代码块用于给类进行初始化, 构造函数是给对象初始化, 构造代码块是给对象进行公共部分初始化 重载和重写的区别 重载: 发生在同一个类中, 指有多个方法的名称相同, 但是参数列表不同, 所谓参数列表不同是指如下几条 参数的个数不同 参数的类型不同 参数的多类型顺序不同 重载与方法的返回值类型无关, 与参数名称无关 重写: 发生在父子类中, 方法名、参数列表必须相同, 返回值范围小于等于父类, 抛出的异常范围小于等于父类, 访问修饰符范围大于等于父类；如果父类方法访问修饰符为 private 则子类就不能重写该方法。 面向对象三大特性封装,继承,多态 封装: 是指隐藏对象的实现细节, 仅对外提供公共访问方式, 提高了代码重用性和健壮性 继承: 提高了代码的复用性, 让类与类之间产生了关系, 给第三个特征”多态”提供了前提 多态: 在Java中有两种形式可以实现多态 12继承基类 (基类可以是 普通类 也可以是 抽象类)实现接口 多态具有 向上转型 与 向下转型 两种转型的概念 向上/下转型 向上转型: 当有子类对象传递给一个父类引用时, 便是向上转型, 多态本身就是向上转型的过程 12使用格式: 父类类型 变量名 = new 子类类型(); 如: Animal animal = new Cat(); 什么时候使用向上转型? 在面向对象编程中, 为了提高扩展性, 你多半都是需要面向抽象(普通基类, 抽象基类 或 接口)编程, 而不是去直接面对具体类型进行编程, 所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定, 而是在程序运行期间才确定, 即一个引用变量倒底会指向哪个具体类的实例对象, 该引用变量发出的方法调用到底是哪个类中实现的方法, 必须在由程序运行期间才能决定 向下转型: 一个已经向上转型的子类对象可以使用强制类型转换, 将父类引用转为子类引用, 这个过程是向下转型; 注意: 如果是直接创建父类对象, 是无法向下转型的!(向下转型的前提是首先进行了向上转型) 123使用格式: 子类类型 变量名 = (子类类型) 父类类型的变量; 如: Animal animal = new Cat(); Cat cat = (Cat) animal 什么时候使用向下转型 当你面向抽象编程时, 某个方法依赖的可能是抽象类型而不是具体类型, 这样的话, 在程序运行时, 具体的子类被传递进来时, 可能就需要使用向下转型来使用子类的一些方法或属性(向下转型的前提是首先进行了向上转型) (很明显在PHP中, 这种弱类型的向下转型是隐式的, 向上转型也是自动的) final 修饰变量 (表示把变量作为常量) 修饰方法: 表示方法不能被子类覆盖重写 (但是本类中依然可以重载) 修饰类: 修饰的类不能有子类, 不能被继承 (比如: Math, String)","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"03. 变量, 常量 基础","slug":"JAVA/2018-12-10-03","date":"2018-12-10T11:33:32.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/10/JAVA/2018-12-10-03/","link":"","permalink":"http://blog.renyimin.com/2018/12/10/JAVA/2018-12-10-03/","excerpt":"","text":"常量 在Java中, 利用关键字 final 指示常量, 例如: 1234567891011public class Constants&#123; public static void main(String[] args) &#123; final double CM_PER_INCH = 2.54; double paperWidth = 8.5; double paperHeight = 11; System.out.println(&quot;Paper size in centimeters:&quot; + paperWidth * CM_PER_INCH + &quot; by &quot; + paperHeight * CM_PER_INCH); &#125;&#125; final 表示这个变量只能被赋值一次, 一旦被赋值后就不能再被更改了 (这可能也就是之前为什么使用final修饰的变量不会自动改变类型) 习惯上, 常量名使用全大写 类常量 在Java中, 经常希望某个常量可以在一个类中的多个方法中使用, 通常将这些常量称为类常量; 可以使用关键字 static final 设置一个类常量, 下面是使用类常量的示例: 123456789101112public class Constants2&#123; final static double CM_PER_INCH = 2.54; public static void main(String[] args) &#123; double paperWidth = 8.5; double paperHeight = 11; System.out.println(&quot;Paper size in centimeters:&quot; + paperWidth * CM_PER_INCH + &quot; by &quot; + paperHeight * CM_PER_INCH); &#125;&#125; const 是Java的保留关键字, 但是目前并未使用, 在Java中, 定义常量必须使用 final 关键字; 字符型常量和字符串常量的区别 形式上: 字符常量是单引号引起的一个字符, 而字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整形值(ASCII 值),可以参加表达式运算; 而字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小: 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志); (注意: char在Java中占两个字节) 注意: Java要确定每种基本类型所占存储空间的大小。它们的大小并不像其他大多数语言那样随机器硬件架构的变化而变化。这种所占存储空间大小的不变性是Java程序比其它大多数语言编写的程序更具可移植性的原因之一。 局部变量和成员变量 成员变量定义在类中, 整个类中都可以访问; 局部变量定义在函数, 局部代码块中, 只在所属的区域有效; 成员变量存在于堆内存的对象中; 局部变量存在于栈内存的方法中; 成员变量随着对象的创建而存在, 随着对象的消失而消失; 局部变量随着所属区域的执行而存在, 随着所属区域的结束而释放; 成员变量都有默认初始化值(boolean false, char \\u0000(null), byte (byte)0, short (short)0, int 0, long 0L, float 0.0f, double 0.0d, String null); 局部变量没有默认初始化值, 使用前必须先声明和初始化; 成员变量可以被 public,private,static 等修饰符所修饰, 而局部变量不能被访问控制修饰符及 static 所修饰; 但是, 成员变量和局部变量都能被 final 所修饰; hashCode 与 equals https://github.com/Snailclimb/JavaGuide/blob/master/Java相关/Java基础知识.md#9-构造器-constructor-是否可被-override 为什么Java中只有值传递1.","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"05. 基础面向对象知识","slug":"JAVA/2018-12-15-05","date":"2018-12-10T05:21:26.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/10/JAVA/2018-12-15-05/","link":"","permalink":"http://blog.renyimin.com/2018/12/10/JAVA/2018-12-15-05/","excerpt":"","text":"抽象类 接口 在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？在一个静态方法内调用一个非静态成员为什么是非法的在 Java 中定义一个不做事且没有参数的构造方法的作用import java和javax有什么区别Object 类 Object类是所有Java类的根基类, 如果类的声明未使用extends关键字指明其基类, 则默认基类为Object类; 位于 package java.lang; 内 __toString() : 默认返回包名 + 类名 + @ + 哈希码 可以重写 继承 vs 组合 “is-a” 关系使用继承 “has-a” 关系使用组合 从代码复用角度来看, 两个设计都能实现代码复用","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"01. 认识 Spring Boot","slug":"Spring boot/2018-12-07-01","date":"2018-12-07T05:42:15.000Z","updated":"2018-12-07T05:49:58.000Z","comments":true,"path":"2018/12/07/Spring boot/2018-12-07-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/07/Spring boot/2018-12-07-01/","excerpt":"","text":"Spring 简介 spring是一个一站式的轻量级java开发框架, 核心是控制反转(IOC)和面向切面(AOP), 针对于开发的WEB层(springMvc)、业务层(Ioc)、持久层(jdbcTemplate)等都提供了多种配置解决方案; SpringMVC springMvc是spring基础之上的一个MVC框架, 主要处理web开发的路径映射和视图渲染, 属于spring框架中WEB层开发的一部分; springMvc属于一个企业WEB开发的MVC框架, 涵盖面包括前端视图开发、文件配置、后台接口逻辑开发等, XML、config等配置相对比较繁琐复杂;Spring Boot简介 Spring 诞生时是 Java 企业版(Java Enterprise Edition, JEE, 也称 J2EE)的 轻量级代替品。无需开发重量级的 Enterprise JavaBean(EJB), Spring 为企业级 Java 开发提供了一种相对简单的方法, 通过依赖注入和面向切面编程, 用简单的Java 对象(Plain Old Java Object, POJO)实现了 EJB 的功能。 虽然 Spring 的组件代码是轻量级的, 但它的配置却是重量级的。 第一阶段：xml配置 在Spring 1.x时代, 使用Spring开发满眼都是xml配置的Bean, 随着项目的扩大, 我们需要把xml配置文件放到不同的配置文件里, 那时需要频繁的在开发的类和配置文件之间进行切换 第二阶段：注解配置 在Spring 2.x 时代, 随着JDK1.5带来的注解支持, Spring提供了声明Bean的注解(例如@Component、@Service), 大大减少了配置量。主要使用的方式是应用的基本配置(如数据库配置)用xml, 业务配置用注解 第三阶段：java配置 Spring 3.0 引入了基于 Java 的配置能力, 这是一种类型安全的可重构配置方式, 可以代替 XML。我们目前刚好处于这个时代, Spring4.x和Spring Boot都推荐使用Java配置。 所有这些配置都代表了开发时的损耗。 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换, 所以写配置挤占了写应用程序逻辑的时间。除此之外, 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了, 你还要知道这些库的哪个版本和其他库不会有冲突, 这难题实在太棘手。并且, 依赖管理也是一种损耗, 添加依赖不是写应用程序代码。一旦选错了依赖的版本, 随之而来的不兼容问题毫无疑问会是生产力杀手。作者：cnn0314来源：CSDN原文：https://blog.csdn.net/m0_37106742/article/details/64438892版权声明：本文为博主原创文章, 转载请附上博文链接！ Spring Boot是由Pivotal团队提供的全新框架, 其设计目的是用来简化新Spring应用的初始搭建以及开发过程; https://blog.csdn.net/forezp/article/details/81040925","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"02. 基本数据类型 相关","slug":"JAVA/2018-12-05-02","date":"2018-12-05T11:40:16.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/05/JAVA/2018-12-05-02/","link":"","permalink":"http://blog.renyimin.com/2018/12/05/JAVA/2018-12-05-02/","excerpt":"","text":"8种基本数据类型数值型 byte: Java中最小的数据类型, 在内存中占8位( bit), 即1个字节, 取值范围-128~127, 默认值 0 (对应包装类 Byte) short: 短整型在内存中占16位, 即2个字节, 取值范围-32768~32717, 默认值0 (对应包装类 Short) int: 整型, 用于存储整数, 在内在中占32位, 即4个字节, 取值范围-2147483648~2147483647, 默认值 0 (对应包装类 Integer) long: 长整型, 在内存中占64位, 即8个字节-2^63~2^63-1, 默认值 0L (对应包装类 Long) float: 浮点型, 在内存中占32位, 即4个字节, 用于存储带小数点的数字(与double的区别在于float类型有效小数点只有6~7位), 默认值 0.0 (对应包装类 Float) double: 双精度浮点型, 用于存储带有小数点的数字, 在内存中占64位, 即8个字节, 默认值 0.0 (对应包装类 Boolean) 非数值类型 char: 字符型, 用于存储单个字符, 占16位, 即2个字节, 取值范围0~65535, 默认值为 空字符 boolean: 布尔类型, 占1个字节, 用于判断真或假(仅有两个值, 即true、false), 默认值 false 注意: Java不支持无符号数据类型; byte, short, int和long都是有符号数据类型。 基本类型的类型转换 首先回顾一下 Java 中的 8 种基本数据类型, 以及它们的占内存的容量大小和表示的范围, 如下图所示 布尔类型boolean占有一个字节, 由于其本身所代表的特殊含义, boolean类型与其他基本类型不能进行类型的转换, 既不能进行自动类型的提升, 也不能强制类型转换, 否则, 将编译出错; Java中甚至没有提供检测基本类型的方法 (typeof), 因此, 使用基本类型的包装类型比较多! 自动类型转换 自动类型转换包括: 数字表示范围小的数据类型可以自动转换成范围大的数据类型 特例情况下, 数字表示范围大的数据类型也可以自动转换成范围小的数据类型 数字表示范围小的数据类型可以自动转换成范围大的数据类型 这点比较好理解, 如下: 1234long l = 100; // 整型字面量数字常量赋值给 表示范围更大的long型int i = 200;long ll = i; 注意: 范围小的数据类型自动转换成范围大的数据类型时, 也可能出现数据精度丢失, 如下 int 转 float 就出现了精度丢失 12float b = 2147483646;System.out.println(b); // 2.14748365E9 精度丢失 特例: 也可以将数字表示范围大的整型常量字面量 直接赋给表示范围小的 byte, short, char等类型变量, 而不需要强制类型转换, 只要不超出其表数范围; 123int count = 100000000;int price = 1999;long totalPrice = count * price; 编译虽然没任何问题, 但结果却输出的是负数, 这是因为两个 int 相乘得到的结果是 int, 相乘的结果超出了 int 的代表范围, 这种情况, 一般把第一个数据转换成范围大的数据类型再和其他的数据进行运算。 123int count = 100000000;int price = 1999;long totalPrice = (long) count * price; // 注意不能写成 long totalPrice = (long) (count * price); 因为 (count * price) 首先就已经超出了int的范围 但是仅限于 字面量, 如果是整型变量, 则需要使用强制类型转换, 如下: 1234567short s1 = 1; // 整型字面量可以赋值给 short 类型 short s2 = 1;s2 = s2 + 1;// s2+1运算中的操作数s2会被自动提升为int类型// s2+1运算结果会成为int类型, 再赋值给short类型的s2时, 编译器会报告需要强制转换类型的错误// 需要改为 s2=(short)(s2+1); 结果就是short类型 自动转换具体如下图: 实线表示自动转换时不会造成数据丢失, 虚线则可能会出现数据丢失问题 自动类型提升 (final修饰) 所谓类型提升就是指在多种不同数据类型的表达式中, 类型会自动向范围表示大的值的数据类型提升 类型提升规则: 所有的byte,short,char型的值将被提升为int型 如果两个操作数其中有一个是double类型, 另一个操作就会转换为double类型 否则, 如果其中一个操作数是float类型, 另一个将会转换为float类型 否则, 如果其中一个操作数是long类型, 另一个会转换为long类型 否则, 两个操作数都转换为int类型 被final修饰的变量不会自动改变类型, 当2个final修饰相操作时, 结果会根据左边变量的类型而转化。 测试 123456byte b1=1,b2=2,b3,b6,b8;final byte b4=4,b5=6,b7;b3=(b1+b2); // 根据上述规则, b1会转换成int, b2会转换int, 所以结果是int, 而b3是byte, 所以Type mismatch: cannot convert from int to byteb6=b4+b5; // 根据上述规则, b4,b5都是final不会转换int, 所以结果是byte, b6也是byte, 所以Type mismatch: cannot convert from int to byteb8=(b1+b4); // 根据上述规则, b1会转换成int, b4不会转换int, 所以结果是int, 而b8是byte , 所以Type mismatch: cannot convert from int to byteb7=(b2+b5); // 根据上述规则, b2会转换成int, b5不会转换int, 所以结果是int, 而b7是byte , 所以Type mismatch: cannot convert from int to byte 强制类型转换 一般是大容量类型转小容量类型时会用, 又称为造型, 用于显示的转换一个数值的类型, 在有可能丢失信息的情况下进行的转换时通过造型来完成, 但可能造成精度降低或溢出 强制类型转换的语法格式: (type) var, 运算符()中的type表示将值var想要转换成的目标数据类型 当将大容量类型强制转换成小容量类型, 而又超出了目标类型的表示范围, 就会被截断成为一个完全不同的值, 例: 12int x = 300; byte y = (byte)x; //值为44 包装类型 Java语言是一个面向对象的语言, 但是Java中的基本数据类型却是不面向对象的, 也就不具有对象的性质, 这在实际使用时存在很多的不便, 为了解决这个不足, 以便与其他对象”接轨”, 在设计类时为每个基本数据类型设计了一个对应的类进行代表, 这八个和基本数据类型对应的类统称为包装类(Wrapper Class), 有些地方也翻译为外覆类或数据类型类 它相当于将基本类型”包装起来”, 使得基本类型具有了对象的性质, 并且为其添加了属性和方法, 丰富了基本类型的操作 包装类均位于 java.lang 包 使用集合类型Collection时, 就一定要使用包装类型而非基本类型 java也没有检测基本类型的方法, 但是可以用instanceof检测包装类型 包装类型: Integer、Long、Short、Byte、Character、Double、Float、Boolean、BigInteger、BigDecmail 其中 BigInteger、BigDecimal 没有相对应的基本类型, 主要应用于高精度的运算 BigInteger 支持任意精度的整数 BigDecimal 支持任意精度带小数点的运算, 主要用于金钱的计算上 基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成 (JDK5的新特性) 包装类型与基本类型异同 声明方式不同, 基本类型不使用new关键字, 而包装类型需要使用new关键字来在堆中分配存储空间 存储方式及位置不同, 基本类型是直接将变量值存储在栈中, 而包装类型是将对象放在堆中, 然后通过引用来使用 初始值不同, 基本类型的初始值如int为0, boolean为false, 而包装类型的初始值为 null 使用方式不同, 基本类型直接赋值直接使用就好, 而包装类型在集合如Collection、Map时会使用到 引用类型 Array 数组可以存放多个类型统一的数据(可以是基本类型也可以是引用类型) 数组创建的格式 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class arr &#123; public static void main(String[] args) &#123; // 一个数组一旦在内存中创建了, 数组的长度就不能发生变化 /** * 静态创建数组: 指定数组内容, 但不显示指定长度 * - 虽然没有指定长度,但系统会自动识别 * - 虽然直接指定了具体元素的内容, 但在内存中仍然有默认值被替换的过程 * - 静态创建数组有如下几种格式 */ // 1. 标准格式 int[] age1 = new int[]&#123;1,2,3,4,5&#125;; // 2. 标准格式拆两步 int[] age2; age2 = new int[]&#123;1,2,3,4,5&#125;; // 3. 简便格式 (1.虽然没有new, 但内存中仍然会开辟堆空间;2.简便格式不能拆两步) int[] age3 = &#123;1,2,3,4&#125;; System.out.println(age3); System.out.println(age3[0]); System.out.println(age3[1]); System.out.println(age3[2]); // 如下超过数组索引范围就会报错 // System.out.println(age3[5]); // 4. 静态创建数组的间便格式还可以如下,但推荐上一种, 因为它将类型 int[](整型数组) 与 变量名分开了 int a1[] = &#123;1,2,3&#125;; System.out.println(a1); /** * 动态创建数组:显示指定数组长度, 但不指定长度 */ int[] age4 = new int[5]; // 拆两步格式 int[] age5; age5 = new int[5]; age5[0] = 1; age5[1] = 2; age5[2] = 3; // 如下超过数组索引范围就会报错 // age5[5] = 4; System.out.println(age5); System.out.println(age5[0]); System.out.println(age5[1]); System.out.println(age5[2]); &#125;&#125; 引用类型 String熟悉String类型 Java没有内置的字符串类型, 而是在标准Java类库中提供了一个预定义类, 叫做String; 每个用双引号括起来的字符串都是String类的一个实例; 截取子串: String类的 substring(起始位置, 结束位置) (不包括结束位置) 拼接串: 和绝大多数程序设计语言一样, Java 允许使用 + 连接两个字符串; 当将一个字符串与一个非字符串的值进行拼接时, 后者会被转换成字符串(任何Java对象都可以转换成一个字符串) 不可变字符串: String对象一旦被创建就是固定不变的了, 对String对象的任何改变都不影响到原对象, 相关的任何change操作都会生成新的对象; String没有提供用于修改字符串的方法, 如果一个字符串s的值为 ‘Hello’, 你希望将其内容修改为 ‘Help!’, 在Java中你是不能直接将s的最后两个位置的字符修改为 ‘p’ 和 ‘!’, 在Java中要实现上述操作, 需要首先提取需要的字符, 然后再批接上要替换的字符 123456789101112String s1 = &quot;Hello&quot;;String s2 = &quot;Hello&quot;;// 一开始两个String的引用是一样的, 都是Hello这个字符串对象的引用System.out.println(System.identityHashCode(s1)); // 992136656System.out.println(System.identityHashCode(s2)); // 992136656System.out.println(System.identityHashCode(&quot;Hello&quot;)); // 992136656// 字符串经过拼接整理后, 其实就是另一个新的字符串对象了s1 = s1.substring(0, 3) + &quot;p!&quot;;System.out.println(s1); // Help!System.out.println(s2); // HelloSystem.out.println(System.identityHashCode(s1));// 511833308 Java虽然不能修改一个字符串中的字符, 但是可以修改字符串变量, 让它引用另外一个字符串; (如上述例子, 其实s指向了一个新的字符串引用) String Pool 虽然看起来修改一个代码单元要比创建一个新字符串更加简洁, 但不可变字符串有一个优点: 编译器可以让字符串共享; JVM为了提升性能和减少内存开销, 避免字符串的重复创建, 维护了一块特殊的内存空间, 即字符串池(String Pool) 检测字符串是否相等 注意: Java中, 你一定不能使用 == 来检测两个字符串是否相等, 这个运算符只能确定两个字符串是否在同一个位置上, 即只能判断两个字符串的引用是否相同; 而Java中完全有可能将两个内容相等的字符串放在不同的位置上; 如果虚拟机始终将相同的字符串共享, 就可以使用 == 运算符检测两个字符串是否相等, 但实际上只有字符串常量是共享的, 而 +, substring 等操作产生的结果并不是共享的; 因此千万不要使用 == 检测字符串的相等行, 以免在程序中出现糟糕的bug, 而且从表面上看, 这种bug很像随机产生的间歇性错误; 在Java中可以使用 s.equals(t) 方法来检测两个字符串是否相等 (相等返回true, 不相等则返回false, s与t可以是字符串常量,也可以是字符串变量) 1234567891011String s12 = &quot;你好!&quot;;String s1 = &quot;你&quot;;String s2 = &quot;好!&quot;;String s1_s2 = s1 + s2;System.out.println(s12); //你好!// System.identityHashCode可以获取对象的内存地址System.out.println(System.identityHashCode(s12)); //511833308System.out.println(s1_s2); //你好!System.out.println(System.identityHashCode(s1_s2)); //1297685781System.out.println(s12 == s1_s2); //falseSystem.out.println(s12.equals(s1_s2)); //true 如果检测两个字符串是否相等而不区分大小写, 可以使用 equalsIgnoreCase 方法: &quot;Hello&quot;.equalsIgnoreCase(&quot;Hello&quot;) main 主函数 格式是固定的, 被jvm所识别和调用 public : 因为权限必须是最大的; static: 不需要对象的, 直接用主函数所属类名调用即可; void: 主函数没有具体的返回值; main: 函数名, 不是关键字, 只是jvm识别的固定的名字; String[] args: 这是主函数的参数列表, 是一个数组类型的参数, 而且元素都是字符串类型; 虚拟机在运行的时候, 可以给主函数传值, 只不过一般情况下不传","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"01. 基础入门","slug":"JAVA/2018-12-02-01","date":"2018-12-02T00:52:28.000Z","updated":"2018-12-22T06:08:10.000Z","comments":true,"path":"2018/12/02/JAVA/2018-12-02-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/02/JAVA/2018-12-02-01/","excerpt":"","text":"三大体系 JAVASE(J2SE): 标准版(Standard Edition), 开发个人计算机上的应用 (基础) (Java5.0版本后, 进行了更名, J2SE-&gt;JAVASE) JAVAEE(J2EE): 企业版(Enterprise Edition), Java EE是在JavaSE的基础上构建的, 提供Web服务、组建模型、管理和通信API, 主要针对企业应用的开发(例如, 电子商务网站、ERP系统), 这也是我们主要面对的版本 JAVAME(J2ME): 微缩版, (例如, 蜂窝电话和可视电话、数字机顶盒、汽车导航系统等等) 跨平台 Java语言的一个非常重要的特点就是与平台的无关性, Java是通过在JVM中运行Java程序实现跨平台特性的, Java语言使用JVM屏蔽了与具体平台相关的信息, 使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码 (.class字节码文件), 就可以在多种平台上不加修改地运行, Java虚拟机在执行字节码时,把字节码解释成具体平台上的机器指令执行。 注意: JVM 并是不跨平台的, 所以我们需要根据具体的平台安装不同版本的JVM虚拟机(下载不同版本的JDK即可) 运行机制 Java程序的运行必须经过 编写, 编译, 运行三个步骤 Java文件编写完之后, 要以 .java 为文件名后缀, 这样得到的就是Java的源代码/源程序, Java的源程序文件操作系统是不能识别的; Java源代码需要被编译成Java虚拟机能识别的 .class字节码文件此过程需要使用JDK里面的 javac.exe编译器工具 对Java源文件进行编译 然后由JVM把javac.exe编译好的.class字节码解释成机器码后运行通过 Java.exe命令 开始启动JVM对编译后的.class类文件进行解释, 不用写.class后缀, 写上类名即文件名即可 字节码文件 是一种和任何具体机器环境及操作系统环境无关的中间代码, 它是一种二进制文件, 是Java源文件由Java编译器编译后生成的目标代码文件 编程人员和计算机都无法直接读懂字节码文件, 它必须由专用的Java解释器来解释执行, 因此Java是一种在编译基础上进行解释运行的语言 Java解释器 负责将字节码文件翻译成具体硬件环境和操作系统平台下的机器代码, 以便执行 (Java程序不能直接运行在现有的操作系统平台上, 它必须在被称为Java虚拟机的软件平台之上解释后才能在操作系统上运行) Java虚拟机(JVM) 是运行Java程序的软件环境, Java解释器就是JVM的一部分: 在运行Java程序时, 首先会启动JVM, 然后由它来负责解释执行Java的字节码(Java字节码只能运行于JVM之上), 利用JVM就可以把Java字节码程序和具体的硬件平台以及操作系统环境分隔开来 采用字节码的好处Java 语言通过字节码的方式, 在一定程度上解决了传统解释型语言执行效率低的问题, 同时又保留了解释型语言可移植的特点所以 Java 程序运行时比较高效, 而且由于字节码并不专对一种特定的机器, 因此, Java程序无须重新编译便可在多种不同的计算机上运行 JVM,JRE,JDK的区别 JVM(Java Virtual Machine Java): JVM 负责将字节码转换为特定机器代码, 提供了内存管理/垃圾回收和安全机制等, 是实现跨平台的核心 JRE(Java Runtime Envrionment): 普通用户而只需要安装 JRE(Java Runtime Environment)来运行 Java 程序 (JRE中会包含JVM), 而程序开发者必须安装JDK来编译、调试程序 JDK (Java Development kit): 顾名思义它是给开发者提供的开发工具箱, 是给程序开发者用的, 它除了包括完整的 JRE(Java运行环境), 还包含了其他供开发者使用的工具包, 作为开发者, 我们要下的就是JDK 两个基础命令 javac.exe 命令是个编译器, 这个命令是用来把.java的源代码编译成.class这个中间字节码文件的; javac.exe这个编译器是在JDK中才有的; JRE中的 java.exe 命令用来调用JVM去解析经过javac.exe编译后的.class字节码文件的; JRE在运行.class文件的时候是利用JRE中包含的JVM来解析.class文件的, JRE只是个运行环境, 它不需要把.java源文件编译成.class字节码文件, 所以不需要有javac.exe;","categories":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/categories/JAVASE/"}],"tags":[{"name":"JAVASE","slug":"JAVASE","permalink":"http://blog.renyimin.com/tags/JAVASE/"}]},{"title":"06. Jenkins 服务器准备","slug":"Jenkins/2018-12-01-06","date":"2018-12-01T04:36:53.000Z","updated":"2018-12-01T13:26:36.000Z","comments":true,"path":"2018/12/01/Jenkins/2018-12-01-06/","link":"","permalink":"http://blog.renyimin.com/2018/12/01/Jenkins/2018-12-01-06/","excerpt":"","text":"环境准备 Vagrant+VirtualBox+Centos7.2(BOX) Vagrantfile 文件内容简单如下 1234567891011Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :jenkins do |jenkins| jenkins.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;jenkins&quot;, &quot;--memory&quot;, &quot;1000&quot;] end jenkins.vm.box = &quot;centos7.2&quot; jenkins.vm.hostname = &quot;lant&quot; jenkins.vm.network :public_network, ip: &quot;192.168.1.232&quot;, bridge: &quot;en0&quot; jenkins.vm.synced_folder &quot;/Users/renyimin/Desktop/jenkins-synced_folder&quot;, &quot;/jenkins-synced&quot; endend vagrant up 启动即可 git 安装参考开发环境准备 依赖准备安装配置jdk环境 下面下载太慢 1wget https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz?AuthParam=1543591644_31c58b16d992a51440a63ce5c4ceb71f 一般在物理机下载下来, 放到vagrantfile中设置的物理机目录 ~/Desktop/jenkins-synced_folder/ 即可 12mv /jenkins-synced/jdk-8u191-linux-x64.tar.gz /usr/local/src/tar -zxvf jdk-8u191-linux-x64.tar.gz 配置环境变量 12345vim /etc/profileexport JAVA_HOME=/usr/local/src/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarsource /etc/profile 安装成功 1234[root@lant bin]# java -versionjava version &quot;1.8.0_191&quot;Java(TM) SE Runtime Environment (build 1.8.0_191-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) 安装 Tomcat 本地下载tomcat8 (https://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.35/bin/apache-tomcat-8.5.35.tar.gz) 12mv /jenkins-synced/apache-tomcat-8.5.35.tar.gz /usr/local/src/tar -zxvf apache-tomcat-8.5.35.tar.gz 配置环境变量 1234567vi /etc/profile.d/tomcat.shATALINA_BASE=/usr/local/src/apache-tomcat-8.5.35PATH=$CATALINA_BASE/bin:$PATHJENKINS_HOME=$CATALINA_BASE/webapps/jenkinsexport PATH CATALINA_BASE JENKINS_HOME. /etc/profile.d/tomcat.sh 查看tomcat安装成功 123456789101112131415[root@lant src]# catalina.sh versionUsing CATALINA_BASE: /usr/local/src/apache-tomcat-8.5.35Using CATALINA_HOME: /usr/local/src/apache-tomcat-8.5.35Using CATALINA_TMPDIR: /usr/local/src/apache-tomcat-8.5.35/tempUsing JRE_HOME: /usr/local/src/jdk1.8.0_191Using CLASSPATH: /usr/local/src/apache-tomcat-8.5.35/bin/bootstrap.jar:/usr/local/src/apache-tomcat-8.5.35/bin/tomcat-juli.jarServer version: Apache Tomcat/8.5.35Server built: Nov 3 2018 17:39:20 UTCServer number: 8.5.35.0OS Name: LinuxOS Version: 3.10.0-327.4.5.el7.x86_64Architecture: amd64JVM Version: 1.8.0_191-b12JVM Vendor: Oracle Corporation[root@lant src]# 配置tomcat 配置server.xml 1234vim /usr/local/tomcat/conf/server.xml----------------------------------------------------------&gt;&lt;Connector port=&quot;80&quot; protocol=&quot;HTTP/1.1&quot; //默认端口为8080, 改为80connectionTimeout=&quot;20000&quot; tomcat有 manager-gui 的管理页面, 想要使用必须配置管理用户, 不使用可以跳过此步。配置tomcat-users.xml: 123vi /usr/local/src/apache-tomcat-8.5.35/conf/tomcat-users.xml&lt;role rolename=&quot;manager-gui&quot;/&gt; //指定用户可以使用的接口为manager-gui&lt;user username=&quot;tomcat&quot; password=&quot;tomcat&quot; roles=&quot;manager-gui&quot;/&gt; //用户名和密码为tomcat, 在manager-gui接口使用 安装Jenkins到Tomcat 本地下载 jenkins的 war 安装包(https://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/2.138.3/jenkins.war) 1234mkdir /usr/local/src/apache-tomcat-8.5.35/webapps/jenkinsmv /jenkins-synced/jenkins.war /usr/local/src/apache-tomcat-8.5.35/webapps/jenkinscd /usr/local/src/apache-tomcat-8.5.35/webapps/jenkinsjar -xvf jenkins.war 安装完成,现在可以开启 Tomcat 来使用 jenkins 了 12345678[root@lant conf]# catalina.sh startUsing CATALINA_BASE: /usr/local/src/apache-tomcat-8.5.35Using CATALINA_HOME: /usr/local/src/apache-tomcat-8.5.35Using CATALINA_TMPDIR: /usr/local/src/apache-tomcat-8.5.35/tempUsing JRE_HOME: /usr/local/src/jdk1.8.0_191Using CLASSPATH: /usr/local/src/apache-tomcat-8.5.35/bin/bootstrap.jar:/usr/local/src/apache-tomcat-8.5.35/bin/tomcat-juli.jarTomcat started.[root@lant conf]# 在 Client 上打开浏览器,访问 http://192.168.1.232/jenkins 即可 首次登陆时, 按照界面上的提示在服务器上找到密码登录即可 进入安装界面, 根据Jenkins向导安装推荐插件 插件安装结束后, 会提示创建 管理员账号 …… 访问 http://192.168.1.232/jenkins/login?from=%2Fjenkins%2F 前两步在安装完成后可能会出现登录后页面空白的问题, 虽然网上有很多方案, 但发现并不太合理, 最后这边是重启tomcat即可正常访问 功能简介PHP项目持续集成构建代码 使用 Jenkins+git 实现php项目的持续集成 构建一个自由风格的 php-deploy (PHP的部署工具) Gernal配置, 丢弃旧的构建, 防止jenkins构建较多之后变臃肿 源码管理: 这里使用git插件(也可以使用svn插件) 以之前在gitlab中初始化的oms项目为例 (本机要有git, 可以从gitlab仓库拉取代码) 需要将本机git的 publicKey 配置到gitlab中 配置如下如图 (默认是主分支) 然后保存 尝试构建 在 本地开发机中(192.168.1.239) 向 gitlab仓库上推送代码 123456git clone git@192.168.1.231:web/oms.gitcd omstouch README.mdgit add README.mdgit commit -m &quot;尝试提交分支到远端&quot;git push -u origin master 然后使用jenkins构建一次, 发现会拉取master的代码库 上面可以看到代码被构建到jenkins服务器上的 /usr/local/src/apache-tomcat-8.5.35/webapps/jenkins/workspace/oms 目录 发布代码到服务器 之前的测试, jenkins已经可以构建代码到jenkins服务器上, 接下来还需要将代码发布到项目代码将要被诶部署的服务器上才行 在通过Jenkins发布php代码之前, 首先需要在Jenkins服务的web页面上检查一下 “Git plugin” 和 “Publish Over SSH” 两个插件是否已安装: “系统管理” → “管理插件” → “已安装”, 然后搜索 “Git plugin” 看看是否已安装, 一般这个插件都是默认安装的 然后到 “可选插件” 里安装 “Publish Over SSH” 插件 安装完后重启jenkins (systemctl restart jenkins) 配置密钥认证 把jenkins服务器之前生成的公钥拷贝到远程的机器上去( cat ~/.ssh/id_rsa.pub ), 也就是把公钥的内容粘贴到 目标server 的 .ssh/authorized_keys 文件里 再将jenkins服务器之前生成的密钥([root@lant .ssh]# cat ~/.ssh/id_rsa)粘贴在jenkins配置中, 点击”系统管理” → “系统设置”, 下拉页面, 大概在网页的最下面可以找到 “Publish Over SSH”注: Jenkins SSH Key 这一栏默认会使用Jenkins管理员admin账户的密码，可以不填则设置为空密码。 然后修改jenkins上之前 oms的(php-deploy) 的构建配置 完成以上操作后, 就可以使用这个刚刚创建的任务去工作了, 点击 “立即构建”, 就能把git上的代码发布到指定的服务器上 并且最后在目标服务器(192.168.1.233)上确实有了项目代码 参考 https://blog.csdn.net/qq_26886929/article/details/54864439 https://blog.csdn.net/lxw983520/article/details/78903419 https://www.cnblogs.com/jimmy-xuli/p/9072015.html","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"05. 一台服务器模拟目标Server","slug":"Jenkins/2018-12-01-05","date":"2018-12-01T03:51:20.000Z","updated":"2018-12-01T13:17:43.000Z","comments":true,"path":"2018/12/01/Jenkins/2018-12-01-05/","link":"","permalink":"http://blog.renyimin.com/2018/12/01/Jenkins/2018-12-01-05/","excerpt":"","text":"环境准备 Vagrant+VirtualBox+Centos7.2(BOX) Vagrantfile 文件内容简单如下 1234567891011Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :jenkinswebserver do |jenkinswebserver| jenkinswebserver.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;jenkins-webserver&quot;, &quot;--memory&quot;, &quot;1000&quot;] end jenkinswebserver.vm.box = &quot;centos7.2_git&quot; jenkinswebserver.vm.hostname = &quot;lant&quot; jenkinswebserver.vm.network :public_network, ip: &quot;192.168.1.233”, bridge: &apos;en0&apos; jenkinswebserver.vm.synced_folder &quot;/Users/renyimin/Desktop/jenkins-synced_folder&quot;, &quot;/jenkins-synced&quot; endend vagrant up 启动即可","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"04. gitlab 服务器准备","slug":"Jenkins/2018-12-01-04","date":"2018-12-01T01:33:44.000Z","updated":"2018-12-01T13:16:47.000Z","comments":true,"path":"2018/12/01/Jenkins/2018-12-01-04/","link":"","permalink":"http://blog.renyimin.com/2018/12/01/Jenkins/2018-12-01-04/","excerpt":"","text":"环境准备 GitLab是利用 Ruby on Rails 一个开源的版本管理系统, 实现的一个自托管的Git项目仓库, 可通过Web界面进行访问公开的或者私人项目, 它拥有与Github类似的功能, 能够浏览源代码, 管理缺陷和注释。可以管理团队对仓库的访问, 它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序(Wall)进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用, 便于日后有需要的时候进行查找。 环境为Vagrant+VirtualBox+Centos7.2(BOX), Vagrantfile 文件内容简单如下 1234567891011Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :jenkinsgitlab do |jenkinsgitlab| jenkinsgitlab.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;jenkins-gitlab&quot;, &quot;--memory&quot;, &quot;1000&quot;] end jenkinsgitlab.vm.box = &quot;centos7.2&quot; jenkinsgitlab.vm.hostname = &quot;lant&quot; jenkinsgitlab.vm.network :public_network, ip: &quot;192.168.1.231&quot;, bridge: &quot;en0&quot; jenkinsgitlab.vm.synced_folder &quot;/Users/renyimin/Desktop/jenkins-synced_folder&quot;, &quot;/jenkins-synced&quot; endend gitlab 安装部署 依赖安装: yum -y install policycoreutils openssh-server openssh-clients postfix 设置postfix开机自启,并启动(postfix支持gitlab发信功能) : systemctl enable postfix &amp;&amp; systemctl start postfix 下载gitlab安装包, 然后安装 centos 6系统的下载地址:https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el6 centos 7系统的下载地址:https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 下载rpm包并安装12wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-9.5.9-ce.0.el7.x86_64.rpmrpm -i gitlab-ce-9.5.9-ce.0.el7.x86_64.rpm 修改gitlab配置文件指定服务器ip和自定义端口: 123vim /etc/gitlab/gitlab.rbexternal_url &apos;http://192.168.1.231&apos; 重置并启动GitLab 12gitlab-ctl reconfiguregitlab-ctl restart 物理机配置gitlab的host, 访问: http://lant.gitlab.com, 初次登录需要修改默认密码(初始账号默认为root) 登陆后, 显现单配置一下基本界面信息 设置gitlab发信功能 发信系统用的默认的postfix, smtp是默认开启的, 两个都启用了, 两个都不会工作 这里设置关闭smtp，开启postfix 1234vim /etc/gitlab/gitlab.rb找到 #gitlab_rails[&apos;smtp_enable&apos;] = true 改为 gitlab_rails[&apos;smtp_enable&apos;] = false修改后执行 gitlab-ctl reconfigure 另一种是关闭postfix, 设置开启smtp, 相关教程请参考官网https://doc.gitlab.cc/omnibus/settings/smtp.html 测试是否可以邮件通知: 登录并添加一个用户, 这里使用qq邮箱添加一个用户 注意: 如果是在内网自己做测试部署的话, 之前配置了域名, 可能就会导致邮件发送不会出去 1234# 查看系统邮件日志tail /var/log/maillog# 可以看到, 邮件使用的是 &lt;gitlab@lant.gitlab.com&gt;, 但是外网可能找不到该路径localhost postfix/smtp[24784]: 02ABC507AAF: to=&lt;gitlab@lant.gitlab.com&gt;, relay=none, delay=2320, delays=2310/0.1/10/0, dsn=4.4.3, status=deferred (Host or domain name not found. Name service error for name=lant.gitlab.com type=MX: Host not found, try again) 因此先修改gitlab的url为ip, 然后即可看到qq邮箱收到了邮件 123456vim /etc/gitlab/gitlab.rb external_url &apos;http://192.168.1.231&apos;gitlab-ctl reconfiguregitlab-ctl restart 最后即可点击邮件中的连接设置用户密码 gitlab项目仓库初始化 为项目创建相关负责人的Group 为web组创初始化一个oms项目仓库 参考: https://www.cnblogs.com/wenwei-blog/p/5861450.html https://blog.csdn.net/qq_26886929/article/details/54864439 https://blog.csdn.net/lxw983520/article/details/78903419","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"03. 本地环境准备","slug":"Jenkins/2018-11-28-03","date":"2018-11-28T14:55:16.000Z","updated":"2018-12-01T13:24:31.000Z","comments":true,"path":"2018/11/28/Jenkins/2018-11-28-03/","link":"","permalink":"http://blog.renyimin.com/2018/11/28/Jenkins/2018-11-28-03/","excerpt":"","text":"环境准备 Vagrant+VirtualBox+Centos7.2(BOX) Vagrantfile 文件内容简单如下 123456789101112Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :jenkins do |jenkins| jenkins.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;jenkins&quot;, &quot;--memory&quot;, &quot;1000&quot;] end jenkins.vm.box = &quot;centos7.2&quot; jenkins.vm.hostname = &quot;lant&quot; jenkins.vm.network :private_network, ip: &quot;192.168.1.230&quot; # 物理机需要提前准备/Users/renyimin/Desktop/jenkins-synced_folder目录, 虚拟机映射目录会自动创建 jenkins.vm.synced_folder &quot;/Users/renyimin/Desktop/jenkins-synced_folder&quot;, &quot;/jenkins-synced&quot; endend vagrant up 启动即可 Git编译安装 源码下载, 此处选择 git-2.9.5.tar.gz 下载完成后, 将压缩包移动到 /Users/renyimin/Desktop/jenkins-synced_folder 目录 然后登入 192.168.1.230 虚拟机, 在 /jenkins-synced 即可看到压缩包 移动压缩包 mv /jenkins-synced/git-2.9.5.tar.gz /usr/local/src 解压 tar -zxvf git-2.9.5.tar.gz 编译安装 1234cd /usr/local/src/git-2.9.5yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-ExtUtils-MakeMaker package./configure --prefix=/usr/local/gitmake &amp;&amp; make install 设置环境变量 123echo &apos;PATH=/usr/local/git/bin:$PATH&apos; &gt;&gt; /etc/profileecho &apos;export PATH&apos; &gt;&gt; /etc/profilesource /etc/profile 如下即安装完成 12[root@lant /]# git --versiongit version 2.9.5 安装完成后的配置 安装完成后, 还需要最后一步设置(因为Git是分布式版本控制系统, 所以, 每个机器都必须自报家门)注意 git config 命令的 —global 参数, 用了这个参数, 表示你这台机器上所有的Git仓库都会使用这个配置, 当然也可以对某个仓库指定不同的用户名和Email地址 12[root@lant /]# git config --global user.name &quot;lant&quot;[root@lant /]# git config --global user.email &quot;564613464@qq.com&quot; 在本地创建SSH Key在用户家目录下, 看看有没有 .ssh 目录(/Users/renyimin/.ssh)如果没有, 则需要创建SSH Key $ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; (一路回车即可)如果一切顺利的话, 最后即可在用户主目录里找到 .ssh 目录, 里面有 id_rsa 和 id_rsa.pub 两个文件, 这两个就是SSH Key的秘钥对, id_rsa是私钥, 不能泄露出去, id_rsa.pub 是公钥, 可以放心地告诉任何人; 公钥可以配置到你的远程仓库存放的平台上, 方便和远程仓库关联;","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"02. 认识 Jenkins","slug":"Jenkins/2018-11-28-02","date":"2018-11-28T14:48:10.000Z","updated":"2018-12-01T13:24:31.000Z","comments":true,"path":"2018/11/28/Jenkins/2018-11-28-02/","link":"","permalink":"http://blog.renyimin.com/2018/11/28/Jenkins/2018-11-28-02/","excerpt":"","text":"Jenkins Jenkins是一个开源软件项目, 是基于Java开发的一种持续集成工具, 用于监控持续重复的工作, 旨在提供一个开放易用的软件平台, 使软件的持续集成变成可能; Hudson是Jenkins的前身, Hudson后来被收购, 成为商业版, 后来创始人又写了一个jenkins, jenkins在功能上远远超过hudson Jenkins 有非常丰富的插件支持我们可以方便的打通版本库、测试构建环境、线上环境的所有环节, 并且丰富友好的通知使用者和开发、管理人员 部署代码上线流程","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"01. 持续集成","slug":"Jenkins/2018-11-28-01","date":"2018-11-28T14:16:08.000Z","updated":"2018-12-02T05:21:47.000Z","comments":true,"path":"2018/11/28/Jenkins/2018-11-28-01/","link":"","permalink":"http://blog.renyimin.com/2018/11/28/Jenkins/2018-11-28-01/","excerpt":"","text":"Continuous integration (CI) 互联网软件的开发和发布, 已经形成了一套标准流程, 最重要的组成部分就是持续集成(Continuous integration, 简称 CI), 下面引用的是百度百科的解释 持续集成是一种软件开发实践, 即团队开发成员经常集成他们的工作, 通常每个成员至少集成一次, 每天可能会发生多次集成。每次集成都通过自动化的构建(包括编译(可选), 发布, 自动化测试)来验证, 从而尽快地发现集成错误, 许多团队发现这个过程可以大大减少集成的问题, 让团队能够更快的开发内聚的软件; 持续集成通俗地来讲, 就是团队中的每个RD每天都会频繁地将代码集成到主干。它的好处主要有两个: 快速发现错误: 每完成一点更新, 就集成到主干, 可以快速发现错误, 定位错误也比较容易 防止分支大幅偏离主干: 如果不是经常集成, 主干又在不断更新, 会导致以后集成的难度变大, 甚至难以集成 持续集成的目的, 就是让产品可以快速迭代, 同时还能保持高质量; 它的核心措施是, 代码集成到主干之前, 必须通过自动化测试, 只要有一个测试用例失败, 就不能集成; 持续集成要求每当开发人员提交了新代码之后, 就对整个应用进行构建, 并对其执行全面的自动化测试集合。根据构建和测试结果, 我们可以确定新代码和原有代码是否正确的集成在一起。如果失败, 开发团队就要停下手中的工作, 立即修复它。 Martin Fowler 说过 “持续集成并不能消除 Bug, 而是让它们非常容易发现和改正。” 与持续集成相关的, 还有两个概念, 分别是 持续交付 和 持续部署 持续交付 持续交付(Continuous delivery)指的是, 频繁地将软件的新版本, 交付给质量团队或者用户以供评审。这样可以以可持续的方式快速向客户端发布新的更改, 如果评审通过, 代码就进入生产阶段。(小版本快速迭代) 持续交付是在持续集成的基础上, 将集成后的代码部署到更贴近真实运行环境的「类生产环境」（production-like environments）中, 比如, 完成单元测试后, 可以把代码部署到 testing 环境中进行更多的测试, 如果代码没有问题, 可以继续手动部署到生产环境中。 持续交付可以看作持续集成的下一步。它强调的是, 不管怎么更新, 软件是随时随地可以交付的。 持续部署 持续部署(continuous deployment)是持续交付的下一步, 指的是代码通过质量团队或者用户评审以后, 把部署到生产环境的过程自动化。 持续部署的目标是, 代码在任何时刻都是可部署的, 可以进入生产阶段。 参考 http://www.ruanyifeng.com/blog/2015/09/continuous-integration.html https://www.sohu.com/a/236703123_711529","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.renyimin.com/tags/Jenkins/"}]},{"title":"01. 认识Elastic-Job","slug":"Elastic-Job/2018-09-15-01","date":"2018-09-15T06:07:21.000Z","updated":"2018-11-22T04:36:35.000Z","comments":true,"path":"2018/09/15/Elastic-Job/2018-09-15-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/15/Elastic-Job/2018-09-15-01/","excerpt":"","text":"引出Elastic-Job 在项目中使用定时任务是避免不了的, 而在部署定时任务时, 由于部署多台机器会导致同一个任务会执行多次, 因此通常只部署一台机器 比如给用户发送邮件的定时任务, 每天定时的给用户下发邮件, 如果部署了多台, 同一个用户将发送多份邮件; 但是只部署一台机器, 可用性又无法保证 Elastic-Job 就可以帮助解决定时任务在集群部署情况下的协调调度问题, 保证任务不重复不遗漏的执行; Elastic-Job简介 Elastic-Job 是当当开源的一款非常好用的分布式调度框架, 有两个互相独立的子项目组成 Elastic-Job-Cloud: 以私有云平台的方式提供集资源、调度以及分片为一体的全量级解决方案, 依赖Mesos和Zookeeper Elastic-Job-Lite: 定位为轻量级无中心化解决方案, 使用jar包的形式提供分布式任务的协调服务 Elastic-Job-Lite 和 Elastic-Job-Cloud 提供同一套API开发作业, 开发者仅需一次开发, 然后可根据需要以Lite或Cloud的方式部署 这里主要以Elastic-Job-Lite进行调研学习 主要功能 定时任务: 基于成熟的定时任务作业框架Quartz cron表达式执行定时任务 作业注册中心: 基于Zookeeper和其客户端Curator实现的全局作业注册控制中心, 用于注册, 控制和协调分布式作业执行 作业分片: 将一个任务分片成为多个小任务项在多服务器上同时执行 弹性扩容缩容: 运行中的作业服务器崩溃或新增加n台作业服务器, 作业框架将在下次作业执行前重新分片, 不影响当前作业执行 支持多种作业执行模式: 支持OneOff, Perpetual和SequencePerpetual三种作业模式 失效转移: 运行中的作业服务器崩溃不会导致重新分片, 只会在下次作业启动时分片, 启用失效转移功能可以在本次作业执行过程中监测其他作业服务器空闲, 抓取未完成的孤儿分片项执行 运行时状态收集: 监控作业运行时状态, 统计最近一段时间处理的数据成功和失败数量, 记录作业上次运行开始时间, 结束时间和下次运行时间 作业停止, 恢复和禁用: 用于操作作业启停, 并可以禁止某作业运行(上线时常用) 被错过执行的作业重触发: 自动记录错过执行的作业, 并在上次作业完成后自动触发, 可参考Quartz的misfire 多线程快速处理数据: 使用多线程处理抓取到的数据, 提升吞吐量 幂等性: 重复作业任务项判定, 不重复执行已运行的作业任务项, 由于开启幂等性需要监听作业运行状态, 对瞬时反复运行的作业对性能有较大影响 容错处理: 作业服务器与Zookeeper服务器通信失败则立即停止作业运行, 防止作业注册中心将失效的分片分项配给其他作业服务器, 而当前作业服务器仍在执行任务, 导致重复执行 Spring支持: 支持spring容器, 自定义命名空间, 支持占位符 运维平台: 提供运维界面, 可以管理作业和注册中心 https://blog.csdn.net/adi851270440/article/details/80493367","categories":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://blog.renyimin.com/categories/ElasticJob/"}],"tags":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://blog.renyimin.com/tags/ElasticJob/"}]},{"title":"14. 建立双索引--- text分词 + 排序","slug":"elasticsearch/2018-06-21-14","date":"2018-06-21T11:30:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/21/elasticsearch/2018-06-21-14/","link":"","permalink":"http://blog.renyimin.com/2018/06/21/elasticsearch/2018-06-21-14/","excerpt":"","text":"如果对一个 text 类型的字段进行排序, 由于该字段会进行分词处理, 这样的话, 排序的结果就可能不是我们想要的结果; 通常的解决方案是在建立 mapping 时, 同时为该字段建立两个索引: 一个进行分词用来进行全文检索 一个不进行分词, 用来进行排序 注意使用 &quot;fielddata&quot;: true 练习: 创建mapping, 构造数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253DELETE /mywebsitePUT /mywebsite&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot; : &quot;keyword&quot; # &quot;index&quot;: false &#125; &#125;, &quot;fielddata&quot;: true &#125;, &quot;contennt&quot;: &#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125;PUT /mywebsite/article/1&#123; &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57&#125;PUT /mywebsite/article/2&#123; &quot;title&quot;: &quot;JAVA Language&quot;, &quot;content&quot;: &quot;Java LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-12&quot;, &quot;author_id&quot;: 32&#125;PUT /mywebsite/article/3&#123; &quot;title&quot;: &quot;C Language&quot;, &quot;content&quot;: &quot;c LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-09&quot;, &quot;author_id&quot;: 86&#125;GET /mywebsite/_mapping/article``` - 测试, 如果使用**title字段的不分词索引进行检索**, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title.raw”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容{“took”: 9,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;PHP Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;JAVA Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;C Language&quot; ] } ]}} 1- 测试, 如果使用title默认的分词索引进行检索, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容, 是title字段分词后的内容{“took”: 8,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;php&quot; ] }, { ...... &quot;sort&quot;: [ &quot;language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;language&quot; ] } ]}}```","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"13. mapping","slug":"elasticsearch/2018-06-19-13","date":"2018-06-19T11:21:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/19/elasticsearch/2018-06-19-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/elasticsearch/2018-06-19-13/","excerpt":"","text":"mapping核心数据类型 es的文档中, 每个字段都有一个数据类型, 可以是: 一个简单的类型, 如 text, keyword, date, long, double, boolean 或 ip-支持JSON的分层特性的类型,如对象或嵌套 或者像 geo_point, geo_shape 或 completion 这样的特殊类型 为不同目的以不同方式索引相同字段通常很有用, 例如, 字符串字段可以被索引为用于全文搜索的文本字段, 并且可以被索引为用于排序或聚合的关键字字段, 或者, 可以使用标准分析器, 英语分析器和法语分析器索引字符串字段; 之前已经了解过: 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping;为了更准确方便地让es理解我们的意图, 一般我们会对index的type手动来创建mapping mapping操作 GET /products/_mapping/computer 只能在创建index时手动创建mapping, 或者新增field mapping, 但是不能 update filed mapping; 手动创建index并设置mapping 1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot; : &quot;date&quot; &#125;, # 如果不想进行分词, 就设置为 keyword &quot;publisher_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 尝试修改某个字段的mapping, 会报错 1234567891011121314151617181920212223242526272829303132PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; &#125;&#125;# 结果报错&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 测试mapping 测试1 123456789101112131415161718192021222324GET website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 注意 只能在不同的索引中对相同的字段设定不同的datatype, 即便是在同一个index中的不同type, 也不能对相同的field设置不同的datatype;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"12. 分词器","slug":"elasticsearch/2018-06-17-12","date":"2018-06-17T09:05:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/17/elasticsearch/2018-06-17-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/17/elasticsearch/2018-06-17-12/","excerpt":"","text":"之前在介绍mapping时, 已经了解到, ES会根据文档的字段类型, 来决定该字段是否需要进行分词和倒排索引, 而分词器的主要工作就是对字段内容进行分词, 通过分词器处理好的结果才会拿去建立倒排索引; ES内置的分词器: standard analyzer simple analyzer whitespace analyzer language analyzer 测试分词器: 12345678910111213141516171819202122232425262728293031GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, # 切换进行测试 &quot;text&quot; : &quot;Test to analyze&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;test&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125; ]&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"11. 诡异的搜索结果 引出 mapping","slug":"elasticsearch/2018-06-16-11","date":"2018-06-16T11:23:16.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-11/","excerpt":"","text":"诡异的搜索结果 构造数据 1234567891011121314151617181920DELETE /websiteGET /website/_mappingPUT /website/article/1&#123; &quot;post_date&quot;: &quot;2018-06-20&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;php is the best language&quot;&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2018-06-21&quot;, &quot;title&quot;: &quot;java&quot;, &quot;content&quot;: &quot;java is the second&quot;&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2018-06-22&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;C++ is third&quot;&#125; 诡异的搜索结果 1234GET /website/article/_search?q=2018 # 3条查询结果GET /website/article/_search?q=2018-06-21 # 3条查询结果GET /website/article/_search?q=post_date:2018-06-22 # 1条查询结果GET /website/article/_search?q=post_date:2018 # 0条查询结果 结果分析 前两个查询之所以能匹配到所有文档, 是因为查询时并没有指定字段进行匹配, 所以默认查询的是_all字段, 而_all是经过分词的并且有倒排索引对于第一个查询来说, 2018 这个值进行分词后还是2018, 自然是可以匹配到所有文档的而对于第二个查询来说, q=2018-06-21 进行分词后也包含2018, 所以也可以匹配到所有文档 对于第三个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 可以匹配到是比较正常的情况 而对于第四个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 但却没有结果 这就要引出 ES 的mapping机制了 Mapping映射 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping; mapping就是index的type的元数据, 每个type都有一个自己的mapping, 决定了该type下文档中每个field的数据类型, 分词及建立倒排索引的行为 以及 进行搜索的行为; ES在自动创建mapping时, 会根据字段值去自行猜测字段的类型, 不同类型的field, 有的是full-text, 有的就是exact-value 对于 full-text型的field, es会对该filed内容进行分词, 时态转换, 大小写转换, 同义词转换等一系列操作后, 建立倒排索引; 对于 exact-value型的field, es则不会进行分词等处理工作 full-text型 和 exact-value型 的不同, 也决定了当你进行搜索时, 其处理行为也是不同的 如果指明要搜索的field, ES也会根据你要搜索的字段的类型, 来决定你发送的搜索内容是否先需要进行全文分析…等一些列操作 当然, 如果你搜索时不指定你具体字段, 则搜索的是 _all, 是会先对你的发送的搜索内容进行分词等操作的 之前诡异的例子中, 其实就是因为在创建文档时, 由于 post_date 字段的值被ES自认为是date类型(exact-value精确值), 所以es不会对这种类型做分词及倒排索引, 所以 GET /website/article/_search?q=post_date:2018 在搜索时候, 其实是去精准匹配post_date字段了, 所以匹配不到; 查看你索引type的默认mapping: 123456789101112131415161718192021222324252627282930313233GET /website/_mapping&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 引出手动创建mapping 除了让es自动为我们创建mapping, 一般我们都是在创建文档前, 先手动去创建index和type, 以及type对应的mapping 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, ES 需要知道每个域中数据的类型 而很显然我们比ES更了解我们的字段类型, ES根据值去判断的话, 很容易出现误判 比如你如果字段是个日期, 可能形式为 2018-06-20 12:13:15 但ES可能不会认为这是个date类型, 如果是 2018-06-20 它又认为是date类型, 所以还是自己手动设置比较好 虽然映射是index的type的, 但事实上, 如果在相同的index中, 即使在不同的type, 你也不能对相同字段做不同的类型指定, 可参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"10. 了解 `_all` field","slug":"elasticsearch/2018-06-16-10","date":"2018-06-16T06:29:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-10/","excerpt":"","text":"_all 对于在学习 query-string 搜索时, 一般这样来用 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc 这种查指定字段进行筛选的方式; 其实ES还可以直接 GET /products/computer/_search?q=diaonao 来进行检索, 这种检索方式会对文档中的所有字段进行匹配; 之所以可以对文档中的所有字段进行匹配, 是 _all 元数据的作用 当你在ES中索引一个document时, es会自动将该文档的多个field的值全部用字符串的方式连接起来, 变成一个长的字符串, 作为 _all field值, 同时对_all分词并建立索引; 之后在搜索时, 如果没有指定对某个field进行搜索, 默认就会搜索 _all field, 而你传递的内容也会进行分词后去匹配 _all 的倒排索引 练习 1234567891011DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 下面的检索都可以搜索到上面的文档GET /products/computer/_search?q=4500GET /products/computer/_search?q=xuhang","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"09. 组合多条件搜索","slug":"elasticsearch/2018-06-16-09","date":"2018-06-16T02:07:26.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-09/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-09/","excerpt":"","text":"查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all : 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match : 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 keyword 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 12345678910111213141516171819202122232425DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot; : &#123; &quot;query&quot;: &quot;language&quot;, &quot;fields&quot;:[&quot;content&quot;, &quot;title&quot;] &#125; &#125;&#125; range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者 keyword字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678910111213141516DELETE /testGET /test/_mapping/languagePUT /test/language/1&#123; &quot;first_name&quot;: &quot;jhon&quot;, &quot;last_name&quot;: &quot;ren&quot;&#125;GET /test/language/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;Jhon&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter, 这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果 (不过, 这只是当should不在must或should下的时候) 只有 must 和 must_not 中的子句是决定了是否能查询出数据 而 should 只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分 (如果should之外的其他语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, # 增加评分 &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 上面那种 should 用法的不同之处 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;, &quot;author_id&quot;: 71&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;, &quot;author_id&quot;: 32&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;, &quot;author_id&quot;: 56&#125;# 下面顺带演示了sort定制排序, 而不是使用默认的相关度排序GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125; a AND (b OR c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句此时的用法和一开始那种不同, 它不仅仅是提升结果评分, 而是直接决定了结果是否匹配 12345678910111213141516GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 123456789101112131415161718192021GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; # 过滤可以使用 constant_score &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; a OR (b AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 1234567891011121314151617181920GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 constant_score &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; 组合过滤 和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"08. 全文检索,结构化精确检索,短语检索,统计 预习","slug":"elasticsearch/2018-06-15-08","date":"2018-06-15T10:56:31.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/15/elasticsearch/2018-06-15-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/elasticsearch/2018-06-15-08/","excerpt":"","text":"查询和过滤 在es中检索文档时候, 对文档的筛选分为 查询 和 过滤, 这两种方式是不太一样的 练习, 搜索商品desc字段中包含 ‘diannao’, 并且售价大于5000的商品 1234567891011121314151617GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot; : &#123;&quot;gt&quot;: 5000&#125; &#125; &#125; &#125; &#125;&#125; 注意: 结构化检索(精确类型字段的检索) 一般会被放到filter过滤语句中, 不会进行分词和相关度排名, 但会对过滤进行缓存 全文检索(全文类型字段的检索) 一般用查询语句进行筛选, 会进行分词和相关度排名 full-text 检索 ES可以进行全文检索并可以进行相关度排名 重新准备数据 1234567891011121314151617181920212223242526272829DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer&quot;, &quot;desc&quot; : &quot;gaoqing hongji diannao&quot;, &quot;price&quot; : 4870, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;chaobao&quot;, &quot;gaoqing&quot;] &#125;PUT /products/computer/3&#123; &quot;name&quot; : &quot;dell&quot;, &quot;desc&quot; : &quot;daier chaoji diannao&quot;, &quot;price&quot; : 5499, &quot;tag&quot; : [&quot;shishang&quot;, &quot;gaoqing&quot;, &quot;gaoxingneng&quot;] &#125;POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125; 练习, 全文检索 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;&#125; 练习 全文高亮检索 12345678910111213GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;desc&quot; : &#123;&#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWbE6HmlWC0s-aachNUv&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;huawei&quot;, &quot;desc&quot;: &quot;china best diannao gaoqing&quot;, &quot;price&quot;: 6080, &quot;tag&quot;: [ &quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;desc&quot;: [ &quot;china best &lt;em&gt;diannao&lt;/em&gt; &lt;em&gt;gaoqing&lt;/em&gt;&quot; ] &#125; &#125;, ...... 结构化精确检索phrase search(短语搜索) 与全文索引不同, 全文索引会对你发送的 查询串 进行拆分(做分词处理), 然后去倒排索引中与之前在存储文档时分好的词项进行匹配, 只要你发送的查询内容拆分后, 有一个词能匹配到倒排索引中的词项, 该词项所对应的文档就可以返回; phrase search(短语搜索)则不会对你发送的 查询串 进行分词, 而是要求在指定查询的字段中必须包含和你发送的查询串一模一样的内容 才算是匹配, 否则该文档不能作为结果返回; 短语搜索 和 结构化搜索还是不一样 结构化搜索是 你的查询串 和 指定的文档字段内容 是完全一致的, 查询串和字段本身都不会做分词, 一般该字段也是精确类型的字段类型; 而 短语搜索 则是, 你的 查询串 不会做分词, 但是你查询的字段可能会做分词, 你的查询串需要包含在 指定字段中; 下一篇可以查看一下terms的用法和效果 搜索商品desc字段中包含 ‘gaoqing diannao’短语 的文档 123456789# 短语检索GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;desc&quot;: &quot;diannao gaoqing&quot; &#125; &#125;&#125; 结果发现, 虽然还是查询的全文字段desc, 但是结果却只有一个 提前了解ES统计语法 统计商品 每个tag下的商品数量, 即, 根据商品的tag进行分组 12345678GET /products/computer/_search&#123; &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 初次运行报错 1234567891011121314151617181920212223242526&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;products&quot;, &quot;node&quot;: &quot;eCgKpl8JRbqwL3QY0Vuz3A&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; &#125; ] &#125;, &quot;status&quot;: 400&#125; 解决方案: 将文本field的 filedata 属性设置为true (现在不用知道这玩意儿, 先尽快解决, 看到聚合分析的预发和效果, 后面讲在详聊该问题) 123456789PUT /products/_mapping/computer&#123; &quot;properties&quot;: &#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 重新执行统计语句, 发现返回中除了分析的结果, 还包含了查询的文档内容; 如果只想显示聚合分析的结果, 可以如下设置size为0: 123456789GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 针对名称中包含”china”的商品, 计算每个tag下的商品数 12345678910111213GET /products/computer/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;gaoqing&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot; : &#123; &quot;terms&quot; : &#123;&quot;field&quot;: &quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格 (先分组, 再计算每组的平均值) 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot; : &#123; &quot;avg&quot; : &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格, 并且按照平均价格进行排序 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;, &quot;order&quot;: &#123;&quot;avg_by_price&quot;:&quot;desc&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 按照指定的价格范围区间进行分组, 然后再每个分组内再按照tag进行分组, 最后在计算每组的平均价格 1234567891011121314151617181920212223242526GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ &#123;&quot;from&quot;:4500, &quot;to&quot;:5000&#125;, &#123;&quot;from&quot;:5000, &quot;to&quot;:5500&#125;, &#123;&quot;from&quot;:5500, &quot;to&quot;:6100&#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;:&#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;4500.0-5000.0&quot;, &quot;from&quot;: 4500, &quot;to&quot;: 5000, &quot;doc_count&quot;: 2, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4870 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5000.0-5500.0&quot;, &quot;from&quot;: 5000, &quot;to&quot;: 5500, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5500.0-6100.0&quot;, &quot;from&quot;: 5500, &quot;to&quot;: 6100, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"07. 查询小优化","slug":"elasticsearch/2018-06-14-07","date":"2018-06-14T02:50:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/14/elasticsearch/2018-06-14-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/14/elasticsearch/2018-06-14-07/","excerpt":"","text":"由于你的每个查询操作都可能会被转发到不同node的shard去执行, 现在假设你的查询, 会打到不同的10个shard上, 每个shard上都要花费1秒钟才能出结果, 这样你总共10s后才会给用户响应, 如果是个商品列表, 用户体验就会非常差 假设本来需要在10秒钟拿到100条数据(每个shard上10条), 现在你可以设置让es在1秒钟就让请求返回, 只拿到部分数据即可 此时可以在查询请求时跟上 timeout 参数(10ms, 1s， 1m): GET /_search?timeout=1ms (可灌入大量数据做测试) 深度分页问题: 假设你的列表每页展示20条数据, 总共1万页, 当我们在使用ES进行分页搜索时, 你想查询第9900页的那20条数据当你的请求到达第一个协调节点后, 它会要求ES给你返回所有该索引对应的primary-shard上的前9900页的数据, 然后es在内存中排序后, 这样会大量占用当前协调节点的计算机资源, 所以尽量避免出现这种深度分页的查询;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"06. mget, bulk 批量操作","slug":"elasticsearch/2018-06-10-06","date":"2018-06-10T09:06:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-06/","excerpt":"","text":"mget 批量查询 批量查询可以只发送一次网络请求, 返回多条查询结果, 能大大缩减网络请求的性能开销 练习 : 12345678GET /_mget&#123; &quot;docs&quot; : [ &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:1&#125;, &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:2&#125;, &#123;&quot;_index&quot;:&quot;blogs&quot;,&quot;_type&quot;:&quot;php&quot;,&quot;_id&quot;:1&#125; ]&#125; bulk 语法: 每个操作要两个json串, 语法如下: 12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; 可以执行的操作类型如: delete: 删除一个文档, 只要一个json串就可以了 create: PUT /index/type/id/_create 创建, 存在会报错 index: 即普通的 put 操作, 可以是创建也可以是全量替换文档 update: 执行部分字段更新 练习: 12345678910111213141516171819DELETE /productsPUT /products/computer/1 # 先创建一个文档&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;GET /products/computer/_searchPOST /products/_bulk&#123;&quot;delete&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 1&#125;&#125; # 删除id为1的文档 (1行json即可)&#123;&quot;create&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 2&#125;&#125; # 创建id为2的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-create-test2&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;&#125;&#125; # 创建一个文档 (es生成id, 2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3&#125;&#125; # 创建一个id为3的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;, &quot;test_field2&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;update&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3, &quot;_retry_on_conflict&quot;: 3 &#125;&#125; # 更改id为3的文档中的test_field字段&#123;&quot;doc&quot; : &#123;&quot;test_field&quot; : &quot;_bulk-index-update-test3&quot;&#125;&#125; bulk操作中, 任何一个操作失败, 不会影响其他的操作, 但是在返回结果里会有异常日志 bulk的请求会被加载到内存中, 所以如果太大的话, 性能反而会下降, 因此需要通过反复测试来获取一个比较合理的bulk size, 一般从1000~5000条数据开始尝试增加数据; 如果看大小的话, 最好在5-15M之间;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"05. ES的搜索方式 Query-string 与 query DSL, multi-index, multi-type搜索模式","slug":"elasticsearch/2018-06-10-05","date":"2018-06-10T06:33:46.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-05/","excerpt":"","text":"Query-string 搜索 之所以叫 query-string, 是因为search的参数都是以http请求的 query-string 来传递的 练习, 搜索全部商品 GET /products/computer/_search 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc query-string这种搜索比较适合在命令行使用curl快速地发一个请求来检索信息, 如果查询比较复杂, 一般不太适用, 正式开发中比较少用; query DSL DSL(Domain Specified Language): 领域特定语言 (这里即 ES的领域特定语言), 是在HTTP的请求体中通过json构建查询语法, 比较方便, 可以构建各种复杂语法; 练习, 查询所有商品 123456GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 1234567891011GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &quot;desc&quot;&#125; ]&#125; 练习, 分页查询商品 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot; : 0, &quot;size&quot; : 2&#125; 练习, 指定需要返回的字段 (使用_source元数据: 可以指定返回哪些field) 1GET /products/computer/1?_source=name,price 1234567GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot; : [&quot;name&quot;, &quot;desc&quot;, &quot;tag&quot;]&#125; query DSL 可以在HTTP请求体中构建非常复杂的查询语句, 所以比较常用; 更多复杂用法后面会聊到; multi-index, multi-type搜索模式 GET /_search : 检索所有index, 所有type下的数据 GET /index/_search : 指定一个index, 搜索其下所有type的数据 GET /index1,index2/_search : 指定多个index, 搜索他们下面所有type的数据 GET /index1,index2/type1,type2/_search : 指定多个index, 搜索他们下面指定的多个type的数据 _all/type1,type2/_search : 搜索所有index下指定的多个type的数据","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"01.","slug":"zookeeper/2018-06-10-01","date":"2018-06-10T05:21:09.000Z","updated":"2018-12-15T08:39:02.000Z","comments":true,"path":"2018/06/10/zookeeper/2018-06-10-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/zookeeper/2018-06-10-01/","excerpt":"","text":"一致性模型 弱一致性 强一致性 同步 Paxos Raft(multi-paxos) ZAB(multi-paxos) CAP 强一致性算法 Paxos 算法 Raft 算法 ZAB 一致性hashnameserverzookeeper 不适合存储大量数据https://github.com/Snailclimb/JavaGuide/blob/master/主流框架/ZooKeeper.md","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.renyimin.com/categories/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.renyimin.com/tags/zookeeper/"}]},{"title":"04. 简单尝试 CURD","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T02:36:57.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"Cat Api ES提供的 Cat Api 可以用来查看 集群当前状态, 涉及到 shard/node/cluster 几个层次 尝试使用 GET /_cat/health?v 查看 时间戳、集群名称、集群状态、集群中节点的数量 等等 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1540815645 20:20:45 elasticsearch yellow 1 1 6 6 0 0 6 0 - 50.0% 返回信息 和 集群健康API(GET _cluster/health) 返回都一样 索引文档ES 中可以使用 POST 或 PUT 来索引一个新文档, 熟悉HTTP协议的话, 应该知道 PUT是幂等的, 而POST是非幂等的, ES也遵循了这一点 PUT PUT 创建文档的时候需要手动设定文档ID (类似已知id, 进行修改) 如果文档不存在, 则会创建新文档; 如果文档存在, 则会覆盖整个文档 (所以需要留意) 虽然使用PUT可以防止POST非幂等引起的多次创建, 但也要留意使用PUT带来的文档覆盖问题 练习: 12345678910111213141516171819202122# 此处创建一个 索引为 products , 类型为 computer, 文档ID为1的商品 PUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 返回&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, # 表示应该写入的有两个分片(1个主分片和1个副本分片, 但注意: 这里代表的可不是总分片数, 显然es的索引默认对应5个主分片, 每个主分片又对应一个副本分片, 总共会有10个分片) &quot;successful&quot;: 1, # 表示成功写入一个分片, 即写入了主分片, 但是副本分片并未写入, 因为目前只启了一个节点 &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 另外, 注意: 使用PUT创建文档时, 如果不指定ID, 则会报错 POST POST 创建文档时不需要手动传递文档ID, es会自动生成全局唯一的文档ID 练习 12345678910111213141516171819202122POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125;# 返回, 可以看到文档ID是自动生成的, 其他字段和使用`PUT`时返回的信息相同&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWa_MgAhWC0s-aachNUS&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 检索文档先尝试最简单的一种 query-string 查询方式: GET /products/computer/_search : 查询/products/computer/下的所有文档 更新文档 PUT、POST PUT 对整个文档进行覆盖更新 1234PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer-hongji&quot;&#125; partial update: 如果只是想更新文档的部分指定字段, 可以使用 POST 结合 _update : (partial update内置乐观锁并发控制) 123456POST /products/computer/2/_update?retry_on_conflict=5&#123; &quot;doc&quot;: &#123; &quot;name&quot; : &quot;acer-hongji-鸿基&quot; &#125;&#125; 这里注意一下_update的内部机制其实是: es先获取整个文档, 然后更新部分字段, 最后老文档标记为deleted, 然后创建新文档此时在标记老文档为deleted时就可能会出现并发问题, 如果线程1抢先一步将老文档标注为deleted, 那么线程2在将新文档标注为deleted时就会失败(version内部乐观锁机制)此时在es内部会做处理, 他内部完成了对乐观锁的实现, 如果失败后, 其实也是进行重试, 你可以手动传递 retry_on_conflict参数来决定其内部的重试次数 PUT如何只创建不替换: 由于创建文档与全量替换文档的语法是一样的, 都是 PUT, 而有时我们只是想新建文档, 不想替换文档 可以使用 op_type=create 来说明此命令只是用来执行创建操作的PUT /index/type/id?op_type=create 或 PUT /index/type/id/_create 可以看到, 此时, 如果文档已经存在, 会进行报错提示冲突, 而不会帮你直接替换1234567PUT /products/computer/1?op_type=create&#123; &quot;name&quot; : &quot;huawei create&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing create&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;, &quot;create&quot;] &#125; 删除文档 ES的文档替换: 上面已经了解过, 其实就是PUT创建文档, 如果传递的文档id不存在, 就是创建, 如果文档id已经存在, 则是替换操作; 注意: es在做文档的替换操作时, 会将老的document标记为deleted, 然后新增我们给定的那个document, 当后续创建越来越多的document时, es会在适当的时机在后台自动删除标记为delete的document; ES的删除: 不会直接进行物理删除, 而是在数据越来越多的时候, es在合适的时候在后台进行删除 练习: 123456789101112131415DELETE /products/computer/2# 返回&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"03. ES 一些基本概念","slug":"elasticsearch/2018-06-09-03","date":"2018-06-09T10:23:07.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-03/","excerpt":"","text":"近实时 从文档被索引到可以被检索会有轻微延时, 约1s Index(索引 n) 这里的Index是个名词, 类似于传统RDS的一个数据库, 是存储document的地方 一个Index可以包含多个 type (索引的复数词为 indices 或 indexes) index 名称必须是小写, 不能用下划线开头, 不能包含逗号 一般将不同的项目数据放到不同的index中 每个index会物理地对应多个分片, 这样, 每个项目都有自己的分片, 互相物理地独立开, 如果有项目是做复杂运算的, 也不会影响其他项目的分片 索引(v) : ES中的还会提到 索引一个文档, 这里的 索引 是动词, 存储文档并建立倒排索引的意思; Type(类型) 一个Index中可以有多个type 代表document属于index中的哪个类别(type 可以对同一个index中不同类型的document进行逻辑上的划分,可以粗略地理解成传统数据库中的数据表?) 名称可以是大小写, 不能用下划线开头, 不能包含逗号 注意: type是对index做的逻辑划分, 而shard是对index做的物理划分 Document(文档) ES中的最小数据单元, ES使用 JSON 作为文档的序列化格式 (ES中的文档可以通俗地理解成传统数据库表中的一条记录) _id: 文档id 可以手动指定, 也可以由es为我们生成; 手动指定id: 根据应用情况来判断是否符合手动指定 document id, 一般如果是从某些其他的系统中导入数据到es, 就会采用这种方式, 就是使用系统中已有的数据的唯一标识作为es中的document的id;比如从数据库中迁移数据到es中, 就比较适合采用数据在数据库中已有的primary key;put /index/type/id 自动生成id: 如果说我们目前要做的系统主要就是将数据存储到es中, 数据产生出来以后直接就会存放到es, 所以不需要手动指定document id的形式, 可以直接让es自动生成id即可;post /index/typees自动生成的id长度为20个字符, URL安全, base64编码, GUID, 分布式并行生成时, es会通过全局id来保证不会发生冲突; Cluster(集群) 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 一个ES节点属于哪个集群, 是由其配置中的 cluster.name 决定的; 节点启动后, 其默认name是elasticsearch, 因此如果在一个机器中启动一堆节点, 那它们会自动组成一个es集群(因为它们的cluster.name都是elasticsearch) 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; Shard(分片): type是对index做的逻辑划分, 而shard是对index做的物理划分 一个分片就是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 分片是数据的容器, 文档其实是保存在分片中的: 当我们将很多条document数据添加到索引中时, 索引实际上是指向一个或者多个物理分片; 因此, 你要存储到索引中的数据其实会被分发到不同的分片中, 而每个分片也仅保存了整个索引中的一部分文档; 当你的集群规模扩大或者缩小时(即增加或减少节点时), ES 会自动的在各节点中迁移分片, 而数据是存放在shard中的, 所以最终会使得数据仍然均匀分布在集群里 shard 可以分为 primary shard(主分片), replica shard(副本分片) replica shard 可以容灾, 水平扩容节点时, 还可以自动分配来提高系统负载 默认情况下, 每个index有5个parimary shard, 而每个parimary shard都有1个replica shard, 即每个index默认会对应10个shard 另外, ES规定了, 每个index的 parimary shard 和 replica shard 不能在全部都在同一个节点上, 相同内容的 replica shard 也不能在同一节点上, 不然起不到容灾作用; 集群状态 yellow 在ES中, 每个索引可能对应多个主分片, 每个主分片也都可能对应多个副本分片 对于每个索引, 要保证不会导致es集群为 yellow, 需要注意: es节点数 &gt;= number_of_replicas+1 当索引的 `number_of_replicas=1` 时, 无论 `number_of_shards` 为多少, 2个节点 (`es节点数 = number_of_replicas+1`) 就可以保证集群是 green; 当索引的 `number_of_replicas&gt;1` 时, 只有当 `es节点数 = number_of_replicas+1` 时, 集群才会变为green; 对于任何一个索引, 由于任何具有相同内容的分片(相同主分片的两个副本分片, 或者主分片和其某个副本分片)不会被放在同一个节点上, 所以如果节点数量不够的话, 有些replica-shard分片会处于未分配状态, 集群状态就不可能是green而是yellow; 比如索引 test 有 3个主分片, 每个主分片对应3个副本分片(该索引总共 3+3*3=12 个分片), 那么至少得4(number_of_replicas+1)个节点, 才能保证每个节点上都不会出现具有相同内容的分片, 即可以保证集群是green;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"02. ES 版本选择及简单安装","slug":"elasticsearch/2018-06-09-02","date":"2018-06-09T06:56:01.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-02/","excerpt":"","text":"版本选择 ES 的版本迭代比较快, 目前(06/2018)为止, 已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X; 此处之所以选用5.5.3来学习调研, 主要是因为公司选用的阿里云服务提供的是 ES 5.5.3版本 (所以你在选择版本时, 也可以根据 自建、购买云服务 来决定) 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (此处使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错(调研使用的是 centos7-minimal 版) 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加如下两行配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 12345678910111213&#123; &quot;name&quot; : &quot;rjAFeY9&quot;, # node 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, # 节点默认的集群名称 (可以在es节点的配置文件elasticsearch.yml中进行配置) &quot;cluster_uuid&quot; : &quot;zaJApkNPRryFohhEMEVH5w&quot;, &quot;version&quot; : &#123; # es 版本号 &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 上面未解释的信息暂时先不用了解 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本和集群相关的信息, 但如果能有一个可视化的环境来操作它可能会更直观一些, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装Kibana 是一个开源的分析和可视化平台, 属于 Elastic stack 技术栈中的一部分, Kibana 主要提供搜索、查看和与存储在 Elasticsearch 索引中的数据进行交互的功能, 开发者或运维人员可以轻松地执行高级数据分析, 并在各种图表、表格和地图中可视化数据;接下来主要就是使用Kibana的DevTools提供的控制台进行ES的学习 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 (因为kibana有个副本分片并没有处于正常状态, 因为当前只有一个节点, 副本分片无法被分配到其他节点, 具体细节先不用着急, 后面会进行分析) 小结到此为止, 应该对ES有了最基础的了解, 且基本环境已经安装完毕, 对于后续的练习暂时就够了","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"01. 初识 Elasticsearch","slug":"elasticsearch/2018-06-09-01","date":"2018-06-09T06:24:25.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-01/","excerpt":"","text":"可以通过如下几个特点来认识ES: 开源 基于 Lucene, 提供比较简单的Restful APILucene 可以说是当下最先进、高性能、全功能的搜索引擎库, 由Apache软件基金会支持和提供(更多细节自行了解)但Lucene非常复杂, ES的目的是使全文检索变得简单, 通过隐藏 Lucene 的复杂性, 取而代之的提供一套简单一致的 RESTful API 高性能全文检索和分析引擎, 并可根据相关度对结果进行排序 可以快速且 近实时 地存储,检索(从文档被索引到可以被检索只有轻微延时, 约1s)以及分析 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级 结构化 或 非结构化 数据 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引 应用场景 当你的应用数据量很大, 数据结构灵活多变, 数据之间的结构比较复杂, 如果用传统数据库, 可能不仅需要面对大量的表设计及数据库的性能问题, 此时可以考虑使用ES, 它不仅可以处理非结构化数据, 而且可以帮你快速进行扩容, 承载大量数据; 具体比如多数据源聚合大列表页: 微服务架构是目前很多公司都采用的架构, 所以经常会面对 多数据源聚合的 大列表页, 一个列表中的筛选字段,展示字段可能会来自多个服务, 同时涉及到分页, 所以传统方案可能比较吃力, 而且也得不到比较好的效果; (RRC这边目前是使用 ES 做 数据视图服务, 对这种大列表页所用到的数据源字段做统一配置和聚合) 日志数据分析, RRC 使用 ElasticStack 技术栈来很方便地对各服务的日志进行查询,分析,统计; 站内搜索(电商, 招聘, 门户 等等)都可以使用 ES 来做全文检索并根据相关性进行排名, 高亮展示关键词等;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"04. 栈的应用 -- 结合后缀表达式求解 四则运算","slug":"DataStructure/2017-11-17-13","date":"2017-11-17T05:44:48.000Z","updated":"2018-12-30T11:44:06.000Z","comments":true,"path":"2017/11/17/DataStructure/2017-11-17-13/","link":"","permalink":"http://blog.renyimin.com/2017/11/17/DataStructure/2017-11-17-13/","excerpt":"","text":"引文 在现实生活中, 对于像 9+(3-1)*3+10/2 这类简单四则运算的题目, 对我们来说自然是非常简单, 但之所简单, 是因为对于我们来说, 从小根深蒂固了 先乘除,后加减, 从左算到右, 先括号内后括号外 这句运算口诀, 你可以在大脑中对运算式快速进行整理并求出结果; 但对计算机来说, 求解的困难就在于: 运算式 9+(3-1)*3+10/2 只是一串顺序输入到计算机中的字符串, 要想让它按照运算口诀对这串字符进行处理, 自然需要设计一套能够符合计算机口味, 让计算机能识别的处理流程; 为了让计算机能对简单的四则运算进行处理, 波兰逻辑学家J.Lukasiewicz 于1929年提出了一种不需要括号的后缀表达式, 也称为 逆波兰(Reverse Polish Notation, RPN)表示 之所以叫后缀表达式, 原因是所有的符号都是要在运算数字的后面出现, 并且表达式中不再包含括号了; 而我们平时所用的标准四则运算表达式, 又叫中缀表达式; 运算式 9+(3-1)*3+10/2, 如果用后缀表示法来表示, 结果为 : 9 3 1 - 3 * + 10 2 / + 对于我们来说, 虽然后缀表达式看起来难以理解, 但对于计算机来说, 就好处理多了, 那么计算机是如何使用后缀表达式进行求解的? (稍后我们会讨论四则运算式是如何转换成后缀表达式的) 后缀表达式求解 后缀表达式的求解过程其实用到了 数据结构中 栈 这种线性结构 (初始化一个空栈, 用来对要运算的数字进出使用) 后缀表达式的求解规则如下: 遇到数字直接压栈 遇到运算符, 则将栈顶数字依次出栈两个, 先出栈的在运算符右边, 后出栈的在左边 将上述运算结果再次压栈 循环 … 示例计算机通过后缀表达式 9 3 1 - 3 * + 10 2 / + 求解的过程: 利用了栈这种数据结构的特性(可以对压栈的数据通过出栈进行回溯) 步骤 栈 描述 1 9 数字9直接压栈 2 9 3 数字3直接压栈 3 9 3 1 数字1直接压栈 4 9 2 运算符-, 1,3出栈,3-1=2,2压栈 5 9 2 3 数字3直接压栈 6 9 6 运算符, 3,2出栈,23=6,6压栈 7 15 运算符+, 6,9出栈,9+6=15,15压栈 8 15 10 数字10直接压栈 9 15 10 2 数字2直接压栈 10 15 5 运算符/, 2,10出栈,10/2=5,5压栈 11 20 运算符+, 5,15出栈,15+5=20,20压栈 12 最终20出栈, 栈为空 推导后缀表达式中缀表达式到后缀表达式 从左到右遍历中缀表达式的每个 数字 和 符号 如果是 数字, 直接输出 如果是 符号, 则需要区分是 括号 还是 算数符号(+ - * /), 而且还需要根据情况进行 压栈出栈 操作: 123456789|如果栈为空: 符号直接压栈|如果栈非空:| ├左括号: 直接压栈| ├右括号: 栈顶元素依次出栈并输出, 直到左括号出栈 (注意:出栈的括号并不需要输出; 注意:最后,右括号不压入栈)| ├算数符号: 判断其与栈顶符号的优先级| │--├当前运算符号和括号没有可比性, 所以如果栈顶是左括号, 则运算符号直接压栈| │--├当前运算符号优先级 &gt; 栈顶运算符号, 则当前运算符号直接压栈| │--└当前运算符号优先级 &lt;= 栈顶符号, 则栈顶符号一直出栈, 直到栈顶的运算符号小于当前运算符号, 最后当前符号压栈 (注意, 直到栈顶运算符**小于**当前符号, 意味着优先级相等的符号也都要往外弹; 且最后当前符号压栈了;) 示例9+(3-1)*3+10/2 转 后缀表达式: 步骤 输出 栈 描述 1 9 空栈 数字直接输出 2 9 + 栈为空, 运算符号直接压栈 3 9 +( 栈不为空, 左括号直接压栈 4 9 3 +( 数字直接输出 5 9 3 +(- 栈不为空, -和栈顶符号左括号,没有可比性,直接压栈 6 9 3 1 +(- 数字直接输出 7 9 3 1 - + 右括号: 栈顶元素依次出栈并输出, 直到左括号出栈 8 9 3 1 - +* 当前运算符号优先级 &gt; 栈顶运算符号, 直接压栈 9 9 3 1 - 3 +* 数字直接输出 10 9 3 1 - 3 * + + 当前运算符号优先级 &lt;= 栈顶符号(…) 11 9 3 1 - 3 * + 10 + 数字直接输出 12 9 3 1 - 3 * + 10 +/ 当前运算符号优先级 &gt; 栈顶运算符号, 直接压栈 13 9 3 1 - 3 * + 10 2 +/ 数字直接输出 14 9 3 1 - 3 * + 10 2 / + 空栈 最后栈顶元素依次弹出即可","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/tags/数据结构/"}]},{"title":"03. 线性表","slug":"DataStructure/2017-11-15-12","date":"2017-11-15T13:41:25.000Z","updated":"2018-12-30T04:56:09.000Z","comments":true,"path":"2017/11/15/DataStructure/2017-11-15-12/","link":"","permalink":"http://blog.renyimin.com/2017/11/15/DataStructure/2017-11-15-12/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/tags/数据结构/"}]},{"title":"02. 算法","slug":"DataStructure/2017-11-13-11","date":"2017-11-13T11:36:19.000Z","updated":"2018-12-30T04:56:13.000Z","comments":true,"path":"2017/11/13/DataStructure/2017-11-13-11/","link":"","permalink":"http://blog.renyimin.com/2017/11/13/DataStructure/2017-11-13-11/","excerpt":"","text":"","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/tags/数据结构/"}]},{"title":"01. 数据结构 概述","slug":"DataStructure/2017-11-13-10","date":"2017-11-13T05:51:13.000Z","updated":"2018-12-30T04:56:18.000Z","comments":true,"path":"2017/11/13/DataStructure/2017-11-13-10/","link":"","permalink":"http://blog.renyimin.com/2017/11/13/DataStructure/2017-11-13-10/","excerpt":"","text":"概述 数据结构: 是相互之间存在一种或者多种特定关系的数据元素的集合; 在计算机中, 数据元素并不是孤立,杂乱无序的, 而是具有内在联系的数据集合; 数据元素之间存在一种或多种特定关系; 为了编写一个”好”的程序, 必须分析待处理对象的特性及各处理对象之间存在的关系, 也就是研究数据结构的意义所在; 上面提到的数据元素间的一种或多种特定关系就是接下来要讨论的 逻辑结构与物理结构 数据元素间的结构关系, 可以从 逻辑结构关系 和 物理结构关系 两个不同的角度去进行讨论 逻辑结构 是面向问题的, 而 物理结构 是面向计算机的(物理结构的基本目标就是将数据及其逻辑关系存储到计算机内存中) 逻辑结构和物理存储结构?? 逻辑结构逻辑结构: 是指数据对象中数据元素之间的相互关系(这也是我们今后最需要关注的问题), 逻辑结构分为以下四种 集合结构集合结构中的数据元素除了同属于一个集合外, 它们之间没有其他关系; 各个数据元素是”独立平等”的; 线性结构线性结构中的数据元素之间是 一对一 的关系; 树形结构树形结构中的数据元素之间存在一种 一对多 的层次关系; 图形结构图形结构的数据元素是 多对多 的关系; 物理结构 物理结构: 很多书中也叫存储结构, 是指数据的 逻辑结构 在计算机中的存储形式; 数据元素之间存在多种不同的 逻辑结构, 无论采用哪种逻辑结构进行数据关系的组织, 这些数据元素最终是要落地到存储器中的 (存储器主要是针对内存而言), 所以 如何存储这些数据元素及其逻辑结构, 是实现物理结构的重点和难点 数据元素的 存储结构(物理结构) 有两种形式: 顺序存储和链式存储; 顺序存储结构 顺序存储结构: 是把数据元素存放在地址连续的存储单元里, 其数据间的 逻辑结构关系 和 物理结构关系 是一致的; 这种存储结构很简单, 说白了就是 排队占位。大家都按顺序排好, 每个人占一小段空间, 谁也别插谁的队; 在Java中学过的 数组 其实就是这样的顺序存储结构 当你告诉计算机要创建一个有9个整型数据的数组时, 计算机就在内存中找了片空地, 按照一个整型所占位置的大小乘以9, 开辟一段连续的空间, 于是第一个数组数据就放在第一个位置, 第二个数据就放在第二个位置, 这样依次摆放 链式存储结构 如果数据元素在存储器中都是顺序存储结构这么简单, 那就好办了, 可实际上是随着数据元素的添加和删除, 数据元素间的 逻辑, 物理结构 也都是时刻都处于变化中; 面对时常需要变化的结构, 顺序结构显然不太科学, 因为每个元素的增加和删除, 顺序结构为了维持自身 连续, 足够的空间, 会显得非常困难, 因此就引出了链式存储结构; 链式存储结构: 是把数据元素存放在任意的存储单元里, 这组存储单元可以是连续的, 也可以是不连续的, 数据元素的存储关系并不能反映数据元素之间的逻辑关系, 因此需要用一个指针存放数据元素的地址, 这样通过地址就可以找到相关联数据元素的位置 链式存储结构相比于顺序存储结构显然要灵活许多, 对于这个结构来说, 数据存储在哪里并不重要, 只要有一个指针存放了相应的地址就能找到它了;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/categories/数据结构/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.renyimin.com/tags/数据结构/"}]},{"title":"01. 基本概念扫盲","slug":"网络编程/2018-11-29-01","date":"2017-11-04T09:11:37.000Z","updated":"2018-11-30T05:07:53.000Z","comments":true,"path":"2017/11/04/网络编程/2018-11-29-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/04/网络编程/2018-11-29-01/","excerpt":"","text":"文件描述符 维基百科 文件描述符(File descriptor)是一个用于表述在操作系统中, 指向一个文件(文件可理解为信息载体, 包括socket)的引用指针的抽象化概念 对一个 socket 的读写也会有相应的描述符, 称为 socketfd(socket 描述符) 文件描述符形式上是一个非负整数, 实际上, 它是一个索引值, 指向内核中为每个进程所维护的该进程打开文件的记录表, 当程序打开一个文件或者新创建一个文件, 内核就会向进程返回一个对应的文件描述符(fd) (这个概念只适用于unix、linux操作系统) 更详细可参考: https://learn-linux.readthedocs.io/zh_CN/latest/system-programming/file-io/file-descriptor.html 上下文当一个进程在执行时, CPU 的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文 用户空间与内核空间 Linux从整体上可以分为内核态与用户态 内核态就是内核所处的空间, 内核负责调用底层硬件资源, 并为上层应用程序提供运行环境, 用户态即应用程序的活动空间 比如一个32位的操作系统, 寻址地址(虚拟内存空间)是2的32次方, 也就是4G, 操作系统将较高的1G字节作为内核空间, 而将较低的3G字节作为用户空间, 内核空间具有用户空间所不具备的操作权限。 应用程序的运行必须依托于内核提供的硬件资源(如cpu\\存储\\IO), 内核通过暴露外部接口以便应用程序调用 — 称为 “SystemCall, 系统调用” 系统调用是操作系统的最小功能单位, 应用程序通常运行在用户空间, 当某些操作需要内核权限时, 就会通过系统调用(System calls), 进入内核态执行, 这也就是一次用户态\\内核态的转换, 切换过程中涉及了各种函数的调用以及数据的复制 虽然用户态下和内核态下工作的程序有很多差别, 但最重要的差别就在于特权级的不同, 即权力的不同。运行在用户态下的程序不能直接访问操作系统内核数据结构和程序 熟悉Unix/Linux系统的人都知道, fork() 的工作实际上是以系统调用的方式完成相应功能的, 具体的工作是由系统级的 sys_fork() 负责实施, 其实无论是不是Unix或者Linux, 对于任何操作系统来说, 创建一个新的进程都是属于核心功能, 因为它要做很多底层细致地工作, 消耗系统的物理资源, 比如分配物理内存, 从父进程拷贝相关信息, 拷贝设置页目录页表等等, 这些显然不能随便让哪个程序就能去做, 于是就自然引出特权级别的概念, 显然, 最关键性的权力必须由高特权级的程序来执行, 这样才可以做到集中管理, 减少有限资源的访问和使用冲突 很多程序开始时运行于用户态, 但在执行的过程中, 一些操作需要在内核权限下才能执行, 这就涉及到一个从用户态切换到内核态的过程, 一般存在以下三种需要切换的情况: 系统调用比如调用fork()函数产生进程的时候 程序异常比如5/0, 当除数为0的时候, 就会产生异常 外围设备的中断: 当外围设备完成用户请求的操作后, 会向CPU发出相应的中断信号, 这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序, 如果先前执行的指令是用户态下的程序, 那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成, 系统会切换到硬盘读写的中断处理程序中执行后续操作等 最终通过内核态、用户态的划分与协助, 保证了操作系统的安全性、稳定性 Socket socket也叫套接字, 起源于Unix, 而Unix/Linux基本哲学之一就是一切皆文件, 都可以用 “打开open –&gt; 读写write/read –&gt; 关闭close” 模式来操作, 而Socket就是该模式的一个实现, 所以可以认为Socket是一种特殊的文件; Socket是应用层与TCP/IP协议族通信的中间软件抽象层, 它是一组接口, 在设计模式中, Socket其实就是一个门面模式, 它把复杂的TCP/IP协议族隐藏在Socket接口后面 对于网络中进程之间的通信, 应用程序几乎都使用的是socket IO模型同步和异步 其实网上有很多资料在描述 同步, 异步, 阻塞, 非阻塞 之前的关系, 往往举了一大堆例子, 最终还是说不太清除。 其实应用程序要进行网络通信, 应用程序 必然会进行 系统调用 对于 同步和异步 来说, 我们不需要关注 同步调用在等待期间 或者 异步调用在立即返回后 应用程序是 休眠等待或者活跃去处理其他任务, 而重点关注的是 消息的通信机制, 也就是结果是应用程序主动去等待获取, 还是由系统将结果通知给应用程序 同步和异步关注的是消息通信机制 同步: 发出一个调用后, 一直等待, 直到这个调用结果完成后, 才返回 (同步主要体现在主动获取结果) 异步: 发出一个调用后, 立刻返回, 不需要等待调用结果, 而是在结果处理完成后, 通过 通知机制 或者 回调函数 进行通知 (异步主要体现在结果的回调通知) 阻塞和非阻塞关注的是程序在等待调用结果（消息, 返回值）时的状态.阻塞：发出调用后, 当前线程被挂起, 直到结果返回。非阻塞：发出调用后, 当前线程继续保持运行状态。总之, 同步异步要看是否需要等待操作执行的结构。阻塞非阻塞指请求后是否能够理解返回保持Running的状态。 参考: https://blog.csdn.net/lemon89/article/details/78290389 https://blog.csdn.net/orchestra56/article/details/81005494 https://learn-linux.readthedocs.io/zh_CN/latest/system-programming/file-io/file-descriptor.html持续扫盲中~~","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://blog.renyimin.com/categories/网络编程/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://blog.renyimin.com/tags/网络编程/"}]},{"title":"03. 小试牛刀","slug":"swoole/2017-11-04-03","date":"2017-11-04T09:11:37.000Z","updated":"2018-11-28T06:13:48.000Z","comments":true,"path":"2017/11/04/swoole/2017-11-04-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/04/swoole/2017-11-04-03/","excerpt":"","text":"前言本篇主要是通过简单过一下Swoole的基本功能点来对swoole有个比较基础的认知 TCP Server server 代码 (在虚拟机中通过 php server.php 启动服务器) 123456789101112131415161718192021222324252627282930313233&lt;?php// 创建tcpserver服务器对象$tcpServer = new swoole_server(&quot;192.168.1.110&quot;, 8088, SWOOLE_PROCESS, SWOOLE_SOCK_TCP);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 * $reactor_id(该形参也常被命名为 $from_id): TCP连接所在的Reactor线程ID */$tcpServer-&gt;on(&apos;connect&apos;, function ($server, $fd, $reactor_id) &#123; echo &quot;Client:Connect, fd:&#123;$fd&#125;, reactor_id:&#123;$reactor_id&#125; \\n&quot;;&#125;);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 * $reactor_id(该形参也常被命名为 $from_id): TCP连接所在的Reactor线程ID * $data: 收到的数据内容，可能是文本或者二进制内容 */$tcpServer-&gt;on(&apos;receive&apos;, function ($server, $fd, $reactor_id, $data) &#123; // 此处还根据收到的消息内容做出不同的响应(当然这属于业务部分) $server-&gt;send($fd, &quot;fd:&#123;$fd&#125;, reactor_id:&#123;$reactor_id&#125;, receive:&#123;$data&#125; \\n&quot;);&#125;);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 */$tcpServer-&gt;on(&apos;close&apos;, function ($server, $fd, $reactor_id) &#123; echo &quot;Client: Close.\\n&quot;;&#125;);$tcpServer-&gt;start(); 物理机上使用 telnet 命令测试 telnet 192.168.1.110 8088 (telnet 退出: ctrl+], 然后输入 quit) UDP Server server 代码 (在虚拟机中通过 php server.php 启动服务器) 123456789101112131415&lt;?php$udpServer = new swoole_server(&quot;192.168.1.110&quot;, 8088, SWOOLE_PROCESS, SWOOLE_SOCK_UDP);/** * UDP服务器与TCP服务器不同, UDP没有连接的概念 * 启动Server后, 客户端无需Connect, 直接可以向Server监听的端口发送数据包, 对应的事件为 onPacket * $clientInfo: 是客户端的相关信息, 是一个数组, 有客户端的IP和端口等内容 * 调用 $server-&gt;sendto 方法向客户端发送数据 */$udpServer-&gt;on(&apos;Packet&apos;, function ($server, $data, $clientInfo) &#123; $server-&gt;sendto($clientInfo[&apos;address&apos;], $clientInfo[&apos;port&apos;], &quot;data: &quot; . $data); var_dump($clientInfo);&#125;);$udpServer-&gt;start(); telnet 用于测试 tcp 协议的端口测试,但貌似无法用于udp协议, UDP服务器可以使用 netcat -u 来连接测试 123456 brew install netcat nc -v 或者 netcat -v netcat (The GNU Netcat) 0.7.1 ``` 3. 测试向UDP服务器发送数据包 : netcat -u 192.168.1.110 8088 hello data: hello 123## HTTP Server1. server 代码 (在虚拟机中通过 php server.php 启动服务器) &lt;?php // 通过几行代码即可写出一个异步非阻塞多进程的Http服务器 /** Http\\Server继承自Server, 是一个完整的http服务器实现, Http\\Server支持同步和异步2种模式*/$http = new Swoole\\Http\\Server(“192.168.1.110”, 8080); $http-&gt;on(‘request’, function ($request, $response) { echo “client request, host: {$request-&gt;header[‘host’]}, getParam: {$request-&gt;get[‘name’]} \\n”; $response-&gt;end(“Hello Swoole. #\" . rand(1000, 9999) . \"“);});$http-&gt;start(); 123456782. 物理机访问 http://192.168.1.110:8080/?name=lant ## [Websocket Server](https://wiki.swoole.com/wiki/page/397.html)## 异步毫秒定时器1. 场景: 定时任务(毫秒级别)2. 示例代码 &lt;?php/** 毫秒精度的定时器 底层基于 epoll_wait 和 setitimer 实现, 数据结构使用最小堆, 可支持添加大量定时器*/ // 循环执行定时器swoole_timer_tick(3000, function () { echo “after 3000ms.\\n”;}); // 单次执行定时器swoole_timer_after(3000, function () { echo “after 14000ms.\\n”;});```","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"02. 环境准备","slug":"swoole/2017-11-03-02","date":"2017-11-03T10:36:52.000Z","updated":"2018-11-27T02:24:26.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-02/","excerpt":"","text":"前置 此处使用的是 Vagrant+VirtualBox+CentOS7.2 进行系统环境部署 (如有对Vagrant不熟悉的兄dei~可以提前去了解一下, 也比较简单) VagrantFile 1234567891011Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :swoole do |swoole| swoole.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;swoole-study&quot;, &quot;--memory&quot;, &quot;1000&quot;] end swoole.vm.box = &quot;centos7.2&quot; swoole.vm.hostname = &quot;lant&quot; swoole.vm.network :private_network, ip: &quot;192.168.1.110&quot; swoole.vm.synced_folder &quot;/Users/renyimin/Desktop/swoole-synced_folder&quot;, &quot;/swoole-synced&quot; endend PHP7.2.12 编译安装 下载 php-7.2.12.tar.gz 到 /Users/renyimin/Desktop/swoole-synced_folder 登录虚拟机, 将 /swoole-synced 下的 php-7.2.12.tar.gz 移动到 /usr/local/src: 1234567[root@lant /]# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv swoole-synced sys tmp usr vagrant var[root@lant /]# cd swoole-synced/[root@lant swoole-synced]# lsphp-7.2.12.tar.gz[root@lant swoole-synced]# mv /swoole-synced/php-7.2.12.tar.gz /usr/local/src/[root@lant swoole-synced]# 解压, configure, make, make install 123tar -zxvf php-7.2.12.tar.gz./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-mysql-sock --with-mysqli --with-libxml-dir --with-openssl --with-mhash --with-pcre-regex --with-zlib --with-iconv --with-bz2 --with-curl --with-cdb --with-pcre-dir --with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --with-gettext --with-gmp --with-mhash --with-libmbfl --with-onig --with-pdo-mysql --with-zlib-dir --with-readline --with-libxml-dir --with-xsl --with-pear --enable-fpm --enable-soap --enable-bcmath --enable-calendar --enable-dom --enable-exif --enable-fileinfo --enable-filter --enable-ftp --enable-gd-jis-conv --enable-json --enable-mbstring --enable-mbregex --enable-mbregex-backtrack --enable-pdo --enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem --enable-sysvshm --enable-wddx --enable-zip --enable-mysqlnd-compression-supportmake &amp;&amp; make install 配置环境变量 12345vi ~/.bash_profile# 添加alias php=/usr/local/php/bin/php# 最后执行source ~/.bash_profile 1234[root@lant tmp]# php -vPHP 7.2.12 (cli) (built: Nov 22 2018 05:50:26) ( NTS )Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies 拷贝配置文件 12345cp /usr/local/src/php-7.2.12/php.ini-production /usr/local/php/etc/php.inicp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf// 把pid 改成 /run/php-fpm.pidvim /usr/local/php/etc/php-fpm.conf 检测php当前使用的配置文件 123[root@lant php-fpm.d]# php -i | grep php.iniConfiguration File (php.ini) Path =&gt; /usr/local/php/etcLoaded Configuration File =&gt; /usr/local/php/etc/php.ini 开机启动php-fpm: 将php-fpm加入到system中管理 12345678910111213141516cd /lib/systemd/systemvim php-fpm.service[Unit]Description=The PHP FastCGI Process ManagerAfter=syslog.target network.target[Service]Type=simplePIDFile=/run/php-fpm.pidExecStart=/usr/local/php/sbin/php-fpm --nodaemonize --fpm-config /usr/local/php/etc/php-fpm.confExecReload=/bin/kill -USR2 $MAINPIDExecStop=/bin/kill -SIGINT $MAINPID[Install]WantedBy=multi-user.target 启动php-fpm: systemctl start php-fpm.service 添加到开机启动: systemctl enable php-fpm.service nginx 编译安装 下载 nginx-1.14.1.tar.gz 到 /Users/renyimin/Desktop/swoole-synced_folder 登录虚拟机, 将 /swoole-synced 下的 nginx-1.14.1.tar.gz 移动到 /usr/local/src 解压, configure, make, make install 1234tar -zxvf nginx-1.14.1.tar.gzyum -y install gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel open openssl-devel./configure --prefix=/usr/local/nginxmake &amp;&amp; make install 设置环境变量 123echo &apos;PATH=/usr/local/nginx/sbin:$PATH&apos; &gt;&gt; /etc/profileecho &apos;export PATH&apos; &gt;&gt; /etc/profilesource /etc/profile 设置开机自启动 1234567891011121314151617// 如果是使用yum安装的nginx, 则会自动创建/lib/systemd/system/nginx.service文件// 由于此处是使用编译安装, 所以需要手动在系统服务目录里创建nginx.service文件vi /lib/systemd/system/nginx.service[Unit]Description=nginxAfter=network.target [Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true [Install]WantedBy=multi-user.target 设置开机启动 12[root@lant system]# systemctl enable nginx.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. 配置nginx 主配置文件, /usr/local/nginx/conf/nginx.conf 1234567891011121314151617181920212223242526272829#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; include conf.d/*.conf;&#125; 在 conf 下创建 conf.d 目录 在 conf.d 目录下创建 default.conf, swool.conf 两个配置文件 default.conf 123456789101112131415161718192021server &#123; listen 80; server_name www.myswooleenv.com; #access_log logs/host.access.log main; location / &#123; root /html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location ~ \\.php$ &#123; root /html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; swoole.conf 123456789101112131415server &#123; listen 80; server_name www.swoolestudy.com; location / &#123; root /swooleroot; index index.html index.htm; &#125; location ~ \\.php$ &#123; root /swooleroot; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 在虚拟机中 /html 下创建phpinfo.php, 最后即可在物理机通过 www.myswooleenv.com/phpinfo.php 访问 swoole编译 git clone 下载 swoole 源码 (注意需要注册码云账号并添加本地git公钥) 12cd /usr/local/src git clone git@gitee.com:swoole/swoole.git 编译安装, 使用 phpize 生成 swoole.so 外挂模块 1234567/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make install# 结果Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20170718/Installing header files: /usr/local/php/include/php/[root@lant swoole]# 也可以访问 phpinfo.php 看到上面的 extension_dir 地址, 可以查看 swoole 扩展的编译结果 123[root@lant ~]# cd /usr/local/php/lib/php/extensions/no-debug-non-zts-20170718[root@lant no-debug-non-zts-20170718]# lsopcache.a opcache.so swoole.so php 支持swoole: vi /usr/local/php/etc/php.ini 加入 extension=swoole.so, 重启php-fpm systemctl restart php-fpm.service 访问: http://192.168.1.110/phpinfo.php, 发现已经有了 swoole 扩展 测试 swoole源码中有示例: cd /usr/local/src/swoole/examples/server 运行测试文件: php echo.php 安装 nestat 命令用于查看端口运行情况 : yum -y install net-tools 查看端口运行情况, 可看到 echo.php 这个示例文件已经被成功运行 12netstat -natp | grep 9501tcp 0 0 0.0.0.0:9501 0.0.0.0:* LISTEN 27927/php 杀掉该进程, 则端口不再被占用 123[root@lant server]# php echo.php^C[root@lant server]# tool 为了方便测试, 需要安装 telnet 工具 : yum install xinetd telnet telnet-server -y 参考 https://wiki.swoole.com/wiki/page/351.html","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"01. swoole简介 及 知识扫盲","slug":"swoole/2017-11-03-01","date":"2017-11-03T02:45:17.000Z","updated":"2018-11-22T09:05:27.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-01/","excerpt":"","text":"简介引用swoole官网对swoole的介绍 Swoole:面向生产环境的 PHP 异步网络通信引擎 使 PHP 开发人员可以编写高性能的异步并发 TCP、UDP、Unix Socket、HTTP，WebSocket 服务。Swoole 可以广泛应用于互联网、移动通信、企业软件、云计算、网络游戏、物联网（IOT）、车联网、智能家居等领域。 使用 PHP + Swoole 作为网络通信框架，可以使企业 IT 研发团队的效率大大提升，更加专注于开发创新产品。 特性 Swoole 使用纯 C 语言编写，提供了 PHP 语言的异步多线程服务器，异步 TCP/UDP 网络客户端，异步 MySQL，异步 Redis，数据库连接池，AsyncTask，消息队列，毫秒定时器，异步文件读写，异步DNS查询。 Swoole内置了Http/WebSocket服务器端/客户端、Http2.0服务器端。 除了异步 IO 的支持之外，Swoole 为 PHP 多进程的模式设计了多个并发数据结构和IPC通信机制，可以大大简化多进程并发编程的工作。其中包括了并发原子计数器，并发 HashTable，Channel，Lock，进程间通信IPC等丰富的功能特性。 Swoole2.0 支持了类似 Go 语言的协程，可以使用完全同步的代码实现异步程序。PHP 代码无需额外增加任何关键词，底层自动进行协程调度，实现异步。 典型应用场景 移动互联网API服务器 物联网(IOT) 微服务(Micro Service) 高性能Web服务器 游戏服务器 在线聊天系统 很多用户案例可以参考 https://wiki.swoole.com/wiki/page/p-case.html","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]}]}