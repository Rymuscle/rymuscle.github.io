{"meta":{"title":"Lant's Blog","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"IO相关的一些概念","slug":"socket/2018-05-13-IO","date":"2018-05-13T03:12:19.000Z","updated":"2018-05-13T06:33:17.000Z","comments":true,"path":"2018/05/13/socket/2018-05-13-IO/","link":"","permalink":"http://blog.renyimin.com/2018/05/13/socket/2018-05-13-IO/","excerpt":"","text":"IO在开始socket编程前, 需要先对 IO 的概念有一定的认识: 我们通常使用php的fopen打开文件关闭文件读读写写, 这叫本地文件IO; 而在socket编程中, 本质其实是网络IO; 同步异步阻塞非阻塞之前反正一直搞不清楚同步和阻塞, 异步和非阻塞的概念, 总感觉同步就是阻塞, 异步就是非阻塞的, 总是搞得晕乎乎的, 于是就重新查了些资料进行了梳理, 如有不对欢迎大家指正; 简单来说同步: 同步体现在, 在等待一件事情的处理结果时, 对方是否提供通知服务, 如果对方不提供通知服务, 则为 同步; 异步: 异步体现在, 在等待一件事情的处理结果时, 对方是否提供通知服务, 如果对方提供通知服务, 则为 异步; 对于常见的Ajax请求, 会等到server端处理完后将数据’通知’到前端页面; 对于消息队列, 则是首先会立即响应, 让程序能非阻塞执行一些其他操作, 响应之后的处理由消息系统来进行, 程序其实也不需要等到最后处理完成后的通知; 阻塞: 阻塞体现在, 在等待一件事情的处理结果时, 你是否还去干点其他的事情, 如果不去, 则为 阻塞; 非阻塞: 非阻塞体现在, 在等待一件事情的处理结果时, 你是否还去干点其他的事情, 如果去了, 则为 非阻塞; 结合例子来说此处找了一位朋友写的例子, 感觉很不错同步阻塞: 你去 甜在心馒头 店买太极馒头, 阿梅说:”暂时没, 正在蒸呢, 你自己看着点儿!”, 于是你就站在旁边只等馒头, 此时的你, 是阻塞的, 也是同步的; 阻塞表现在你除了等馒头，别的什么都不做了; 同步表现在等馒头的过程中, 阿梅不提供通知服务, 你不得不自己主动检查 ＂馒头出炉＂ 的消息; 同步非阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 你自己看着点儿!＂, 于是你就站在旁边发发微信, 然后问一句:＂好了没？＂, 然后玩玩QQ游戏, 然后再问一句:＂好了没？＂, 此时的你, 是非阻塞的, 不过却还是同步的; 非阻塞表现在你除了等馒头, 自己还在干别的事情; 同步表现在等馒头的过程中, 由于阿梅不提供通知服务, 你不得不自己主动检查 ＂馒头出炉＂ 的消息; 异步阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 蒸好了我打电话告诉你!＂, 但你依然站在旁边只等馒头, 此时的你, 是阻塞的, 是异步的; 阻塞表现在你除了等馒头, 也没去干点别的什么(比如玩玩手机啥的); 异步表现在等馒头的过程中, 阿梅提供电话通知＂馒头出炉＂的消息, 你只需要等阿梅的电话; 异步非阻塞: 你去甜在心馒头店买太极馒头, 阿梅说:＂暂时没, 正在蒸呢, 蒸好了我打电话告诉你!＂, 于是你就走了, 去买了双新球鞋, 看了看武馆, 总之, 从此不再过问馒头的事情, 一心只等阿梅电话, 此时的你, 是非阻塞的, 是异步的 非阻塞表现在你除了等馒头, 自己还去干点别的事情; 异步表现在等馒头的过程中, 阿梅提供电话通知＂馒头出炉＂的消息, 你只需要等阿梅的电话; 参考","categories":[{"name":"Socket","slug":"Socket","permalink":"http://blog.renyimin.com/categories/Socket/"}],"tags":[{"name":"Socket","slug":"Socket","permalink":"http://blog.renyimin.com/tags/Socket/"}]},{"title":"PHP-X","slug":"PHP/2018-05-09-PHP-X","date":"2018-05-09T12:29:31.000Z","updated":"2018-05-09T12:39:45.000Z","comments":true,"path":"2018/05/09/PHP/2018-05-09-PHP-X/","link":"","permalink":"http://blog.renyimin.com/2018/05/09/PHP/2018-05-09-PHP-X/","excerpt":"","text":"https://wiki.swoole.com/wiki/index/prid-15","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"01.","slug":"oauth/2018-04-28-CAS","date":"2018-04-28T13:10:12.000Z","updated":"2018-05-02T08:07:37.000Z","comments":true,"path":"2018/04/28/oauth/2018-04-28-CAS/","link":"","permalink":"http://blog.renyimin.com/2018/04/28/oauth/2018-04-28-CAS/","excerpt":"","text":"Single Sign On单点登录(Single Sign On), 简称为SSO, 是目前比较流行的企业业务整合的解决方案之一;SSO的定义是在多个应用系统中, 用户只需要登录一次就可以访问所有相互信任的应用系统; CAS简介 CAS由Yale耶鲁大学研发的一款开源的,企业级,SSO单点登录解决方案; CAS Github 源码下载, 这里使用的是CAS5.1.9 CAS 文档CAS系统组件CAS系统架构由 CAS Server 和 CAS Client 两个物理组件构成, 它们通过各种协议进行通信;CAS Server CAS服务器是基于Spring框架构建的Java servlet, 其主要职责是验证用户, 并通过发布和验证票证来对 启用CAS的服务(通常称为CAS客户端)的访问权限 进行授权;服务器在成功登录后, 会向用户授予 票证(TGT)时创建SSO会话;根据用户的请求, 通过使用TGT作为标记的浏览器重定向向服务发出服务票据(ST);ST随后通过反向信道通信在CAS服务器上进行验证。 CAS Protocol文档中详细描述了这些交互; CAS ClientCAS Server搭建 CAS 可以分为两部分 CAS Server 和 CAS Client CAS Server 用来负责用户的认证工作, 就像是把第一次登录用户的一个标识存在这里, 以便此用户在其他系统登录时验证其需不需要再次登录; CAS Client 就是我们自己的应用, 需要接入CAS Server端; 当用户访问我们的应用时, 首先需要重定向到CAS Server端进行验证, 要是原来登陆过, 就免去登录, 重定向到下游系统, 否则进行用户名密码登陆操作; 术语Ticket Granting ticket (TGT): 可以认为是CAS Server根据用户账号和密码生成的一张票, 存在CAS Server端;Ticket-granting cookie (TGC): 其实就是一个cookie, 存放用户身份信息, 由CAS Server发给CAS Client端;Service ticket (ST): 由TGT生成的一次性票据, 用于验证, 只能用一次。相当于server发给client一张票, 然后client拿着这是个票再来找server验证, 看看是不是server签发的;就像是我给了你一张我的照片, 然后你拿照片再来问我, 这个照片是不是你, 没错，就是这么无聊 TGT(Ticket Grangting Ticket) TGT是CAS为用户签发的登录票据, 拥有了TGT, 用户就可以证明自己在CAS成功登录过; TGT封装了Cookie值以及此Cookie值对应的用户信息; 用户在CAS认证成功后, CAS生成cookie, 写入浏览器, 同时生成一个TGT对象, 放入自己的缓存; TGT对象的ID就是cookie的值 当HTTP再次请求到来时, 如果传过来的有CAS生成的cookie, 则CAS以此cookie值为key查询缓存中有无TGT, 如果有的话, 则说明用户之前登录过, 如果没有, 则用户需要重新登录; ST（Service Ticket）ST是CAS为用户签发的访问某一service的票据。用户访问service时，service发现用户没有ST，则要求用户去CAS获取ST。用户向CAS发出获取ST的请求，如果用户的请求中包含cookie，则CAS会以此cookie值为key查询缓存中有无TGT，如果存在TGT，则用此TGT签发一个ST，返回给用户。用户凭借ST去访问service，service拿ST去CAS验证，验证通过后，允许用户访问资源。 PGT（Proxy Granting Ticket）Proxy Service的代理凭据。用户通过CAS成功登录某一Proxy Service后，CAS生成一个PGT对象，缓存在CAS本地，同时将PGT的值（一个UUID字符串）回传给Proxy Service，并保存在Proxy Service里。Proxy Service拿到PGT后，就可以为Target Service（back-end service）做代理，为其申请PT。","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"03. 一些注意点","slug":"OOP/PHP/2018-04-21-OOP-PHP-03","date":"2018-04-21T07:50:21.000Z","updated":"2018-04-21T11:21:42.000Z","comments":true,"path":"2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-03/","link":"","permalink":"http://blog.renyimin.com/2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-03/","excerpt":"","text":"类常量 类常量是公共的 只能通过类名, 使用 :: 静态访问类常量 (也可以使用实例化的对象通过::来访问类常量) 客户端代码不能对其进行改变 常量属性使用 const 关键字来声明, 不需要使用 $ 符号 常量属性的值只能包含基本数据类型的值, 不能将一个对象指派给常量 当需要在类的所有实例中都能访问某个属性, 并且属性值无需改变时, 应该使用常量 抽象类 不能实例化抽象类 抽象类中应该至少包含一个抽象方法 (但也不是强制性的) 抽象方法使用abstract关键字声明, 不能有具体的内容 抽象方法不能有消息体{}, 并且需要以;结束 继承抽象方法的子类必须实现抽象类中的所有抽象方法 继承抽象方法的子类实现了抽象类中的所有抽象方法之后, 还应该注意: 新实现的这个方法的访问控制权限不能比抽象方法更严格; 接口 接口相对于抽象类, 可能更是彻底的, 一个纯粹的模板; 接口只能定义功能, 而不包含实现的内容; 任何实现了接口的类都需要实现接口中所定义的所有方法, 否则该类必须声明为abstract; 接口中的所有方法都必须声明为 public(或者不声明权限, 默认就为public) 接口中的所有方法不能有消息体{}, 并且需要以;结束 一个类可以同时继承一个父类和实现任意个接口, extends子句应该在inplements 子句之前; 一个比较重要的概念是: 实现接口的类接受了它继承的类以及实现的接口的类型; (对于面向对象来说, 依赖抽象而不依赖具体) final关键字 ~~未完待续","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"02. 延迟静态绑定static关键字","slug":"OOP/PHP/2018-04-21-OOP-PHP-02","date":"2018-04-21T07:03:35.000Z","updated":"2018-04-21T10:42:26.000Z","comments":true,"path":"2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-02/","link":"","permalink":"http://blog.renyimin.com/2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-02/","excerpt":"","text":"延迟静态绑定是在PHP5.3引入的; static类似于self, 但它指的是调用 包含static的方法 的调用类, 而不是包含static的类; 一般 static 和 self 的使用语法有下面两种 new static() 与 new self() (用作实例化) self:: 与 static:: (静态方法调用) 如何理解第2点中的意思 如下, Male与Female 两个类分别继承自 Human, 并且继承了createSelf()方法, 结果却发现, Male 和 Female 调用createSelf()后, 创建的却是 Human 对象 12345678910111213141516171819202122&lt;?phpclass Human&#123; public function createSelf() &#123; return new self(); &#125;&#125;class Male extends Human&#123;&#125;class Female extends Human&#123;&#125;$boy = new Male();var_dump($boy-&gt;createSelf());$girl = new Female();var_dump($girl-&gt;createSelf()); 如果Human是个抽象类, 你甚至无法使用 new self() 而使用了new static()效果就不一样了 下面, Male与Female 两个类分别继承自 Human, 并且继承了createSelf()方法, 结果 Male 和 Female 调用createSelf()后, 创建的也都是各自的对象实例12345678910111213141516171819202122&lt;?phpabstract class Huma&#123; public function createSelf() &#123; return new static(); &#125;&#125;class Male extends Huma&#123;&#125;class Female extends Huma&#123;&#125;$boy = new Male();var_dump($boy-&gt;createSelf());$girl = new Female();var_dump($girl-&gt;createSelf()); static 和 self 在调用静态方法时也是一样的 如下结果, 两个类都是 Hello! 123456789101112131415161718192021222324252627282930313233&lt;?phpabstract class Huma&#123; public static function createSelf() &#123; self::say(); &#125; public static function say() &#123; echo &quot;Hello!&quot;; &#125;&#125;class Male extends Huma&#123; public static function say() &#123; echo &quot;I am a Male&quot;; &#125;&#125;class Female extends Huma&#123; public static function say() &#123; echo &quot;I am a Female&quot;; &#125;&#125;$boy = new Male();$boy-&gt;createSelf();$girl = new Female();$girl-&gt;createSelf(); 如果想在父类的公共静态方法中, 调用子类自己重载的静态方法, 那就需要使用 static:: 123456789101112131415161718192021222324252627282930313233&lt;?phpabstract class Huma&#123; public static function createSelf() &#123; static::say(); &#125; public static function say() &#123; echo &quot;Hello!&quot;; &#125;&#125;class Male extends Huma&#123; public static function say() &#123; echo &quot;I am a Male&quot;; &#125;&#125;class Female extends Huma&#123; public static function say() &#123; echo &quot;I am a Female&quot;; &#125;&#125;$boy = new Male();$boy-&gt;createSelf(); // I am a Male$girl = new Female();$girl-&gt;createSelf(); // I am a Female","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"01. 普通非静态 与 静态","slug":"OOP/PHP/2018-04-21-OOP-PHP-01","date":"2018-04-21T06:16:26.000Z","updated":"2018-04-21T10:51:02.000Z","comments":true,"path":"2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-01/","link":"","permalink":"http://blog.renyimin.com/2018/04/21/OOP/PHP/2018-04-21-OOP-PHP-01/","excerpt":"","text":"对于 属性 来说类内部的调用方式 静态属性是类的属性 普通属性是类具体实例化出的对象的属性 所以二者是完全不同的, 调用方式也非常不同 静态属性 self::$静态属性名 、类名::$静态属性名 普通属性 $this-&gt;普通属性名 类外部的调用方式 静态属性是类的属性 普通属性是类具体实例化出的对象的属性 所以二者是完全不同的, 调用方式也非常不同 静态属性 类名::$静态属性名 普通属性 实例化对象-&gt;普通属性名 对于 方法 来说对于方法, 无论 普通还是静态, 它们都是属于类的, 所以理论上, 它们的调用方式 -&gt;/:: 是可以互换的, 但事实上, 还是应该严格区分, 否则可能会报 Deprecated...; 类内部的调用方式 普通方法 可以使用 $this-&gt;普通方法名() 来进行调用; 也可以使用 self::$普通方法名() 静态方法 可以使用 self::$静态方法名() / 类名::$静态方法名() 来进行调用; 也可以使用 $this-&gt;$静态方法名() 即类内部 普通方法 和 静态方法 的调用方式可以互换; 类外部的调用方式 普通方法 可以使用 实例化对象-&gt;普通方法名() 来进行调用; 注意: 使用 类名::普通方法名() / 实例化对象::普通方法名() 都会报 Deprecated:非静态方法不应该按照镜头盖方法的调用方式来使用 静态方法 可以使用 类名::$静态方法名() 来进行调用; 注意：静态方法也可以使用 实例化对象-&gt;普通方法名() 来进行调用; (静态属性不可以) 静态/非静态方法中的调用权限 静态方法中只能使用 静态调用方式::来调用 静态属性, 静态方法, 非静态方法; 不能使用::调用非静态属性; 非静态普通方法中可以调用 静态/非静态 的 属性/方法; 小结 静态属性/方法, 只能通过 :: 来访问; 普通属性只能通过 -&gt; 来访问; 普通方法能通过 -&gt;/:: 来访问; (类外部只能通过-&gt;)","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"Git","slug":"2018-04-08-rsa-01","date":"2018-04-08T02:41:16.000Z","updated":"2018-04-08T05:14:04.000Z","comments":true,"path":"2018/04/08/2018-04-08-rsa-01/","link":"","permalink":"http://blog.renyimin.com/2018/04/08/2018-04-08-rsa-01/","excerpt":"","text":"对称加密算法加密和解密使用同样的规则(“密钥”); 这种加密模式有一个最大弱点: 甲方必须把加密规则告诉乙方, 否则乙方无法解密; 这样, 保存和传递密钥 就成了最头疼的问题; 非对称加密算法简介 非对称加密算法模式 123乙方生成两把密钥(公钥和私钥): 公钥是公开的, 任何人都可以获得, 私钥则是保密的;甲方获取乙方的公钥, 然后用它对信息加密;乙方得到加密后的信息, 用私钥解密; 如果公钥加密的信息只有私钥解得开, 那么只要私钥不泄漏, 通信就是安全的; 所谓非对称加密，其实很简单，就是加密和解密需要两把钥匙: 一把公钥和一把私钥; RSA算法 1977年, 三位数学家Rivest、Shamir 和 Adleman 设计了一种算法, 可以实现非对称加密, 这种算法用他们三个人的名字命名, 叫做RSA算法。从那时直到现在, RSA算法一直是最广为使用的”非对称加密算法”。 毫不夸张地说, 只要有计算机网络的地方, 就有RSA算法; 这种算法非常可靠, 密钥越长, 它就越难破解。根据已经披露的文献, 目前被破解的最长RSA密钥是768个二进制位, 也就是说, 长度超过768位的密钥, 还无法破解(至少没人公开宣布)。 因此可以认为, 1024位的RSA密钥基本安全, 2048位的密钥极其安全; RSA算法的原理RSA算法并不难, 只需要一点数论知识就可以理解 (要用到的四个数学概念) ~~ 未完待续 互质关系 如果两个正整数, 除了1以外, 没有其他公因子，我们就称这两个数是互质关系(coprime), 比如, 15和32没有公因子, 所以它们是互质关系; 这说明, 不是质数也可以构成互质关系。(质数(prime number)又称素数, 有无限个, 质数定义为在大于1的自然数中, 除了1和它本身以外不再有其他因数) 关于互质关系, 不难得到以下结论: 任意两个质数构成互质关系, 比如13和61; 一个数是质数, 另一个数只要不是前者的倍数, 两者就构成互质关系, 比如3和10; 如果两个数之中, 较大的那个数是质数, 则两者构成互质关系, 比如97和57; 1和任意一个自然数是都是互质关系，比如1和99; p是大于1的整数, 则p和p-1构成互质关系, 比如57和56; p是大于1的奇数, 则p和p-2构成互质关系, 比如17和15; http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html","categories":[{"name":"RSA","slug":"RSA","permalink":"http://blog.renyimin.com/categories/RSA/"}],"tags":[{"name":"RSA","slug":"RSA","permalink":"http://blog.renyimin.com/tags/RSA/"}]},{"title":"PHP7语法新特性","slug":"PHP/2018-04-04-php7new-base","date":"2018-04-04T01:32:02.000Z","updated":"2018-04-04T02:30:39.000Z","comments":true,"path":"2018/04/04/PHP/2018-04-04-php7new-base/","link":"","permalink":"http://blog.renyimin.com/2018/04/04/PHP/2018-04-04-php7new-base/","excerpt":"","text":"null合并运算符 ?? ?? : 如果变量存在且值不为NULL， 它就会返回自身的值，否则返回它的第二个操作数; 先回顾一下 isset 用法: 只有 显示声明为null 或者 未声明 的变量, isset的结果为false 1234567891011121314151617181920212223242526272829&lt;?php$a = 0;$b = 0.0;$c = &apos;&apos;;$d = &apos;0&apos;;$e = &apos;0.0&apos;;$f = &apos;null&apos;;$g = [];$h = [&apos;name&apos; =&gt; &apos;renyimin&apos;, &apos;age&apos; =&gt; null];$i = null;var_dump(isset($a)); // truevar_dump(isset($b)); // truevar_dump(isset($c)); // truevar_dump(isset($d)); // truevar_dump(isset($e)); // truevar_dump(isset($f)); // truevar_dump(isset($g)); // truevar_dump(isset($h[&apos;age&apos;])); // false// 注意: array_key_exists 即使键的值为null, 结果也是truevar_dump(array_key_exists(&apos;age&apos;, $h)); // truevar_dump(isset($h[&apos;address&apos;])); // false// 注意: array_key_exists 即使键的值为null, 结果也是true, 除非键真的不存在var_dump(array_key_exists(&apos;address&apos;, $h)); // falsevar_dump(isset($h[&apos;address&apos;])); // falsevar_dump(isset($i)); // falsevar_dump(isset($j)); // false ?? 示例: 1234&lt;?php$a = 0;$c = $a ?? 10; // 相当于 $c = isset($a) ? $a : 10;echo $c; ~~ 未完待续 ###","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"103. 分区管理","slug":"mysql/2018-03-28-mysql-103","date":"2018-03-28T02:41:26.000Z","updated":"2018-03-28T02:10:14.000Z","comments":true,"path":"2018/03/28/mysql/2018-03-28-mysql-103/","link":"","permalink":"http://blog.renyimin.com/2018/03/28/mysql/2018-03-28-mysql-103/","excerpt":"","text":"分区管理MySQL5.1 提供了许多修改分区表的方式, 添加、删除、重新定义、合并或拆分已经存在的分区是可能的, 所有这些操作都可以通过使用ALTER TABLE命令的分区扩展来实现; 当然, 也有获得分区表和分区信息的方式; drop删除分区及数据 mysql&gt; alter table user drop partition p4; 注意: 只能对每个分区进行删除, 不能针对每个子分区进行删除操作, 删除分区后子分区连同数据一并被删除; 删除分区后, 数据也被删除了; drop partition删除分区的语法, 只能用于 range/list 分区 (如果用来删除hash分区或者key分区,则会报错) 如果要删除 hash/key 分区, 则直接使用下面remove来移除分区即可, 一般也不直接删除数据; remove移除分区 使用remove移除分区, 注意仅仅是移除分区, 并不会删除数据 (和drop PARTITION不一样, 后者会连同数据一起删除) ALTER TABLE tablename REMOVE PARTITIONING; 这样就可以将一个原本分区的数据表变成不分区的表 移除分区前 123456789$ ls |grep user |xargs du -sh0B user#P#p1.MYD4.0K user#P#p1.MYI4.0K user#P#p2.MYD4.0K user#P#p2.MYI4.0K user#P#p3.MYD4.0K user#P#p3.MYI12K user.frm4.0K user.par ALTER TABLEuserREMOVE PARTITIONING; 移除分区后 1234$ ls |grep user |xargs du -sh4.0K user.MYD4.0K user.MYI12K user.frm 注意: 该语法是可以适用于 range/list/key/hash 类型的分区的; 创建不同类型分区分区创建索引(整表创建索引)新增分区新增不同类型分区合并分区 原本分区结构 12345678910111213$ ls |grep user |xargs du -sh4.0K user#P#p0.MYD4.0K user#P#p0.MYI4.0K user#P#p1.MYD4.0K user#P#p1.MYI4.0K user#P#p2.MYD4.0K user#P#p2.MYI4.0K user#P#p3.MYD4.0K user#P#p3.MYI4.0K user#P#p4.MYD4.0K user#P#p4.MYI12K user.frm4.0K user.par 合并 p2, p3这两个分区 分区并无子分区123456ALTER TABLE tb_sub_ev REORGANIZE PARTITION p2,p3 INTO ( PARTITION m_p2_p3 VALUES LESS THAN (2000) ( SUBPARTITION n0, SUBPARTITION n1 ) ); 拆分分区重新分区每日自动新增分区未完待续","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"102. 分区键, 主键, 唯一索引关系","slug":"mysql/2018-03-27-mysql-102","date":"2018-03-27T07:46:51.000Z","updated":"2018-03-28T02:09:40.000Z","comments":true,"path":"2018/03/27/mysql/2018-03-27-mysql-102/","link":"","permalink":"http://blog.renyimin.com/2018/03/27/mysql/2018-03-27-mysql-102/","excerpt":"","text":"若表有 primary key 或 unique key, 在对表进行分区时, 需要注意: 分区键 必须包含在primary key或unique key列内, 这是为了确保主键的效率, 否则同一主键区的数据一个在Ａ分区, 一个在Ｂ分区, 显然会比较麻烦; 可以说: 在分区表上, 用于分区表达式里的每一个字段都必须是唯一性索引的一部分; 如何理解上面的概念? 如果表中既有主键, 也有唯一索引: 无论单列键还是多列键分区都会失败 单列唯一索引键分区报错 1234567891011121314151617CREATE TABLE IF NOT EXISTS `user` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;, `name` varchar(50) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;名称&apos;, `sex` int(1) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;0为男，1为女&apos;, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, PRIMARY KEY (`id`), UNIQUE KEY `age_unique` (`age`)) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (`age`)( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN (6), PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN (MAXVALUE));// 1503 - A PRIMARY KEY must include all columns in the table&apos;s partitioning function, Time: 0.011000s 单列主键分区报错CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL AUTO_INCREMENT COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, PRIMARY KEY (id), UNIQUE KEY `age_unique` (`age`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id)( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN (6), PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN (MAXVALUE));// 1503 - A UNIQUE INDEX must include all columns in the table’s partitioning function, Time: 0.008000s 1- 多列(主键和唯一索引键)分区报错 // 两列做分区也是失败的, 如下会报错CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL AUTO_INCREMENT COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, PRIMARY KEY (id), UNIQUE KEY `age_unique` (`age`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id,age)( PARTITION p0 VALUES LESS THAN (3, 10), PARTITION p1 VALUES LESS THAN (6, 15), PARTITION p2 VALUES LESS THAN (9, 20), PARTITION p3 VALUES LESS THAN (12, 25), PARTITION p4 VALUES LESS THAN (MAXVALUE,MAXVALUE));// 1503 - A PRIMARY KEY must include all columns in the table’s partitioning function, Time: 0.012000s 123 4. 表中只有`主键`: 分区键属于主键内的键即可 (多列分区, 需要将列与主键一起作为主键才行) - 分区键属于主键内的键即可 CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL AUTO_INCREMENT COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, PRIMARY KEY (id)) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id)( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN (6), PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN (MAXVALUE)); 1- 如果想多列分区, 则需要将多余的列与主键一起作为主键 CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL AUTO_INCREMENT COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, PRIMARY KEY (id, age)) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id,age)( PARTITION p0 VALUES LESS THAN (3, 10), PARTITION p1 VALUES LESS THAN (6, 15), PARTITION p2 VALUES LESS THAN (9, 20), PARTITION p3 VALUES LESS THAN (12, 25), PARTITION p4 VALUES LESS THAN (MAXVALUE,MAXVALUE)); 125. 表中只有`唯一索引`: 分区键属于`唯一索引键`内的键即可 (多列分区, 需要将多个列一起作为唯一索引) - 分区键属于`唯一索引键`内的键即可 CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, UNIQUE KEY (id, age)) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id)( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN (6), PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN (MAXVALUE)); 1- 多列分区报错(竟然是报主键错): 1503 - A PRIMARY KEY must include all columns in the table&apos;s partitioning function, Time: 0.012000s CREATE TABLE IF NOT EXISTS user (id int(11) NOT NULL COMMENT ‘用户ID’,name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’,sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, age int(2) NOT NULL DEFAULT ‘0’ COMMENT ‘年龄’,UNIQUE KEY id_u (id), UNIQUE KEY age_u (age)) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id, age)(PARTITION p0 VALUES LESS THAN (3,10),PARTITION p1 VALUES LESS THAN (6,15),PARTITION p2 VALUES LESS THAN (9,20),PARTITION p3 VALUES LESS THAN (12,25),PARTITION p4 VALUES LESS THAN (MAXVALUE,MAXVALUE)); 1- **多个唯一索引: 无论单列分区还是多列分区, 都会报错** // 单列分区报错CREATE TABLE IF NOT EXISTS user ( id int(11) NOT NULL COMMENT ‘用户ID’, name varchar(50) NOT NULL DEFAULT ‘’ COMMENT ‘名称’, sex int(1) NOT NULL DEFAULT ‘0’ COMMENT ‘0为男，1为女’, `age` int(2) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;年龄&apos;, UNIQUE KEY id_u (id), UNIQUE KEY age_u (`age`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8PARTITION BY RANGE COLUMNS (id)( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN (6), PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN (MAXVALUE));``` 小结 要进行分区, 表中不能同时存在主键和唯一索引键, 也不能存在多个唯一索引键; 分区的键必须包含在主键内 或者 包含在唯一索引键内; 本篇只是测试了range这种分区类型, 其实上面的限制对于range/list/hash/key类型的分区都适用;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"101. 分区,分表,分库","slug":"mysql/2018-03-27-mysql-101","date":"2018-03-27T03:37:12.000Z","updated":"2018-03-28T02:09:09.000Z","comments":true,"path":"2018/03/27/mysql/2018-03-27-mysql-101/","link":"","permalink":"http://blog.renyimin.com/2018/03/27/mysql/2018-03-27-mysql-101/","excerpt":"","text":"前言 MySQL是从5.1开始支持分区功能的, 在MySQL中, 数据是以文件的形势存在磁盘上的, 默认放在 ‘/mysql/data/‘ 下(可以通过my.cnf中的datadir来指定) MyISAM引擎中, 一张表主要对应着三个文件: .frm(与表相关的元数据信息都存放在frm文件, 包括表结构的定义信息等) .myd(存放表数据) .myi(存表索引) InnoDB引擎中, 一张表也是对应着三个文件: .frm(和MyISAM差不多) .ibd文件和.ibdata文件, 都是存放innodb数据的文件, 之所以用两种文件来存放innodb的数据, 是因为innodb的数据存储方式能够通过配置来决定是使用共享表空间存放存储数据, 还是用独享表空间存放存储数据独享表空间存储方式使用.ibd文件,并且每个表一个ibd文件;共享表空间存储方式使用.ibdata文件，所有表共同使用一个ibdata文件可在mysql的配置文件通过innodb_file_per_table进行配置 如果一张表的数据量太大, .ibd, .myd, .myi 之类的文件就会变的很大, 查找数据就会变的很慢, 此时就可以利用mysql的分区功能, 在物理上将这一张表对应的三个文件, 分割成许多个小块, 这样之后, 如果查找一条数据时, 就不用全部查找了, 只要知道这条数据在哪一块, 然后在那一块找即可; 如果表的数据太大, 可能一个磁盘放不下, 此时, 还可以把数据分配到不同的磁盘里面去; 查看当前MySQL版本是否支持分区 对于MySQL5.6以下版本, 如果查询结果显示Empty, 表示不支持分区: 12mysql&gt; show variables like &apos;%partition%&apos;;Empty set (0.00 sec) 对于mysql5.6以及以上版本, 需要使用下面的查询命令: 1mysql&gt; show plugins; 上面的查询方法会显示所有插件, 如果有如下插件的话, 表示支持分区: 1| partition | ACTIVE | STORAGE ENGINE | NULL | GPL | 分区的2种方式横向分区横向分区: 比如, 有1000W条数据, 分成十份, 前10W条数据放到第一个分区, 第二个10W条数据放到第二个分区, 依此类推; 也就是把表分成了十份(和使用merge来分表有点像, 取出一条数据的时候, 这条数据包含了表结构中的所有字段); 横向分区, 并没有改变表的结构; 纵向分区纵向分区: 比如, 在设计用户表的时候, 开始的时候没有考虑好, 把用户的所有信息都放到了一张表里面去, 这样这个表里面就会有比较大的字段, 如个人简介..等, 而这些简介也许不会有好多人去看，所以等到有人要看的时候, 再去查找(分表的时候, 可以把这样的大字段与主表分开来); 横向分区mysql提供的分区属于第一种 横向分区, 并且细分成很多种方式 range分区 按照RANGE分区的表是通过如下方式进行分区的, 分区表达式的值位于一个给定的连续区间内的那些行, 会被放到一个分区中 创建表同时进行分区 12345678910111213CREATE TABLE IF NOT EXISTS `user` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;, `name` varchar(50) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;名称&apos;, `sex` int(1) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;0为男，1为女&apos;, PRIMARY KEY (`id`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 PARTITION BY RANGE (id) ( PARTITION p0 VALUES LESS THAN (3), PARTITION p1 VALUES LESS THAN ( 6 ),PARTITION p2 VALUES LESS THAN (9), PARTITION p3 VALUES LESS THAN (12), PARTITION p4 VALUES LESS THAN MAXVALUE ); 查看分区效果 (到数据表文件的存放处 $ cd /Library/Application\\ Support/appsolute/MAMP\\ PRO/db/mysql56/) 12345678910111213$ ls |grep user |xargs du -sh0B user#P#p0.MYD4.0K user#P#p0.MYI0B user#P#p1.MYD4.0K user#P#p1.MYI0B user#P#p2.MYD4.0K user#P#p2.MYI0B user#P#p3.MYD4.0K user#P#p3.MYI0B user#P#p4.MYD4.0K user#P#p4.MYI12K user.frm4.0K user.par 插入数据 12345INSERT INTO `test`.`user` (`name` ,`sex`)VALUES (&apos;tank&apos;, &apos;0&apos;) ,(&apos;zhang&apos;,1),(&apos;ying&apos;,1),(&apos;张&apos;,1),(&apos;映&apos;,0),(&apos;test1&apos;,1),(&apos;tank2&apos;,1) ,(&apos;tank1&apos;,1),(&apos;test2&apos;,1),(&apos;test3&apos;,1),(&apos;test4&apos;,1),(&apos;test5&apos;,1),(&apos;tank3&apos;,1) ,(&apos;tank4&apos;,1),(&apos;tank5&apos;,1),(&apos;tank6&apos;,1),(&apos;tank7&apos;,1),(&apos;tank8&apos;,1),(&apos;tank9&apos;,1) ,(&apos;tank10&apos;,1),(&apos;tank11&apos;,1),(&apos;tank12&apos;,1),(&apos;tank13&apos;,1),(&apos;tank21&apos;,1),(&apos;tank42&apos;,1); 再次查看分区效果如下可以看到, 文件大小都是4.0K, 从这儿我们可以看出MyISAM引擎下, 分区的最小区块是4K (InnoDB貌似是96k) 12345678910111213$ ls |grep user |xargs du -sh4.0K user#P#p0.MYD4.0K user#P#p0.MYI4.0K user#P#p1.MYD4.0K user#P#p1.MYI4.0K user#P#p2.MYD4.0K user#P#p2.MYI4.0K user#P#p3.MYD4.0K user#P#p3.MYI4.0K user#P#p4.MYD4.0K user#P#p4.MYI12K user.frm4.0K user.par 数据测试 初始数据为25条 1234567mysql&gt; select count(id) as count from user;+-------+| count |+-------+| 25 |+-------+1 row in set (0.00 sec) 删除第四个分区 123mysql&gt; alter table user drop partition p4; Query OK, 0 rows affected (0.15 sec)Records: 0 Duplicates: 0 Warnings: 0 可以发现, 存放在第四个分区里面的14条数据丢失了, 剩下的3个分区只有11条数据 1234567mysql&gt; select count(id) as count from user; +-------+| count |+-------+| 11 |+-------+1 row in set (0.00 sec) 查看分区文件, 发现第四个分区确实被删除了 1234567891011$ ls |grep user |xargs du -sh4.0K user#P#p0.MYD4.0K user#P#p0.MYI4.0K user#P#p1.MYD4.0K user#P#p1.MYI4.0K user#P#p2.MYD4.0K user#P#p2.MYI4.0K user#P#p3.MYD4.0K user#P#p3.MYI12K user.frm4.0K user.par 小结: 可以发现, 在进行range分区后 会生成一个 .par文件,用来存储分区信息; MyISAM/InnoDB引擎, 原有的 .frm 表结构文件没有被分隔; MyISAM引擎, 原有的 .MYD数据文件, .MYI索引文件都被分隔了; InnoDB引擎, 原有的 .idb数据文件被分隔了; 注意 当往分区列中插入null值, RANG分区会将其当作最小值来处理即插入最小的分区中 list分区中, NULL值必须出现在分区枚举值中, 否在在插入数据是会报错 hash和key会将NULL当做0处理 List分区 RANGE分区是从属于一个连续区间值的集合, 而LIST分区是基于某列的值从属于一个值列表集中的一个值 如果不用主键, 如下list分区可以创建成功，一般情况下, 一张表肯定会有一个主键(所以如果需要用其他键来做分区, 参考下一篇博文分区键, 主键, 唯一索引关系) 123456789101112131415161718192021CREATE TABLE IF NOT EXISTS `list_part` ( `id` int(11) NOT NULL COMMENT &apos;用户ID&apos;, `province_id` int(2) NOT NULL DEFAULT 0 COMMENT &apos;省&apos;, `name` varchar(50) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;名称&apos;, `sex` int(1) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;0为男, 1为女&apos;) ENGINE=INNODB DEFAULT CHARSET=utf8PARTITION BY LIST (province_id) ( PARTITION p0 VALUES IN (1,2,3,4,5,6,7,8), PARTITION p1 VALUES IN (9,10,11,12,16,21), PARTITION p2 VALUES IN (13,14,15,19), PARTITION p3 VALUES IN (17,18,20,22,23,24));// 创建成功, 会看到分区效果 (InnoDB默认分区最小是96k)$ ls |grep list_part |xargs du -sh96K list_part#P#p0.ibd96K list_part#P#p1.ibd96K list_part#P#p2.ibd96K list_part#P#p3.ibd12K list_part.frm4.0K list_part.par 注意 当往分区列中插入null值, RANG分区会将其当作最小值来处理即插入最小的分区中 list分区中, NULL值必须出现在分区枚举值中, 否在在插入数据是会报错 hash和key会将NULL当做0处理 RANGE分区必须的连续的且不能重叠(3,6,9,12,MAXVALUE可以, 3,6,5,12,MAXVALUE就会报错) hash分区 HASH分区主要用来确保数据在预先确定数目的分区中平均分布, 你所要做的只是 对将要被哈希的列值, 指定一个列值或表达式; 指定被分区的表将要被分割成的分区数量; 对HASH分区，使用的用户函数必须返回一个大于0的整数值; 创建表同时进行hash分区 12345678CREATE TABLE IF NOT EXISTS `hash_part` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;评论ID&apos;, `comment` varchar(1000) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;评论&apos;, `ip` varchar(25) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;来源IP&apos;, PRIMARY KEY (`id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 PARTITION BY HASH(id) PARTITIONS 3; 查看分区效果 123456$ ls |grep hash_part |xargs du -sh96K hash_part#P#p0.ibd96K hash_part#P#p1.ibd96K hash_part#P#p2.ibd12K hash_part.frm4.0K hash_part.par 注意 当往分区列中插入null值, RANG分区会将其当作最小值来处理即插入最小的分区中 list分区中, NULL值必须出现在分区枚举值中, 否在在插入数据是会报错 hash和key会将NULL当做0处理 RANGE分区必须的连续的且不能重叠(3,6,9,12,MAXVALUE可以, 3,6,5,12,MAXVALUE就会报错) key分区 按照KEY进行分区, 类似于按照HASH分区 HASH分区是使用用户定义的表达式 而KEY分区的哈希函数是由MySQL服务器提供 创建表同时进行key分区 12345678CREATE TABLE IF NOT EXISTS `key_part` ( `news_id` int(11) NOT NULL COMMENT &apos;新闻ID&apos;, `content` varchar(1000) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;新闻内容&apos;, `u_id` varchar(25) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;来源IP&apos;, `create_time` DATE NOT NULL DEFAULT &apos;0000-00-00 00:00:00&apos; COMMENT &apos;时间&apos; ) ENGINE=INNODB DEFAULT CHARSET=utf8 PARTITION BY LINEAR HASH(YEAR(create_time)) PARTITIONS 3; 查看分区效果 123456$ ls |grep key_part |xargs du -sh96K key_part#P#p0.ibd96K key_part#P#p1.ibd96K key_part#P#p2.ibd12K key_part.frm4.0K key_part.par 注意 当往分区列中插入null值, RANG分区会将其当作最小值来处理即插入最小的分区中 list分区中, NULL值必须出现在分区枚举值中, 否在在插入数据是会报错 hash和key会将NULL当做0处理 RANGE分区必须的连续的且不能重叠(3,6,9,12,MAXVALUE可以, 3,6,5,12,MAXVALUE就会报错) 子分区 子分区是分区表中每个分区的再次分割, 对于已经通过RANGE或LIST分区了的表再进行子分区是可能的, 子分区既可以使用HASH分区, 也可以使用KEY分区; 这也被称为复合分区(composite partitioning) 如果一个分区中创建了子分区, 其他分区也要有子分区 如果创建了子分区, 每个分区中的子分区数必须相同 同一分区内的子分区, 名字不相同, 不同分区内的子分区名子可以相同(5.1.50不适用) 创建表同时, 进行子分区操作 123456789101112131415161718CREATE TABLE IF NOT EXISTS `sub_part` ( `news_id` int(11) NOT NULL COMMENT &apos;新闻ID&apos;, `content` varchar(1000) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;新闻内容&apos;, `u_id` int(11) NOT NULL DEFAULT 0 COMMENT &apos;来源IP&apos;, `create_time` DATE NOT NULL DEFAULT &apos;0000-00-00 00:00:00&apos; COMMENT &apos;时间&apos; ) ENGINE=INNODB DEFAULT CHARSET=utf8 PARTITION BY RANGE(YEAR(create_time)) SUBPARTITION BY HASH(TO_DAYS(create_time))( PARTITION p0 VALUES LESS THAN (1990)( SUBPARTITION s0,SUBPARTITION s1,SUBPARTITION s2 ), PARTITION p1 VALUES LESS THAN (2000)( SUBPARTITION s3,SUBPARTITION s4,SUBPARTITION good ), PARTITION p2 VALUES LESS THAN MAXVALUE( SUBPARTITION tank0,SUBPARTITION tank1,SUBPARTITION tank3 ) ); 查看分区后, 数据表文件结构 123456789101112$ ls |grep sub_part |xargs du -sh96K sub_part#P#p0#SP#s0.ibd96K sub_part#P#p0#SP#s1.ibd96K sub_part#P#p0#SP#s2.ibd96K sub_part#P#p1#SP#good.ibd96K sub_part#P#p1#SP#s3.ibd96K sub_part#P#p1#SP#s4.ibd96K sub_part#P#p2#SP#tank0.ibd96K sub_part#P#p2#SP#tank1.ibd96K sub_part#P#p2#SP#tank3.ibd12K sub_part.frm4.0K sub_part.par","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"Restful","slug":"http/2018-03-06-restful","date":"2018-03-16T11:36:23.000Z","updated":"2018-05-09T06:31:19.000Z","comments":true,"path":"2018/03/16/http/2018-03-06-restful/","link":"","permalink":"http://blog.renyimin.com/2018/03/16/http/2018-03-06-restful/","excerpt":"","text":"简介REST本身并没有创造新的技术, 组件或服务, 主要指的是一组架构约束条件和原则, 隐藏在RESTful背后的理念就是使用Web的现有特征和能力, 更好地使用现有Web标准中的一些准则和约束; 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。 虽然REST本身受Web技术的影响很深, 但是理论上REST架构风格并不是绑定在HTTP上, 只不过目前HTTP是唯一与REST相关的实例; 所以通常描述的REST也是通过HTTP实现的REST; URI的设计 URI的设计应该遵循可寻址性原则, 具有自描述性, 需要在形式上给人以直觉上的关联; 比如: 用_或-来让URI可读性更好例如国内比较出名的开源中国社区, 它上面的新闻地址就采用这种风格, 如 http://www.oschina.net/news/38119/oschina-translate-reward-plan 使用/来表示资源的层级关系例如 https://github.com/rymuscle/chat/issues 就表示了一个多级的资源, 指的是rymuscle用户的chat项目的issues列表 使用?用来过滤资源 (如果记录数量很多，服务器不可能都将它们返回给用户, 比如分页等筛选条件)很多人只是把?简单的当做是参数的传递, 很容易造成URI过于复杂、难以理解; 其实可以把?用于对资源的过滤;例如 https://github.com/rymuscle/chat/pulls 用来表示git项目的所有推入请求;而 /pulls?state=closed 用来表示git项目中已经关闭的推入请求, 这种URL通常对应的是一些特定条件的查询结果或算法运算结果; ,或;可以用来表示同级资源的关系 URI里带上版本号, 如 https://api.example.com/v1/ (Github就是这样做的) 另一种做法是, 将版本号放在HTTP头信息中, 但不如放入URL方便和直观; URI中只应该描述清楚资源的名称, 而不应该包括资源的操作(因为统一资源接口要求使用标准的HTTP方法对资源进行操作, 方法都是有语义的, 所以已经明确描述了这次的操作含义) URI不应该再使用动作来描述, 如下就是一些不符合统一接口要求的URI (它们都在URI中对操作进行了描述):1234GET /getUser/1POST /createUserPUT /updateUser/1DELETE /deleteUser/1 HTTP动词接口应该使用标准的HTTP方法如GET，PUT和POST，并遵循这些方法的语义(通过明确的方法来描述操作), 可以参考博文 HTTP各请求方法详解 或者 Restful架构详解 状态码状态码应该使用HTTP标准状态码 可以参考博文 HTTP协议预览 更多参考","categories":[{"name":"Restful","slug":"Restful","permalink":"http://blog.renyimin.com/categories/Restful/"},{"name":"HTTP","slug":"Restful/HTTP","permalink":"http://blog.renyimin.com/categories/Restful/HTTP/"}],"tags":[{"name":"Restful","slug":"Restful","permalink":"http://blog.renyimin.com/tags/Restful/"},{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"yield 协程","slug":"PHP/2018-01-13-yield","date":"2018-01-13T05:20:31.000Z","updated":"2018-04-04T09:46:45.000Z","comments":true,"path":"2018/01/13/PHP/2018-01-13-yield/","link":"","permalink":"http://blog.renyimin.com/2018/01/13/PHP/2018-01-13-yield/","excerpt":"","text":"迭代器迭代生成器简介 (迭代)生成器也是一个函数, 返回的是一个迭代器, 而这个迭代器实现了Iterator接口, 所以叫迭代生成器; 和普通函数不同, 由于(迭代)生成器返回的是一个迭代器, 所以(迭代)生成器函数中的返回值可以依次返回, 而不是只返回一个单独的值; 或者换句话说, (迭代)生成器使你能更方便的实现了迭代器接口; 引用网上常见的示例 1234567891011&lt;?phpfunction xrange($start, $end, $step = 1) &#123; for ($i = $start; $i &lt;= $end; $i += $step) &#123; yield $i; &#125;&#125;// var_dump(xrange(1, 1000000)); // object(Generator)[1]foreach (xrange(1, 1000000) as $num) &#123; echo $num;&#125; 上面这个 xrange() 函数提供了和PHP的内建函数range()一样的功能, 但是不同的是range()函数返回的是一个包含值从1到100万0的数组; 而xrange()函数返回的是依次输出这些值的一个迭代器, 而不会真正以数组形式返回; 这种方法的优点是显而易见的, 它可以让你在处理大数据集合的时候不用一次性的加载到内存中, 甚至你可以处理无限大的数据流; 当然, 也可以不通过生成器来实现这个功能, 而是可以通过继承Iterator接口实现, 但通过使用生成器实现起来会更方便, 不用再去实现iterator接口中的5个方法了; 生成器内部 要从生成器认识协程, 理解它内部是如何工作是非常重要的: 生成器是一种可中断的函数, 在它里面的yield构成了中断点; 像上面的例子, 调用xrange(1,1000000)的时候, xrange()函数里代码其实并没有真正地运行, 它只是返回了一个迭代器; 调用迭代器的方法一次, 其中的代码运行一次, 例如: 如果你调用生成器返回的迭代器的rewind方法 $range-&gt;rewind(), 那么xrange()里的代码就会运行到控制流第一次出现yield的地方, 而函数内传递给yield语句的返回值, 可以通过$range-&gt;current()获取; 为了继续执行生成器中yield后的代码, 你就需要调用迭代器的 $range-&gt;next() 方法, 这将再次启动生成器, 直到下一次yield语句出现; 因此, 连续调用next()和current()方法, 你就能从生成器里获得所有的值,直到再没有yield语句出现; 对xrange()来说, 这种情形出现在$i超过$end时, 在这种情况下, 控制流将到达函数的终点, 因此将不执行任何代码, 一旦这种情况发生, vaild()方法将返回假, 这时迭代结束; 示例1 123456789101112131415161718192021&lt;?phpfunction gen() &#123; var_dump(&apos;cat&apos;); yield &apos;t&apos;; var_dump(&apos;dog&apos;);&#125;$gen = gen();$gen-&gt;rewind(); // 指针会走到第一个yield处, 所以会经过 catvar_dump($gen-&gt;current()); // 获取第一个yield处的值var_dump(&apos;----------&apos;);$gen-&gt;next(); // 会往下走, 走到下一个yield处, 所以会经过 dogvar_dump($gen-&gt;current()); // 获取下一个yield处的值 (但是后面已经没有yield了, 所以为null)// 结果:string(3) &quot;cat&quot;string(1) &quot;t&quot;string(10) &quot;----------&quot;string(3) &quot;dog&quot;NULL 示例2 1234567891011121314151617181920212223242526272829303132333435363738&lt;?phpfunction gen() &#123; var_dump(&apos;cat&apos;); yield &apos;t&apos;; var_dump(&apos;dog&apos;); yield &apos;g&apos;; var_dump(&apos;pig&apos;); yield &apos;p&apos;;&#125;$gen = gen();$gen-&gt;rewind(); // 指针会走到第一个yield处, 所以会经过 catvar_dump($gen-&gt;current()); // 获取第一个yield处的值var_dump(&apos;----------&apos;);$gen-&gt;next(); // 会往下走, 走到下一个yield处, 所以会经过 dogvar_dump($gen-&gt;current()); // 获取下一个yield处的值 (但是后面已经没有yield了, 所以为null)var_dump(&apos;----------&apos;);$gen-&gt;next(); // 会往下走, 走到下一个yield处, 所以会经过 pigvar_dump($gen-&gt;current()); // 获取下一个yield处的值 (p)var_dump(&apos;----------&apos;);$gen-&gt;next(); // 会往下走, 什么也没有var_dump($gen-&gt;current()); // 获取下一个yield处的值 (为null)// 结果string(3) &quot;cat&quot;string(1) &quot;t&quot;string(10) &quot;----------&quot;string(3) &quot;dog&quot;string(1) &quot;g&quot;string(10) &quot;----------&quot;string(3) &quot;pig&quot;string(1) &quot;p&quot;string(10) &quot;----------&quot;NULL 上面示例中使用(迭代)生成器时, yield 都只是简单地作为一个语句来使用, 这样只能实现 生成器到调用者的单向通信; 迭代器在进行循环(迭代)前, 需要先进行rewind; 简单理解 协程(Coroutine) 协程的支持是在迭代生成器的基础上, 增加了调用者可以回送数据给生成器的功能(调用者发送数据给被调用的生成器函数), 这就把生成器到调用者的单向通信转变为两者之间的双向通信; 调用者传递数据给生成器, 是通过迭代器的send()方法实现的; 下面就是一个简单的协程示例, 用来演示这种通信如何运行的 下面例子可以看到在生成器内部, yield 不再是简单的语句, 而是一个可以接收调用者参数并进行赋值的表达式 12345678910111213141516171819202122232425262728293031323334353637&lt;?phpfunction gen() &#123; var_dump(&apos;cat&apos;); $ret1 = (yield &apos;yield1&apos;); var_dump($ret1 . &apos;dog&apos;); $ret2 = (yield &apos;yield2&apos;); var_dump($ret2 . &apos;pig&apos;);&#125;$gen = gen();$gen-&gt;rewind(); // 指针会走到第一个 yield 处, 所以会经过 cat// catecho &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;current());// yield1echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;send(&apos;lala&apos;)); // 如果第一次send之前没有rewind将指针指向第一个yield, 则send会: 自动先进行一次rewind // 2. 由于指针还在第一个yield处, 此时send传递参数还是传到第一个yield处 (yield &apos;yield1&apos;) // 3. 执行next 到下一个yield (会经过 $ret.dog ) // 4. 返回next之后的yield值(yield &apos;yield2&apos; )// laladog// yield2echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;current()); // 到了第二个yield处, 由于没有传递至, 所以可以通过current直接获取 yield &apos;yield2&apos; 传给调用者的 &apos;yield2&apos;// yield2echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;send(&apos;haha&apos;)); // 由于指针没变, 还在第二个yield处, 传递了 haha 给生成器 // 然后执行了next 继续往下走, 会经过 var_dump($ret . &apos;pig&apos;); // 返回next之后的yield值 (已经进行了next, 而之后没有yield了, 所以会返回null)// hahapig// nullecho &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;current()); // 由于上面已经走了next, 而下面啥都没有了......// null 第一次send会默认执行rewind 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?phpfunction gen() &#123; var_dump(&apos;cat&apos;); $ret1 = (yield &apos;yield1&apos;); var_dump($ret1 . &apos;dog&apos;); $ret2 = (yield &apos;yield2&apos;); var_dump($ret2 . &apos;pig&apos;);&#125;$gen = gen();echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;send(&apos;lala&apos;)); // 如果第一次send之前没有rewind将指针指向第一个yield, 则send会: 自动先进行一次rewind (所以也会经过 cat ) // 2. 由于指针还在第一个yield处, 此时send传递参数还是传到第一个yield处 (yield &apos;yield1&apos;) // 3. 执行next 到下一个yield (会经过 $ret.dog ) // 4. 返回next之后的yield值(yield &apos;yield2&apos; )// cat// laladog// yield2echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;current()); // 到了第二个yield处, 由于没有传递至, 所以可以通过current直接获取 yield &apos;yield2&apos; 传给调用者的 &apos;yield2&apos;// yield2echo &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;send(&apos;haha&apos;)); // 由于指针没变, 还在第二个yield处, 传递了 haha 给生成器 // 然后执行了next 继续往下走, 会经过 var_dump($ret . &apos;pig&apos;); // 返回next之后的yield值 (已经进行了next, 而之后没有yield了, 所以会返回null)// hahapig// nullecho &apos;+++++++++++++++++++&apos;;var_dump($gen-&gt;current()); // 由于上面已经走了next, 而下面啥都没有了......// null//function foo() &#123;// $string = yield;// echo $string;// for ($i = 1; $i &lt;= 3; $i++) &#123;// yield $i;// &#125;//&#125;////$generator = foo();//$generator-&gt;send(&apos;Hello world!&apos;);//foreach ($generator as $value) echo &quot;$value\\n&quot;; 更多小例子 (可参考PHP手册) 可以迭代三次 123456789101112131415161718&lt;?phpfunction printer() &#123; $i = 3; while ($i&gt;0) &#123; echo 33; $string = yield; echo $string; $i--; echo $i; &#125;&#125;$printer = printer();$printer-&gt;send(&apos;Hello world!&apos;);echo &quot;&lt;br/&gt;++++++++++&lt;br/&gt;&quot;;$printer-&gt;send(&apos;haha&apos;);echo &quot;&lt;br/&gt;++++++++++&lt;br/&gt;&quot;;$printer-&gt;send(&apos;heihei&apos;); 可以一直迭代 1234567891011121314151617&lt;?phpfunction printer() &#123; while (true) &#123; echo 33; $string = yield; echo $string; &#125;&#125;$printer = printer();$printer-&gt;send(&apos;Hello world!&apos;);echo &quot;&lt;br/&gt;++++++++++&lt;br/&gt;&quot;;$printer-&gt;send(&apos;haha&apos;);echo &quot;&lt;br/&gt;++++++++++&lt;br/&gt;&quot;;$printer-&gt;send(&apos;heihei&apos;);echo &quot;&lt;br/&gt;++++++++++&lt;br/&gt;&quot;;$printer-&gt;send(&apos;heihei2&apos;); 个人感觉重点在于理解调用者发出send()后, send()的执行包括next()及返回 (以及, 如果是初次执行send的话, 会隐式执行rewind); 也可以参考鸟哥博客对send的说明 协程实现任务调度 上面的例子比较简单, 可能无法体系会到协程的优点, 接下来可以尝试利用协程去实现多任务调度 要解决的问题是当你想并发地运行多任务(或者”程序”)时, 我们都知道CPU在一个时刻只能运行一个任务(不考虑多核的情况), 因此处理器需要在不同的任务之间进行切换, 而且总是让每个任务运行一小会儿; 多任务协作这个术语中的协作很好的说明了如何进行这种切换的: 它要求当前正在运行的任务自动把控制传回给调度器, 这样就可以运行其他任务了;这与抢占多任务相反, 抢占多任务是这样的: 调度器可以中断运行了一段时间的任务, 不管它喜欢还是不喜欢;协作多任务在Windows的早期版本(windows95)和Mac OS中有使用, 不过它们后来都切换到使用抢先多任务了, 理由相当明确: 如果你依靠程序自动交出控制的话, 那么一些恶意的程序将很容易占用整个CPU, 不与其他任务共享; 现在你应当明白协程和任务调度之间的关系: yield指令提供了任务中断自身的一种方法, 然后把控制交回给任务调度器; 因此协程可以运行多个其他任务, 更进一步来说, yield可以用来在任务和调度器之间进行通信; 手册中这个例子有空可以稍稍微品味一下: 123456789101112131415161718&lt;?phpfunction nums() &#123; for ($i = 0; $i &lt; 5; ++$i) &#123; //get a value from the caller $cmd = (yield $i); if($cmd == &apos;stop&apos;) return;//exit the function &#125;&#125;$gen = nums();foreach($gen as $v)&#123; if($v == 3)//we are satisfied $gen-&gt;send(&apos;stop&apos;); echo &quot;&#123;$v&#125;\\n&quot;;&#125; 多任务调度的实现 两个使用协程函数的任务: 12345678910111213function task1() &#123; for ($i = 1; $i &lt;= 10; ++$i) &#123; echo &quot;This is task 1 iteration $i.\\n&quot;; yield; &#125;&#125;function task2() &#123; for ($i = 1; $i &lt;= 5; ++$i) &#123; echo &quot;This is task 2 iteration $i.\\n&quot;; yield; &#125;&#125; Task任务类包装 任务协程函数 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?php/** * 如代码, 一个任务就是用任务ID标记的一个协程(函数). * 使用setSendValue()方法, 你可以指定哪些值将被发送到下次的恢复(在之后你会了解到我们需要这个), * run()函数确实没有做什么, 除了调用send()方法的协同程序 * 要理解为什么添加了一个 beforeFirstYieldflag变量 */class Task &#123; protected $taskId; protected $coroutine; protected $sendValue = null; protected $beforeFirstYield = true; public function __construct($taskId, Generator $coroutine) &#123; $this-&gt;taskId = $taskId; $this-&gt;coroutine = $coroutine; &#125; public function getTaskId() &#123; return $this-&gt;taskId; &#125;// public function setSendValue($sendValue) &#123;// $this-&gt;sendValue = $sendValue;// &#125; public function run() &#123; // 由于send的特性, 会导致先next, 所以下面在设置了一个初始标志beforeFirstYield, 初始时, 先获取了current if ($this-&gt;beforeFirstYield) &#123; $this-&gt;beforeFirstYield = false; return $this-&gt;coroutine-&gt;current(); &#125; else &#123; // 注意: 执行send之后会返回next之后的yield值 $retval = $this-&gt;coroutine-&gt;send(&apos;目前还没用到双向通信&apos;); return $retval; &#125; &#125; public function isFinished() &#123; return !$this-&gt;coroutine-&gt;valid(); &#125;&#125; 实现Scheduler调度器类 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?phpclass Scheduler &#123; protected $maxTaskId = 0; protected $taskMap = []; // taskId =&gt; task protected $taskQueue; public function __construct() &#123; $this-&gt;taskQueue = new SplQueue(); &#125; public function newTask(Generator $coroutine) &#123; // 生成任务id $tid = ++$this-&gt;maxTaskId; // 将协程函数封包到Task类中 作为任务 $task = new Task($tid, $coroutine); // 放入调度器的任务池中 $this-&gt;taskMap[$tid] = $task; // 将任务放入调度器的队列中 $this-&gt;schedule($task); return $tid; &#125; public function schedule(Task $task) &#123; $this-&gt;taskQueue-&gt;enqueue($task); &#125; public function run() &#123; while (!$this-&gt;taskQueue-&gt;isEmpty()) &#123; // 从调度器队列中弹出任务, 准备执行 $task = $this-&gt;taskQueue-&gt;dequeue(); // 执行任务 $task-&gt;run(); // 如果任务执行完毕, 则从调度器的任务池中清除任务 if ($task-&gt;isFinished()) &#123; unset($this-&gt;taskMap[$task-&gt;getTaskId()]); &#125; else &#123; // 否则, 再次将弹出的任务放入调度器的任务队列中 $this-&gt;schedule($task); &#125; &#125; &#125;&#125; 测试 test.php 12345678910111213141516171819202122&lt;?phprequire_once &apos;Task.php&apos;;require_once &apos;Scheduler.php&apos;;function task1() &#123; for ($i = 1; $i &lt;= 10; ++$i) &#123; echo &quot;This is task 1 iteration $i.\\n&quot;; yield; &#125;&#125;function task2() &#123; for ($i = 1; $i &lt;= 5; ++$i) &#123; echo &quot;This is task 2 iteration $i.\\n&quot;; yield; &#125;&#125;$scheduler = new Scheduler;$scheduler-&gt;newTask(task1());$scheduler-&gt;newTask(task2());$scheduler-&gt;run(); 当然, 上面是具有两个简单任务(没什么意义的任务)的调度器; 任务和调度器之间的通信 上面可以看到, 调度器已经运行了, 那么我们来看下一个问题: 任务和调度器之间的通信; 接下来将模拟 进程和操作系统进行会话时的方式 – 系统调用, 来做任务和调度器之间的通信我们需要系统调用的理由是操作系统与进程相比它处在不同的权限级别上, 因此为了执行特权级别的操作(如杀死另一个进程), 就不得不以某种方式把控制传回给内核, 这样内核就可以执行所说的操作了; 再说一遍, 这种行为在内部是通过使用中断指令来实现的. 过去使用的是通用的int指令, 如今使用的是更特殊并且更快速的syscall/sysenter指令; 我们的任务调度系统将反映这种设计: 不是简单地把调度器传递给任务(这样就允许它做它想做的任何事), 我们将通过给yield表达式传递信息来与系统调用通信, 这儿yield即是中断, 也是传递信息给调度器(和从调度器传递出信息)的方法; 为了说明系统调用, 我们对可调用的系统调用做一个小小的封装: 1 ~~未完待续","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"08. 并行连接, 持久连接","slug":"http/2017-12-06-HTTP-08","date":"2017-12-06T12:10:11.000Z","updated":"2018-05-09T06:26:48.000Z","comments":true,"path":"2017/12/06/http/2017-12-06-HTTP-08/","link":"","permalink":"http://blog.renyimin.com/2017/12/06/http/2017-12-06-HTTP-08/","excerpt":"","text":"常被误解的Connection首部 HTTP允许在客户端和最终的源端服务器之间存在一串HTTP中间实体(代理, 高速缓存等)。可以从客户端开始, 逐跳地将HTTP报文经过这些中间设备, 转发到源端服务器上去(或者进行反向传递)。 HTTP的 Connection 首部字段中有一个由,分隔的连接标签列表; Connection首部可以承载3种不同类型的标签, 因此非常令人费解: HTTP首部字段名, 列出了只与此链接有关的首部; 任意标签值, 用于描述此链接的非标准选项; close, 说明操作完成之后需要关闭这条持久连接; 如果连接标签中包含了一个HTTP首部字段的名称, 那么这个首部字段就包含了一些连接有关的信息, 不能将其转发出去, 在将报文转发出去之前, 必须删除Connection首部列出的所有首部字段。 由于Connection首部可以防止无意中对本地首部的转发, 因此将逐跳字首部名放入Connection首部被称为”对首部的保护”。(Connection首部是个逐跳首部, 只适用于单条传输链路, 不应该沿着传输链路向下传输 (参考P101)) 并行连接 在串行请求时, 浏览器可以先完整地请求原始的HTML页面, 然后请求第一个嵌入对象, 然后请求第二个嵌入对象等, 以这种简单的方式对每个嵌入式对象进行串行处理, 很明显这样处理很慢!! HTTP允许客户端打开多条连接, 并行地执行多个HTTP事务, 如下图, 并行加载了四幅嵌入式图片, 每个事务都有自己的TCP连接: 并行连接可能会提高页面的加载速度 包含嵌入对象的组合页面如果能通过并行连接克服单条连接的空载时间和带宽限制, 加载速度也会有所提高。时延可以重叠起来, 而且如果单条连接没有充分利用客户端的因特网带宽, 可以将为用带宽分配来装载其他对象。 如下图, 串行和并行的对比, 并行情况下, 先装载的是封闭的HTML页面, 然后并行处理其余3个事务, 每个事务都有自己的连接。(图片的装载是并行的, 连接的时延也是重叠的) 由于软件开销的存在, 每个连接请求之间总会有一些小的时延, 但连接请求和传输时间基本上都是重叠起来的! 并行连接不一定更快 即使并行连接的速度可能会更快, 但是并不一定总是更快 因为在客户端的网络带宽如果不足时, 大部分的时间可能都是用来传送数据的。在这种情况下, 一个连接到速度较快服务器上的HTTP事务就会很容易耗尽所有可用的Modem带宽。 如果并行加载多个对象, 每个对象都会去竞争这有限的带宽, 每个对象都会以较慢的速度按比例加载, 这样带来的性能提升就很小, 甚至没什么提升。 而且打开大量连接会消耗很多内存资源, 从而引发自身性能问题。 复杂的Web有可能会有数十或数百个内嵌对象, 客户端可能可以打开数百个连接, 但Web服务器通常要同时处理很多其他用户的请求, 所以很少有Web服务器希望出现这样的情况。 一百个用户同时发出申请, 每个用户打开100个连接, 服务器就要负责处理1W个连接, 这会造成服务器性能的严重下降。对高负荷的代理来说也同样如此。 实际上, 浏览器确实使用了并行连接, 但它们会将并行连接的总数限制为一个较小的值(通常是四个)。服务器可以随意关闭来自特定客户端的超量连接。 并行连接可能让人”感觉”更快一些通过上面的介绍, 我们知道并行连接并不总是能使页面加载更快, 但即使实际上没有加快页面的传输速度, 并行连接通常也会让用户觉得页面加载的更快了,因为多个组件对象同时出现屏幕上时, 用户能够看到加载的进展。如果整个屏幕上有很多动作在进行, 即使实际上整个页面的下载时间更长, 用户也会认为Web页面加载得更快一些。 持久连接 HTTP/1.1(以及HTTP/1.0的各种增强版本)允许HTTP设备在事务处理结束之后将TCP连接保持在打开状态, 以便未来的HTTP请求能够重用现存的连接。在事务处理结束之后仍然保持在打开状态的TCP连接被称为持久连接。 非持久连接会在每个事务结束之后关闭, 持久连接会在不同事务之间保持打开状态, 直到客户端或服务器其决定将其关闭为止。 重用已对目标服务器打开的空闲持久连接, 就可以避开缓慢的连接建立阶段。而且已经打开的连接还可以避免慢启动的拥塞使用阶段, 以便更快速地进行数据的传输。 持久连接和并行连接 之前已经了解过”并行连接可以提高复合页面的传输速度, 但并行连接也有一些缺点”;而持久连接有一些比并行连接更好的地方,持久连接降低了时延和连接建立的开销, 将连接保持在已调谐状态, 而且减少了打开连接的潜在数量。但是, 管理持久连接时要特别小心, 不然就会积累大量的空闲连接, 耗费本地以及远程客户端和服务器上的资源。 持久连接与并行连接配合使用可能是更高效的方式。现在, 很多Web应用程序都会打开少量的并行连接, 其中的每一个都是持久连接。 持久连接有两种类型: 比较老的 HTTP/1.0+&quot;keep-alive&quot; 连接, 以及现代的 HTTP/1.1 &quot;persistent&quot; 连接。 HTTP/1.0+keep-alive连接 前言:大约从1996年开始, 很多HTTP/1.0浏览器和服务器都进行了扩展, 以支持一种被称为keep-alive连接的早期实验型持久连接。这些早期的持久连接收到了一些互操作性设计方面问题的困扰, 这些问题在后期的HTTP/1.1版本中都得到了修正, 但很多客户端和服务器仍然在使用这些早期的keep-alive连接。 下图在”串行连接上实现了4个HTTP事务的时间线” 与 “在一条持久连接上实现同样事务” 所需的时间线进行了比较, 显示了keep-alive连接的一些性能优点 由于去除了创建连接和关闭连接的开销, 所以时间线有所缩减 Keep-Alive操作客户端和服务器要配合 keep-alive已经不再使用了, 而且在当前的HTTP/1.1规范中也已经没有了对它的说明了。但浏览器和服务器对keep-alive握手的使用仍然相当广泛, 因此HTTP的实现者应该做好与之进行交互操作的准备. 实现HTTP/1.0 keep alive连接的客户端可以通过包含Connection: Keep-Alive首部请求将一条连接保持在打开状态。 如果服务器愿意为下一条请求将连接保持在打开状态, 就在响应中包含相同的首部。如果响应中没有Connection: Keep-Alive首部, 客户端就认为服务器不支持keep-alive, 会在发回响应报文之后关闭连接。 还有keep-alive首部 注意, keep-Alive首部只是请求将连接保持在活跃状态。发出keep-alive请求之后, 客户端和服务器并不一定会同意进行keep-alive会话。 它们可以在任意时刻关闭空闲的keep-alive连接, 并可随意限制keep-alive连接所处理事务的数量。 可以用Keep-Alive通用首部字段中指定的, 有逗号分隔的选项来调节keep-alive的行为: 参数timeout: 是在Keep-Alive响应首部发送的, 它估计了服务器希望将连接保持在活跃状态的时间。这并不是一个承诺值。 参数max: 是在Keep-Alive响应首部发送的, 它估计了服务器还希望为多少个事务保持此连接的活跃状态。这并不是一个承诺值。 Keep-Alive首部还可以支持任意未经处理的属性, 这些属性主要用于诊断和调试。语法为 name [=value]。 Keep-Alive首部完全是可选的, 但只有在提供了 Connection:Keep-Alive 时才能使用它。 下面这个例子说明服务器最多还会为另外5个事务保持连接的打开状态, 或者将打开状态保持到连接空闲了2分钟之后。 12Connection: Keep-AliveKeep-Alive: max=5, timeout=120 keep-alive连接的限制和规则 在HTTP/1.0中, keep-alive并不是默认使用的。客户端必须发送一个 Connection: Keep-Alive 请求首部来激活keep-alive连接。 Connection: Keep-Alive 首部必须随所有希望保持持久连接的报文一起发送。 如果客户端没有发送Connection: Keep-Alive首部, 服务器就会在那条请求之后关闭连接。 客户端如果探明响应中没有Connection: Keep-Alive响应首部, 就可以知道服务器发出响应之后是否会关闭连接了。 一般都是在检测到连接关闭之后, 就可以确定报文实体主体部分的长度。如果想”无需检测到连接关闭 就能确定报文实体主体部分的长度”, 那你的响应报文的实体主体部分必须有正确的Connect-Length, 有多部件媒体类型, 或者用分块传输编码的方式进行了编码。 在一条keep-alive信道中回送错误的 Connection-Length 是很糟糕的事, 这样的话, 事务处理的另一端就无法精确地检测出一条报文的结束和另一条报文的开始了。 代理和网关必须执行Connection首部的规则, 代理或网关必须在将报文转发出去或将其高速缓存之前, 删除在Connection首部中命名的所有首部字段以及Connection首部本身。 严格来说, 不应该与无法确定是否支持Connection首部的代理服务器建立keep-alive连接, 以防止出现下面要介绍的哑代理问题, 在实际应用中不是总能做到这一点的。 从技术上来讲, 应该忽略所有来自HTTP/1.0设备的Connection首部字段(包括Connection:Keep-Alive), 因为他们可能是由比较老的代理服务器误转发的。但是实际上, 尽管可能会有在老代理上挂起的危险, 有些客户端和服务器还是会违反这条规则。 除非重复发送请求会产生其他副作用, 否则 “如果在客户端受到完整响应之前连接就关闭了, 那么客户端一定要做好重试请求的准备”。 Keep-Alive和哑代理 正常情况下, 如果客户端与一台服务器对话, 客户端可以发送一个 Connection:Keep-Alive 首部来告知服务器它希望保持连接的活跃状态, 如果服务器支持keep-alive, 就回送一个 Connection:Keep-Alive 首部, 否则就不回送。 问题是出在代理上 — 尤其是那些不理解Connection首部, 而且不知道在沿着转发链路将报文转发出去之前应该将Connection首部删除的代理。 很多老式或简单的代理都是盲中继(blind relay), 他们只是将字节从一个连接转发到两一个连接中去, 不对Connection首部进行特殊处理。 下图就是一个Web客户端通过一个作为盲中继使用的哑代理与Web服务器进行对话的例子: 更多参考: P101 盲中继的更多问题参考 4.5.7 (??) 为了防止此类代理通信问题的发生, 现在的代理都决不能转发Connection首部和所有名字出现在Connection值中的首部。 另外还有几个不能作为Connection首部的值, 并且也不能被代理转发或作为缓存响应使用的首部: Proxy-Authenticate, Proxy-Connection, Transfer-Encoding 和 Upgrade; HTTP/1.1 persistent连接 HTTP/1.1主键停止了对keep-alive连接的支持, 用一种名为持久连接(persistent connection)的改进型设计取代了它。 持久连接的目的与keep-alive连接的目的相同, 但机制更优一些。 与HTTP/1.0的keep-alive连接不同, HTTP/1.1持久连接在默认情况下是激活的。除非特别指明, 否则HTTP/1.1假定所有连接都是持久的。 要在事务处理结束之后将连接关闭, HTTP/1.1应用程序必须向报文中显示地添加一个Connection:close首部。 这是与以前的HTTP协议很重要的区别, 在以前的版本中, keep-alive连接要么是可选的, 要么根本就不支持。 HTTP/1.1客户端假定在收到响应后, 除非响应中包含了 Connection:close首部, 不然HTTP/1.1连接就仍维持在打开状态。 但是, 客户端和服务器仍然可以随时关闭空闲的连接。 不发送 Connection:close 并不以为这服务器承诺永远将连接保持在打开状态。 persistent连接的限制和规则 (??) 发送了 Connection:close 请求首部之后, 客户端就无法在那条连接上发送更多的请求了。 如果客户端不想在连接上发送其他请求了, 就应该在最后一条请求中发送一个 Connection:close 请求首部。 只有当连接上所有的报文都有正确的, 自定义报文长度时 – 也就是, 实体主体部分的长度都和响应 Connect-Length 一致, 或者是用分块传输编码方式编码的 — 连接才能持久保持。 HTTP/1.1的代理必须能够分别管理与客户端和服务器的持久连接 — 每个持久连接都值适用于一跳传输。 (由于较老的代理会转发Connection首部, 所以)HTTP/1.1的代理服务器不应该与HTTP/1.0客户端建立持久连接, 除非他们了解客户端的处理能力。 实际上, 这一点是很难做到的, 很多厂商都违背了这一原则。 尽管服务器不应该试图在传输报文的过程中关闭连接, 而且在关闭连接之前至少应该响应一条请求, 但不管Connection首部取了什么值, HTTP/1.1设备都可以在任意时刻关闭连接。 HTTP/1.1应用程序必须能够从异步的关闭中恢复出来, 只要不存在可能会累积起来的副作用, 客户端都应该重试这条请求。(??) 除非重复发送请求会产生其他副作用, 否则 “如果在客户端收到完整响应之前连接就关闭了, 那么客户端必须要重新发送请求” 一个用户客户端对任何服务器或代理, 最多只能维护两条持久连接, 以防服务器过载。 代理可能需要更多到服务器的连接来支持并发用户的通信, 所以如果有N个用户试图访问服务器的话, 代理最多要维持2N条到任意服务器或父代理的连接。 管道化连接 HTTP/1.1允许在持久连接上可选地会用请求管道。这是在keep-alive连接上的进一步性能优化。在相应到达之前, 可以将多条请求放入队列。 当第一条请求通过网络流向地球另一端的服务器时, 第二条和第三条也可以开始发送了。 在高时延网络条件下, 这样做可以降低网络的回环时间, 提高性能。 如下图: 对管道化连接的限制 如果HTTP客户端无法确认连接是持久的, 就不应该使用管道。 必须按照与请求相同的顺序回送HTTP响应。HTTP报文中没有序列号标签, 因此如果收到的响应失序了, 就没办法将其与请求匹配起来了。 HTTP客户端必须做好连接会在任意时刻关闭的准备, 还要准备好重发所有未完成的管道化请求。 如果客户端打开了一条持久连接, 并立即发出了10条请求, 服务器可能在只处理了5条请求后关闭了连接, 剩下的5条请求会失败, 客户端必须能够应对这些过早关闭连接的情况, 重新发出这些请求。 HTTP客户端不应该用管道化的方式发送回产生副作用的请求(比如POST)。 总之, 出错的时候, 管道化方式会阻塞客户端了解服务器执行的是一系列管道化请求中的哪一些。由于无法安全地重试POST这样的非幂等请求, 所以出错时, 就存在某些方法永远不会被执行的风险。 关闭连接“任意”解除连接所有HTTP客户端, 服务器或代理都可以在任意时刻关闭一条TCP传输连接, 通常会在一条报文结束时关闭连接, 但出错的时候, 也可能在首部行中间, 或其他奇怪的地方关闭连接。对管道化持久连接来说, 这种情形是很常见的。HTTP应用程序可以在经过任意一段时间之后，关闭持久连接。比如，在持久连接空闲一段时间之后，服务器可能会决定将其关闭。但是，服务器永远都无法确定在它关闭”空闲”连接的那一刻，在线路的那一头的客户端有没有数据要发送。如果出现这种情况，客户端就会在写入半截请求报文时发现出现了连接错误。 Conetent-Length 及 截尾操作每条HTTP响应都应该有精确的Content-Length首部，用来描述响应主体的尺寸。如果老的HTTP服务器省略了Content-Length或者包含错误的长度指示，这样就要一来服务器发出连接关闭来说明数据的真是末尾。 连接关闭容限,重试及幂等性即使在非错误情况下,连接也可以在任意时刻关闭。HTTP应用程序要做好正确处理非预期关闭的准备。如果在客户端执行事务的过程中, 传输连接关闭了, 那么, 除非事务处理会带来一些副作用, 否则客户端就应该重新打开连接, 并重试一次。对管道化连接来说, 这种情况更加严重一些。客户端可以将大量请求放入队列中排队, 但源端服务器可以关闭连接, 这样就会留下大量未处理的请求, 需要重新调度。 副作用是很重要的问题, 如果在发送出一些请求数据之后, 收到返回结果之前, 连接关闭了, 客户端就无法百分之百地确定服务器端实际激活了多少事务。有些事务, 比如GET一个静态的HTML页面, 可以反复执行多次, 也不会有什么变化。而其他一些事务, 比如向一个在线书店POST一张订单, 就不能重复执行, 不然会有下多张订单的危险。 如果一个事务， 不管是执行一次还是很多次，得到的结果都相同, 这个事务就是幂等的。实现者们可以认为GET、HEAD、PUT、DELETE、TRACE和OPTIONS方法都共享这一特性。客户端不应该以管道化方式传送非幂等请求(比如POST)。否则，传输连接的过早终止就会造成一些不确定的后果。要发送一条非幂等请求，就需要等待来自前一条清求的响应状态。 尽管用户Agent代理可能会让操作员来选择是否对请求进行重试，但一定不能自动重试非幂等方法或序列。比如，大多数浏览器都会在重载一个缓存的POST响应时提供一个对话框，询问用户是否希望再次发起事务处理。 正常关闭连接 正常关闭连接TCP连接是双向的。TCP连接的每一端都有一个输入队列和一个输出队列, 用于数据的读或写。放入一端输出队列中的数据最终会出现在另一端的输入队列中。 完全关闭与半关闭应用程序可以关闭TCP输入和输出信道中的任意一个, 或者将两者都关闭了。套接字调用close()会将TCP连接的输入和输出信道都关闭了, 这被称作 “完全关闭”。还可以用套接字调用shutdown()单独关闭输入或输出信道。这被称为”半关闭”。 TCP关闭及重置错误…. 参考《HTTP权威指南》– 第四章《图解HTTP协议》https://developer.mozilla.org/en-US/docs/Web/HTTPhttps://tools.ietf.org/html/rfc2616","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"11. 内容协商与转码","slug":"http/2017-12-06-HTTP-11","date":"2017-12-06T10:50:27.000Z","updated":"2018-05-09T06:26:42.000Z","comments":true,"path":"2017/12/06/http/2017-12-06-HTTP-11/","link":"","permalink":"http://blog.renyimin.com/2017/12/06/http/2017-12-06-HTTP-11/","excerpt":"","text":"前言 一个URL常常需要代表若干不同的资源 例如那种需要以多种语言提供其内容的网站站点。 如果某个站点有 ‘说法语的’ 和 ‘说英语的’ 两种用户, 它可能想用这两种语言提供网站站点信息; 理想情况下，服务器应当向英语用户发送英文版，向法语用户发送法文版; 而用户只要访问网站主页就可以得到相应语言的内容。 HTTP提供了 内容协商 方法，允许客户端和服务器作这样的决定。 通过这些方法，单一的URL就可以代表不同的资源(比如，同一个网站页面的法语版和英语版)，这些不同的版本称为变体。 除了根据 内容协商 来决定URL代表的是那种版本的资源。另外, 对于有些特定的URL来说, 服务器还可以根据一些其他原则来决定发送什么内容给客户端最合适。在有些场合下, 服务器甚至可以自动生成定制的页面。比如，服务器可以为手持设备把HTML页面转换成WML页面，这类动态内容变换被称为转码。这些变换动作是HTTP客户端和服务器之间进行内容协商的结果。 内容协商技术 共有3种不同的方法可以决定服务器上哪个页面最适合客户端: 让客户端来选择, 服务器自动判定, 或 让中间代理来选。这3种技术分别称为客户端驱动的协商、服务器驱动的协商 以及 透明协商 内容协商技术摘要如下: 客户端驱动 (300)1.对于服务器来说，收到客户端请求时只是发回响应，在其中列出可用的页面，让客户端决定要看哪个，这是最容易的事情。 很显然，这是服务器最容易实现的方式，而且客户端很可能选择到最佳的版本(只要列表中有让客户端选择的足够信息)。 不利之处是每个页面都需要两次请求: 第一次获取列表，第二次获取选择的副本。这种技术速度很慢且过程枯燥乏味，让用户厌烦。 2.从实现原理上来说，服务器实际上有两种方法为客户端提供选项: 一是发送回一个HTML文档，里面有到该页面的各种版本的链接和每个版本的描述信息; 另一种方法是发送回HTTP/1.1响应时，使用 300 Multiple Choices 响应代码。客户端浏览器收到这种响应时，在前一种情况下(发回html文档的情况)，会显示一个带有链接的页面; 在后一种情况下，可能会弹出对话窗口，让用户做选择。不管怎么样，决定是由客户端的浏览器用户作出的 3.除了增加时延并且对每个页面都要进行繁琐的多次请求之外, 这种方法还有一个缺点: 它需要多个URL, 公共页面要一个, 其他每种特殊页面也都要一个。 服务器驱动1.之前已经知道了客户端驱动的协商存在的若干缺点。大部分缺点都涉及客户端和服务器之间通信量的增长, 这些通信量用来决定什么页面才是对请求的最佳响应。 2.而减少额外通信量的一种方法是让服务器来决定发送哪个页面回去，但为了做到这一点，客户端必须发送有关客户偏好的足够信息，以便服务器能够作出准确的决策。服务器通过 客户端请求的首部集 来获得这方面的信息(客户偏好)!! 有以下两种机制可供HTTP服务器评估发送什么响应给客户端比较合适： 检査 客户端请求中的内容协商首部集: 服务器察看客户端发送的 Accept内容协商首部集, 设法用相应的响应首部与之匹配; 根据其他(非内容协商)首部进行变通, 例如，服务器可以根据客户端发送的 User-Agent 首部来发送响应 客户端内容协商首部集1.客户端可以用下面列出的HTTP首部集发送用户的偏好信息 Accept : 告知服务器发送何种媒体类型Accept-Language : 告知服务器发送何种语言Accept-Charset : 告知服务器发送何种字符集Accept-Encoding : 告知服务器采用何种编码 2.实体首部集 和 内容协商首部集 注意: 内容协商首部集与实体首部非常类似(比如 Accept-Encoding 和 Content-Encoding)。不过, 这两种首部的用途截然不同: 实体首部集,像运输标签,它们描述了把报文从服务器传输给客户端的过程中必须的各种报文主体属性; 如下列出的实体首部集来匹配客户端的Accept内容协商首部集 12345Accet首部 实体首部Accept Content-TypeAccept-Language Content-LanguageAccept-Charset Content-TypeAccept-Encoding Content-Encoding （由于HTTP是无状态的协议，表示服务器不会在不同的请求之间追踪客户端的偏好，所以客户端必须在每个请求中都发送其偏好信息） 而内容协商首部集是由客户端发送给服务器用来告知其偏好信息的, 以便服务器可以从文档的不同版本中选择出最符合客户端偏好的那个来提供服务; 内容协商首部中的质量值1.HTTP协议中定义了质量值，允许客户端为每种偏好类别列出多种选项，并为每种偏好选项关联一个优先次序。 例如，客户端可以发送下列形式的Accept-Language首部：Accept-Language: en; q=0.5, fr; q=0.0 , nl; q=1.0, tr; q=0.0 其中q值的范围从0.0-1.0(0.0是优先级最低的，而1.0是优先级最高的)。 上面列出的那个首部，说明该客户端最愿意接收荷兰语(缩写为nl)文档，但英语(缩写为en)文档也行; 无论如何，这个客户端都不愿意收到法语(缩写为fr)或土耳 其语(缩写为tr)的版本; 2.注意: 偏好的排列顺序并不重要，只有与偏好相关的q值才是重要的; 客户端其它请求首部集1.服务器也可以根据客户端其他请求首部集来匹配响应, 比如 User-Agent 首部。例如, 服务器知道老版本的浏览器不支持JavaScript语言，这样就可以向其发送不含有JavaScript的页面版本。 2.由于缓存需要尽力提供所缓存文档中正确的”最佳”版本，HTTP协议定义了服务器在响应中发送的 Vary 首部。 这个首部告知缓存, 客户端, 和所有下游的代理, 服务器根据哪些首部来决定发送响应的最佳版本。 透明协商(vary首部)1.了支持透明内容协商，服务器必须有能力告知代理，服务器需要检査哪些请求首部，以便对客户端的请求进行最佳匹配。但是HTTP/1.1规范中没有定义任何透明协商机制, 不过却定义了 Vary 首部。服务器在响应中发送了Vary首部，以告知中间节点需要使用哪些请求首部进行内容协商 2.代理缓存可以为通过单个URL访问的文档保存不同的副本, 如果服务器把它们的决策过程传给代理,这些代理就能代表服务器与客户端进行协商。（缓存同时也是进行内容转码的好地方，因为部署在缓存里的通用转码器能对任意服务器，而不仅仅是一台服务器传来的内容进行转码） 3.对内容进行缓存的时候是假设内容以后还可以重用。然而，为了确保对客户端请求回送的是正确的已缓存响应, 缓存必须应用服务器在回送响应时所用到的大部分决策逻辑; 4.之前我们已经了解了客户端发送的Accept内容协商首部集; 也了解到, 为了给每条请求选择最佳的响应, 服务器使用了哪些与这些首部集匹配的相应实体首部集。其实, 代理缓存也必须使用相同的首部集来决定回送哪个已缓存的响应。 5.下图展示了涉及缓存的正确及错误的操作序列。 缓存把第一个请求转发给服务器，并存储其响应。 对于第二个请求，缓存根据URL査找到了匹配的文档。但是，这份文档是法语版的，而请求者想要的是西班牙语版的。如果缓存只是把文档的法语版本发给请求者的话，它就犯了错误; 像上面2中提到的, 代理缓存也必须要根据客户端发送来的内容协商首部来给客户端返回正确的响应 Vary首部1.下面是浏览器和服务器发送的一些典型的请求及响应首部: 2.然而, 如果服务器的决策不是依据Accept首部集，而是比如User-Agent首部的话，情况会如何？例如, 服务器可能知道老版本的浏览器不支持JavaScript语言, 因此可能会回送不包含JavaScript的页面版本。如果服务器是根据其他首部来决定发送哪个页面的话, 和Accept首部集一样, 缓存也必须知道这些首部是什么, 这样才能在选择回送的页面时做出同样的逻辑判断。 3.HTTP的 Vary 响应首部中列出了所有客户端请求首部, 服务器可用这些首部来选择文档或产生定制的内容(在常规的内容协商首部集之外的内容)。例如, 若所提供的文档取决于User-Agent首部, Vary首部就必须包含User-Agent; 小结 当新的请求到达时, 代理缓存会根据内容协商首部集来寻找最佳匹配。但在把文档提供给客户端之前, 它还必须检査服务器有没有在已缓存响应中发送Vary首部。 如果有Vary首部, 那么新请求中那些首部的值必须与旧的已缓存的响应的请求首部相同。(也就是说,代理缓存也会保存旧的请求的请求首部和响应首部, 下面一句话更加肯定这一点) 因为服务器可能会根据客户端请求的首部来改变响应, 为了实现透明协商, 代理缓存就必须为每个已缓存变体保存客户端请求首部和相应的服务器响应首部) 如果某服务器的Vary首部看起来像 Vary: User-Agent, Cookie 这样，大量不同的User-Agent和Cookie值将会产生非常多的变体, 而代理缓存必须为每个变体保存其相应的文档版本。当缓存执行査找时，首先会对内容协商首部集进行内容匹配，然后比较请求的变体与缓存的变体。如果无法匹配，缓存就从原始服务器获取文档 转码 我们已经讨论了一个机制, 该机制可以让客户端和服务器从某个URL的一系列文档中挑选出最适合客户端的文档。但是, 实现这些机制的前提是，存在一些满足客户端需求的文档—不管是完全满足还是在一定程度上满足; 然而, 如果服务器没有能满足客户端需求的文档会怎么样呢？服务器可以给出一个错误响应。但理论上，服务器可以把现存的文档转换成某种客户端可用的文档, 这种选项称为转码; 下面列出了一些假设的转码 1234567转换之前 转换之后HTML文档 WML文档高分辨率图像 低分辨率图像彩色图像 黑白图像有多个框架的复杂页面 没有很多框架或图像的简单文本页面有Java小应用程序的HTML页面 没有Java小应用程序的HTML页面有广告的页面 去除广告的页面 有3种类别的转码: 格式转换、信息综合以及内容注入 格式转换 格式转换是指将数据从一种格式转换成另一种格式, 使之可以被客户端査看。通过HTML到WML的转换, 无线设备就可以访问通常供桌面客户端査看的文档了。通过慢速连接访问Web页面的客户端并不需要接收高分辨率图像, 如果通过格式转换降低图像分辨率和颜色来减小图像文件大小的话, 这类客户端就能更容易地査看图像比较丰富的页面了。 格式转换可以由如下内容协商首部集来驱动, 但也能由 User-Agent 首部来驱动。注意: 内容转换或转码 与 内容编码 或 传输编码 是不同的, 后两者一般用于更高效或安全地传输内容, 而前两者则可使访问设备能够査看内容; Accet首部 实体首部 Accept Content-Type Accept-Language Content-Language Accept-Charset Content-Type Accept-Encoding Content-Encoding 信息综合 从文档中提取关键的信息片段称为信息综合(information synthesis), 这是一种有用的转码操作。这种操作的例子包括根据小节标题生成文档的大纲，或者从页面中删除广告和商标 根据内容中的关键字对页面分类是更精细的技术, 有助于总结文档的精髓。这种技术常用于Web页面分类系统中，比如门户网站的Web页面目录 内容注入参见P423 转码与静态预生成的对比 转码的替代做法是在Web服务器上建立Web页面的不同副本, 例如一个是HTML, 一个是WML, 一个图像分辨率高，一个图像分辨率低；一个有多媒体内容，一个没有。 但是，这种方法不是很切合实际，原因很多： 某个页面中的任何小改动都会牵扯很多页面，需要很多空间来存储各页面的不同版本，而且使页面编目和Web服务器编程(以提供正确的版本)变得更加困难。 有些转码操作，比如广告插入(尤其是定向广告插入)，就不能静态实现, 因为插入什么广告和请求页面的用户有关 对单一的根页面进行即时转换，是比静态的预生成更容易的解决方案。 但这样会在提供内容时增加时延。不过有时候其中一些计算可以由第三方进行，这样就减少了Web服务器上的计算负荷——比如可以由代理或缓存中的外部Agent完成转换 下图显示了在代理缓存中进行的转码 参考","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"05. 缓存相关","slug":"http/2017-11-30-HTTP-05","date":"2017-11-30T13:27:36.000Z","updated":"2018-05-09T06:26:52.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-05/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-05/","excerpt":"","text":"Web缓存简介1.Web缓存是指可以自动保存常见文档副本的HTTP设备。当Web请求抵达缓存设备时, 如果缓存设备本地有”已缓存的”副本, 就可以从本地设备而不是原始服务器中提取这个文档。 2.使用缓存的优点 减少了冗余的数据传输, 节省了你的网络费用 缓解了网络本身的瓶颈问题, 不需要更多的带宽就能够更快地加载页面 降低了对原始服务器的要求, 服务器可以更快地响应, 避免过载的出现 降低了距离时延, 因为从较远的地方加载页面会更慢一些即使带宽不是问题, 距离也可能成为问题。每台网络路由器都会增加因特网流量的时延。即使客户端和服务器之间没有太多的路由器, 光速自身也会造成显著的时延。 缓存命中、缓存未命中 如果一些请求到达缓存设备时, 缓存设备可以用本地已有的副本为这些请求提供服务, 就被称为缓存命中; 如果一些请求到达缓存设备时, 缓存设备本地没有副本提供给这些请求, 而将请求转发给原始服务器, 这就被称为缓存未命中; 后面还有 再验证命中 和 再验证未命中 的概念; 文档过期时间 原始服务器的内容可能会发生变化，所以缓存还需要不时地对其进行检测, 看看他们保存的副本是否仍然是服务器上最新的副本; 原始服务器通过 老式的HTTP/1.0+的实体首部字段 Expires 或 新式的HTTP/1.1的通用首部字段 Cache-Control:max-age 可以向每个文档附加一个过期日期; Expires实体首部字段 和 Cache-Control:max-age这个通用首部字段, 所做的事情本质上是一样的(但由于 Cache-Control 首部能使用相对时间, 所以更倾向与使用比较新的 Cache-Control 首部); 而 Expires 绝对日期依赖于计算机时钟的正确设置 在缓存文档过期之前, 缓存设备可以随意使用这些副本, 而且无需与服务器做任何联系, 除非 客户端请求中包含 “阻止提供缓存” 的首部 Cache-Control:no-store; 或者客户端请求中包含”只有经过验证才能返回缓存副本”的首部 Cache-Control:no-cache; 但是一旦已缓存文档过期, 缓存设备就必须与服务器进行再验证核对, 除非你设置了 Cache-Control:only-if-cached,要求只使用缓存; 文档过期算法为: expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) 注意: 有些服务器会回送一个 Expires:0 响应头, 试图将文档置于永远过期的状态, 但这种语法是非法的, 可能给某个软件带来问题, 应该试着支持这种结构的输入, 但是不应该产生这种结构的输出; 而 Cache-Control 的 max-age 则可以设置 Cache-Control: max-age=0; 缓存再验证文档过期后, 缓存需要到原始服务器查看其缓存文档有没有发生变化, 这种 新鲜度检测 被称为 HTTP缓存再验证; 但是仅仅是已缓存文档过期了, 还不能说明该过期文档和原始服务器上的文档有实际的区别, 这只是意味着到时间进行再验证了; 因为客户端还可以设置一些缓存控制项来进行影响; 再验证依据 为了有效地进行再验证, HTTP定义了一些特殊的请求, 不用从服务器上获取整个对象, 就可以快速检测出内容是否是最新的, HTTP的条件方法可以高效地实现再验证; HTTP定义了5个条件请求首部, 对 缓存再验证 来说有用的2个首部是 If-Mofified-Since 和 If-None-Match, 所有的条件首部都以前缀If-开头。 再验证 请求首部字段 If-Modified-Since 如果自If-Modified-Since指定日期之后, 文档被修改了, If-Modified-Since 条件就为真, 通常GET就会成功执行, 携带新首部的新文档会被返回给缓存, 新首部除了其他信息之外, 还包含了一个新的过期日期; 如果自If-Modified-Since指定日期之后, 文档没被修改, If-Modified-Since 条件就为假, 会向客户端返回一个小的 304 Not Modified响应报文, 为了提高有效性, 不会返回文档主体; 否则, 如果文档发生了变化, 就返回带有新主体的200响应; 请求首部字段 If-Modified-Since 和 实体首部字段 Last-Modified 配合工作; 再验证 请求首部字段 If-None-Match 有些文档可能会被周期性地重写, 但实际包含的数据常常却是一样的。尽管内容没有发生变化, 但是修改日期会发生变化; 有些文档可能内容被修改了, 但是所做的修改并不重要, 不需要让世界范围内的缓存都重装数据(比如对拼写或注释的修改), 涉及到弱验证器; 有些服务器无法准确地判定其页面的最后修改日期; 有些服务器提供的文档会在亚秒间隙发生变化(比如,实时监视器), 对这些服务器来说, 以秒为粒度的修改日期可能就不够用了; 为了解决上述问题, HTTP有一个被称为 实体标签(ETag) 的 版本标识符, 这个实体标签是附加到文档上的任意标签, 标签可能可能包含了文档序列号或版本名, 或是文档内容的校验及其他指纹信息。当对文档进行修改时, 可以修改文档的实体标签来说明这个新的版本。这样, 如果实体标签被修改了, 缓存就可以用 If-None-Match 条件首部来GET文档的新副本了; 如果If-None-Match 与 If-Modified-Since 同时存在, 两个都要进行验证; 请求首部字段 If-None-Match 和 响应首部字段 ETag 配合工作; 再验证命中 缓存对副本进行再验证时, 会向原始服务器发送一个小的再验证请求。如果发现内容没有变化, 服务器会以一个小的 304 Not Modified 进行响应; 只要缓存知道副本仍然有效, 就会再次将副本标识为暂时新鲜的, 并将副本提供给客户端, 这被称为再验证命中(revalidate hit) 或 缓慢命中(slow hit); 这种方式确实还是需要与原始服务器进行核对, 所以会比单纯的缓存命中要慢, 但是它并没有从服务器中获取对象数据, 所以要比缓存未命中要快一些。 再验证未命中 如果缓存发现原始服务器对象与已缓存副本不同, 则服务器会向客户端发送一条普通的, 带有完整内容的 HTTP 200 OK 响应; 这种方式 不仅需要与原始服务器进行核对, 而且会从服务器中获取对象数据, 所以理论上貌似要比缓存未命中要慢一些; 如果再验证发现服务器对象已经被删除, 服务器就回送一个 404 Not Found 响应, 缓存也会将其本地副本删除; 小结成功的再验证 比 缓存未命中 要快失败的再验证 几乎和 缓存未命中 速度一样 弱验证器1.只要原始服务器内容发生变化, 则实体标签就会变化, 正常情况下, 强验证器就会对比失败, 导致服务器会在一个 200 OK 响应中返回新的内容以及新的Etag标签; 2.有时, 服务器希望对文档进行一些不重要的修改, 并且不需要使所有已缓存副本都失效 HTTP1.1支持的”弱验证器”, 就允许对一些内容做修改, 此时服务器会用前缀 W/ 来标识弱验证器。3.不管相关的实体值以何种方式发生了变化, 强实体标签都要发生变化, 而相关实体在语义上发生了比较重要的变化时, 弱实体标签页应该发生变化。 公有和私有缓存1.通用首部字段(general header fields)Cache-Control有两个缓存响应指令: public 和 private2.缓存可以是单个用户专用的, 也可以是数千名用户共享的 ; 专用缓存被称为私有缓存(private cache), 私有缓存是个人的缓存, 包含了单个用户最常用的页面 ; 共享缓存被称为公有缓存(public cache), 公有缓存包含了某个用户团体常用页面 ; 私有缓存私有缓存不需要很大的动力或存储空间, 这样就可以将其做的很小, 很便宜。Web浏览器中就有内建的私有缓存—大多数浏览器都会将常用文档缓存在你个人电脑的磁盘和内存中, 并且允许用户去配置缓存的大小和各种设置; 公有缓存1.公有缓存是特殊的共享代理服务器, 被称为缓存代理服务器(caching proxy server), 或者更常见地被称为代理缓存(proxy cache)。2.代理缓存会从自己本地缓存中给用户提供缓存资源, 或者代表用户与服务器进行联系。公有缓存会接受来自多个用户的访问, 所以通过它可以更好地减少冗余流量。3.如下图: 每个客户端都会重复地访问一个(还不在私有缓存中的)新的”热门”文档。每个私有缓存都要获取同一份文档, 这样它就会多次穿过网络。 而使用共享的公有缓存时, 对于这个流行的对象, 缓存只要取一次就行了, 它会用共享的副本为所有的请求服务, 以降低网络流量。 缓存控制1.之前在各通用首部字段详解已经对各选项进行了说明; 2.补充 (HTTP Cache-Control: max-age和max-stale=s的区别) immutable : 属于缓存控制的一个扩展属性, 参考 当一个支持immutable的客户端浏览器看到这个属性时, 它应该知道, 如果没有超过过期时间，那么服务器端该页面内容将不会改变, 这样浏览器就不应该再发送有条件的重新验证请求(比如通过If-None-Match 或 If-Modified-Since等条件再向服务器端发出更新检查);也就是说, 通常过去我们使用304回复客户端该页面内容没有变化，但是如果用户按浏览器的刷新或F5键，浏览器会再次向服务器端发出该页面内容请求，服务器端如果确认该页面没有变化，那么发回304给客户端，不再发送该页面的实体内容，虽然这样节省了来回流量，但是如果大型网站的很多用户为了得到及时信息，经常会刷新浏览器，这就造成了大量刷新请求，向服务器端求证该页面是否改变，这会影响网站的带宽，也增加服务器端验证压力。而新的选项immutable可以杜绝这种现象。immutable可以节省HTTP请求,缩短请求时间,这是因为服务器不必再处理304响应了。","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"04. 各通用首部字段详解","slug":"http/2017-11-30-HTTP-04","date":"2017-11-30T11:50:02.000Z","updated":"2018-05-09T06:26:56.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-04/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-04/","excerpt":"","text":"Cache-Control 缓存能力控制1.Cache-Control 缓存指令是单向的, 即请求中存在一个指令并不意味着响应中将存在同一个指令; 该指令是可选的; 指令可以有多个选项, 选项之间通过 , 分隔; 2.可以按请求和响应分为(P377) 缓存请求指令(选项如下) 缓存响应指令(选项如下) 3.注意 no-cache 和 must-revalidate 的区别 no-cache: 告诉浏览器、缓存服务器，不管本地副本是否过期，使用资源副本前，一定要到源服务器进行副本有效性校验; must-revalidate：告诉浏览器、缓存服务器，本地副本过期前，可以使用本地副本；本地副本一旦过期，必须去原始服务器进行有效性校验。(这应该是缓存系统的默认行为, 但must-revalidate指令使得这个要求是明确的参考; 可参考 更多参考后面的博文缓存相关中的介绍 Pragma 兼容Pragma是HTTP1.1之前版本的历史遗留字段, 仅作为与HTTP1.0做向后兼容;除了与只理解 Pragma:no-cache 的HTTP/1.0应用程序进行交互时使用; HTTP/1.1应用程序都应该使用Cache-Control:no-cache; Date 当前时间简单来说就是HTTP报文的创建日期, 它会参与到缓存的过期时间运算中; 公式: expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) 默认情况下, Date 的值为当前时间 而响应首部字段中的 Age 头字段, 是告诉客户端, 源服务器创建的缓存在代理服务器上已经保存了多久, 字段的单位为秒; 代理创建响应时必须加上首部字段Age; Connection 该首部字段具备两个作用: 控制不再转发给代理的首部字段 和 管理持久连接; 更多参考HTTP - 并行连接, 持久连接 未完待续… Via 使用首部字段Via, 是为了追踪客户端与服务器之间的请求和响应报文的传输路径;报文在经过代理或者网关时, 会先在首部字段Via中附加该服务器的信息, 然后再进行转发。 Via首部是为了追踪传输路径, 所以也经常会和TRACE方法一起使用, 比如代理服务器受到由TRACE方法发送过来的请求(其中Max-Forward:0)时, 代理服务器就不能再转发该请求了,这种情况下, 代理服务器会将自身的信息附加到Via首部后, 返回该请求的响应。 未完待续… Transfer-Encoding传输编码 该通用首部字段规定了传输报文主体时采用的编码方式; HTTP/1.1的传输编码方式仅对分块传输编码有效, 即只能设置为 Transfer-Encoding:chunked; 参考 MDN Doc; Content-Encoding 和 Transfer-Encoding 二者经常会结合来用, 其实就是针对 Transfer-Encoding 的分块再进行 Content-Encoding 压缩; 对比 请求首部字段Accept-encoding内容编码 (P369) 请求首部字段 Accept-encoding 是将客户端用户代理(浏览器)所支持的内容编码方式(通常是某种压缩算法) 及 内容编码方式的优先级顺序, 通知给服务器; 通过内容协商, 服务端会选择一个客户端支持的方式, 使用并在 响应报文的实体首部字段 Content-Encoding 中通知客户端, 服务器选择了哪种内容编码方式; 另外, 可以一次性指定多种内容编码! 也可以使用权重q值来表示相对优先级; 正常情况下, 主要采用4种编码方式: gzip, compress, deflate, identity; 即使客户端和服务器都支持某些相同的压缩算法，但如果Accept-encoding:identity, 表示客户端告诉服务器对响应主体不要进行压缩, 出现的两种情况的常见情形是： 要发送的数据已经经过压缩, 再次进行压缩不会导致被传输的数据量更小, 一些图像格式的文件会存在这种情况; 服务器超载, 无法承受压缩需求导致的计算开销, 通常, 如果服务器使用超过80%的计算能力, 微软建议不要压缩; 只要identity(表示不需要进行任何编码)没有被明确禁止使用(没有明确通过 identity;q=0 或是 *;q=0 指令明确设置 identity 的权重值为0), 一旦禁止就表示 服务端必须进行编码; 没有禁止identity, 则服务器禁止返回表示客户端错误的 406 Not Acceptable 响应; 禁止identity后, 服务器才可能因为实在没有合适的内容编码类型而, 返回表示客户端错误的406 Not Acceptable 参考MDN 对比 实体首部字段 Content-encoding内容编码实体首部字段 Content-encoding 会告诉客户端, 服务器对实体的主体部分选用的内容编码方式 对比 请求首部字段Accept-charset请求首部字段 Accept-charset 会告诉服务器, 用户代理(浏览器)所支持的字符集和字符集的相对优先顺序。(P388) 可以一次性指定多种字符集; 也可以使用权重q值来表示相对优先级; 对比 请求首部字段Accept-Language请求首部字段 Accept-charset 会告诉服务器, 用户代理(浏览器)所支持的语言和语言的相对优先顺序。 可以一次性指定多种字符集; 也可以使用权重q值来表示相对优先级; 小结 请求首部字段Accept-encoding 和 实体首部字段Content-encoding来决定内容压缩方式; 通用首部字段Transfer-Encoding用来决定响应实体是否分块; 请求首部字段Accept-charset, Accept-Language 是客户端告诉服务端自己能支持的字符编码和语言及其中的优先顺序; TrailerUpgradeWarning~~未完待续","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"03. HTTP状态码详解","slug":"http/2017-11-30-HTTP-03","date":"2017-11-30T06:30:12.000Z","updated":"2018-05-09T12:01:44.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03/","excerpt":"","text":"1xx 101: 参考博文WebSocket简单示例分析 (做协议升级, 还会响应: Connection: Upgrade)~~未完待续 2xx Web API的设计与开发 P109 200 OK : 200码非常出名, 似乎没有对它进一步说明的必要; 201 Created : 当在服务器端创建数据成功时, 会返回201状态码; 用于请求 创建服务器对象, 比如 PUT操作 成功创建资源后, 就应该返回 201码; 202 Accepted : 在异步处理客户端请求时, 它用来表示服务器端已经接受了来自客户端的请求, 但处理尚未结束; 在文件格式转换, 处理远程通知(Apple Push Notification等)这类很耗时的场景中, 如果等到所有处理都结束后才向客户端返回响应消息, 就会花费相当长的时间, 造成应用可用性不高; 这时采用的方法是服务器向客户端返回一次响应消息, 然后立刻开始异步处理。 202状态码就被用于告知客户端服务器端已经开始处理请求, 但整个处理过程尚未结束; 比如: 以LinkedIn的参与讨论的API为例如果成功参与讨论并发表意见, 服务器端通常会返回201状态码;但如果需要得到群主的确认, 那么所发表的意见就无法立即在页面显示出来, 这时服务器端就需要返回202状态码; 从广义上来看, 该场景也属于异步处理, 但和程序设计里的异步执行当然不同; 204 No Content : 正如其字面意思, 当响应消息为空时会返回该状态码。 其实就是告诉浏览器, 服务端执行成功了, 但是没什么数据返回给你, 所以你不用刷新页面, 也不用导向新的页面; 在用DELETE方法删除数据时, 服务器端通常会返回204状态码(阮一峰博文也提到过, 对DELETE适用); 除此之外, 也有人认为在使用 PATCH 方法更新数据时, 因为只是更新已有数据, 所以返回204状态码更加自然; 关于204状态码的讨论可以参考 p111; 205 Reset Content : 告诉浏览器, 页面表单需要被重置; 205的意思是服务端在接收了浏览器POST请求以后, 处理成功以后, 告诉浏览器, 执行成功了, 请清空用户填写的Form表单, 方便用户再次填写; 206 Partial Content : 成功执行了一个部分或Range(范围)的请求; 206响应中, 必须包含 Content-Range, Date 以及 ETag或Content-Location首部; 3xx300 Multiple Choices : 客户端驱动方式进行内容协商时, 服务器可能返回多个连接供客户端进行选择 (比如多语言网站可能会出现); 301 Moved Permanently : 在请求的URL已经被移除时使用, 响应的Location首部中应该包含资源现在所处的URL; (比较适合永久重定向) 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是301; 则即便稍后取消了location.php中的跳转(或者修改了跳转地址), 由于浏览器还是会认为你之前的跳转是永久性的, 再次访问www.test.com/location.php仍然会跳转到之前的跳转链接(除非清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 302 Found: 与301类似, 但是客户端应该使用Location首部给出的URL来进行临时定位资源, 将来的请求仍应该使用老的URL; 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是302; 如果稍后取消了location.php中的跳转, 再次访问www.test.com/location.php, 会发现不会进行跳转, 而是访问到 location.php 修改后的代码 (不用清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 303 See Other : HTTP/1.1使用303来实现和302一样的临时重定向; 307 Temporary Redirect HTTP/1.1规范要求用307来取代302进行临时重定向; (302临时重定向留给HTTP/1.0) 所以他也具备302临时重定向的特点; 但是, 与 302, 303 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 308 Permanent Redirect 貌似不是rfc2616的标准 具备和301永久重定向的特点, 需要清除浏览器缓存才行; 但是, 与 301 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 304 Not Modified : 参考博文缓存相关 4xx Web API的设计与开发 P1134字头状态码主要用于描述因客户端请求的问题而引发的错误。也就是说, 服务器端不存在问题, 但服务器端无法理解客户端发送的请求, 或虽然服务器端能够理解但请求却没有被执行, 当遇到这些情况引发的错误时, 服务器端便会向客户端返回这一类别的状态码。因此, 当服务器端返回4字头的状态码时, 就表示客户端的访问方式发生了问题, 用户需要检查一下客户端的访问方式或访问的目标资源等。 400 Bad Request : 表示其他错误的意思, 即其他4字头状态码都无法描述的错误类型; 401 Unauthorized : 表示认证(Authentication)类型的错误 比如当需要先进行登录操作, 而却没有告诉服务器端所需的会话信息(比如token..), 服务器端就会返回401状态码, 告知客户端出错的大致原因; 403 Forbidden : 和401状态码比较相似, 所以也经常被混淆; 其实403表示的是授权(Authotization)类型的错误, 授权和认证的不同之处是: 认证表示”识别前来访问的是谁”, 而授权则表示”赋予特定用户执行特定操作的权限” 通俗地说: 401状态码表示”我不知道你是谁”, 403状态码表示”虽然知道你是谁, 但你没有执行该操作的权限” 404 Not Found : 表示访问的数据不存在, 但是 例如当客户端湿度获取不存在的用户信息时, 或者试图访问原本就不存在的端点时, 服务器就会返回404状态码; 所以, 如果客户端想要获取用户信息, 却得到服务器端返回的404状态码, 客户端仅凭”404 Not Found”将难以区分究竟是用户不存在, 还是端点URI错误导致访问了原本不存在的URI; 405 Method Not Allowed : 表示虽然访问的端点存在, 但客户端使用的HTTP方法不被服务器端允许; 比如客户端使用了POST方法来访问只支持GET方法的信息检索专用的API; 又比如客户端用了GET方法来访问更新数据专用的API等; 406 Not Acceptable : 服务器端API不支持客户端指定的数据格式时, 服务器端所返回的状态码; 比如, 服务器端只支持JSON和XML输出的API被客户端指定返回YAML的数据格式时, 服务器端就会返回406状态码; 408 Request Timeout : 当客户端发送请求至服务器端所需的时间过长时, 就会触发服务器端的超时处理, 从而使服务器端返回该状态码; 409 Conflict: 用于表示资源发生冲突时的错误 (est中就会有该错误码) 比如通过指定ID等唯一键值信息来调用注册功能的API时, 倘若已有相同ID的数据存在, 就会导致服务器端返回409状态码; 在使用邮箱地址及Facebook ID等信息进行新用户注册时, 如果该邮箱地址或者ID已经被其他用户注册, 就会引起冲突, 这时服务器端就会返回409状态码告知客户端该邮箱地址或ID已被使用; 410 Gone : 和 404状态码 相同, 都表示访问资源不存在, 只是410状态码不单表示资源不存在, 还进一步告知资源曾经存在, 只是目前已经消失了; 因此服务器端常在访问被删除的数据时返回该状态码, 但是为了返回该状态码, 服务器必须保存该数据已被删除的信息, 而且客户端也应该知晓服务器端保存了这样的信息; 但是在通过邮箱地址搜索用户信息的API中, 从保护个人信息的角度来说, 返回410状态码的做法也会受到质疑; (所以在此种资源不存在的情况下, 为了稍微安全一些, 返回410状态码需要慎重) 413 Request Entity Too Large : 413也是比较容易出现的一种状态码, 表示请求实体过大而引发的错误 请求消息体过长是指, 比如在上传文件这样的API中, 如果发送的数据超过了所允许的最大值, 就会引发这样的错误; 414 Request-URI Too Large : 414是表示请求首部过长而引发的错误 如果在进行GET请求时, 查询参数被指定了过长的数据, 就会导致服务器端返回414状态码 415 Unsupported Media Type : 和406比较相似 406我们知道是表示服务器端不支持客户端想要接收的数据格式 而415表示的是服务器端不支持客户端请求首部 Content-Type 里指定的数据格式, 也就是说, 当客户端通过POST,PUT,PATCH等方法发送的请求消息体的数据格式不被服务器支持时, 服务器端就会返回415状态码; 例如在只接收JSON格式的API里, 如果客户端请求时发送的是XML格式的数据去请求服务器端, 或者在 Content-Type 首部指定 application/xml, 都会导致该类型错误; 429 Too Many Requests : 是2012年RFC6585文档中新定义的状态码, 表示访问次数超过了所允许的范围; 例如某API存在一小时内只允许访问100次的访问限制, 这种情况下入股哦客户端视图进行第101次访问, 服务器便会返回该状态码; 表示在一定的时间内用户发送了太多的请求, 即超出了”频次限制”, 在响应中，可以提供一个 Retry-After 首部来提示用户需要等待多长时间之后再发送新的请求; 5xx 5字头状态码表示错误不发生在客户端, 而是由服务器自身问题引发的。 500 Internal Server Error : 是web应用程序开发里非常常见的错误, 当服务器代码里存在bug, 输出错误信息并停止运行等情况下, 就会返回该类型的错误; 因此, 不仅限于API, 对于5字头状态码的错误, 都要认真监视错误日志, 使系统在出错时及时告知管理员, 以便在错误发生时做好应对措施, 防止再次发生。 501 Not Implemented : ??? 502 Bad GateWay : ??? 503 Service Unavaliable : 用来表示服务器当前处于暂不可用状态 可以回送:响应首部 Retry-After 表示多久恢复; 不同的客户端与服务器端应用对于 Retry-After 首部的支持依然不太一致; 不过，一些爬虫程序，比如谷歌的爬虫程序Googlebot, 会遵循Retry-After响应首部的规则, 将其与503(Service Unavailable,当前服务不存在)响应一起发送有助于互联网引擎做出判断,在宕机结束之后继续对网站构建索引。 参考:https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After 504 Gateway Time-out: 复现这个错误码比较简单, 让你的php程序模拟耗时请求, 如下代码 123&lt;?phpsleep(70);//模拟耗时，睡70秒echo &quot;睡醒了&quot;; 就会返回 ``` 504 Gateway Time-out nginx/1.11.4 ``` 505 HTTP Version Not Supported: 服务器收到的请求, 使用的是它无法支持的HTTP协议版本; 参考:《HTTP权威指南》、《Web API的设计与开发》","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. HTTP各请求方法详解","slug":"http/2017-11-30-HTTP-02","date":"2017-11-30T03:29:12.000Z","updated":"2018-05-09T06:27:03.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-02/","excerpt":"","text":"前言 HTTP/1.1 中实现的method, 参考RFC2616, 可以看到有: OPTIONS, GET, HEAD, POST, PUT, DELETE, TRACE, CONNECT RFC2616中提到: PATCH，LINK，UNLINK方法被定义，但并不常见;(&lt;图解http协议&gt;中也提到 LINK, UNLINK已经被http1.1废弃); 不同应用各自的实现不同, 有些应用会完整实现, 有些还会扩展, 有些可能会实现一部分; PUT(对比POST) PUT: 对已有资源进行更新操作, 所以是 update 操作; put和post的区别 在HTTP中, PUT被定义为 idempotent(幂等性) 的方法，POST则不是，这是一个很重要的区别 应该用PUT还是POST ？ 取决于这个REST服务的行为是否是idempotent(幂等)的 假如发送两个请求, 希望服务器端是产生两个新数据，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用POST方法; 但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的, 那就应该使用PUT方法; 虽然POST和PUT差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦 POST POST: 上面已经提过了, POST是非幂等的; POST和PUT都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的; PATCH(对比PUT)PATCH不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题 对已有资源的操作 用于资源的部分内容进行更新 (例如更新某一个字段, 具体比如说只更新用户信息的电话号码字段); 而PUT则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变; 当资源不存在时: PATCH 可能会去创建一个新的资源, 这个意义上像是 saveOrUpdate 操作。 参考: https://segmentfault.com/q/1010000005685904/ https://unmi.cc/restful-http-patch-method/ http://restcookbook.com/HTTP%20Methods/patch/ https://tools.ietf.org/html/rfc5789 HEADHEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回，而仅仅返回HTTP头信息;比如: 欲判断某个资源是否存在, 我们通常使用GET, 但这里用HEAD则意义更加明确。 GET比较简单, 直接获取资源; OPTIONS这个方法使用比较少, 它用于获取当前URL所支持的方法;若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST;之前跨域相关博文 CORS方案 not-so-simple request 中的”预检”请求用的请求方法就是 OPTIONS; CONNECT要求用隧道协议连接代理, 如使用SSL TRACE~~未完待续 DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"01. HTTP协议预览","slug":"http/2017-11-30-HTTP-01","date":"2017-11-30T03:25:12.000Z","updated":"2018-05-09T06:27:09.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-01/","excerpt":"","text":"简介 HTTP(Hypertext Transfer Protocol 超文本传输协议), 是在万维网上进行通信时所使用的协议; 协议版本 HTTP/0.9 有很多设计缺, 很快就被HTTP/1.0所取代了;(…只支持GET方法…) HTTP/1.0 第一个得到广泛应用的HTTP版本; HTTP/1.O+ 在HTTP/1.0上扩展了很多非官方的特性, 是非正式的HTTP扩展版本;(支持持久的keep-alivel连接) HTTP/1.1 是当前使用的版本 HTTP-NG(HTTP/2.0) 关注的是性能提升, 目前还未普及 用于HTTP协议交互的信息被称为HTTP报文 请求端(客户端)的报文叫 请求报文; 响应端(服务器端)的叫 响应报文; 请求报文结构报文首部 请求行 : 包含了 请求方法, 请求URL, 客户端请求报文使用的HTTP协议版本 (如: GET / HTTP/1.1) 首部块 请求首部字段 通用首部字段 实体首部字段 其他 空行(CRLF)报文主体响应报文结构报文首部 状态行 : 包含了 服务端响应报文使用的HTTP协议版本, 状态码, 原因短语 (如: HTTP/1.1 200 OK) 首部块 响应首部字段 通用首部字段 实体首部字段 其他 空行(CRLF)报文主体请求方法上面已经了解到, 在 “请求报文” -&gt; “报文首部” -&gt; “请求行” 中, 包含了 请求方法, 具体可参考 各请求方法详解 首部字段从上面还可以看到, 报文中的首部块有如下几种首部字段 请求首部字段响应首部字段通用首部字段Cache-Control 缓存能力控制Pragma 兼容Cache-controlDate 当前时间Transfer-Encoding 传输编码Accept-encoding 内容编码 (请求首部字段) (可能会导致服务器返回 406 Not Acceptable)Content-encoding 内容编码 (实体首部字段)Accept-charset 请求首部字段可参考博文：各通用首部字段详解 实体首部字段状态码详解可参考博文：HTTP状态码详解 ~~未完待续","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"70. 查询性能优化","slug":"mysql/2017-09-27-mysql-70","date":"2017-09-27T12:50:37.000Z","updated":"2018-03-08T06:03:46.000Z","comments":true,"path":"2017/09/27/mysql/2017-09-27-mysql-70/","link":"","permalink":"http://blog.renyimin.com/2017/09/27/mysql/2017-09-27-mysql-70/","excerpt":"","text":"前言之前已经了解了索引优化的相关内容, 它对于高性能是必不可少的, 但还不够, 还需要合理设计查询; 如果查询写的很糟糕, 即使库表结构再合理, 索引再合适, 也无法实现高性能; 查询优化, 库表结构优化, 索引优化需要齐头并进, 一个不落; 慢查询基础优化数据访问 确认应用程序是否在检索大量超过需要的数据, 你可能访问了太多的行, 也可能是太多的列;比如: 总是返回全部的列; 只展示5条数据,你却查出100条; 确认MySQL服务器层是否在分析大量超过需要的数据行; (注意: 索引是在存储引擎层, 一旦服务器层分析的数据过多, 可能你的索引不太合适, 没有在存储引擎层过滤掉数据) 未完待续~~","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"查询性能优化","slug":"查询性能优化","permalink":"http://blog.renyimin.com/tags/查询性能优化/"}]},{"title":"50. EXPLAIN 分析","slug":"mysql/2017-09-25-mysql-50","date":"2017-09-25T13:23:08.000Z","updated":"2018-03-16T06:30:27.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-50/","excerpt":"","text":"准备环境1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB, DEFAULT CHARSET = utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`))ENGINE = InnoDB,DEFAULT CHARSET = utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) select_type select_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 (最常见的查询类别就是 SIMPLE 了) PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 如果使用了UNION查询, 那么EXPLAIN 输出结果类似如下: 123456789101112mysql&gt; EXPLAIN ( SELECT * FROM user_info WHERE id IN ( 1, 2, 3 ) ) UNION( SELECT * FROM user_info WHERE id IN ( 3, 4, 5 ) );+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+3 rows in set (0.00 sec) mysql&gt; type type 字段比较重要, 它提供了判断查询是否高效的重要依据依据; 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等; type 常用的取值有: system: 表中只有一条数据, 这个类型是特殊的 const 类型; ?? const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据(const 查询速度非常快, 因为它仅仅读取一次即可) eq_ref: 此类型通常出现在多表的join查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果, 并且查询的比较操作通常是 =, 查询效率较高, 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using where; Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | NULL |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询, 例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5;+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+2 rows in set (0.00 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录; 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL (没用到索引), 并且 key_len 字段是此次查询中使用到的索引的最长的那个 1234567mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+1 row in set (0.00 sec) 下面对比, 都使用了范围查询, 但是一个可以使用索引范围查询, 另一个不能使用索引 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 另外, 可参考 P185: in语句虽然有时候 type结果也是range (不过, 对于真正的范围查询, 确实是无法使用范围列后面的其他索引了, 但是对于”多个等值条件查询”则没有这个限制) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过ALL类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据, 即 做的是覆盖索引, 当是这种情况时, Extra 字段会显示 Using index 下面的例子中, 查询的 name 字段恰好是一个索引(做到了覆盖索引), 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据;因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index; 1234567mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 下面不但使用了全索引扫描, 而且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;nihao&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) 但是, 如果不使用索引的话, 下面type就是ALL, 表示使用了全表扫描, 并且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age=10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 下面 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一, 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. type小结type 类型的性能比较 : 通常来说, 不同的 type 类型的性能关系如: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的; 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快; 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了; possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引;注意: 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到;(MySQL 在查询时具体使用了哪些索引, 由 key 字段决定) key此字段是 MySQL 在当前查询时所真正使用到的索引 rowsrows 也是一个重要的字段, MySQL 查询优化器根据统计信息, 估算SQL要查找到结果集需要到表中扫描读取的数据行数(上面的例子可以看到, 基本上使用到了索引的话, 真正扫描的行数都很少); 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好 ExtraExplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 比如下面, 使用索引扫描做排序 和 不使用索引扫描做排序 的效果: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY name;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY age;+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using filesort |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+1 row in set (0.00 sec) Using index 与 Using index condition “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 比如下面, 第一个做到了覆盖索引扫描, 后面两个都没做到 mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name,age FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT * FROM user_info where name=&apos;haha&apos;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"49. 索引和锁","slug":"mysql/2017-09-25-mysql-49","date":"2017-09-25T13:10:40.000Z","updated":"2018-03-08T02:57:03.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-49/","excerpt":"","text":"索引可以让查询锁定更少的行 因为InnoDB只有在访问行的时候才会对其加锁, 而索引能够减少InnoDB访问的行数, 从而减少锁的数量;但这只有当InnoDB在存储引擎层就能过滤掉所有不需要的行时才行, 如果索引(处在存储引擎层)无法过滤掉无效的行, 那么在InnoDB检索到数据并发送给服务器层以后, 服务器层才能应用where子句, 这时已经无法避免锁定行了;虽然InnoDB的行锁效率很高, 内存使用也很少, 但是锁定行的时候仍然会带来额外开销;锁定超过需要的行会增加锁争用并减少并发性;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"48. 冗余和重复索引","slug":"mysql/2017-09-25-mysql-48","date":"2017-09-25T10:27:40.000Z","updated":"2018-03-08T02:39:49.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-48/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-48/","excerpt":"","text":"重复索引 是指在相同的列上按照相同的顺序创建的相同类型的索引; 应该避免这样的重复索引, 发现后也应该立即删除; MySQL允许在相同的列上创建多个索引, 但是MySQL需要单独维护重复的索引, 并且优化器在优化查询的时候也需要逐个地进行考虑, 这会影响性能; 冗余索引 和 重复索引 不同, 如果创建了索引(A,B), 在创建索引(A)就是冗余索引, 因为这只是(A,B)索引的前缀索引; 大多数情况下都不需要冗余索引; 因此索引(A,B)也可以当做索引(A)来使用 但是如果再创建索引(B,A), (B) 则都不是冗余索引 有时候为了让两个查询都变快, 也会需要冗余索引 (P179) 应该尽量扩展已有的索引而不是创建新的索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"47. 使用索引扫描来做排序","slug":"mysql/2017-09-25-mysql-47","date":"2017-09-25T10:25:11.000Z","updated":"2018-03-08T05:38:13.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-47/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-47/","excerpt":"","text":"简介 只有当索引的列顺序 和 ORDER BY 子句的顺序完全一致, 并且所有列的排序方向都一样时(要么都是正序, 要么都是倒序), MySQL才能够使用索引来对结果做排序; 如果查询需要关联多张表, 则只有当 ORDER BY 子句引用的字段全部为第一个表时, 才能使用索引做排序; ORDER BY 子句 和 查找型查询的限制是一样的, 需要满足索引的最左前缀的要求, 否则, MySQL都需要亲自去执行排序操作, 而无法利用索引排序; 有一种情况下, ORDER BY 子句可以不用满足最左前缀的要求, 那就是前导列为常量的时候; 比如一张表的索引是 key(a,b,c) , 而 查询语句是 ... where a=100 order by b,c, 即使 order by 不满足最左前缀的要求, 也可以使用索引做排序; P177 列出了很多不可以使用索引做排序的查询; 当查询同时有 ORDER BY 和 LIMIT 子句的时候 像select &lt;col...&gt; from profiles where sex=&#39;m&#39; order by rating limit 10;这种查询语句, 同时使用了order by和limit, 如果没有索引就会很慢; 即使有索引, 如果用户界面有翻页, 翻页比较靠后时, 也会非常慢, 因为随着偏移量的增加, MySQL需要花费大量的时间来扫描需要丢弃的数据; 但是sex的选择性又很低, 如何优化呢? 对于选择性非常低的列, 如果要做排序的话, 可以增加一些特殊的索引来做排序, 例如, 可以创建 (sex, rating)索引 然后采用 延迟关联 , 通过覆盖索引先查询返回需要的主键, 在根据这些主键关联原表获得需要的行;select &lt;col...&gt; from profiles INNER JOIN (select &lt;primart key&gt; from profiles where x.sex=&#39;m&#39; order by rating limit 100000, 10) as x using(primary key)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"46. 覆盖索引","slug":"mysql/2017-09-25-mysql-46","date":"2017-09-25T09:30:26.000Z","updated":"2018-03-08T02:29:53.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-46/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-46/","excerpt":"","text":"介绍 通常, 大家都会根据查询的WHERE条件来创建合适的索引, 不过这只是索引优化的一个方面, 设计优秀的索引应该考虑到整个查询, 而不单单是where条件部分。 覆盖索引 索引确实是一种查找数据的高效方式, 但是MySQL也可以使用索引就能直接获取列的数据, 这样就不需要再去读取数据行; 如果叶子节点中已经包含了要查询的数据, 那么就没有必要再回表查询了; 即, 如果一个索引包含(或者说覆盖)所有需要查询的字段值, 我们就称之为 覆盖索引; 覆盖索引是非常有用的工具, 能够极大地提高性能; 拿InnoDB来说, 覆盖索引就非常有用, 如果你的查询能够做到覆盖你的二级索引列, 那么只需要遍历一次B-Tree(可以直接在二级索引中找到数据), 可以避免对聚簇索引的二次查询; 其他更多参考 P171 注意: 覆盖索引必须要保存索引列的值, 而 哈希索引, 空间索引 和 全文索引 等都不存储索引列的值; 所以MySQL只能使用B-Tree索引做覆盖索引; 如果索引不能覆盖查询所需的全部列, 那就不得不每扫描一次索引记录, 就回表查询一次对应的行, 优化小案例 select * .... : 因为查询从表中选择了所有的列, 而一般你不会创建覆盖了所有列的二级索引, 所以这种局域肯定不会用到覆盖索引; .... where title LIKE &#39;%ren&#39;: Mysql只能在where条件中做索引的 最左前缀匹配的LIKE比较, 而这里的where条件是以通配符开头的LIKE查询; 查看 (P171) 的优化案例 (做表的自关联, 子句使用覆盖索引, 外部不用) 这样虽然无法使用索引覆盖整个查询, 但总算比完全无法利用覆盖索引要好","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"45. 聚簇索引","slug":"mysql/2017-09-24-mysql-45","date":"2017-09-24T12:10:31.000Z","updated":"2018-03-07T13:32:55.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-45/","excerpt":"","text":"简介 聚簇索引并不是一种单独的索引类型, 而是一种 数据存储方式; 因为是存储引擎负责实现索引, 所以不是所有的存储引擎都支持聚簇索引, 这里主要讨论的是 InnoDB引擎的聚簇索引; InnoDB的 聚簇索引 实际上在同一个结构中保存了 B-Tree索引 和 数据行 当表有聚簇索引时, 它的数据行实际上存放在索引的叶子页中(叶子页包含了数据行的 全部列数据) 因为无法同时把数据行存放在两个不同的地方, 所以一个表只能有一个聚簇索引 不过覆盖索引, 可以模拟多个聚簇索引的情况 InnoDB默认会创建聚簇索引: InnoDB通过主键来作为聚簇索引, 如果没有定义主键, 则会选择一个唯一的非空索引代替, 如果连非空索引都没有, InnoDB会隐式定义一个主键来作为聚簇索引; 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上，若使用where id = 14这样的条件查找数据; 则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据; InnoDB只聚集 在同一个磁盘页面中的记录, 因此, 如果数据在物理上是相邻的, 那么在索引上就也是相邻的; 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的; 但是包含相邻键值的页面可能物理上会相距甚远; 聚簇索引的优点 访问速度更快: 聚簇索引将索引和数据保存在同一个B-Tree中, 因此从聚簇索引中获取数据通常比在非聚簇索引中查询要快; 使用覆盖索引的查询, 可以直接使用页节点中的主键值; 聚簇索引缺点 聚簇索引最大限度地提高了I/O密集型应用的性能, 但如果数据全部都放在内存中, 则访问顺序就没那么重要了, 聚簇索引也就没什么优势了; 插入速度严重依赖于插入顺序, 按照主键的顺序插入是速度最快的方式, 但如果不是按照主键顺序, 在完成操作后最好执行 OPTIMIZE TABLE 命令重新组织一下表; 更新聚簇索引的代价很高, 因为会强制InnoDB将每个被更新的行移动到新的位置; 基于聚簇索引的表在插入新行, 或者主键被更新导致需要移动行的时候, 可能面临 “页分裂” 问题; 当前主键值要求必须将这一行插入到某个已满的页中时, 存储引擎会将该页分裂成两个页面来容纳该行, 这就是一次页分裂操作。 页分裂操作会导致表占用更多的磁盘空间 聚簇索引会导致全表扫描变慢, 尤其是行比较稀疏, 或者由于页分裂导致数据存储不连续的时候; 二级索引(非聚簇索引)可能比想象的要更大, 因为在二级索引的叶子节点包含了引用行的主键列; 二级索引访问需要两次索引查找, 而不是一次 二级索引叶子几点保存的 “行指针” 是行的主键; 这意味着通过二级索引查找行, 存储引擎需要找到二级索引叶子节点获得对应的主键值; 然后根据这个主键值去聚簇索引中查找对应的行数据; 这里做了重复工作, 两次 B-Tree 查找, 而不是一次。 InnoDB 和 MyISAM 索引对比 InnoDB支持聚簇索引, 而MyISAM不支持; MyISAM中主键索引和其他索引在索引结构上没有区别; 而InnoDB中 (主键)聚簇索引 和 二级索引(普通索引) 是有区别的;(P167) 上图总结","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"44. 高性能索引策略 -- 多列索引","slug":"mysql/2017-09-24-mysql-44","date":"2017-09-24T09:25:31.000Z","updated":"2018-03-07T12:23:09.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-44/","excerpt":"","text":"前言 在多个列上建立独立的单列索引, 在大部分情况下并不能提高MySQL的查询性能; 例如: film_actor 表在 film_id 和 actor_id 上各有一个单列索引, 但是对于下面这个查询WHERE条件, 这两个单列索引都不是好的选择 select film_id, actor_id from actor where actor_id=1 OR film_id=1; 对于上面的查询 老版本的MySQL会使用全表扫描; 而新版本会使用 索引合并策略(参考P158) 来进行优化, 但这更说明了表上的索引建的很糟糕 接下来除了 之前已经在博文: B-Tree索引中介绍过的多列索引的 左前缀策略; 你还需要关注的是创建索引时, 索引列的顺序 选择合适的索引列顺序 在一个多列索引中, 索引列的顺序首先决定了最左前缀策略在查询时是如何进行的; 其次还意味着索引首先按照最左列进行排序, 其次是第二列, 等等; 所以多列索引的列顺序至关重要; 在不需要考虑排序和分组的时候, 将选择性最高的列放在前面通常是很好的, 这时候索引的作用只是用于优化where条件的查找。 然而, 性能不知是依赖于所有索引列的选择性(整体基数), 也和查询条件的具体值有关, 也就是和值的分布有关; ~~未完待续","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"43. 高性能索引策略 - 独立的列, 前缀索引","slug":"mysql/2017-09-24-mysql-43","date":"2017-09-24T09:20:31.000Z","updated":"2018-03-08T05:22:03.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-43/","excerpt":"","text":"前言前面已经对常见的索引类型及其对应的优缺点有了一定的了解, 接下来要考虑的就是如何 高效正确地选择并使用索引 独立的列独立的列是指: 在查询条件中, 索引列 不能是表达式的一部分, 也不能是函数的参数; (如: select actor_id from actor where actor_id+1=5; 就无法使用索引, 应该始终将索引列单独放在比较符号的一侧, select actor_id from actor where actor_id=4;); 前缀索引 因为B-Tree索引中存储了实际的列值, 所以如果你需要索引的列的内容很长, 就会导致 索引变得大且慢; 对于BLOB, TEXT 或者很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 正常情况下, 不论是创建 单列索引 还是 多列索引, 创建的索引都是以 某个列完整的值 来创建索引, 而 前缀索引则是以 列开始的部分字符 来创建索引, 从而大大节约索引空间, 提高索引效率, 但是这样会降低索引的选择性; 索引选择性 索引选择性: 是指不重复的索引值(基数) 和 数据表的记录总数的比值(当然, 唯一索引的所有索引值都不同, 选择性是1, 这是最好的索引选择性, 性能也是最好的); 假设一张订单表, 按照 city(城市全名) 来分 和 按照 city(第一个字)来分组 , 那肯定前一种情况分出来的组比较多, 也就是不重复的索引值多; 如果按照 city 字段的前3个字符来分组的话, 效果如下 如果按照 city 字段的前7个字符来分组的话, 可以想到, 自然可能会是 分组会更多, 每组的数据会更少 分的组越多, 也就是如果以此长度的前缀创建索引的话, 不重复的索引值也就越多, 那么选择性就越高; 选择性越高, 则查询效率越高, 因为MySQL在查找时能够通过索引就过滤掉更多的行, 否则一个索引还是对应了很多的数据行, 那效率还是很低; 而我们要做的其实就是让我们的 前缀选择性 接近 完整列的选择性 简单点说, 让 city(n) 接近 city 的选择性; 计算完整列的选择性 下面给出了同一个查询中计算不同前缀长度的选择性 前缀索引是一种能使索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 有无法使用前缀索引做覆盖扫描 创建前缀索引: alter table city add key(city(7)), 表示以 city 字段的前7个字符 来创建索引; 参考p189: 根据传统经验, 不应该在选择性低的裂伤创建索引, 但是如果很过查询都用到该列, 比如一个表中的 gender 列, 考虑到使用的频率, 还是建议在创建不同组合的索引时, 将 (sex) 作为前缀!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"42. 哈希索引","slug":"mysql/2017-09-24-mysql-42","date":"2017-09-24T07:01:31.000Z","updated":"2018-03-07T10:04:36.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-42/","excerpt":"","text":"简介 哈希索引(hash index)基于哈希表实现, 只有精确匹配索引中所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code): 哈希索引将所有的哈希码存储在索引中 同时在哈希表中保存指向每个数据行的指针 在MySQL中, 只有Memory引擎显示支持哈希索引, 这也是Memory引擎表的默认索引类型, Memory引擎也支持B-Tree索引。 Memory引擎支持 非唯一哈希索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码, 如果出现这种情况, 索引会以链表的方式存放多个行指针到同一个哈希条目中 查找时, 会1. 先在索引中按照哈希码来找到指向数据行的指针, 2. 然后比较数据行的值是否是你查找的行 哈希索引的限制因为索引自身只要存储对应的哈希值和行指针, 所以索引的结构十分紧凑, 这也让哈希索引的查找速度非常快, 然而哈希索引也有它的限制: 哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快; 哈希索引数据并不是按照索引值顺序排序的, 所以无法用于排序; 不支持 部分索引列匹配查找 , 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 不支持范围查询 (只支持如 =, &lt;&gt;, in 等一些 等值比较) 哈希索引数据非常快, 除非有很多哈希冲突 (因为memory引擎支持非唯一索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码)当出现哈希冲突时, 存储引擎必须遍历 冲突的哈希值 所对应的 链表 中所有的行指针, 逐行到表中进行比较, 直到找到所有符合条件的行; 哈希冲突如果哈希冲突很多的话, 一些索引维护操作的代价也为很高, 如果表中删除一行数据, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的指针, 冲突越多代价越大;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"41. B-Tree索引","slug":"mysql/2017-09-24-mysql-41","date":"2017-09-24T06:50:19.000Z","updated":"2018-03-07T11:41:40.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-41/","excerpt":"","text":"简介 当人们谈论索引时, 如果没有特别指明索引类型, 多半说的就是 B-Tree 索引, 它使用 B-Tree 数据结构来存储数据; MySQL的大多数存储引擎都支持这种索引 (Archive引擎不支持) 存储引擎以不同的方式使用 B-Tree 索引, 性能也各有优劣: MyISAM使用 前缀压缩技术 使得索引更小 InnoDB则按照原数据格式进行存储 MyISAM的索引是通过 数据的物理位置 引用被索引的行叶子页中的值指向被索引的行的物理地址 InnoDB的索引则是通过 主键 引用被索引的行叶子页中的值指向被索引的行的主键 建立在B-Tree结构(从技术上来说是B+Tree)上的索引 注意: B-Tree索引中存储了 被索引的列实际的列值, 指向数据行的指针 (上面的图可能没体现出来, 结合下面多列索引的图找找感觉~~) B-Tree 通常意味着: 所有的值都是 按顺序 存储的;B-Tree对索引列是顺序组织存储的, 所以很适合查找范围数据; 每一个叶子页到根的距离都是相同的； 叶子页比较特别, 他们的指针指向的是被索引的数据, 而不是其他的节点页(不同引擎的”指针”类型不同) 下图显示了多列索引是如何组织数据存储的 对于下表中的每一行数据, 索引中包含了 last_name, first_name, dob(date of birth, 即出生日期) 列的值 1234567CREATE TABLE People last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(&apos;m&apos;, &apos;f&apos;) not null, key(last_name, first_name, dob)); 注意: 索引对多个列的值进行排序的依据是定义索引时列的顺序 (如上图中, 最后两个条目, 两个人的姓和名都一样, 则根据他们的出生日期来进行排列顺序) 最左前缀效应之前提到过 “索引可以包含一个或多个列的值。如果包含多个列, 那么列的顺序也十分重要, 因为MySQL只能高效地使用索引的最左前缀列“ 拿之前的 People 表来做参考(其创建的索引是 key(last_name, first_name, dob)) 全值匹配 : 指的是如果查询条件和某个索引中的所有列值进行匹配, 这样就可以利用到上面创建的索引; 比如, 查找 ‘姓为Allen,名为Cuba,出生日期为1960-01-01’ 的人时 并且指的注意的是, 如果你的查询条件做到了全值匹配, 那么即使你查询条件的顺序不是依照左前缀原则, MySQL也会做优化; 匹配最左前缀: 比如, 查找 ‘姓为Allen’ 的人, 即使用索引的第一列, 这样就可以利用到上面创建的索引; 匹配最左前缀列的前缀: 可以用来匹配索引中第一列的值的开头部分, 比如, 查找 姓以’J’开头 的人, 这样就可以利用到上面创建的索引; 匹配最左前缀范围值: 查找 ‘姓在Allen和Barrymore’ 之间的人, 这样就可以利用到上面创建的索引 精确匹配第一列 并 范围匹配第二列: 查找 ‘姓为Allen并且名字以K开头’ 的人, 这样就可以利用到上面创建的索引 只用访问索引的查询 : 这种查询只需要访问索引, 而无需访问数据行; 后面会单独讨论这种 覆盖索引 的优化。 B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了 B-tree索引的限制 如果查询时, 查询条件不是按照索引的最左列开始写, 则无法使用索引; 上面例子中, 索引就无法用于 ‘查找名字为Bill的人’, 也无法查找 ‘生日为1960-01-01的人’, 因为这两列都不是最左数据列。 也无法用于查找 ‘姓以某个字母结尾的人’ (你建索引时指定的列顺序, 列的值内容 都要符合最左前缀才能利用到索引) 查询条件不能跳过索引中的列 比如, 查询 ‘姓为Smith 并且 生日为1960-01-01’ 就无法使用到索引 如果查询条件中有某个列是范围查询, 则其右边的所有列都无法使用索引来优化查找 比如, where last_name=&#39;Smith&#39; and first_name LIKE &#39;J%&#39; AND dob=&#39;1976-12-23&#39; 这个查询只能使用索引的前两列 如果范围查询的列的值结果有限, 比如数据表中只有2个人是 ‘名字以J开头’ 的, 那你也别用范围查找了, 直接用多个等于条件来代替就行比如: where last_name=&#39;Smith&#39; and (first_name=&#39;Jack&#39; or first_name=&#39;Jieke&#39; ) AND dob=&#39;1976-12-23&#39; 到这里可以看到: 索引列的顺序是非常重要的! 在优化性能的时候, 可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"40. 索引基础","slug":"mysql/2017-09-24-mysql-40","date":"2017-09-24T06:30:25.000Z","updated":"2018-03-07T11:37:16.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-40/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-40/","excerpt":"","text":"索引基础 索引是存储引擎用于快速找到记录的一种数据结构; 索引优化是对查询性能优化最有效的手段了; 索引可以包含 一个 或 多个列 的值; 如果如果索引包含多个列, 索引中列的顺序也非常重要, 因为MySQL只能高效地使用索引的最左前缀列; 创建一个包含两个列的索引 和 创建两个只包含一个列的索引 是大不相同的; 在MySQL中, 索引是在 存储引擎层实现, 而不是在服务器层实现: 索引有很多种类型, 可以为不同场景提供更好的性能; 但并不是每个存储引擎都支持所有的索引类型; 不同存储引擎的索引, 其工作方式和底层实现也可能是不一样的; MySQL支持的索引类型B-Tree索引哈希索引数据空间索引全文索引其他索引类别索引优点索引可以让服务器快速地定位到表的指定位置; 但这并不是索引的唯一作用, 到现在可以看到, 根据创建索引的数据结构不同, 索引也有一些其他的附加组作用 最常见的B-Tree索引, 按照顺序存储数据, 所以MySQL可以用来做 ORDER BY 和 GROUP BY 操作; 因为数据是有序的, 所以B-Tree也就会将相关的列值都存储在一起 因为B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"08.","slug":"composer/2017-09-22-composer-08","date":"2017-09-22T13:20:17.000Z","updated":"2018-02-03T07:34:43.000Z","comments":true,"path":"2017/09/22/composer/2017-09-22-composer-08/","link":"","permalink":"http://blog.renyimin.com/2017/09/22/composer/2017-09-22-composer-08/","excerpt":"","text":"","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"07. autoload之psr-4, psr-0, classmap, files","slug":"composer/2017-09-22-composer-07","date":"2017-09-22T05:10:30.000Z","updated":"2018-02-28T08:31:04.000Z","comments":true,"path":"2017/09/22/composer/2017-09-22-composer-07/","link":"","permalink":"http://blog.renyimin.com/2017/09/22/composer/2017-09-22-composer-07/","excerpt":"","text":"自动加载 vendor/ 库 对于composer安装到vendor/目录下的库 如何在项目中自动加载到? composer安装完库之后, 会生成一个 vendor/autoload.php 文件, 你可以在项目中简单的引入这个文件, 这样就可以自动加载composer管理的vendor/下的这些库; require ‘vendor/autoload.php’; 当然, 你自己发布的packagist包一定要注意其中也需要设置自动加载规则(命名空间和目录对应关系) composer 自动加载类型psr-4 如果你的项目中还没有准备好自动加载功能(来实现对你项目各个目录中类的自动加载),现在你已经不需要自己去准备了, 因为一旦你引入了composer, 它就已经为你准备好了这一功能!!(当然, 引入composer很简单, 无论你是自己 composer init初始化composer.json文件还是通过composer require安装一个包来生成 composer.json文件都可以) 它不仅能像上面说的那样帮你实现自动加载composer帮你管理的包, 也可以帮你在项目中创建自己的自动加载!! 你可以在 composer.json 的 autoload 字段中增加自己的 autoloader 12345&#123; &quot;autoload&quot;: &#123; &quot;psr-4&quot;: &#123;&quot;Acme\\\\&quot;: &quot;src/&quot;&#125; &#125;&#125; 像上面那样, 你就定义一个从 命名空间 到 目录 的映射关系, 此时 src 应该和vendor目录同级, 都在你项目的根目录下; 最后, 你在src目录下写的类就应该是Acme命名空间; 如果指定的 Acme 是个顶级命名空间, 那src下不管目录多深, 都可以从这个顶级开始找到, 不用再一一配置子命名空间与目录的对应关系了; 如果 Acme 不是个顶级命名空间, 那么和src同级的目录也得配置其命名空间和对应关系 像上面那样配置好之后, composer 将注册一个 PSR-4 autoloader 到 Acme 命名空间 添加完 autoload 字段后，你应该再次运行 install 命令来生成 vendor/autoload.php 文件 注意: 此时虽然修改了 composer.json 文件, 但是由于并没有涉及到包信息(比如版本信息)的修改, 所以install和update都一样; 引用这个文件也将返回 autoloader 的实例，你可以将包含调用的返回值存储在变量中，并添加更多的命名空间。这对于在一个测试套件中自动加载类文件是非常有用的。 可参考: http://docs.phpcomposer.com/01-basic-usage.html#Autoloading psr-0不推荐…. 这里就不扯了 classmap classmap 需要写在autoload内; classmap 所配置的目录下的所有 .php 和 .inc 文件里的类, 都会在 install/update 过程中存储到 vendor/composer/autoload_classmap.php 文件中的map数组中; vendor/composer/autoload_classmap.php文件中的映射关系, key是扫描到的类的namespace\\类名 或者 类名(没有命名空间的), value是类文件的路径 格式 12345&#123; &quot;autoload&quot;: &#123; &quot;classmap&quot;: [&quot;src/&quot;, &quot;lib/&quot;, &quot;test/Something.php&quot;] // 可以看到不仅可以指定目录, 也可以指定文件 &#125;&#125; files 也需要写在autoload内; Files方式就是手动指定供直接加载的文件, 比如说我们有一系列全局的helper functions，可以放到一个helper文件里然后直接进行加载;&quot;autoload&quot;: { &quot;psr-4&quot;: { //一些自己写的类库 &quot;Test\\\\&quot;: &quot;src/&quot;, &quot;Test1\\\\&quot;: &quot;src1/&quot;, &quot;Test2\\\\&quot;: &quot;src2/&quot; }, &quot;classmap&quot;: [&quot;src/&quot;, &quot;src1/hehe.php&quot;], // 一些类 &quot;files&quot;: [&quot;common/functions.php&quot;] // 一些公共函数 }","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"06. 发布自己的包","slug":"composer/2017-09-20-composer-06","date":"2017-09-20T10:40:17.000Z","updated":"2018-03-01T02:20:43.000Z","comments":true,"path":"2017/09/20/composer/2017-09-20-composer-06/","link":"","permalink":"http://blog.renyimin.com/2017/09/20/composer/2017-09-20-composer-06/","excerpt":"","text":"前言 GitHub官方提供了和Packagist相关的钩子服务; Packagist主要提供Composer包发布和索引, 默认Composer从Packagist获取资源。(可以使用你的GitHub帐号登录Packagist) 也可以理解为你真正的项目代码是在Gihub仓库中, 而相关介绍及索引信息是在Packagist平台 发布步骤github仓库部分 github创建仓库 将本地目录与github中仓库关联 在本地项目中初始化 composer.json 文件 composer init(使用composer自带的初始化命令，创建一个composer.json描述文件)。 如果想手动编辑，可以去composer官网阅读相关文档获得帮助。 在本地开发一个功能包, 并上传到github仓库中 开发功能包的注意事项: 需要在composer.json文件中配置好包内部的自动加载规则, 比如: 12345\"autoload\": &#123; \"psr-4\": &#123; \"Lant\\\\\": \"./\" &#125; &#125; 否则, 即使你下载下来你的包, 并且你引入了 vendor/autoload.php 文件, 也无法正常自动加载到你的包代码 如果你的包composer.json文件中指定了 顶级命名空间名 与 目录 的关系, 子命名空间和目录就不用设置了 只用在子目录的类文件中声明 namespace 顶级命名空间/本命名空间 即可! 如果你包中的是几个同级目录, 那你可能就需要为每个目录设置 命名空间 和 目录的对应关系! Packagist部分 访问Packagist主页，确认自己已经登录，然后点击右上角大大的Submit Package，然后填入我们创建的仓库的地址，点击Check，然后没问题，再点击Submit。 为了让Packagist平台可以自动更新github仓库中的信息, 需要我们配置Github仓库的钩子服务 进入仓库, 点击 “setting” 点击左侧 “Integrations &amp; services” 然后 “Add service” - 选择 “Packagist” 然后填写表单, username为你的packagist账户名(如果使用github账户登录, 则为github账户名), Token是packagist中的Token; 注意有时候你的包修改完之后, 发现github仓库中有了, packagist中的同步时间也有了, 那你得看一下中国全量镜像站点的同步时间是否正确 (说好的一分钟同步一次, 但..不尽然, 被坑过); 参考http://note.youdao.com/noteshare?id=8265aa9789dba4451ada428a95048f33","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"05. require 和 create-project","slug":"composer/2017-09-20-composer-05","date":"2017-09-20T10:21:46.000Z","updated":"2018-02-03T07:27:07.000Z","comments":true,"path":"2017/09/20/composer/2017-09-20-composer-05/","link":"","permalink":"http://blog.renyimin.com/2017/09/20/composer/2017-09-20-composer-05/","excerpt":"","text":"require是在当前项目目录下进行包安装, 一般安装到vendor/下; create-project 是指把包当成一个项目来安装, 也就是如果你创建的这个包是一个完整的项目, 你就可以来直接把这个包当成项目来创建 composer create-project lant/packagist_test 项目在本地的目录名 项目版本(如:dev-master)","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"04.更新包的方式(require, install, update)对比","slug":"composer/2017-09-18-composer-04","date":"2017-09-18T14:23:17.000Z","updated":"2018-03-01T09:52:19.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-04/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-04/","excerpt":"","text":"require虽然是安装, 但也可以用来更新包到指定版本, 并同时更新composer.json和composer.lock文件, 如果没有则会创建! composer require 包名 新版本 当你手动修改了composer.json文件中包的版本之后, 可以执行 composer update 来重新安装该包, 并更新composer.lock文件, 如果没有则会创建! 当然, 你可以指定你需要更新的包 composer update 包名, 包名... 另外, 还应该注意一下 install 和 update 的一个小细节:当你修改了composer.json文件, 如果不是对包做增删改(比如增加一个包, 删除一个包, 或者修改包的版本信息), 而是增删改其他信息(比如配置自动加载之类的信息), 那么你使用 install 和 update 是没有区别的! 小结 install, require, update都可以做包安装 (如果在composer.json中做了像自动加载的信息的配置, install和update会重新进行类关系映射) require 和 update 都可以做包更新 (如果在composer.json文件中做了包版本的增删改查, install无法进行更新) 另外: install: 主要是在部署阶段使用，以便在生产环境和开发环境使用的都是composer.lock文件中相同的依赖项，保证线上部署环境与本地开发环境的一致性。 update: 主要是在开发阶段使用，根据我们在composer.json文件中指定的内容升级项目的依赖包。 此更新非彼更新比如packagist中的包现在是v2.0, 你本地的是v1.0, 你直接执行 composer update, 不要指望composer会自动帮你更新到v2.0, 要更新你得在composer.json中指明版本号, 然后composer update才会根据composer.json文件去更新!这样, 至于你的更新是升级还是降级就看你在composer.json文件中如何指定的了!","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"03.安装包的方式(require, install, update)对比","slug":"composer/2017-09-18-composer-03","date":"2017-09-18T13:20:31.000Z","updated":"2018-03-01T09:50:48.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-03/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-03/","excerpt":"","text":"初次做包安装 如果你的项目刚引入composer (composer init), 现在是第一次进行包的安装, 也就是只有composer.json文件, 并没有composer.lock文件, 可以, 直接执行 composer require 包名 包版本 该命令会帮你安装所需的包到 vendor/ 目录下 生成 composer.json 文件, 并将依赖信息写入文件中 生成 composer.lock 文件 composer将会通过composer.json来读取需要的包和相对的版本, 然后创建composer.lock文件 所以，对于除了包版本之外的其他配置, 如自动加载…等, composer.lock 文件不会包含! (所以当改变这些信息之后, install 和 update的效果一样) 非初次安装 如果你的项目已经安装过一些包了, 即已经有 composer.json, composer.lock 文件; 此时, 你有两种方式: 和初次安装一样, 使用 composer require 包名 包版本 进行安装 手动在composer.json文件中进行配置, 然后运行 composer update注意, 此时不能使用 composer install, 因为该命令是依据 composer.lock 文件来进行安装的, 而composer.lock文件又是依据composer.json文件中的包及包版本信息生成的,而此时你改变了 composer.json 文件, 并且是新增了一个包(及版本对应关系), 所以你需要更新 composer.lock 文件! 如果你执行的是 composer install, 则会警告: Warning: The lock file is not up to date with the latest changes in composer.json. You may be getting outdated dependencies. Run update to update them. 当然, 你也可以删除 composer.lock 文件, 然后手动在composer.json文件中进行配置, 最后执行 composer install install, require, update都可以做包安装","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"02. 设置中国全量镜像","slug":"composer/2017-09-18-composer-02","date":"2017-09-18T09:40:11.000Z","updated":"2018-03-01T09:54:16.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-02/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-02/","excerpt":"","text":"前言 一般情况下,安装包的数据(主要是 zip 文件) 一般是从 github.com 上下载的, 安装包的元数据是从 packagist.org 上下载的。然而，由于众所周知的原因，国外的网站连接速度很慢，并且随时可能被“墙”甚至“不存在”。“Packagist 中国全量镜像”所做的就是缓存所有安装包和元数据到国内的机房并通过国内的 CDN 进行加速，这样就不必再去向国外的网站发起请求, 从而达到加速 composer install以及 composer update 的过程, 并且更加快速、稳定。因此, 即使 packagist.org、github.com 发生故障(主要是连接速度太慢和被墙), 你仍然可以下载、更新安装包。 原理图下面是一张从网上找的图 配置方法1.修改 composer 的全局配置文件打开命令行窗口（windows用户）或控制台（Linux、Mac 用户）并执行命令: composer config -g repo.packagist composer https://packagist.phpcomposer.com 2.修改当前项目的 composer.json 配置文件 打开命令行窗口（windows用户）或控制台（Linux、Mac 用户）, 进入你的项目的根目录（也就是 composer.json 文件所在目录），执行命令: composer config repo.packagist composer https://packagist.phpcomposer.com 上述操作将会在 当前项目的 composer.json 文件的末尾自动添加镜像的配置信息（你也可以自己手工添加）： 123456\"repositories\": &#123; \"packagist\": &#123; \"type\": \"composer\", \"url\": \"https://packagist.phpcomposer.com\" &#125; &#125; 参考: https://pkg.phpcomposer.com/","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"01.安装","slug":"composer/2017-09-18-composer-01","date":"2017-09-18T09:10:11.000Z","updated":"2018-02-28T08:16:31.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-01/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-01/","excerpt":"","text":"composer.phar文件下载先下载composer.phar文件, 3种方式: curl -sS https://getcomposer.org/installer | php php -r &quot;readfile(&#39;https://getcomposer.org/installer&#39;);&quot; | php 手动下载Composer, 地址 Linux/Unix/OS 全局安装 : mv composer.phar /usr/local/bin/composer 局部安装 : mv composer.phar /局部目录/composer windows 全局安装 : 配置composer.phar文件路径(D:\\WWW\\composer)到环境变量中 ; 接下来需要在composer.phar同级目录下新建文件composer.bat : echo @php &quot;%~dp0composer.phar&quot; %*&gt;composer.bat 这就安装好了 局部安装 : 直接把下载的composer.phar文件放到项目根目录下; 运行命令 : php composer.phar install 然后就安装成功了(每个局部目录都需要这么安装, 比较麻烦) 推荐全局安装即可. composer自我更新 self-update 将 Composer 自身升级到最新版本, 只需要运行 composer self-update 命令。它将替换你的 composer.phar 文件到最新版本。 如果你想要升级到一个特定的版本，可以 composer self-update 1.0.0-alpha7 自我更新还可以跟参数 --rollback (-r): 回滚到你已经安装的最后一个版本。 --clean-backups: 在更新过程中删除旧的备份，这使得更新过后的当前版本是唯一可用的备份。","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"27. REPEATABLE READ 可重复读","slug":"mysql/2017-09-17-mysql-27","date":"2017-09-17T14:10:52.000Z","updated":"2018-03-07T09:19:48.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-27/","excerpt":"","text":"前言 该隔离级别可以解决不可重复读问题, 脏读问题; 也就是它既可以让事务只能读其他事务已提交的的记录, 又能在同一事务中保证多次读取的数据即使被其他事务修改,读到的数据也是一致的。 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免 脏读, 不可重复读 这两个问题的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MVCC 多版本并发控制","slug":"mysql/2017-09-17-mysql-26","date":"2017-09-17T10:10:21.000Z","updated":"2018-03-07T09:19:44.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-26/","excerpt":"","text":"简介Multiversion Concurrency Control 阿里数据库内核’2017/12’月报中对MVCC的解释是: 多版本控制: 指的是一种 提高并发 的技术。最早的数据库系统, 只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。 &lt;高性能MySQL&gt;中对MVCC的部分介绍 MySQL的大多数事务型存储引擎实现的其实都不是简单的行级锁。基于提升并发性能的考虑, 它们一般都同时实现了多版本并发控制(MVCC)。不仅是MySQL, 包括Oracle,PostgreSQL等其他数据库系统也都实现了MVCC, 但各自的实现机制不尽相同, 因为MVCC没有一个统一的实现标准。可以认为MVCC是行级锁的一个变种, 但是它在很多情况下避免了加锁操作, 因此开销更低。虽然实现机制有所不同, 但大都实现了非阻塞的读操作，写操作也只锁定必要的行。MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。 相关概念 read view 主要是用来做可见性判断的, 比较普遍的解释便是”本事务不可见的当前其他活跃事务”, 但正是该解释, 可能会造成一节理解上的误区, 所以此处提供两个参考, 用来避开理解误区:read view中的高水位low_limit_id可以参考 对于read view快照的生成时机, 也非常关键, 也正是因为生成时机的不同, 造成了RC, RR两种隔离级别的不同可见性 可以参考; 在innodb(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来; 在innodb(默认repeatable committed级别), 事务中每条select语句都会创建一个快照(read view); 参考1234With REPEATABLE READ isolation level, the snapshot is based on the time when the first read operation is performed.使用REPEATABLE READ隔离级别，快照是基于执行第一个读操作的时间。With READ COMMITTED isolation level, the snapshot is reset to the time of each consistent read operation.使用READ COMMITTED隔离级别，快照被重置为每个一致的读取操作的时间。 undo-log 可以参考数据库内核月报2015/04/01 前言Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中, 但从5.6开始，也可以使用独立的Undo表空间。Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作。大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除, 而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo; 在回滚段中的undo logs分为: insert undo log 和 update undo loginsert undo log : 事务对insert新记录时产生的undolog, 只在本事务回滚时需要, 并且在事务提交后就可以立即丢弃;update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除。 InnoDB存储引擎在数据库每行数据的后面添加了三个字段 6字节的事务ID(DB_TRX_ID)字段: 用来标识最近一次对本行记录做修改(insert|update)的事务的标识符, 即最后一次修改(insert|update)本行记录的事务id。至于delete操作，在innodb看来也不过是一次update操作，更新行中的一个特殊位将行表示为deleted, 并非真正删除。 7字节的回滚指针(DB_ROLL_PTR)字段: 指写入回滚段(rollback segment)的 undo log record (撤销日志记录)。如果一行记录被更新, 则 undo log record 包含 ‘重建该行记录被更新之前内容’ 所必须的信息。 6字节的DB_ROW_ID字段: 包含一个随着新行插入而单调递增的行ID, 当由innodb自动产生聚簇索引时, 聚簇索引会包括这个行ID的值, 否则这个行ID不会出现在任何索引中。结合聚簇索引的相关知识点, 大概可以理解为: 如果我们的表中没有主键或合适的唯一索引, 也就是无法生成聚簇索引的时候, InnoDB会帮我们自动生成聚集索引, 但聚簇索引会使用DB_ROW_ID的值来作为主键; 如果我们有自己的主键或者合适的唯一索引, 那么聚簇索引中也就不会包含 DB_ROW_ID 了 , 如果有误, 希望指正, 谢谢。 可见性比较算法 假设要读取的数据行, 最后完成事务提交的事务id(即, 让当前数据行最后落地的事务id)为 trx_id_current ; 当前新开事务id为 new_id 当前新开事务创建的快照 read view 中最早的事务id为 up_limit_id, 最迟的事务id为 low_limit_id(注意 low_limit_id=未开启的事务id=当前最大事务id+1) 比较: 1.trx_id_current &lt; up_limit_id, 这种情况比较好理解, 表示, 新事务在读取该行记录时, 该行记录的最后稳定事务ID小于系统当前所有活跃的事务ID, 所以当前行稳定数据对新事务可见, 跳到步骤5; 2.trx_id_current &gt;= trx_id_last, 这种情况也比较好理解, 表示, 该行记录的 最后稳定事务ID 是在 本次新事务 创建之后才开启的,但是却在本次新事务执行第二个select前就commit了, 所以该行记录的当前值对本次新事务不可见(RR级别), 跳到步骤4; 3.trx_id_current &lt;= trx_id_current &lt;= trx_id_last, 表示: 该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见, 调到步骤4,否则表示可见。 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 trx_id_current，然后跳到步骤1重新开始判断; 5.将该可见行的值返回。 案例分析1.下面是一个非常简版的演示事务对某行记录的更新过程, 当然, InnoDB引擎在内部要做的工作非常多: 2.下面是一套比较算法的应用过程也可参考https://github.com/zhangyachen/zhangyachen.github.io/issues/68中的案例 当前读和快照读 MySQL的InnoDB存储引擎默认事务隔离级别是RR(可重复读), 是通过 “行排他锁+MVCC” 一起实现的, 不仅可以保证可重复读, 还可以部分防止幻读; 为什么是部分防止幻读, 而不是完全防止? 效果: 在如果事务B在事务A执行中, insert了一条数据并提交, 事务A再次查询, 虽然读取的是undo中的旧版本数据(防止了部分幻读), 但是事务A中执行update或者delete都是可以成功的!! 因为在innodb中的操作可以分为当前读(current read)和快照读(snapshot read): 快照读 和 当前读 可参考之前的博文幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read) 小结 InnoDB所谓的MVCC 一般我们认为MVCC有下面几个特点： 每行数据都存在一个版本，每次数据更新时都更新该版本 修改时Copy出当前版本, 然后随意修改，各个事务之间无干扰 保存时比较版本号，如果成功(commit)，则覆盖原记录, 失败则放弃copy(rollback) 就是每行都有版本号，保存时根据版本号决定是否成功，听起来含有乐观锁的味道, 因为这看起来正是，在提交的时候才能知道到底能否提交成功 而InnoDB实现MVCC的方式是: 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） 二者最本质的区别是: 当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ Innodb的实现真算不上MVCC, 因为并没有实现核心的多版本共存, undo log 中的内容只是串行化的结果, 记录了多个事务的过程, 不属于多版本共存。但理想的MVCC是难以实现的, 当事务仅修改一行记录使用理想的MVCC模式是没有问题的, 可以通过比较版本号进行回滚, 但当事务影响到多行数据时, 理想的MVCC就无能为力了。 比如, 如果事务A执行理想的MVCC, 修改Row1成功, 而修改Row2失败, 此时需要回滚Row1, 但因为Row1没有被锁定, 其数据可能又被事务B所修改, 如果此时回滚Row1的内容，则会破坏事务B的修改结果，导致事务B违反ACID。 这也正是所谓的 第一类更新丢失 的情况。 也正是因为InnoDB使用的MVCC中结合了排他锁, 不是纯的MVCC, 所以第一类更新丢失是不会出现了, 一般说更新丢失都是指第二类丢失更新。 MVCC 高并发特点写不影响读，读不影响写，写只影响写只有写写会阻塞!!! 参考 关于read view创建时机: http://www.sohu.com/a/194511597_610509 https://www.cnblogs.com/digdeep/p/4947694.html https://www.zhihu.com/question/265280455/answer/292022808 关于比较算法 low_limit_id 高水位事务: https://github.com/zhangyachen/zhangyachen.github.io/issues/68 https://www.zhihu.com/question/66320138 https://www.zhihu.com/question/265280455/answer/292022808 大咖问答: https://www.zhihu.com/inbox/4577674200 更多可以参考数据库内核月报: https://yq.aliyun.com/articles/303200?spm=5176.100240.searchblog.9.271fd153pQ9FgV 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"MVCC","slug":"MVCC","permalink":"http://blog.renyimin.com/tags/MVCC/"}]},{"title":"25. READ COMMITTED","slug":"mysql/2017-09-17-mysql-25","date":"2017-09-17T06:50:52.000Z","updated":"2018-03-07T09:19:39.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-25/","excerpt":"","text":"前言 READ COMMITTED 隔离级别可以解决高并发场景下, 事务 脏读 的问题; 也就是可以让事务只能读其他事务已完成(提交/回滚)的落地数据; 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免脏读的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"24. READ UNCOMMITTED 未提交读","slug":"mysql/2017-09-17-mysql-24","date":"2017-09-17T06:40:04.000Z","updated":"2018-03-07T09:19:35.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-24/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-24/","excerpt":"","text":"简介 READ UNCOMMITTED 隔离级别会造成 脏读(Dirty Read) 现象的出现:也就是说, 事务 可以读取 其他事务 未提交的数据(事务A中对数据所做的修改, 即使还没有被提交, 对其他事务也都是可见的); 这个级别会导致很多问题, 而且从性能上来说, READ COMMITTED 并不会比其他的级别好太多, 却缺乏其他级别的很多好处, 在实际应用中一般很少使用。 虽然该隔离级别很少使用, 但还是有必要了解一下, 它这个隔离级别究竟是如何进行隔离的, 竟还能容许很多问题的存在? 测试准备环境 先准备一张测试表test_transaction: 1234567891011121314DROP TABLE IF EXISTS `test_transaction`;CREATE TABLE `test_transaction` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `user_name` char(20) NOT NULL COMMENT &apos;姓名&apos;, `age` tinyint(3) NOT NULL COMMENT &apos;年龄&apos;, `gender` tinyint(1) NOT NULL COMMENT &apos;1:男, 2:女&apos;, `desctiption` text NOT NULL COMMENT &apos;简介&apos;, PRIMARY KEY (`id`), KEY `name_age_gender_index` (`user_name`,`age`,`gender`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;INSERT INTO `test_transaction` VALUES (1, &apos;金刚狼&apos;, 127, 1, &apos;我有一双铁爪&apos;);INSERT INTO `test_transaction` VALUES (2, &apos;钢铁侠&apos;, 120, 1, &apos;我有一身铁甲&apos;);INSERT INTO `test_transaction` VALUES (3, &apos;绿巨人&apos;, 0, 2, &apos;我有一身肉&apos;); 如下: 123456789mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 2 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 脏读效果 先查看 客户端1 事务的隔离级别: SELECT @@SESSION.tx_isolation;; 可以看到, InnoDB默认事务隔离级别为 REPEATABLE READ 123456789mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; 重新设置 会话端1 的事务隔离级别为 read uncommitted: SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; 注意, 此时只是当前会话端的隔离级别被改, 其余 会话端 还是默认的 REPEATABLE READ 隔离级别 接下来将 会话端2 的事务隔离级别也设置为read uncommitted; 客户端1 开启事务, 并执行一个查询 ‘读取数据’, 注意, 客户端1 的事务并未提交 1234567891011121314151617181920mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction where id=2;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 |+----+-----------+-----+--------+--------------------+1 row in set (0.00 sec) mysql&gt; 客户端2 开启事务, 并修改客户端1查询的数据 123456789101112131415mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-托尼&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 此时发现, 客户端2 可以对 客户端1 正在读取的记录进行修改 注意, 客户端2此时的事务也并未提交 回到 客户端1, 再次查询数据, 发现数据已经变成了’钢铁侠-托尼’; 然后客户端2 rollback 事务, 再到客户端1中查询, 发现user_name又变成了’钢铁侠’, 那之前读到 ‘钢铁侠-托尼’ 就是脏数据了, 这就是一次 脏读 小结: 可以用一张图来演示整个实验过程 分析该隔离级别如何加锁重新构造测试条件 客户端1开启事务, 然后对数据做修改 1234567mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rymuscle&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 注意, 客户端1此时的事务并未提交 客户端2开启事务, 对相同的数据行做修改 12345mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rym&apos; where id=2;....阻塞等待了 最终会如下: 注意: 在上面的过程, 在客户端2阻塞阶段, 你可以通过一个新的客户端来分析, 客户端2 在锁等待的情况下的 加锁情况 和 事务状态: 查看表的加锁情况: select * from information_schema.INNODB_LOCKS; 事务状态 select * from information_schema.INNODB_TRX; 小结 所以: READ UNCOMMITTED 隔离级别下, 写操作是会加锁的, 而且是X排他锁, 直到客户端1事务完成, 锁才释放, 客户端2才能进行写操作 接下来你肯定会纳闷 “既然该隔离级别下事务在修改数据的时候加的是x锁, 并且是事务完成后才释放, 那一次场测试中, 客户端2 在事务中修改完数据之后, 为什么还没提交事务, 也就是x锁还在, 结果客户端1却能读取到客户端2修改的数据”？ 这完全不符合排他锁的特性啊(排他锁会阻塞除当前事务之外的其他事务的读,写操作) 其实网上已经有人在sqlserver的官网上找到了相关资料: 12345ansactions running at the READ UNCOMMITTED level do not issue shared locks to prevent other transactions from modifying data read by the current transaction. READ UNCOMMITTED transactions are also not blocked by exclusive locks that would prevent the current transaction from reading rows that have been modified but not committed by other transactions. When this option is set, it is possible to read uncommitted modifications, which are called dirty reads. Values in the data can be changed and rows can appear or disappear in the data set before the end of the transaction. This option has the same effect as setting NOLOCK on all tables in all SELECT statements in a transaction. This is the least restrictive of the isolation levels. 翻译翻译, 在思考思考, 大概说的是在 READ UNCOMMITTED 级别运行的事务不会发出共享锁: 也就是事务A在读取数据时, 什么锁都不加; 这样的话, 事务B就可以对同样的数据进行修改(同时会加上排他锁);而事务A要读取事务B未提交的修改, 也不会被事务B所加的排他锁阻止, 因为排他锁会阻止其他事务再对其锁定的数据加读写锁, 但是可笑的是, 事务在该隔离级别下去读数据的话根本什么锁都不加, 这就让排他锁无法排它了; 所以可以得出: READ UNCOMMITTED隔离级别下, 读不会加任何锁。而写会加排他锁，并到事务结束之后释放。(读写不阻塞, 写写阻塞 (但会读到脏数据))","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"23. 事务隔离级别(READ UNCOMMITTED) 与 锁","slug":"mysql/2017-09-17-mysql-23","date":"2017-09-17T06:20:52.000Z","updated":"2018-03-07T09:19:29.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-23/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-23/","excerpt":"","text":"前言 之前几篇博文已经介绍了 Mysql事务, 高并发下事务将会面对的问题 及 解决方案;从之前的博文中可以了解到: MySQL在高并发场景下, 主要采用 事务隔离性中的4种隔离级别 及 MVCC机制 来解决事务可能会面临的一些问题; 接下来主要探讨一下, 在MySQL中, 事务的各隔离级别是如何实现的, 如何解决问题的, 隔离级别和锁之间是什么关系? 隔离级别 和 锁的关系 对于事务中各隔离级别与锁的关系, 先看下一参考美团技术博客中对事务隔离级别的介绍: 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 从上面的博文大概可以了解到: 事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略。 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的。 READ UNCOMMITTED 未提交读READ UNCOMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料:-《高性能MySQL》 MySQL官方文档 慕课mark_rock同学手记 美团技术博客","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"22. 幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read)","slug":"mysql/2017-09-16-mysql-22","date":"2017-09-16T06:10:07.000Z","updated":"2018-03-07T09:19:20.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-22/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-22/","excerpt":"","text":"前言RR + MVCC 虽然解决了 幻读 问题, 但严格来说, 只是部分解决幻读问题 演示 打开 客户端1 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 打开 客户端2 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 1234567891011121314mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) mysql&gt; 在 客户端2 之前开启的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了select幻读)!! 12345678910111213141516171819202122232425262728mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; //并且, 此时`update/delete`也是可以操作这条在事务中看不到的记录的! //( 后面会看到: update，delete也都是当前读) 问题的出现 虽然多次select读取,发现已经克服了幻读问题 但当 在客户端2事务中 insert插入一条id为4的新数据, 发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 – 一致性非阻塞读 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库状态的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果您插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。 如果事务更新或删除由不同事务提交的行, 则那些更改对当前事务就变得可见。 但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 小结 也就是RR隔离级别, 在同一事务中多次读取的话, 只是对 select 克服了幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的select都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 在RR级别下 快照读 是通过MVVC(多版本控制)和undo log来实现的 而当前读 是需要加 record lock(记录锁) 和 gap lock(间隙锁) 来实现的, 如果需要实时显示数据，还是需要通过加锁来实现, 这个时候会使用next-key技术来实现。 快照读, 读取专门的快照(对于RC，快照(ReadView)会在每个语句中创建, 对于RR, 快照是在事务启动时创建的), 快照读的操作如下: 1简单的select操作 (不包括: select ... lock in share mode, select ... for update) 当前读, 读取最新版本的记录, 没有快照, 在InnoDB中, 当前读取根本不会创建任何快照。当前读的操作如下, 可以看到 insert, update, delete都是快照读, 所以这几个操作会察觉到其他事务对数据做的更改, 而select察觉不到: 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后, 使用隔离性的最高隔离级别SERIALIZABLE也可以解决幻读, 但该隔离级别在实际中很少使用!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"21. MySQL高并发事务问题 及 解决方案","slug":"mysql/2017-09-16-mysql-21","date":"2017-09-16T05:40:53.000Z","updated":"2018-04-07T10:39:44.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-21/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-21/","excerpt":"","text":"前言上一篇MySQL事务简介简单介绍了MySQL事务的相关概念及特性; 但在实际场景中, 可能经常会面对一些高并发应用, 此时, 简单了解事务概念和特性就不足以应对问题了; 高并发的事务问题在并发量比较大的时候, 很容易出现 多个事务同时进行 的情况。 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以最终导致了一些问题的出现; 脏读 Mysql中, 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读。 因为 ‘事务A’ 此时读取到的是 尚未被持久化 的数据 (事务B中修改的数据还不具备事务的持久性特性) 事务A此时读取的数据也叫 脏数据事务B 最终可能会因为内部其他后续操作的失败或者系统后续突然崩溃等原因, 导致事务B最终整体提交失败, 从而导致事务A读取的数据被回滚;那么最终 事务A 拿到的自然就是脏的数据了, 因为 事务A 拿到的数据已经和数据表中持久化的真实数据不一致了。 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身特性 隔离性 的 – READ COMMITED或以上隔离级别 解决了这个问题; READ COMMITED 级别保证了: 在事务中, 某条语句执行前, 已经被其他事务提交的数据, 对该语句都是可见的。 不可重复读 现在, 上面的 脏读问题 已经被解决了, 那就意味着事务中每次读取到的数据都是 持久性 的数据(被别的事务最终 提交/回滚 的落地数据)。 但脏读问题的解决, 也仅仅只能保证你在事务中每次读到的数据都是持久性的数据而已。 试想, 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了; 那么, 多次读取数据不一致, 有什么危害呢? 首先, 一个事务中为什么要多次读取同一数据, 什么场景下需要这么做? 1234查询余额: 100给别人汇款: 20记录日志: ....展示余额(你可以直接计算然后返回80, 但如果你是查询的话, 就应该保证你查到的是80, 而不受其他事务干扰) 解决方案 : RR+ 在MySQL中, 事务已经用自身特性 隔离性 的 – REPEATABLE READ或以上隔离级别 解决了这个问题; REPEATABLE READ 级别保证了:1.在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的;2.在事务中, 多次读取同一个数据 (在两次读取操作之间, 无论数据被 提交/回滚 多少次(即无论落地过多少遍), 每次读取的结果都应该是一样的; 幻读 (区分不可重复读取)1.之前经常搞混 不可重复读 和 幻读 这两个概念（这两者确实非常相似） 但 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录)。(可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 2.解决方案: RR + MVCC 其实对于 幻读, 在Mysql的InnoDB存储引擎中, 事务默认的 RR 级别已经通过 MVCC机制 帮我们解决了(并非完全解决), 所以该级别下, 你也模拟不出幻读的场景; 退回到 RC 隔离级别的话, 你又容易把 幻读 和 不可重复读 搞混淆, 所以这可能就是比较头痛的点吧! 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 3.想了解更多, 可以参考下一篇幻读的延伸 更新丢失 最后聊一下 高并发事务 的另一个问题: 丢失更新问题, 该问题和之前几个问题需要区分开, 因为解决方案不是一类! 第一类丢失更新: A事务撤销时, 把已经提交的B事务的更新数据覆盖了不过这种情况在Mysql中不会出现, 因为即使是InnoDB事务的最低隔离级别(READ UNCOMMITED): 会在事务(假设事务A)中对写数据设置x锁, 其他事务(假设事务B)想对相同数据做修改, 要等到事务A数据修改完毕并提交(释放了x锁才行); 由于最低隔离级别会出现脏读, 所以即使事务B在事务A提交之后, 对数据做了修改, rollback回滚到的也是事务A最后的提交数据, 所以不会覆盖; (而更高的RC,RR隔离级别使用了MVCC技术, 事务B回滚的话, 也是回滚到事务A最后的那次提交) 第二类丢失更新: A事务 覆盖 B事务 已经提交的数据，造成B事务所做操作丢失 下图可以看到, 搞笑的是: 也正是因为RR隔离级别所保证的可重复读, 给此类问题的出现提供了基础条件; 这种丢失更新, 无法依靠前三种隔离级别来解决, 只能手动使用 乐观锁(在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制), 悲观锁 来解决。 注意注意: 最高隔离级别Serializable在实际应用场景中并不被采用 (并且, 它也无法解决更新丢失的问题) 因为虽然serialize将事务串行化了, 但对于事务, 只要开启事务的条件都满足, 事务A,B,C假设并行到来, 他们都会顺利依次开启事务(只不过以前是并行开启)); 但是开启之后, 虽然读写都会互相阻塞(经过测试), 但是这只是代表事务之间需要等待, 但最终都会执行, 所以还会造成超卖!; 所以还是会出现 更新丢失 的高并发事务问题: 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 参考资料: 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 《高性能MySQL》","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"20. MySQL事务简介","slug":"mysql/2017-09-16-mysql-20","date":"2017-09-16T03:01:07.000Z","updated":"2018-03-08T13:36:24.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-20/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-20/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的 工作单元, 在这个 独立的 工作单元中, 可以有 一组 操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败。 仍然通过最经典的银行转账应用来解释一下: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过该银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，其实就需要打包在一个事务中, 这样就可以保证一组操作可以作为一个 独立的工作单元 来执行。并且在 独立工作单元(即事务) 中的这三个操作, 只要有任何一个操作失败, 则事务整体就应该是失败的, 那就必须回滚所有已经执行了的步骤。 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚。(其实这里也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发场景离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元, 整个事务中的所有操作要么全部提交成功, 要么全部失败回滚。对于一个事务来说, 不能只成功执行其中的一部分操作, 这就是事务的原子性。 Consistency 一致性你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性总是能够保证 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态; 比如之前转账的例子:转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15)转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115)转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的;比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取A账户的余额, 那么这个程序读到的应该是没有被减100的余额才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务隔离性 的四个 隔离级别, 到时候就知道这里为什么说通常来说对其他事务是不可见的; (但确实也还有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 它仍然无法解决更新丢失的问题(可以参考) Durability 持久性一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久 保存到数据库中; (所谓永久可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方);","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"12. MySQL备份与恢复","slug":"mysql/2017-08-23-mysql-12","date":"2017-08-23T10:51:28.000Z","updated":"2018-03-18T07:29:49.000Z","comments":true,"path":"2017/08/23/mysql/2017-08-23-mysql-12/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/mysql/2017-08-23-mysql-12/","excerpt":"","text":"## ##","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"Git","slug":"2017-08-21-Git","date":"2017-08-21T02:07:12.000Z","updated":"2018-03-19T11:16:31.000Z","comments":true,"path":"2017/08/21/2017-08-21-Git/","link":"","permalink":"http://blog.renyimin.com/2017/08/21/2017-08-21-Git/","excerpt":"","text":"暂时没迁移云笔记, 可以点击此处进行阅读, 经常会更新重建笔记 git rebase 与 git merge: http://gitbook.liuhui998.com/4_2.html","categories":[{"name":"Git","slug":"Git","permalink":"http://blog.renyimin.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://blog.renyimin.com/tags/Git/"}]},{"title":"05. 复制","slug":"mysql/2017-08-18-mysql-05","date":"2017-08-18T10:51:28.000Z","updated":"2018-05-06T03:58:57.000Z","comments":true,"path":"2017/08/18/mysql/2017-08-18-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2017/08/18/mysql/2017-08-18-mysql-05/","excerpt":"","text":"概述 MySQL内建的复制功能是构建基于MySQL的大规模、高性能应用的基础, 这类应用使用所谓的水平扩展的架构。 可以通过为服务器配置一个或多个备库的方式来进行数据同步, 复制功能不仅有利于构建高性能的应用, 同时也是高可用、可扩展性、灾难恢复、备份以及数据仓库等工作的基础。 复制解决的基本问题是让一台服务器的数据与其他服务器的数据保持同步。一台主库的数据可以同步到多台备库上, 备库本身也可以被配置成另外一台服务器的主库。主库和备库之间可以有多种不同的组合方式。 MySQL版本对主从复制的影响: 新版本服务器可以作为老版本服务器的备库, 但是老版本服务器作为新版本服务器的备库通常是不可行的, 因为老版本可能无法解析新版本所采用的新特性或语法; 另外, 所使用的二进制文件格式也可能不相同; 小版本升级通常是兼容的; 开销 复制通常不会增加主库的开销, 主要是启用二进制带来的开销, 但出于对备份或及时从崩溃中恢复的目的, 这点开销也是必要的; 每个备库也会对主库增加一些负载(例如网络I/O开销), 尤其当备库请求从主库读取旧的二进制日志文件时, 可能会造成更高的I/O开销; 另外, 锁竞争也可能阻碍事务的提交; 最后, 如果是从一个高吞吐量的主库上复制到多个备库, 唤醒多个复制线程发送事件的开销将会累加; 复制解决的问题 数据分布: 可以在不同的地理位置来分布数据备份; 负载均衡: 通过MySQL复制可以将读操作分布到多个服务器上, 实现对读密集型应用的优化, 并且实现很方便, 通过简单的代码修改就能实现基本的负载均衡, 备份: 对于备份来说, 复制是一项很有意义的技术补充; 高可用性和故障切换: 复制能够帮助应用程序避免MySQL单点失败, 一个包含复制的设计良好的故障切换系统能够显著地缩短宕机时间; MySQL升级测试: 这种做法比较普遍, 使用一个更高版本的MySQL作为备库, 保证在升级全部实例前, 查询能够在备库按照预期执行; 复制原理概述 简单来说, MySQL的复制有如下三个步骤 在主库上把数据更改记录到二进制日志(Binary Log)中(这些记录被称为二进制日志事件) 备库将主库上的日志复制到自己的中继日志(Relay Log)中 备库读取中继日志中的事件, 将其重放到备库数据之上 高性能MySQL中用下图描述了上面三步 二进制日志记录格式 事实上, MySQL支持两种复制方式: 基于行的复制 和 基于语句的复制; 这两种复制方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制(这也就意味着, 在同一时间点, 备库上的数据可能与主库存在不一致, 并且无法保证主备之间的延迟, 一些大的语句可能导致备库产生几秒,几分钟甚至几个小时的延迟) 这两种方式主要是指在主库在记录二进制日志时所采用的日志格式(Binary Logging Formats), 其实有 STATEMENT, ROW, MIXED 三种配置; 基于语句的日志记录: 早在MySQL3.23版本中就存在; 可以通过使用 --binlog-format = STATEMENT 启动服务器来使用此格式; 基于行的日志记录: 在5.1版本中才被加进来(在5.0之前的版本中是只支持基于语句的复制); 可以通过以 --binlog-format = ROW 启动它来使服务器使用基于行的日志记录; 基于混合日志记录: 对于混合日志记录, 默认情况下使用基于语句的日志记录, 但在某些情况下, 日志记录模式会自动切换为基于行的; 当然, 您可以通过使用--binlog-format = MIXED选项启动mysqld来显式使用混合日志记录; 优缺点未完待续~~ 三个线程 MySQL使用3个线程来执行复制功能(其中1个在主服务器上, 另外两个在从服务器上); 当从服务器发出START SLAVE时, 从服务器创建一个I/O线程, 以连接主服务器并让它发送记录在其二进制日志中的语句; 主服务器创建一个binlog dump线程, 将二进制日志中的内容发送到从服务器; 从服务器I/O线程读取主服务器Binlog dump线程发送的内容, 并将该数据拷贝到从服务器的中继日志中; 第3个线程是从服务器的SQL线程, 是从服务器创建用于读取中继日志并执行日志中包含的更新; 问题?未完待续~~ 配置复制 master服务器上进行sql写操作的时候, 是会引起磁盘变化的; 所以slave服务器要想和master上的数据保持一致, 可以有两种办法: slave按照master服务器上每次的sql写语句来执行一遍; slave按照master服务器磁盘上的变化来做一次变化 ; 主服务器master上的写操作都会被记录到binlog二进制日志中, 从服务器slave去读主服务器的binlog二进制日志, 形成自己的relay中继日志, 然后执行一遍 ; 所以主从配置需要做到 主服务器要配置binlog二进制 从服务器要配置relaylog(中继日志) master要授予slave账号: 从服务器如何有权读取主服务器的 binlog (binlog非常敏感, 不可能让谁去随便读) 从服务器用账号连接master 从服务器一声令下开启同步功能 start slave 注意: 一般会在集群中的每个sql服务器中加一个server-id来做唯一标识 ; 配置启动主从 主服务器配置 1234#主从复制配置server-id=4 #服务器起一个唯一的id作为标识log-bin=mysql-bin #声明二进制日志文件名binlog-format= #二进制日志格式 mixed,row,statement 主服务器的 binlog二进制日志 有三种记录方式 mixed, row, statement ; statement: 二进制记录执行语句, 如 update….. row: 记录的是磁盘的变化 如何选择 binlog二进制日志 记录方式? update salary=salary+100; // 语句短, 但影响上万行, 磁盘变化大, 宜用statement update age=age+1 where id=3; // 语句长而磁盘变化小, 宜用row 你要是拿不准用哪个? 那就设置为mixed, 由系统根据语句来决定; 从服务器配置 首先从服务器肯定要开启relaylog日志功能 ; 从服务器一般也会开启binlog, 一方面为了备份, 一方面可能还有别的服务器作为这台从服务器的slave ; 主从之间建立关系 : 主服务器上建立一个用户: grant replication client,replication slave on *.* to repl@&#39;192.168.56.%&#39; identified by &#39;repl&#39; 告诉从服务器要连接哪个主服务器: 在从服务器上进入mysql执行如下语句 1234567reset slave #可以把之前的从服务器同步机制重置一下change master to master_host=&apos;192.168.56.4&apos;,master_user=&apos;repl&apos;,master_password=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,#当前主服务器产生的binlog走到哪儿了(需要在主服务器上`show maste status`来查看file名和position指针位置)master_log_pos=349 然后查看从服务器的slave状态 show slave status 发现已经连上主服务器了 从服务器中启动slave: start slave 注意: 可以一主多从, 但一个从有多个主就会傻逼了 ; 此时我们做的只是mysql主从复制, 但不是读写分离, 距离读写分离还差一小步; 因为读写分离还需要对sql语句进行判断(可以在php层面判断sql语句进行路由, 决定哪种sql去哪个服务器) 可以参考本人有道笔记上的记录","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"02.","slug":"oauth/2017-07-12-02","date":"2017-07-12T13:10:12.000Z","updated":"2018-04-26T12:20:32.000Z","comments":true,"path":"2017/07/12/oauth/2017-07-12-02/","link":"","permalink":"http://blog.renyimin.com/2017/07/12/oauth/2017-07-12-02/","excerpt":"","text":"签名认证腾讯云api安全签名 包含防重放 Base64(HMAC-SHA1(secretKey, orignal)) 阿里云api安全签名 uppercase (hex (hmac_sha1 (s, secretKey)) 京东云宙斯api安全签名 md5 七牛云api安全签名 hmac_sha1 美团云api安全签名 HmacSHA256算法 百度云api安全签名: 包含防重放 华为API网关 即使上面的所有安全算法均被破解;即使秘钥被获取;也就是说你即便截获了算法+秘钥, 构建了自己的攻击性url;此时, url中用户的token是具有时效性的, 一旦token过期, 服务器会刷新出新的token给到客户端;你还需要每隔一段时间截获生成的token, 换到自己的url中;这一系列操作下来, 你截获的还只是一个用户的, 构不成多大威胁!!! 授权被授权之后, 如何安全地访问API接口, 这是上面的 签名认证 要做的!大概是签名认证中也经常用到诸如 ‘权限’ 这个词, 所以经常搞混 签名认证 和 授权 百度对于认证方式: 需要通过使用Access Key Id / Secret Access Key 加密的方法来验证某个请求的发送者身份, Access Key Id(AK)用于标示用户, Secret Access Key(SK)是用户用于加密认证字符串, 和百度云用来验证认证字符串的密钥, 其中SK必须保密, 只有用户和百度云知道;当百度云接收到用户的请求后, 系统将使用相同的SK和同样的认证机制生成认证字符串, 并与用户请求中包含的认证字符串进行比对, 如果认证字符串相同, 系统认为用户拥有指定的操作权限, 并执行相关操作; 如果认证字符串不同, 系统将忽略该操作并返回错误码; 注意: signingKey签名Key百度云不直接使用SK对待签名串生成摘要, 相反的, 百度云首先使用SK和认证字符串前缀生成signingKey, 然后用signingKey对待签名串生成摘要;其实一般都是使用SK对canonicalRequest生成签名摘要signature; 百度这么做, 感觉意义也不大;Authorization = bce-auth-v1/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/2015-04-27T08:23:49Z/1800//b02547aedd91dc438d6bb6d1be68b35917828271b340b9e7d1a7bcd051869054","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"01.","slug":"oauth/2017-07-12-01","date":"2017-07-12T12:10:12.000Z","updated":"2018-04-25T07:44:10.000Z","comments":true,"path":"2017/07/12/oauth/2017-07-12-01/","link":"","permalink":"http://blog.renyimin.com/2017/07/12/oauth/2017-07-12-01/","excerpt":"","text":"为什么使用token? 而不使用账号密码? OAuth 的身份验证过程, 其实就是一个获取 access_token 的过程, 为什么要用token而不直接用账号密码来登录? 经常使用 OAuth 身份认证的开发人员, 可能也会感觉到使用 token 都是出现在 让第三方网站访问用户数据提供服务 场景(比如第三方网站接入的微信, 微博登录, 可以提供用户在微信, 微博中的用户数据给第三方站点); 其实 token 与 账号+密码 最大的不同, 就是 token 必然是有时效性的, 并且可被回收, 即使不小心 token 被攻击者获取, 他也没有办法永远能访问你的数据; 而 OAuth 正是提供了让第三方获取 token 这个临时钥匙的通用解决方案。 https://www.chrisyue.com/security-issue-about-oauth-2-0-you-should-know.html 微信:公众平台: https://mp.weixin.qq.com/cgi-bin/home?t=home/index&amp;lang=zh_CN&amp;token=208919843 (qq邮箱)开放平台： https://open.weixin.qq.com/cgi-bin/index?t=home/index&amp;lang=zh_CN (126邮箱)","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"31. Lua脚本","slug":"Redis/2017-06-29-redis-31","date":"2017-06-29T10:40:07.000Z","updated":"2018-03-10T12:12:08.000Z","comments":true,"path":"2017/06/29/Redis/2017-06-29-redis-31/","link":"","permalink":"http://blog.renyimin.com/2017/06/29/Redis/2017-06-29-redis-31/","excerpt":"","text":"前言 之前已经了解到, 在redis事物中, 为了检测事务即将操作的keys, 是否有多个clients同时改变而引起冲突, 这些keys将会在事务开始前使用 watch 被监控; 如果至少有一个被监控的key在执行exec命令前被修改, 则事务会被打断, 不执行任何动作, 从而保证原子性操作。 之前的这种方案是在解决一种叫做 CAS(check-and-set) (检查后再设置) 的问题, 这种问题也可以使用 Lua 脚本来进行解决; Lua 脚本功能是Reids 2.6版本的最大亮点, 通过内嵌对Lua环境的支持, Redis解决了长久以来不能高效地处理CAS(check-and-set检查后再设置) 命令的缺点, 并且可以通过组合使用多个命令, 轻松实现以前很难实现或者不能高效实现的模式。 脚本的原子性Redis 使用单个 Lua 解释器去运行所有脚本,并且Redis 也保证脚本会以原子性(atomic)的方式执行;(当某个脚本正在运行的时候,会有其他脚本或 Redis 命令被执行, 这和使用 MULTI / EXEC 包围的事务很类似, 在其他别的客户端看来，脚本的效果要么是不可见的，要么就是已完成的;另一方面其实也意味着, 执行一个运行缓慢的脚本并不是一个好主意, 写一个跑得很快很顺溜的脚本并不难, 因为脚本的运行开销(overhead)非常少, 但是当你不得不使用一些跑得比较慢的脚本时, 请小心, 因为当这些蜗牛脚本在慢吞吞地运行的时候, 其他客户端会因为服务器正忙而无法执行命令; Redis中执行Lua脚本的好处 Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令; (一个Lua脚本中的命令, 就相当于类似Redis中GETSET这种原生命令, 是具备原子性的) Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这些命令常驻在Redis内存中，实现复用的效果; Lua脚本可以将多条命令一次性打包，有效地减少网络开销; 不应该使用Lua什么时候不应该把脚本嵌入到Redis里面?因为Redis的实现是堵塞的, 即, 一个Redis server, 在同一时候, 只能执行一个脚本;因此, 如果你写了一个逻辑非常复杂的脚本, 这个脚本的执行时间非常长, 这样, 这个时候如果有别的请求进来, 就只能排队, 等待这个脚本结束了, 这个Redis server才能处理下一个请求。在这个时候, 就连仅仅是获取数据的命令都会被堵住。这个Redis-Server的效率就会被大大的拖慢, 因此, 如果你的脚本执行非常复杂耗时, 那么这个时候你是不应该把它放在Redis里面执行的。 Lua 基础hello.lua hello.lua: 这个Lua脚本比较简单, 仅仅返回一个字符串, 没有与redis-Server进行比较有意义的操作(比如获取或设置数据) 12local msg = &quot;Hello, world!&quot; --定义了一个本地变量msg存储我们的信息return msg 保存这个文件到hello.lua, 运行: 123456 renyimindembp:test renyimin$ redis-cli eval \"$(cat /Users/renyimin/Desktop/test/test.lua)\" 0 \"Hello, world!\"-- 下面这种方法也行 renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0 \"Hello, world!\" renyimindembp:test renyimin$ 运行这段代码会打印”Hello,world!” EVAL 指令 语法: VAL script numkeys key [key ...] arg [arg ...] script 参数: 是一段lua脚本程序, 它会被运行在Redis服务器上下文中, 可以直接写在命令行, 也可以引入.lua文件; numkeys 参数: 指定即将传入lua脚本中的key的个数 (如果没有参数的话, 也需要指明参数个数为0, 否则会报错) 从第三个参数开始的numkerys个参数, 就是你要传入脚本的键名参数, 这些键名参数可以在Lua脚本中通过全局变量KEYS数组(索引从1开始)的形式访问(如:KEYS[1], KEYS[2]); 剩下的在命令最后的附加参数 arg [arg ...], 可以在Lua中通过全局变量 ARGV 数组访问, 访问的形式和KEYS变量类似; 例子: (..是lua中的字符串拼接语法) 123127.0.0.1:6379&gt; EVAL &apos;return &quot;K1: &quot;..KEYS[1]..&quot; K2: &quot;..KEYS[2]..&quot; A1: &quot;..ARGV[1]..&quot; A2: &quot;..ARGV[2]&apos; 3 k1 k2 k3 a1 a2 a3 a4&quot;K1: k1 K2: k2 A1: a1 A2: a2&quot;127.0.0.1:6379&gt; 带宽和 EVALSHA EVAL 命令要求你在每次执行脚本的时候都发送一次脚本, 但是Redis有一个内部的缓存机制, 因此它不会每次都重新编译脚本, 这样, eval发送脚本主体就是在浪费带宽了; 为了减少带宽的消耗, Redis实现了 EVALSHA 命令, 它的作用和EVAL一样, 都用于对脚本求值但它接受的第一个参数不是脚本, 而是脚本的SHA1校验和(sum)(SHA1校验和 的生成看下一节SCRIPT LOAD); EVALSHA执行后, 如果服务器还记得给定的 SHA1校验和 所代表的脚本, 那么执行这个脚本; 如果服务器不记得给定的 SHA1校验和 所代表的脚本, 那么它返回一个特殊的错误, 提醒用户使用EVAL代替EVALSHA以下是示例： 123456789101112// 现在是有a这个key的127.0.0.1:6379&gt; get a&quot;haha&quot;// 开始测试127.0.0.1:6379&gt; SCRIPT LOAD &quot;return redis.call(&apos;GET&apos;,&apos;a&apos;)&quot;&quot;8a2f221803757b26fc7d283bec7ba834d91202c9&quot;127.0.0.1:6379&gt; evalsha 8a2f221803757b26fc7d283bec7ba834d91202c9 0&quot;haha&quot;// 如下瞎写的 sha1校验和, 服务器就不认识了, 要你使用 eval 命令来直接使用脚本127.0.0.1:6379&gt; evalsha lalalala 0(error) NOSCRIPT No matching script. Please use EVAL.127.0.0.1:6379&gt; SCRIPT LOAD如果使用 EVALSHA 发送校验和给服务器, 从而调用正确脚本的话, 这个 校验和 如何生成? 这就要用到 SCRIPT LOAD 命令了;SCRIPT LOAD: 将脚本加载到脚本缓存, 而不执行它 (在将指定的命令加载到脚本缓存中之后, 之后你将需要使用EVALSHA命令和脚本的正确SHA1摘要来调用它) 该脚本被保证永远留在脚本缓存中(除非调用SCRIPT FLUSH) lua调用redis命令 Lua脚本中可以使用两个不同的Lua函数来调用Redis的命令 redis.call() redis.pcall() redis.call() 与 redis.pcall()的区别 这两个命令很类似, 他们唯一的区别是当redis命令执行结果返回错误时, redis.call()将返回给调用者一个错误; 而redis.pcall()会将捕获的错误以Lua表的形式返回; 测试 (redis.call() 和 redis.pcall() 两个函数的参数可以是任意的 Redis 命令) 12345127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;set&apos;,&apos;foo&apos;,&apos;bar&apos;)&quot; 0OK127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;get&apos;,&apos;foo&apos;)&quot; 0&quot;bar&quot;127.0.0.1:6379&gt; .lua 脚本文件 其实写lua脚本还是有很多注意点的, 可以参考中纯函数脚本这一小节; 本篇学习主要是将来能写一些简单的原子命令, 并不会涉及一些复杂的逻辑操作, 所以只是简单了解了一下上面那些注意点; 演示在redis设置一个num为10的库存 set num 10; 下面通过lua脚本写一个原子操作, 将检测库存和最终减少库存放在一起 123456local good = redis.call(&apos;get&apos;, &apos;num&apos;);if tonumber(good) &gt; 0then redis.call(&apos;decr&apos;, &apos;num&apos;)endreturn &apos;ok&apos; 调用该脚本 12renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0&quot;ok&quot; 会正常减少数字 laravel中实现如下 出现超卖的代码 123456789101112131415/** * 使用redis模拟并发超卖 * 由于check和set是分两步执行的 */public function redisOversell()&#123; $good = Redis::get(&apos;num&apos;); if ($good &gt; 0) &#123; Redis::multi(); usleep(500000); //预先已经设置好库存为10个了 Redis::decr(&apos;num&apos;); Redis::exec(); &#125;&#125; 定义脚本: 将check 和 set放到一个脚本里(但这个脚本由于没有设置sleep, 所以只能得出理论上是没有问题的) 123456protected $lua = &quot; local good = redis.call(&apos;get&apos;, &apos;num&apos;);&quot; . &quot; if tonumber(good) &gt; 0 then&quot; . &quot; redis.call(&apos;decr&apos;, &apos;num&apos;)&quot; . &quot; end&quot; . &quot; return &apos;ok&apos;&quot;; 执行 1234567/** * lua脚本解决超卖 */public function redisLua()&#123; Redis::eval($this-&gt;lua, 0);&#125; 参考https://segmentfault.com/a/1190000009811453#articleHeader0https://www.cnblogs.com/huangxincheng/p/6230129.htmlhttps://segmentfault.com/a/1190000007615411","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"23. redis 主从, 哨兵","slug":"Redis/2017-06-24-redis-23","date":"2017-06-24T12:50:07.000Z","updated":"2018-03-09T07:50:19.000Z","comments":true,"path":"2017/06/24/Redis/2017-06-24-redis-23/","link":"","permalink":"http://blog.renyimin.com/2017/06/24/Redis/2017-06-24-redis-23/","excerpt":"","text":"前言 和MySQL主从复制的原因一样, Redis虽然读取写入的速度都特别快, 但是也会产生读压力特别大的情况, 为了分担读压力, redis支持主从复制; Redis的主从结构可以采用 一主多从 或者 级联结构 , 下图为级联结构: 主从复制(Replication)实现读写分离, 提高服务器负载能力 Redis的主从复制架构中, 一类是主数据库(master), 一类是从数据库(slave)master可以进行读写操作, 当发生写操作的时候自动将数据同步到从数据库;而从数据库一般是只读的, 并接收主数据库同步过来的数据; 一个主数据库可以有多个从数据库, 而一个从数据库只能有一个主数据库; Redis主从复制可以根据是否全量分为 全量同步 和 增量同步 Redis全量复制 一般发生在Slave初始化阶段, 这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器, 并发送SYNC命令 ; 主服务器接收到SYNC命令后, 开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令; 主服务器BGSAVE执行完后, 向所有从服务器发送快照文件, 并在发送期间继续记录被执行的写命令; 从服务器收到快照文件后丢弃所有旧数据,载入收到的快照; 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令; 从服务器完成对快照的载入, 开始接收命令请求, 并执行来自主服务器缓冲区的写命令 ; 注意：redis2.8之前的版本, 当主从数据库同步的时候从数据库因为网络原因断开重连后会重新执行上述操作, 不支持断点续传, redis2.8之后支持断点续传。 Redis增量复制 Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令, 从服务器接收并执行收到的写命令。 小结 主从刚刚连接的时候, 进行全量同步; 全同步结束后, 进行增量同步; 当然, 如果有需要, slave 在任何时候都可以发起全量同步; redis 策略是, 无论如何, 首先会尝试进行增量同步, 如不成功, 则要求从机进行全量同步; Redis主从配置方法动态配置 在启动新的redis实例的时候, 设置这个新的实例为某个redis主服务器的Slave: redis-server --port 6379 --slaveof 127.0.0.1 6379 尝试向从服务器发送写命令, 发现不会成功; 在master上添加一个键值对, 发现从服务器上立马就会同步到这条数据; 动态配置对一个已经启动的redis服务发送命令: slaveof ip port, 则会将当前服务器状态从Master修改为别的服务器的Slave 配置文件中配置为每个准备启动的redis从服务, 创建各自的配置文件, 然后在启动redis新实例的时候, 为新实例指定各自的配置文件即可 比如一个6380的从实例, 其配置文件为: redis-server /usr/local/redis/etc/redis-6380.conf 配置文件内容如下: 12slaveof 127.0.0.1 6379port 6380 上面三种方式都可以很轻松地配出主从效果来! sentinel哨兵引出sentinel 之前的主从配置可以为我们解决一个redis实例读压力大的问题, 可以在一台服务器上进行主从多实例的配置, 也可以在不同机器之间进行主从配置; 如果slave服务器挂掉, 只是读性能会下降, 现在的问题是, 如果一旦主redis服务器(也就是master实例)挂了, 你目前貌似只能手动去把这台master实例启动起来; 如果确实这台机器/实例就是启动不起来的话, 那你可能就需要手动去将当前的某一个slave实例切换为master:slaveof no one //先把这个slave实例变成一个master实例slaveof ip port //在之前的从实例中执行, 将他们的master重新切换到这个新的master上 这种手动操作明显不可能被接收, 我们需要当master主挂了之后, 自动有一个slave能够勇于承担地顶上来(因为slave相对于之前的master除了分担读压力外, 还是之前master的备份), 这就引出了redis的sentinel哨兵; 简介 sentinel哨兵 是redis官方提供的高可用解决方案, 可以用它来管理多个redis服务的实例; 之前在编译安装好redis之后, 就可以在 /usr/local/redis/src/ 目录下看到 redis-sentinel 等很多命令; sentinel的监控: 它会不断地检查master和slaves是否正常; 一个sentinel可以监控任意多个master及其下的slaves; 当然, sentinel也可能挂掉, 也有单点问题 (还好Sentinel是一个分布式系统, 可以在一个架构中运行多个Sentinel进程, 他们可以组成一个sentinel网络, 之间是可以互相通信的, 可以通过”投票”来决定master是否挂了) 当一个sentinel认为被监控的服务已经下线时, 它会向网络中的其他sentinel进行确认, 判断该服务器是否真的下线; 如果下线的是一个主服务器, 那么sentinel将会对下线的主服务器进行 自动故障转移通过将下线主服务器的某个从服务器提升为新的主服务器;并将下线主服务器下的从服务器重新指向新的主服务器;来让系统从新回到正常; 之前的master下线后, 如果重新上线了, sentinel会让它作为一个salve, 去新的master中同步数据 实战 启动sentinel: 将 /usr/local/src/ 目录下的redis-sentinel程序文件复制到 /usr/local/redis/bin 目录下; 启动一个运行在sentinel模式下的redis服务实例(两种方式): redis-sentinel 或者 redis-server /usr/local/redis/sentinel.conf --sentinel sentinel配置 Sentinel之间的自动发现机制虽然sentinel集群中各个sentinel都互相连接彼此来检查对方的可用性以及互相发送消息, 但其实你是不用在任何一个sentinel中配置任何其它的sentinel的节点的, 因为sentinel利用了master的发布/订阅机制去自动发现其它也监控了同一master的sentinel节点。 同样，你也不需要在sentinel中配置某个master的所有slave的地址，sentinel会通过询问master来得到这些slave的地址的。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"21. 了解redis持久化","slug":"Redis/2017-06-24-redis-21","date":"2017-06-24T05:40:54.000Z","updated":"2018-03-09T07:49:23.000Z","comments":true,"path":"2017/06/24/Redis/2017-06-24-redis-21/","link":"","permalink":"http://blog.renyimin.com/2017/06/24/Redis/2017-06-24-redis-21/","excerpt":"","text":"Redis的两种持久化模式介绍 RDB(Redis DB)简介 RDB持久化功能可以将redis服务器包含的所有数据库数据以二进制文件(.rdb文件)的形式保存到硬盘; RDB 默认是开启的, 关闭方式有: 注释掉配置文件中save策略行; 在所有save策略下添加save &quot;&quot;; 创建RDB文件, 常见的三种方式 给redis服务器发送SAVE命令; 给redis服务器发送BGSAVE命令; 在redis配置文件中配置rdb持久化的save策略, 如果配置选项被满足, 则服务器会自动执行BGSAVE; 这三种创建RDB的方式, 前两种需要手动去执行;而第三种是服务器自动执行的; rdb文件每次都是覆盖创建, 所以为了安全起见, 需要不时地去备份生成的rdb文件; 三种方式的区别 通过使用客户端向redis服务器发送SAVE命令, 可以命令服务器去创建一个新的RDB文件; 注意: 在redis服务器执行SAVE命令的过程中(也即创建RDB文件的过程中), redis服务器将被阻塞, 服务器端无法再去处理客户端发送的命令请求, 只有在SAVE命令执行完毕之后(也即RDB文件创建完毕之后), 服务器才会重新开始处理客户端发送的命令请求; 注意: 如果rdb文件已经存在, 服务器会自动使用新的rdb文件去代替旧的rdb文件;所以: 之前的rdb文件你可以用定时脚本定时地拷走, 可以防止外一执行了flush db并且服务器进行了save, 此时旧的rdb会被新的rdb文件覆盖掉, 那就完蛋了!!! 通过使用客户端向redis服务器发送BGSAVE命令,也可以命令服务器去创建一个新的RDB文件; 注意: BGSAVE命令不会造成服务器阻塞, 也就是说redis服务器在执行BGSAVE命令的过程中, redis服务器仍然可以正常的处理其他客户端发送的命令请求;BGSAVE命令不会造成服务器阻塞的原因在于: 12当redis服务器接收到BGSAVE命令后,它不会亲自去创建rdb持久化文件,而是通过fork一个子进程,然后由子进程去负责这个rdb文件的生成, 自己则继续处理客户端的命令请求;当子进程创建好rdb文件并退出后, 它会向父进程(也就是负责处理命令请求的redis服务器)发送一个信号,告知redis服务器rdb文件已经创建完毕; 注意: 由于创建子进程会消耗额外的内存, 所以save创建RDB文件的速度其实会比bgsave快, 因为它可以集中资源来创建rdb文件; save 和 bgsave 没有孰好孰坏, 你要考虑哪个更适合你如果你的数据库正在上线当中, 自然使用 bgsave (让服务器以非阻塞方式进行最好);相反,如果你需要在凌晨3点钟维护你的redis, 比如维护需要停机一小时, 这时系统被阻塞了也是没关系的, 这时候你使用save命令就会好一点; 自动创建rdb文件 通过在redis配置文件中配置redis服务器创建rdb文件的条件, 就可以让服务器在客户端操作满足save策略所指定的条件时, 自动去创建rdb文件; 比如, redis默认情况下就是开启rdb持久化的, 并且制定了默认的save条件:123456save 900 1 #900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化） save 300 10 #300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化） save 60 10000 #60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）这些默认的save选项的意思是:只要三个条件中的任意一个条件被满足时,服务器就会自动执行 BGSAVE 命令来创建新的rdb文件; 每次创建完rdb文件之后,服务器为实现自动持久化会将&apos;为实现持久化而设置的时间计数器和次数计数器清零并重新开始计数&apos;, 所以多个保存条件的效果是不会叠加的; RDB持久化问题 redis关机持久化: 当你正常关闭redis的时候, redis服务器不会参考配置中的save策略; 而是会直接先调用save命令, 将redis所有数据持久化到磁盘之后才会真正进行退出; redis - crash不持久化问题: 而当redis服务器意外断电宕机的时候, 你会发现从上一次快照之后的数据将全部丢失; rdb持久化的优点 重建快: 在redis里, 默认使用RDB模式, 因为RDB模式重建数据库比较快, 这里的重建数据库是指将数据从硬盘移到内存,并建立起数据库的过程; 因为对于RDB模式来说, 重建就是把 dump.rdb 文件加载到内存, 并解压字符串,就建立起了数据库; 而对于AOF模式来说, 则是在启动Redis服务器的时候, 运行appendonly.aof日志文件, 在内存中重新建立数据库; 所以从这里的描述就可以看出,AOF的重建过程是要比RDB慢的; 使用RDB模式的话， 系统会将内存中数据库的快照每隔一段时间间隔更新到硬盘中(dump.rdb 文件里), 这个更新的频率是可以指定的。 在redis.conf中有三个配置用来指定内存数据更新到硬盘的频率： 1234//格式是：save &lt;seconds&gt; &lt;changes&gt;save 900 1 //如果仅有1-9次更改操作，那么要900s才写入硬盘一次save 300 10 //如果仅有10-9999次更改操作，那么要300s才写入硬盘一次save 60 10000 //如果超过10000次更改操作，那么60s才会写入硬盘一次 900s(也就是15分钟), 300s(就是5分钟): 这个时间挺长的, 这正是RDB模式的缺点所在如果服务器宕机的话, 可能会造成最后几分钟, 保存在内存中还来不及刷入硬盘的数据丢失, 如果数据很重要那就惨了; 但是如果数据不是那么重要, 丢失几分钟数据也没什么关系, 那么RDB模式是最好的选择。 rdb持久化的缺点 由于创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作, 所以服务器需要隔一段事件再来创建一个新的rdb文件, 也就是说rdb文件的创建操作不能执行的过于频繁, 否则将会严重影响服务器的性能; 由于只能隔一段时间再去创建一下rdb文件, 所以如果在间隔的这段时间中服务器宕机, 那这段间隔中的数据就丢失了, 比如redis服务在创建rdb文件时, 如果使用了默认的save选项, 可能就会丢失60秒的数据, 这就有点严重了!!! 所以为了解决这个问题, rdb可以结合aof来一起进行持久化 aof就解决了服务器不能频繁执行rdb持久化的问题! AOF (Append Only File)简介 为了解决rdb持久化在服务器意外宕机的情况下造成的数据严重丢失的问题, redis还提供了另外一种持久化方案, 那就是 aof持久化! AOF模式是将操作日志记在appendonly.aof文件里, 每次启动服务器就会运行appendonly.aof里的命令重新建立数据库; 因为要重新运行命令, 所以appendonly.aof是比较慢的, 默认AOF模式也是关闭的; 你可以在redis配置文件(redis.conf)中, 配置 appendonly yes 打开AOF模式; aof需要注意的问题 在aof持久化的模式下, 虽然redis服务器在执行修改数据的命令后, 会把执行的命令写入到aof文件中, 但这并不意味着aof文件持久化不会丢失任何数据; 在常见的操作系统中, 执行系统调用write函数, 将一些内容写到某个文件中时, 为了提高效率, 系统通常不会直接将内容写入到磁盘里面, 而是先将内容放入一个内存缓冲区(buffer)里面, 等到缓冲区被填满, 或者用户执行fsync调用和fdatasync调用时, 系统才会将存储在缓冲区里面的内容真正写入到硬盘里; 所以对AOF持久化来说, 当一条命令真正的被写入到硬盘里面时, 这条命令才不会因停机而意外丢失; 因此,aof持久化在遭遇停机时丢失命令的数量, 取决于命令被写入到硬盘的时间;越早将命令写入到硬盘,发生意外停机时丢失的数据就越少;而越迟将命令写入硬盘,发生意外停机时丢失的数据就越多; 所以aof持久化模式还有三种执行策略供你选择; AOF模式三种追加策略这三种追加策略主要就是用来指定什么时机可以将操作日志真正追加到appendonly.aof文件里; always : 服务器每写入一个命令, 就调用一次fdatasync, 将缓冲区里面的命令写入到磁盘文件中, 在这种模式下, 服务器即使遭遇意外停机, 也不会丢失任何自己已经成功执行的命令数据; 比较安全, 但比较慢; (类似mysql了) everysec: 服务器每一秒重新调用一次fdatasync, 将缓冲区里面的操作日志写入到磁盘文件中, 这是系统默认的方式, 是一种权衡折衷; 通常这种方式会比较好, 在这种模式下, 服务器即使遭遇意外停机时, 最多只丢失一秒钟的内执行的命令; no: 服务器不主动调用fdatasync, 由操作系统去决定什么时候将缓冲区里面的命令写入到硬盘里面; 在这种模式下,服务器遭遇意外停机时, 丢失的命令数量是不确定的; 可以在redis.conf中配置这三种策略 可以看到默认使用的是everysec这种折中策略;123# appendfsync alwaysappendfsync everysec# appendfsync no AOF文件中的冗余命令(aof文件重写) 随着服务器的不断运行,为了记录数据库发生的变化, 服务器会将越来越多的命令写入到aof文件中, 使得aof文件的体积不断增大; 为了让aof文件的大小控制在合理的范围, 避免它疯狂增长, redis提供了AOF重写功能, 通过这个功能, 服务器可以产生一个新的aof文件: 新的aof文件记录的内容和原有的aof文件记录的内容在redis服务器重建数据后, 数据完全一样; 新的aof文件会使用尽可能少的命令来记录数据库数据, 因此新的aof文件的体积通常会比原有aof文件的体积要小得多; aof重写期间, 服务器不会被阻塞, 可以正常处理客户端发送的命令请求; 有两种方法可以触发aof文件重写 客户端向服务器发送 BGREWRITEAOF 命令; 通过设置配置文件选项来让服务器自动执行BGWRITEAOF命令; 1234567auto-aof-rewrite-min-size &lt;size&gt;触发aof重写所需的最小体积:只要aof文件的体积大于等于size时,服务器才会考虑是否需要进行aof重写,这个选项用于避免对体积过小的aof文件进行重写auto-aof-rewrite-percentage 100指定触发重写所需的aof文件体积百分比, 当aof文件的体积大于auto-aof-rewrite-min-size指定的体积,并且超过上一次重写之后的aof文件体积的percent%时, 就会触发aof重写,(如果服务器启动刚刚不就,还没有进行过aof重写,那么使用服务器启动时载入的aof文件体积来作为基准值)将这个值设置为0表示关闭自动aof重写; 例子 1234//只有当aof文件的增量大于100%的时候才进行重写auto-aof-rewrite-percentage 100//当aof文件大于64mb之后才考虑进行aof重写,还需要看上一条的百分比增量够不够auto-aof-rewrite-min-size 64mb 如果AOF文件出错了 服务器可能在程序正在对AOF文件进行写入时停机, 如果停机造成了AOF文件出错(corrupt), 那么 Redis 在重启时会拒绝载入这个 AOF 文件, 从而确保数据的一致性不会被破坏; 当发生这种情况时,可以用以下方法来修复出错的 AOF 文件： 首先先为现有的 AOF 文件创建一个备份; 然后使用 Redis 附带的 redis-check-aof 程序, 对原来的 AOF 文件进行修复: $ redis-check-aof --fix (可选)使用 diff -u 对比修复后的 AOF 文件和原始 AOF文件的备份, 查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"11. Redis -- task queue","slug":"Redis/2017-06-17-redis-11","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-16T06:08:00.000Z","comments":true,"path":"2017/06/17/Redis/2017-06-17-redis-11/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/Redis/2017-06-17-redis-11/","excerpt":"","text":"前言现在有很多专门的队列软件(如ActiveMQ, RabbitMQ等)，在缺少专门的任务队列可用的情况下, 也可以使用Redis的队列机制; FIFO 先进先出队列 Redis的列表结构, 允许通过 RPUSH和LPUSH以及RPOP和LPOP, 从列表的两端推入和弹出元素。 假设一个操作中, 有向用户发送电子邮件这一功能, 由于这一功能可能会有非常高的延迟,甚至可能会出现发送失败, 所以这里就不能采用平时常见的代码流方式来执行这个邮件发送操作; 为此, 可以使用 任务队列 来记录 “邮件的收信人,邮件内容,发送邮件的原因”, 以 先到先服务 的方式发送邮件(此处采用的是 从右推入队列, 从左弹出元素) 队列: redis服务中的列表类型就充当了队列服务 消费者: 阻塞等待 (在Laravel将其做成命令进行启动, 该消费者就会阻塞等待) 12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email&apos;], 0); return $res;&#125; 生产者: 直接访问如下代码推送多条消息 1234567891011121314151617181920public function rPush()&#123; $data1 = json_encode([ &apos;user_id&apos; =&gt; 11, &apos;content&apos; =&gt; &apos;最近推出新款汽车--宝马, 售价:$12W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data2 = json_encode([ &apos;user_id&apos; =&gt; 12, &apos;content&apos; =&gt; &apos;最近推出新款汽车--奔驰, 售价:$15.8W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data3 = json_encode([ &apos;user_id&apos; =&gt; 13, &apos;content&apos; =&gt; &apos;最近推出新款汽车--大众, 售价:$20W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); //一旦运行就会阻塞起来 Redis::rpush(&apos;queue:email&apos;, [$data1, $data2, $data3]);&#125; 最终消费者正常拿到消息, 并且顺序也是正确的; 多种任务的队列 一般情况下, 我们为每种任务单独使用一个队列的; 但如果有一个队列处理多种任务的场景, 实现起来也很方便; 只用在消息中指明消息所需要调用的回调函数即可; priority 优先级队列优先级队列其实在redis中可以依靠 BRPOP和BLPOP的特性; 当 BLPOP 被调用时, 如果给定key(队列)列表中, 至少有一个非空列表, 那么弹出遇到的第一个非空列表的头元素, 并弹出元素所属的列表名字一起, 组成结果返回给调用者; 也就是说, BLPOP 给定的队列列表中, 靠前的就是优先级高的;假设你有 ‘重置密码的邮件’, ‘提醒邮件’, ‘发广告的邮件’, 三种队列, 如果你期望他们按照优先级依次排列, 那么只用设置为:12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email:resetpwd&apos;, &apos;queue:email:warning&apos;, &apos;queue:email:advertisement&apos;], 0); return $res;&#125; 想优先推送的, 你就放入第一个队列’queue:email:resetpwd’中(BRPOP)也类似 也可参考: http://blog.csdn.net/woshiaotian/article/details/44757621 延迟队列未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"12. Redis -- 发布订阅","slug":"Redis/2017-06-18-redis-12","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-09T09:55:58.000Z","comments":true,"path":"2017/06/17/Redis/2017-06-18-redis-12/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/Redis/2017-06-18-redis-12/","excerpt":"","text":"发布订阅其实 和 队列 非常相似: 这里的 频道, 就类似于任务队列中所讨论的 队列; 而订阅者就类似于 任务队列中所讨论的 消费者; 但他们还是有区别的: 对于队列来说, 即使消费者不在线, 消息也一直在队列中存放着, 等待消费者上线后进行处理; 而对于 发布订阅 中的 订阅者来说, 如果消息发布到频道中时, 订阅者不在线(没有一直阻塞等待), 那么这条消息也不会为它保存着; 另外对于队列来说, 其中的消息只要被一个消费者处理了, 其他消费者就不用处理了; 而对于发布订阅来说, 向频道中发布一条消息, 则所有在线的订阅者都可以收到这个消息; (貌似是比较是个做聊天室之类的东西) 实例可以参考有道笔记","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"08. redis锁 -- 不太安全?","slug":"Redis/2017-06-12-redis-08","date":"2017-06-12T08:35:08.000Z","updated":"2018-03-16T06:16:10.000Z","comments":true,"path":"2017/06/12/Redis/2017-06-12-redis-08/","link":"","permalink":"http://blog.renyimin.com/2017/06/12/Redis/2017-06-12-redis-08/","excerpt":"","text":"前言一般来说, 在对数据进行”加锁”时, 程序首先需要获取锁来得到对数据进行排他性访问的能力, 然后才能对数据执行一系列操作, 最后还要将锁释放给其他程序;之前已经了解过, Redis使用WATCH命令来代替对数据进行加锁, 因为WATCH只会在数据被其他客户端抢先修改了的情况下通知执行了这个命令的客户端, 所以这个命令被称为乐观锁; SETEX 实现锁介绍为了对数据进行排他性访问, 程序首先要做的就是获取锁; 而 Redis 的 SETEX 命令天生就适合用来实现锁的获取功能; 这个命令只会在键不存在的情况下为键设置值, 而其他进程一旦发现键存在, 那就只能等待之前锁的释放; 准备环境 数据表准备 123456789DROP TABLE IF EXISTS `goods`;CREATE TABLE `goods` ( `id` int(10) NOT NULL AUTO_INCREMENT, `goods_name` varchar(100) NOT NULL, `num` int(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `goods` VALUES (1, &apos;iphone 6 plus&apos;, 10); 超卖代码: 12345678public function mysqlOverSell()&#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(500000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125;&#125; Jmeter压测配置: 结果发现超卖: (后来设置压测为每秒3个线程, 也超卖了1件) 使用redis的setex加锁 redis 123127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; 代码 123456789101112131415public function setnx()&#123; $lock = Redis::setnx(&apos;tag&apos;, 1); // 如果加锁成功, 则可以进行如下操作(其他客户端只能等待锁释放) if ($lock) &#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(200000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125; // 注意: 执行完成之后, 必须释放锁 Redis::del(&apos;tag&apos;); &#125;&#125; 修改jmeter访问路由, 重新测试, 发现确实不会出现超卖现象了 SETEX锁 - 问题死锁客户端的突然下线导致锁不被释放因为客户端即使在使用锁的过程中也可能会因为这样或那样的原因而下线, 所以为了防止客户端在取得锁之后崩溃, 并导致锁一直处于 已获取 状态;所以还应该为锁的实现加上 超时限制 的特性: 如果获得锁的进程未能在指定时限内完成操作, 那么可能会认为客户端已经crash掉线, 所以锁将需要被自动释放; SETEX锁设超时限制 为锁加超时限制的普通方法如下 1234567$lock = Redis::setnx(&quot;tag&quot;, 1)if ($lock) &#123; Redis::expire(&quot;my:lock&quot;, 10); // ... do something Redis::del(&quot;my:lock&quot;)&#125; 如果客户端是在 Redis::expire(&quot;my:lock&quot;, 10); 之前就崩溃, 锁不被释放的问题还是存在; 从 setnx 到 set 所以, 从redis2.6.12开始(set新增了可选选项), 官方建议使用set命令替代setnx来实现锁, 如下 12345if (Redis::set(&quot;tag&quot;, 1, &quot;nx&quot;, &quot;ex&quot;, 10)) &#123; ... do something Redis::del(&quot;tag&quot;)&#125; 释放了其他进程的锁如果持有锁的进程因为操作时间过长, 而导致锁超时被自动释放, 但进程本身不知道这一点, 并且其他进程可能已经获取了锁; 这样, 进程在稍后做释放锁操作时, 如果只是简单的 Redis::del(&quot;tag&quot;&quot;), 这可能会释放掉其他进程持有的锁; 因此, 在获取锁的时候, 需要设置一个token, 放入自己的锁中, 在释放锁的时候, 用来保证释放的是自己的锁; 锁释放注意 如果客户端A是因为执行超时, 而导致锁被自动释放, 那么当客户端A最后在释放锁时, 可能此时客户端B已经加上了自己的锁, 所以在锁释放时需要做两个操作 检查token是否一致 释放锁 注意: 在上面两步之间, 比如说刚检查完token一致, 也可能发生超时而自动释放锁, 导致锁token被换上别的客户端的, 所以释放锁这一步应该放在事务中, 并提前用watch监控代表锁的那个key, 伪代码如下: 123456Redis::watch(&apos;tag&apos;);if (Redis::get(&apos;tag&apos;) == $token) &#123; Redis::multi(); Redis::del(&apos;tag&apos;); Redis::exec();&#125; 多个客户端同时获取锁 假设客户端A超时后, 锁被自动释放, 此时客户端B拿到了锁, 如果客户端B也因为超时导致锁被释放(此时客户端A还没执行完, B也没执行完), 那么客户端C也能拿到锁; 这样就同时存在3个客户端拿到了锁; 这个问题貌似就是一直没解决的问题!! 可能你需要评估业务的复杂度, 来设置超时时间为多长; 参考:基于Redis的分布式锁到底安全吗(上)https://segmentfault.com/q/1010000013626041?_ea=3427544~~未完待续","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"07. redis事务与WATCH乐观锁","slug":"Redis/2017-06-11-redis-07","date":"2017-06-11T13:14:46.000Z","updated":"2018-03-16T05:41:21.000Z","comments":true,"path":"2017/06/11/Redis/2017-06-11-redis-07/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/Redis/2017-06-11-redis-07/","excerpt":"","text":"Redis事务与单线程 因为Redis是单线程的, 所以即使多个客户端同时来对同一数据发来很多命令, 也会被串行挨个执行; 但是需要注意的是, 这和mysql的srialize隔离级别一样, 即使是串行执行命令, redis也逃不过高并发事务时的 更新丢失 问题; mysql是使用乐观锁, 悲观锁来解决的 参考MySQL高并发事务问题 及 解决方案 而redis的事务也是通过与WATCH(乐观锁)的结合才得以解决这个问题 Redis事务与WATCH 演示Redis事务在客户端高并发时出现的 丢失更新 问题 redis 为了解决高并发事务时这种 丢失更新 的问题, 提供了 WATCH WATCH介绍 redis 并没有实现典型的加锁功能(比如MySQL中, 在访问以写入为目的数据时 SELECT FOR UPDATE), 因为加这种悲观锁可能会造成长时间的等待; 所以redis为了尽可能地减少客户端的等待时间, 采用了WATCH监控机制, 如果某个客户端A在事务开始之前 WATCH 了一个key, 那么其实就相当于对该key加了乐观锁 事务中正常执行你要执行的操作 (乐观地认为不会有其他客户端抢在你前面去改动那个key) 直到当客户端A真正exec的时候, 才会验证客户端A之前WATCH(监控)的key是否被变动过, 如果变动过, 则客户端A可以进行重试; 测试:","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"06. Redis 事务","slug":"Redis/2017-06-11-redis-06","date":"2017-06-11T11:10:03.000Z","updated":"2018-03-16T05:37:18.000Z","comments":true,"path":"2017/06/11/Redis/2017-06-11-redis-06/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/Redis/2017-06-11-redis-06/","excerpt":"","text":"Redis事务介绍 redis事务是使用队列以先进先出(FIFO)的方法保存命令, 较先入队的命令会被放到数组的前面, 而较后入队的命令则会被放到数组的后面; redis事务从开始到结束通常会通过三个阶段: 事务开始 命令入队 事务执行 Redis事务通常会使用 MULTI, EXEC, WATCH等命令来完成; redis事务的ACID特性在redis中, 事务具有 原子性(Atomicity) , 一致性(Consistency) 和 隔离性(Isolation);并且当redis运行在某种特定的持久化模式下,事务也具有 持久性(Durability); (弱)原子性 对于redis的事务来说, 事务队列中的命令也是要么就全部执行, 要么就一个都不执行, 因此redis的事务是具有原子性的; 不过需要注意的是: redis事务的原子性有两种情况需要区分 一种是 语法错误导致redis事务执行出错, 比如, 你事务中的某条命令语法错误, 那么你在exec的时候, 所有的命令都不会执行: 1234567891011127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age(error) ERR wrong number of arguments for &apos;set&apos; command127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; 另一种是 无语法错误,可以运行成功, 但是运行完之后会返回运行错误, 这种情况下, 错误之前的命令不会回滚; 123456789101112131415161718192021222324//比如命令或命令的参数格式错误,那么事务就会出现有可能部分命令成功,而部分命令失败的情况127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age 10QUEUED//这里就会出现运行错误127.0.0.1:6379&gt; incr nameQUEUED127.0.0.1:6379&gt; set addr yunchengQUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) (error) ERR value is not an integer or out of range4) OK127.0.0.1:6379&gt; keys *1) &quot;addr&quot;2) &quot;age&quot;3) &quot;name&quot;127.0.0.1:6379&gt; 需要说明的是: 上述的第二种情况, 从表面上看, 不符合”原子性”, 因为你所执行的命令中有一条出错后, redis并没有进行回滚; 其实是由于你的此种错误命令不属于语法错误, 对redis来说, 不会导致执行出错,所以你的这条错误命令是可以执行成功的, 不过成功后会返回错误提示, 所以redis并不会进行回滚; redis的作者在事务相关的文档中解释说:1234不支持事务回滚是因为这种复杂的功能和redis追求的简单高效的设计主旨不符合,并且他认为, redis事务的执行时错误通常都是编程错误造成的,这种错误通常只会出现在开发环境中, 而很少会在实际的生产环境中出现,所以他认为没有必要为redis开发事务回滚功能。 所以, 在事务中执行redis命令时, 最好确保所有命令都能执行成功; 一致性redis通过谨慎的错误检测和简单的设计来保证事务一致性。 隔离性 事务的隔离性指的是, 即使数据库中有多个事务并发在执行, 各个事务之间也不会互相影响, 并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同; 因为redis使用单线程的方式来执行事务(以及事务队列中的命令), 并且服务器保证, 在执行事务期间不会对事务进行中断, 因此, redis的事务总是以串行的方式运行的, 并且事务也总是具有隔离性的; Redis为单进程单线程模式, 采用队列模式将并发访问变为串行访问(Redis本身没有锁的概念, Redis对于多个客户端连接并不存在竞争) 持久性因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能, 所以redis事务的耐久性由redis使用的持久化模式(rdb/aof)来决定: 关于持久化, 可以参考博文:了解redis持久化","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"03. 一些规范点","slug":"Redis/2017-06-07-redis-03","date":"2017-06-07T13:31:47.000Z","updated":"2018-03-16T05:26:33.000Z","comments":true,"path":"2017/06/07/Redis/2017-06-07-redis-03/","link":"","permalink":"http://blog.renyimin.com/2017/06/07/Redis/2017-06-07-redis-03/","excerpt":"","text":"将格式约定: 表名:字符串1:字符串2:····· 不要使用keys正则匹配操作, 包括但不限于各种形式的模糊匹配操作 因为Redis是单线程处理, 在线上KEY数量较多时, 操作效率极低(时间复杂度为O(N)), 该命令一旦执行会严重阻塞线上其它命令的正常请求, 而且在高QPS情况下会直接造成Redis服务崩溃 可靠的消息队列服务Redis List经常被用于消息队列服务;假设消费者程序在从队列中取出消息后立刻崩溃, 但由于该消息已经被取出且没有被正常处理, 那么可以认为该消息已经丢失, 由此可能会导致业务数据丢失, 或业务状态不一致等现象发生;为了避免这种情况, Redis提供了 RPOPLPUSH 命令, 消费者程序 会 原子性 的 从主消息队列中取出消息并将其插入到备份队列中, 并且会返回弹出队列的数据然后等到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除;同时还可以提供一个守护进程, 当发现备份队列中的消息过期时，可以重新将其再放回到主消息队列中, 以便其它的消费者程序继续处理。 未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"Xhgui","slug":"PHP/2017-04-26-Xhgui","date":"2017-04-26T10:16:30.000Z","updated":"2018-05-10T02:28:08.000Z","comments":true,"path":"2017/04/26/PHP/2017-04-26-Xhgui/","link":"","permalink":"http://blog.renyimin.com/2017/04/26/PHP/2017-04-26-Xhgui/","excerpt":"","text":"","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"Xhprof","slug":"PHP/2017-04-23-Xhprof","date":"2017-04-23T12:10:32.000Z","updated":"2018-05-10T02:29:11.000Z","comments":true,"path":"2017/04/23/PHP/2017-04-23-Xhprof/","link":"","permalink":"http://blog.renyimin.com/2017/04/23/PHP/2017-04-23-Xhprof/","excerpt":"","text":"是否需要装扩展 还是php已内置? http://blog.oneapm.com/apm-tech/235.html","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"03. OPcache","slug":"PHP/2017-04-21-OPcache","date":"2017-04-21T05:20:28.000Z","updated":"2018-05-10T02:29:11.000Z","comments":true,"path":"2017/04/21/PHP/2017-04-21-OPcache/","link":"","permalink":"http://blog.renyimin.com/2017/04/21/PHP/2017-04-21-OPcache/","excerpt":"","text":"OPCachehttp://www.laruence.com/tag/opcachehttps://www.cnblogs.com/jysdhr/p/6924178.html APCeAccelerator","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"03. php.ini配置文件","slug":"PHP/2017-04-15-php.ini","date":"2017-04-15T12:16:32.000Z","updated":"2018-05-09T12:34:00.000Z","comments":true,"path":"2017/04/15/PHP/2017-04-15-php.ini/","link":"","permalink":"http://blog.renyimin.com/2017/04/15/PHP/2017-04-15-php.ini/","excerpt":"","text":"## ##","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"11. basic_qos, prefetch_count","slug":"rabbitmq/2017-04-15-rabbitmq-11","date":"2017-04-13T13:12:26.000Z","updated":"2018-03-14T01:23:59.000Z","comments":true,"path":"2017/04/13/rabbitmq/2017-04-15-rabbitmq-11/","link":"","permalink":"http://blog.renyimin.com/2017/04/13/rabbitmq/2017-04-15-rabbitmq-11/","excerpt":"","text":"正常消息轮发的问题生产者12345678910111213141516171819public function noPrefetchCount()&#123; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); // 声明 $exchange = &apos;prefetch_exchange&apos;; $queueName = &apos;test_queue&apos;; // 声明交换机 $channel-&gt;exchange_declare($exchange, &apos;direct&apos;, false, true, false); // 声明队列 $channel-&gt;queue_declare($queueName, false, true, false, false); $channel-&gt;queue_bind($queueName, $exchange); for ($i=0; $i&lt;10; $i++) &#123; $message = new AMQPMessage(&apos;hello world--&apos; . $i, [&apos;content_type&apos; =&gt; &apos;text/plain&apos;, &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT,]); $channel-&gt;basic_publish($message, $exchange, null, true); &#125; $channel-&gt;close(); $connection-&gt;close();&#125; 消费者1通过sleep(10)来模拟消费者1的复杂业务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class Consumer4 extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer4&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); // 声明 $exchange = &apos;prefetch_exchange&apos;; $queueName = &apos;test_queue&apos;; // 声明交换机 $channel-&gt;exchange_declare($exchange, &apos;direct&apos;, false, true, false); // 声明队列 $channel-&gt;queue_declare($queueName, false, true, false, false); $channel-&gt;queue_bind($queueName, $exchange); /** * @param \\PhpAmqpLib\\Message\\AMQPMessage $message */ $backCall = function ($message) &#123; echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; //echo $message-&gt;delivery_info[&apos;delivery_tag&apos;]; echo &quot;\\n--------\\n&quot;; sleep(10); $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; $channel-&gt;basic_consume($queueName, &apos;&apos;, false, false, false, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 消费者212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class Consumer5 extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer5&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); // 声明 $exchange = &apos;prefetch_exchange&apos;; $queueName = &apos;test_queue&apos;; // 声明交换机 $channel-&gt;exchange_declare($exchange, &apos;direct&apos;, false, true, false); // 声明队列 $channel-&gt;queue_declare($queueName, false, true, false, false); $channel-&gt;queue_bind($queueName, $exchange); /** * @param \\PhpAmqpLib\\Message\\AMQPMessage $message */ $backCall = function ($message) &#123; echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; //echo $message-&gt;delivery_info[&apos;delivery_tag&apos;]; echo &quot;\\n--------\\n&quot;; $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; $channel-&gt;basic_consume($queueName, &apos;&apos;, false, false, false, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 启动两个消费者, 并且执行生产者; 结果 消费者2 (Consumer5) 很快打印完 1，3，5，7，9后就歇着了 123456789101112131415161718192021renyimindembp:laravel renyimin$ php artisan Consumer5--------hello world--1----------------hello world--3----------------hello world--5----------------hello world--7----------------hello world--9-------- 而消费者1(Consumer4) 还在缓慢的打印 0,2,4,6,8 上面情况在实际场景中比较常见, 由于消费者自身处理能力有限, 从rabbitmq获取一定数量的消息后, 希望rabbitmq不再将队列中的消息推送过来, 当对消息处理完后(即对消息进行了ack, 并且有能力处理更多的消息)再接收来自队列的消息。在这种场景下, 我们可以通过设置basic.qos信令中的prefetch_count来达到这种效果 basic_qos rabbitmq对basic.qos信令的处理 首先, basic.qos是针对channel进行设置的, 也就是说只有在channel建立之后才能发送basic.qos信令。 在rabbitmq的实现中, 每个channel都对应会有一个rabbit_limiter进程, 当收到basic.qos信令后, 在rabbit_limiter进程中记录信令中prefetch_count的值, 同时记录的还有该channel未ack的消息个数; 注: 其实basic.qos里还有另外两个参数可进行设置, prefetch_size和global, 但是RabbitMQ没有实现prefetch_size, 并在3.3.0版本中对global这个参数的含义进行了重新定义, 即glotal=true时表示在当前channel上所有的consumer都生效, 否则只对设置了之后新建的consumer生效; prefetch_count控制消息轮发只用在消费者创建完channel后, 简单加上 $channel-&gt;basic_qos(&#39;&#39;, 1, true); 即可(当然, 为了统一, 生产者最好也加上) 这就表示: 消费者完成一个ack后, 才能给他继续发送, 如果消费者业务过于复杂, 就先发给其他闲着的消费者 果然, 经过测试, Consumer4第一个hello world--0还没有执行完成的时候, Consumer5已经执行完了 1，2，3，4，5，6，7，8，9了","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"10. 事务机制 VS PublisherConfirm","slug":"rabbitmq/2017-04-15-rabbitmq-10","date":"2017-04-13T10:20:39.000Z","updated":"2018-03-13T08:36:17.000Z","comments":true,"path":"2017/04/13/rabbitmq/2017-04-15-rabbitmq-10/","link":"","permalink":"http://blog.renyimin.com/2017/04/13/rabbitmq/2017-04-15-rabbitmq-10/","excerpt":"","text":"背景之前已经了解过, 在使用RabbitMQ的时候, 我们可以通过 消息持久化 来解决因为服务器的异常奔溃导致的消息丢失; 除此之外我们还会遇到一个问题, 那就是当消息的发布者在一次发布多条消息时, 将消息发送出去之后, 消息到底有没有正确到达服务器呢? 如果不进行特殊配置的话, 默认情况下发布操作是不会返回任何信息给生产者的, 即, 默认情况下我们的生产者是不知道消息有没有正确到达服务器的; 如果在消息到达服务器之前已经丢失的话, 持久化操作也解决不了这个问题, 因为消息根本就没到达服务器, 你怎么进行持久化; 那么这个问题该怎么解决?其实RabbitMQ为我们提供了两种方式: 通过AMQP事务机制实现, 这也是从AMQP协议层面提供的解决方案; 通过将channel设置成confirm模式来实现; 事务机制 php-amqplib 中与事务机制有关的方法有三个, 分别是Channel里面的txSelect(), txCommit() 以及 txRollback(); txSelect(): 用于将当前Channel设置成是transaction模式; txCommit(): 用于提交事务; txRollback(): 用于回滚事务; 在通过txSelect()开启事务之后, 我们便可以发布消息给服务器了 如果txCommit()提交成功了, 则消息一定是到达了服务器; 如果在txCommit()执行之前broker异常奔溃或者由于其他原因抛出异常, 这个时候我们便可以捕获异常通过txRollback()回滚事务了; 测试代码: 生产者123456789101112131415161718192021222324252627282930313233343536 public function tx() &#123; $exchange = &apos;tx_exchange&apos;; $queueName = &apos;tx_queue&apos;; $bindingKey = &apos;tx_bindingkey&apos;; $connection = new AMQPStreamConnection(&apos;localhost&apos;, &apos;5672&apos;, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); $channel-&gt;exchange_declare($exchange, &apos;direct&apos;, false, true, false); $channel-&gt;queue_declare($queueName, false, true, false, false); $channel-&gt;queue_bind($queueName, $exchange, $bindingKey); try &#123; //开启事务 $channel-&gt;tx_select(); for ($i = 0; $i &lt; 10; $i++) &#123; $message = new AMQPMessage( &apos;msg-&apos; . $i, array( &apos;content_type&apos; =&gt; &apos;text/plain&apos;, &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, )); $channel-&gt;basic_publish($message, $exchange, $bindingKey); // 模拟事务失败的情况 (在发送第3条消息的时候, 模拟服务器异常)// if (2 == $i) &#123;// 1 / 0; // 出现异常// &#125; &#125; //提交事务 $channel-&gt;tx_commit(); &#125; catch (\\Exception $e) &#123; $channel-&gt;tx_rollback(); &#125; $channel-&gt;close(); $connection-&gt;close(); &#125; // 可以发现, 正常情况下, 队列中的消息是10条; // 在模拟事务失败的情况下, 之前的两条消息也都会被回滚, 队列中消息是0条; 使用事务确实能够解决发布者与broker代理服务器之间的消息确认, 只有消息成功被broker接收事务提交才能成功, 否则我们便可以在捕获异常进行事务回滚操作同时进行消息重发; 但是使用事务机制的话会降低RabbitMQ的性能，就拿上面的程序发送1000条消息，使用事务的话需要58244毫秒，而不使用事务的话仅仅需要89毫秒，因此在实际中使用事务会带来很大的性能损失; 那么有没有更好的方法既能保证发布者知道消息已经正确到达, 又能基本上不带来性能上的损失呢？ 从AMQP协议的层面看是没有更好的方法的，但是RabbitMQ提供了一个更好的方案，即将channel信道设置成confirm模式; channel-confirm模式生产者确认模式实现原理 生产者将信道设置成confirm模式, 一旦信道进入confirm模式, 所有在该信道上面发布的消息都将会被指派一个唯一的ID(从1开始, 即我们之前常见的 delivery_tag) 一旦消息被投递到所有匹配的队列之后, broker就会发送一个确认给生产者(包含消息的唯一ID), 这就使得生产者知道消息已经正确到达目的队列了; 如果消息和队列是可持久化的, 那么服务器会先将消息写入磁盘, 然后传给生产者的确认消息中delivery-tag(确认消息的序列号); 此外broker也可以设置basic_ack()的第二个参数multiple, 表示到这个序列号之前的所有消息都已经得到了处理; confirm模式最大的好处在于他是异步的, 一旦发布一条消息, 生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息, 当消息最终得到确认之后, 生产者应用便可以通过回调方法来处理该确认消息, 如果RabbitMQ因为自身内部错误导致消息丢失, 就会发送一条nack消息, 生产者应用程序同样可以在回调方法中处理该nack消息; 生产者在回调中处理ack消息: 12345$channel-&gt;set_ack_handler( function (AMQPMessage $message) &#123; echo &quot;Message acked with content &quot; . $message-&gt;body . PHP_EOL; &#125;); 生产者在回调中处理nack消息: 12345$channel-&gt;set_nack_handler( function (AMQPMessage $message) &#123; echo &quot;Message nacked with content &quot; . $message-&gt;body . PHP_EOL; &#125;); 最后, 需要 $channel-&gt;wait_for_pending_acks(); 来等待消费者的回应注意: wait_for_pending_acks() 可以放在哪些 basic_publish 之后, 就只等待在其代码之上的 消息的回应; 注意: 这里等待的确认, 和 消费者的确认 貌似不一样(响应给队列的), 回调函数也不会监控到 消费者的确认 ; 开启confirm模式的方法: 生产者通过调用channel的confirm_select()方法将channel设置为confirm模式; 注意一点, 已经在transaction事务模式的channel是不能再设置成confirm模式的,即这两种模式是不能共存的; 生产者实现confirm模式有两种编程方式: 普通confirm模式, 每发送一条消息, 调用 wait_for_pending_acks()或wait_for_pending_acks_returns() 方法等待服务端confirm, 这实际上是一种串行的confirm, 每publish一条消息之后就等待服务端confirm, 如果服务端返回false或者超时时间内未返回, 客户端进行消息重传; 批量confirm模式, 每发送一批消息之后, 调用 wait_for_pending_acks()或wait_for_pending_acks_returns() 方法等待服务端confirm, 这种批量确认的模式极大的提高了confirm效率, 但是如果一旦出现confirm返回false或者超时的情况, 客户端需要将这一批次的消息全部重发, 这会带来明显的重复消息, 如果这种情况频繁发生的话, 效率也会不升反降; 上代码1.了解了基本的原理之后, 可以写一个生产者的例子 12345678910111213141516171819202122232425262728293031323334353637383940414243public function confirm()&#123; $exchange = &apos;confirm_test_exchange&apos;; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 创建信道 $channel = $connection-&gt;channel(); // 设置信道为confirm模式 $channel-&gt;confirm_select(); // 创建对ack回应的回调操作 $channel-&gt;set_ack_handler( function (AMQPMessage $message) &#123; echo &quot;Message acked with content &quot; . $message-&gt;body; echo &apos;&lt;br/&gt;&apos;; &#125; ); // 创建对nack回应的回调操作 $channel-&gt;set_nack_handler( function (AMQPMessage $message) &#123; echo &quot;Message nacked with content &quot; . $message-&gt;body; echo &apos;&lt;br/&gt;&apos;; &#125; ); $channel-&gt;exchange_declare($exchange, &apos;fanout&apos;, false, false, true); // 普通confirm模式 $i = 1; $msg = new AMQPMessage($i, [&apos;content_type&apos; =&gt; &apos;text/plain&apos;]); $channel-&gt;basic_publish($msg, $exchange); // 下面只能等待上面这一条消息发送后, 服务器的回应 $channel-&gt;wait_for_pending_acks(); // 批量confirm模式 while ($i &lt;= 11) &#123; $msg = new AMQPMessage($i++, array(&apos;content_type&apos; =&gt; &apos;text/plain&apos;)); $channel-&gt;basic_publish($msg, $exchange); &#125; // 下面将等待上面所有的消息发送后, 服务器最终的回应(一条没有确认, 则所有的都失败) $channel-&gt;wait_for_pending_acks(); $channel-&gt;close(); $connection-&gt;close();&#125; 换 wait_for_pending_acks_returns() 方法: 123456789101112131415161718192021222324252627282930313233343536373839public function confirm1()&#123; $exchange = &apos;confirm1_test_exchange&apos;; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); $channel-&gt;confirm_select(); $channel-&gt;set_ack_handler( function (AMQPMessage $message) &#123; echo &quot;Message acked with content &quot; . $message-&gt;body . PHP_EOL; echo &apos;&lt;br/&gt;&apos;; &#125; ); $channel-&gt;set_nack_handler( function (AMQPMessage $message) &#123; echo &quot;Message nacked with content &quot; . $message-&gt;body . PHP_EOL; echo &apos;&lt;br/&gt;&apos;; &#125; ); $channel-&gt;set_return_listener( function ($replyCode, $replyText, $exchange, $routingKey, AMQPMessage $message) &#123; echo &quot;Message returned with content &quot; . $message-&gt;body . PHP_EOL; echo &apos;&lt;br/&gt;&apos;; &#125; ); $channel-&gt;exchange_declare($exchange, &apos;fanout&apos;, false, false, true); $i = 1; $message = new AMQPMessage($i, array(&apos;content_type&apos; =&gt; &apos;text/plain&apos;)); $channel-&gt;basic_publish($message, $exchange, null, true); $channel-&gt;wait_for_pending_acks_returns(); while ($i &lt;= 11) &#123; $message = new AMQPMessage($i++, array(&apos;content_type&apos; =&gt; &apos;text/plain&apos;)); $channel-&gt;basic_publish($message, $exchange, null, true); &#125; $channel-&gt;wait_for_pending_acks_returns(); $channel-&gt;close(); $connection-&gt;close();&#125;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"09. RPC远程调用及注意事项","slug":"rabbitmq/2017-04-13-rabbitmq-09","date":"2017-04-13T10:20:39.000Z","updated":"2018-03-14T01:20:52.000Z","comments":true,"path":"2017/04/13/rabbitmq/2017-04-13-rabbitmq-09/","link":"","permalink":"http://blog.renyimin.com/2017/04/13/rabbitmq/2017-04-13-rabbitmq-09/","excerpt":"","text":"RabbitMQ中实现RPC的机制 客户端发送请求(消息)时, 在消息的属性(MessageProperties)中设置两个值: replyTo: Queue名称, 用于告诉服务器处理完成后将通知我的消息发送到这个Queue中; correlationId: 此次请求的标识号, 服务器处理完成后需要将此属性返还, 客户端将根据这个id了解哪条请求被执行了; 服务器端收到消息并处理 服务器端处理完消息后, 将生成一条应答消息到replyTo指定的Queue, 同时带上correlationId属性; 客户端之前已通过另一个消费者订阅replyTo指定的Queue, 从中收到服务器的应答消息后, 根据其中的correlationId属性分析哪条请求被执行了, 根据执行结果进行后续业务处理; 其中可以看出, 服务端既是 消费者 又是 生产者; 引用网上常见的简图 客户端(生产者)请求远程调用 并 声明回执队列 注意: 回执队列在绑定exchange时, bindingkey 要和 回执队列名 一样 (因为服务端接收到的 reply_to 稍后在basic_publish发布消息时, 会当做 routingkey 使用) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public function rpc()&#123; // 创建connection(TCP)连接 $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 声明 $rpcExchange = &apos;rpc_exchange&apos;; $rpcQueue = &apos;rpc&apos;; $rpcQueueRoutingKey = &apos;rpc_routing_key&apos;; $replyQueue = &apos;rpc_reply&apos;; // 注意回执队列绑定时, 队列名要和bindingkey一样 $replyQueueRoutingKey = &apos;rpc_reply&apos;; $channel-&gt;exchange_declare($rpcExchange, &apos;direct&apos;, false, true, false); // 声明 rpc 队列 $channel-&gt;queue_declare($rpcQueue, false, true, false, false); // 绑定rpc队列和交换机 $channel-&gt;queue_bind($rpcQueue, $rpcExchange, $rpcQueueRoutingKey); // 声明 回执 队列 $channel-&gt;queue_declare($replyQueue, false, true, false, false); // 绑定回执队列和交换机 $channel-&gt;queue_bind($replyQueue, $rpcExchange, $replyQueueRoutingKey); $data = [ &apos;id&apos; =&gt; &apos;5&apos;, &apos;age&apos; =&gt; &apos;100&apos;, &apos;gender&apos; =&gt; &apos;male&apos;, ]; // 创建消息对象 $msg = new AMQPMessage(json_encode($data, JSON_UNESCAPED_UNICODE), [ &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, // 设定reply_to &apos;reply_to&apos; =&gt; $replyQueue, &apos;correlation_id&apos; =&gt; &apos;data-5&apos;, ] ); // 发布消息 $channel-&gt;basic_publish($msg, $rpcExchange, $rpcQueueRoutingKey); // 关闭信道 $channel-&gt;close(); // 关闭connection $connection-&gt;close();&#125; 服务端接收(消费者, 生产者)接收请求 并 响应调用结果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;class Consumer extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); // 声明 $rpcExchange = &apos;rpc_exchange&apos;; $rpcQueue = &apos;rpc&apos;; $rpcQueueRoutingKey = &apos;rpc_routing_key&apos;; // 声明交换机 $channel-&gt;exchange_declare($rpcExchange, &apos;direct&apos;, false, true, false); // 声明队列 $channel-&gt;queue_declare($rpcQueue, false, true, false, false); // 绑定队列和交换机 $channel-&gt;queue_bind($rpcQueue, $rpcExchange, $rpcQueueRoutingKey); // 创建 接收消息的回调函数 $backCall = function ($message) use($rpcExchange) &#123; $body = $message-&gt;body; echo &quot;\\n--------\\n&quot;; var_dump(json_decode($body, true)); echo &quot;\\n--------\\n&quot;; // 收到消息后 $msg = new AMQPMessage( $body, // 也可以使用$message-&gt;delivery_info[&apos;correlation_id&apos;]获取correlation_id [&apos;correlation_id&apos; =&gt; $message-&gt;get(&apos;correlation_id&apos;)] ); // 业务处理逻辑...... // ...... // ...... // 处理完成后进行回执 echo $message-&gt;get(&apos;reply_to&apos;); $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_publish($msg, $rpcExchange, $message-&gt;get(&apos;reply_to&apos;)); // 消息ack应答 $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; // 消费 $channel-&gt;basic_consume($rpcQueue, &apos;&apos;, true, false, true, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 客户端(消费者)订阅回执队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class Consumer1 extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer1&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); $channel = $connection-&gt;channel(); // 声明 $rpcExchange = &apos;rpc_exchange&apos;; $replyQueue = &apos;rpc_reply&apos;; $replyQueueRoutingKey = &apos;rpc_reply&apos;; // 声明交换机 $channel-&gt;exchange_declare($rpcExchange, &apos;direct&apos;, false, true, false); // 声明队列 $channel-&gt;queue_declare($replyQueue, false, true, false, false); // 绑定队列和交换机 $channel-&gt;queue_bind($replyQueue, $rpcExchange, $replyQueueRoutingKey); // 创建 接收消息的回调函数 $backCall = function ($message) &#123; echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; echo $message-&gt;delivery_info[&apos;delivery_tag&apos;]; echo &quot;\\n--------\\n&quot;; // 消息ack应答 $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; // 消费 $channel-&gt;basic_consume($replyQueue, &apos;&apos;, true, false, true, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 效果1.访问远程调用的路由, rpc队列中会有消息2.启动服务端进程, 效果如下, 并且rpc队列中消息被消费, rpc_reply队列中开始有消息 1234567891011121314renyimindembp:laravel renyimin$ php artisan Consumer--------array(3) &#123; [&quot;id&quot;]=&gt; string(1) &quot;5&quot; [&quot;age&quot;]=&gt; string(3) &quot;100&quot; [&quot;gender&quot;]=&gt; string(4) &quot;male&quot;&#125;--------rpc_reply 3.启动客户端的消费者, 消费rpc_reply队列中的消息, 效果如下 12345renyimindembp:laravel renyimin$ php artisan Consumer1--------&#123;&quot;id&quot;:&quot;5&quot;,&quot;age&quot;:&quot;100&quot;,&quot;gender&quot;:&quot;male&quot;&#125;1--------","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"02. php-fpm配置文件","slug":"PHP/2017-04-12-fpm.conf","date":"2017-04-12T08:40:32.000Z","updated":"2018-05-09T12:15:46.000Z","comments":true,"path":"2017/04/12/PHP/2017-04-12-fpm.conf/","link":"","permalink":"http://blog.renyimin.com/2017/04/12/PHP/2017-04-12-fpm.conf/","excerpt":"","text":"多个配置文件在下载并编译安装完成php7.2之后, 在安装目录下的 etc/ 目录中, 有几个和php-fpm相关的配置文件需要了解一下 php-fpm.conf.default 文件, 一般会用该文件拷贝一份 php-fpm.conf 文件, 作为php-fpm的全局配置文件; php-fpm.d/www.conf.default 文件, 一般会用该文件拷贝一份 www.站点名.conf 文件, 作为php-fpm的一个pool配置文件; php-fpm的pool 如上面介绍的那样, php-fpm的配置文件可能不止一份, 因为而在一台承载了多个网站的php服务器上，为了实现权限隔离, 管理员通常会将网站分配到不同的 pool 里, 换句话说, 在现实环境里, php-fpm的配置文件可能不止一个 每个pool可以监听不同的sock、tcp/ip 比如nginx有好几个站点, 如果所有的网站使用同一个池, 那其中一个网站发生一些故障, 比如程序员写的一些程序有问题, 就会把php资源耗尽, 也就是当其中的一个站点502(可能是php资源不够)了, 这样的结果就是其他站点的php也会502, 所以有必要把每一个站点隔离开, 每个pool的名字要唯一; 有一份全局配置文件php-fpm.conf, 另外, 在php-fpm.d下, 还可以创建多个 pool 配置文件; nginx可以配置不同的虚拟主机去使用php-fpm的不同pool 测试php-fpm的pool php-fpm配置参考: https://blog.csdn.net/mrding991124/article/details/79018917 nginx 配置 php-fpm各配置项分析 pid设置, 默认在PHP安装目录(/usr/local/php72)中的 var/run/php-fpm.pid, 建议开启 pid = run/php-fpm.pid 错误日志, 默认在安装目录中的 var/log/php-fpm.log error_log = log/php-fpm.log 错误级别, 可用级别为: alert(必须立即处理), error(错误情况), warning(警告情况), notice(一般重要信息), debug(调试信息); 默认为: notice. log_level = notice 表示在 emergency_restart_interval 所设值的时间内, 出现 SIGSEGV 或者 SIGBUS 错误的php-cgi进程数如果超过 emergency_restart_threshold 个, php-fpm就会优雅重启, 这两个选项一般保持默认值: 123#如下, 表示在60秒内出现 SIGSEGV 或者 SIGBUS错误 的php-cgi进程数如果超过10个, php-fpm就会重启emergency_restart_threshold = 60emergency_restart_interval = 60s 12345SIGSEGV --- 段错误, 遇到此错误的可能情况是:1. 缓冲区溢出 --- 通常由指针引用超出范围引起;2. 堆栈溢出 --- 请记住默认堆栈大小为8192K;3. 禁止文件访问 --- 文件操作在我们的评判系统上禁止;其中的第三条, 跟本问题的关系比较大, 也就是php-cgi访问了一个不存在的或者没有权限访问的文件; 设置子进程接受主进程复用信号的超时时间, 可用单位: s(秒), m(分), h(小时), 或者 d(天) ; 默认单位: s(秒); 默认值:0; process_control_timeout = 0 进程复用 先来简单谈谈PHP请求处理过程吧Nginx与PHP的交互依赖于CGI接口, 因为两者都实现了CGI接口, 所以Nginx可以把收到的请求交给PHP, 并从PHP获得相应的结果回传给客户端;最基本的CGI实现是每次请求都新建一个PHP进程, 处理完成后关闭进程, 这种方式会消耗很多的资源在进程的启动和关闭上, 所以效率并不高;进而出现了FastCGI这种实现方式, 也就是启动一个进程后让它处理多个请求再关闭, 这种方式就是解决每次请求都打开和关闭进程的消耗的, 但FastCGI有个缺点, 就是因为一个进程只能同时处理一个请求, 如果同时收到多个请求, 它们只能排队等待FastCGI进程的处理;解决FastCGI只能同时处理一个请求的方式很简单, 就是开启多个FastCGI进程, 不过开启多个FastCGI进程的话就存在对这些进程的管理问题, 比如究竟要开多少个进程, 怎么根据需要分配请求到这些进程上等等;而PHP-FPM就是这样一个管理FastCGI进程的管理程序, Nginx先将请求传递给PHP-FPM，再由PHP-FPM选择合适的FastCGI处理进程进行处理;在PHP-FPM将请求传递给FastCGI处理进程的时候, 就涉及到进程复用了, 原则上, PHP-FPM会选择空闲的FastCGI进程去处理请求, 在处理之前, PHP-FPM会发送进程复用信号给FastCGI进程, 用来让FastCGI进程准备好接受请求并处理; 但是, FastCGI进程并不总是能够处理请求, 也就是不一定能够响应进程复用信号(比如说出现假死的情况), 所以这个参数就表示了PHP-FPM留给FastCGI进程多久时间去响应进程复用信号, 如果超时了, PHP-FPM会选择其他的方式(例如使用其他的FastCGI进程)去处理请求;参考segmentfault问答 后台执行fpm, 默认值为yes，如果为了调试可以改为no, 在FPM中, 可以使用不同的设置来运行多个进程池; 这个配置可以针对每个进程池单独设置 daemonize = yes FPM进程池? fpm监听端口, 即 nginx 中配置的php的处理地址, 一般默认值即可， 可用格式为: ip:port, port, /path/to/unix/socket, 每个进程池都需要设置 listen = 127.0.0.1:9000 backlog数, -1表示无限制, 由操作系统决定, 此行注释掉就行, backlog含义参考: http://www.3gyou.cc/?p=41 listen.backlog = -1 rrc的oms系统, 线上backlog设置的是65535 backlog 关于backlog更多详情, 可以参考此文 允许访问FastCGI进程的IP, 设置 any 为不限制IP, 如果要设置其他主机的nginx也能访问这台FPM进程, listen处要设置成本地可被访问的IP, 默认值是any; 每个地址是用逗号分隔, 如果没有设置或者为空, 则允许任何服务器请求连接 listen.allowed_clients = 127.0.0.1 unix socket 需要设置的选项, 如果使用tcp方式访问, 这里注释即可 123listen.owner = www # sock通信文件的属主, 和nginx通信listen.group = www # sock通信文件的属组, 和nginx通信listen.mode = 0666 启动进程的帐户和组 user = www group = www 这个和nginx中配置的账户和组有什么关联么? 如何控制子进程, 选项有 static 和 dynamic, 对于专用服务器, pm可以设置为static pm = dynamic 如果选择static, 则由 pm.max_children 指定固定的子进程数, 如果选择dynamic, 则由下开参数决定: pm.max_children # 子进程最大数 pm.start_servers # 启动时的进程数 pm.min_spare_servers # 保证空闲进程数最小值, 如果空闲进程小于此值, 则创建新的子进程 pm.max_spare_servers # 保证空闲进程数最大值, 如果空闲进程大于此值, 此进行清理 设置每个子进程重生之前服务的请求数, 对于可能存在内存泄漏的第三方模块来说是非常有用的, 如果设置为 0 则一直接受请求, 等同于 PHP_FCGI_MAX_REQUESTS 环境变量, 默认值:0 pm.max_requests = 1000 FPM状态页面的网址, 如果没有设置, 则无法访问状态页面, 默认值: none, munin监控 会使用到 pm.status_path = /status rrc 配置为 pm.status_path = /status/fpm FPM监控页面的ping网址, 如果没有设置, 则无法访问ping页面, 该页面用于外部检测FPM是否存活并且可以响应请求, 请注意必须以斜线开头 (/) ping.path = /ping rrc oms系统线上没开启 用于定义ping请求的返回相应, 返回为 HTTP 200 的 text/plain 格式文本, 默认值: pong ping.response = pong rrc oms系统线上没开启 设置单个请求的超时中止时间 request_terminate_timeout = 0 该选项可能会对 php.ini 设置中的 max_execution_time 因为某些特殊原因没有中止运行的脚本有用; 设置为 0 表示 Off, 当经常出现502错误时可以尝试更改此选项 php.ini 设置中的 max_execution_time ：这设置了脚本被解析器中止之前允许的最大执行时间，默认是30s 也就是php.ini 设置中的 max_execution_time如果是30秒过期, 而fpm中的request_terminate_timeout设置为0, 则是php.ini在30秒后终止了进程;php有Fatal error超时日志，http状态码为500 而如果php.ini 设置中的 max_execution_time如果是30秒过期, 而fpm中的request_terminate_timeout设置为15, 则是phpfpm的解析器在15秒后终止了进程;php无Fatal error超时日志，http状态码为502，php-fpm日志中有杀掉子进程日志 参考 当一个请求超过该设置的超时时间后, 就会将对应的PHP调用堆栈信息完整写入到慢日志中, 设置为 ‘0’ 表示 ‘Off’ request_slowlog_timeout = 10s rrcoms系统线上设置为 request_slowlog_timeout = 1 慢请求的记录日志,配合request_slowlog_timeout使用 slowlog = log/$pool.log.slow rrcoms系统线上因为只有一个站点, 所以也是只有一个默认的 www.conf 池配置文件, 所以慢日志文件为 www.log.slow 设置文件打开描述符的rlimit限制, 默认值:系统定义值默认可打开句柄是1024，可使用 ulimit -n 查看, ulimit -n 2048修改 rlimit_files = 1024 rrc oms系统线上没配置 设置核心 rlimit 最大限制值, 可用值: ‘unlimited’ 、0或者正整数; 默认值: 系统定义值 rlimit_core = 0 rrc oms系统线上没配置 启动时的Chroot目录, 所定义的目录需要是绝对路径, 如果没有设置, 则 chroot 不被使用. chroot = rrc oms系统线上没配置 设置启动目录，启动时会自动Chdir到该目录. 所定义的目录需要是绝对路径. 默认值: 当前目录，或者/目录（chroot时） chdir = rrc oms系统线上没配置 重定向运行过程中的 stdout 和 stderr 到主要的错误日志文件中, 如果没有设置, stdout 和 stderr 将会根据FastCGI的规则被重定向到 /dev/null, 默认值:空 catch_workers_output = yes rrc oms系统线上配置为 yes php命令行直接运行php文件, 比如laravel中自定义的命令脚本(例如消费者脚本)和通过php-fpm解析的程序有什么区别? php.ini中的opcache缓存会对两种运行方式都生效么? Nginx使用Unix域Socket通信(Nginx和php-fpm在同一台服务器): https://www.cnblogs.com/JohnABC/p/4531107.html","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"06. Persistent (持久化)","slug":"rabbitmq/2017-04-11-rabbitmq-06","date":"2017-04-11T14:10:51.000Z","updated":"2018-03-13T07:12:47.000Z","comments":true,"path":"2017/04/11/rabbitmq/2017-04-11-rabbitmq-06/","link":"","permalink":"http://blog.renyimin.com/2017/04/11/rabbitmq/2017-04-11-rabbitmq-06/","excerpt":"","text":"RabbitMQ 默认情况下, Exchange, 队列, 消息 都是非持久的, 这意味着一旦消息服务器重启, 所有已声明的 Exchange, 队列, 以及 队列中的消息 都会丢失 要做到消息持久化, 其实只用设置: exchange: durable属性为true queue: durable属性为true message对象: 12345678$msg = new AMQPMessage( &apos;Hello World!&apos;, [ // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, &apos;content_type&apos; =&gt; &apos;text/plain&apos;, ]); 通过设置Exchange和MessageQueue的durable属性为true,可以使得队列和Exchange持久化; 但是这还不能使得队列中的消息持久化,这需要生产者在发送消息的时候,将delivery mode设置为2,只有这3个全部设置完成后,才能保证服务器重启不会对现有的队列造成影响。 这里需要注意的是, 只有durable为true的Exchange 和 durable为ture的Queues才能绑定, 否则在绑定时, RabbitMQ都会抛错的。 持久化会对RabbitMQ的性能造成比较大的影响;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"05. Priority(队列设定消息最大优先级, 消息优先级)","slug":"rabbitmq/2017-04-11-rabbitmq-05","date":"2017-04-11T12:21:07.000Z","updated":"2018-03-14T01:18:39.000Z","comments":true,"path":"2017/04/11/rabbitmq/2017-04-11-rabbitmq-05/","link":"","permalink":"http://blog.renyimin.com/2017/04/11/rabbitmq/2017-04-11-rabbitmq-05/","excerpt":"","text":"前言在系统应用中会根据业务的优先级来决定哪些内容优先被解决, 在RabbitMQ 3.5+版本中支持了队列优先级 和 消息优先级。 队列优先级 您可以使用 x-max-priority 参数声明优先级队列, 这个参数应该是一个整数, 指出队列应该支持的最大优先级 在使用 php-amqplib 中, 即: 在 query_declare() 方法的 arguments参数 中加上 x-max-priority 元素; 然后, 您可以在创建消息时, 在 AMQPMessage类 的第二个参数 properities 中设置 priority 属性来发布优先消息, 数字越大表示优先级越高; 没有优先级属性的消息被视为优先级为0 优先级高于队列最大值的消息被视为以最高优先级发布 注意: 如果生产者的队列声明的是优先级队列, 也就是 $argument 参数中有 x-max-priority 属性, 消费者中声明队列的时候, 也必须带上该参数属性, 否则消费者起不来! 文档 与AMQP 0-9-1规格相反，默认情况下，RabbitMQ队列不支持优先级。在创建优先级队列时，您可以根据需要指定任意数量的优先级。注意：每个队列的每个优先级有一些内存和磁盘上的成本。还有一个额外的CPU成本，特别是在消费时，所以你可能不希望创造大量的级别。消息优先级字段被定义为无符号字节，所以实际上优先级应该在0到255之间。没有优先级属性的消息被视为优先级为0.优先级高于队列最大值的消息被视为以最高优先级发布。如果需要优先级队列，我们​​建议在1到10之间使用。目前使用更多优先级将消耗更多资源(Erlang进程) 注意: 只有当消费者不足, 不能及时进行消费的情况下, 优先级队列才会生效 ; 生产者设置队列中消息的最大优先级为101234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;?phpnamespace App\\Http\\Controllers\\Test;use App\\Http\\Controllers\\Controller;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class RabbitController extends Controller&#123; public function producerPriority() &#123; // 创建connection(TCP)连接 $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 声明交换机 $channel-&gt;exchange_declare(&apos;test&apos;, &apos;direct&apos;, false, true, false); // 声明队列, 设定队列优先级别最大为10 (消息仍然可以设置比最大优先级大的级别) $arguments = new AMQPTable([&quot;x-max-priority&quot; =&gt; 20]); $channel-&gt;queue_declare(&apos;hello&apos;, false, true, false, false,false, $arguments); // 绑定队列和交换机 $channel-&gt;queue_bind(&apos;hello&apos;, &apos;test&apos;, &apos;hello&apos;); // 创建消息对象 $msg4 = new AMQPMessage(&apos;msg-4&apos;, [ &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, &apos;priority&apos; =&gt; 4 ] ); // 发布消息 $channel-&gt;basic_publish($msg4, &apos;test&apos;, &apos;hello&apos;); // 创建消息对象 $msg5 = new AMQPMessage(&apos;msg-5&apos;, [ &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, &apos;priority&apos; =&gt; 5 ] ); // 发布消息 $channel-&gt;basic_publish($msg5, &apos;test&apos;, &apos;hello&apos;); // 创建消息对象 $msg3 = new AMQPMessage(&apos;msg-3&apos;, [ &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, &apos;priority&apos; =&gt; 3 ] ); // 发布消息 $channel-&gt;basic_publish($msg3, &apos;test&apos;, &apos;hello&apos;); // 创建消息对象 $msg20 = new AMQPMessage(&apos;msg-20&apos;, [ &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, &apos;priority&apos; =&gt; 20 ] ); // 发布消息 $channel-&gt;basic_publish($msg20, &apos;test&apos;, &apos;hello&apos;); // 关闭信道 $channel-&gt;close(); // 关闭connection $connection-&gt;close(); &#125;&#125; 消费者注意: 如果生产者的队列声明的是优先级队列, 也就是 $argument 参数中有 x-max-priority 属性, 消费者中声明队列的时候, 也必须带上该参数属性, 否则消费者起不来!1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class Consumer1 extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer1&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; // 创建connection(TCP)连接 $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 声明交换机 $channel-&gt;exchange_declare(&apos;test&apos;, &apos;direct&apos;, false, true, false); // 声明队列 $arguments = new AMQPTable([&quot;x-max-priority&quot; =&gt; 20]); $channel-&gt;queue_declare(&apos;hello&apos;, false, true, false, false,false, $arguments); // 绑定队列和交换机 $channel-&gt;queue_bind(&apos;hello&apos;, &apos;test&apos;, &apos;hello&apos;); // 创建 接收消息的回调函数 $backCall = function ($message) &#123; echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; echo &quot;\\n--------\\n&quot;; // 消息ack应答 $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; // $channel-&gt;basic_consume(&apos;hello&apos;,&apos;&apos;,false, false, false, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 效果当只启动一个消费者的时候, 可以看到消息都是按照优先级来被消费的: 1234567891011121314151617renyimindembp:laravel renyimin$ php artisan Consumer1--------msg-20----------------msg-5----------------msg-4----------------msg-3-------- 当然, 如果消费者足够的情况下, 就不一定了! 貌似和redis默认的队列优先级不同: redis是按照队列本身的优先级来的; rabbit是按照队列中消息的优先级来的, 只能在队列内部来设定消息的优先级, 队列本身的优先级貌似没有; 参考官网手册","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"04. Exclusive Queue (排他队列)","slug":"rabbitmq/2017-04-11-rabbitmq-04","date":"2017-04-11T11:28:16.000Z","updated":"2018-03-14T01:17:04.000Z","comments":true,"path":"2017/04/11/rabbitmq/2017-04-11-rabbitmq-04/","link":"","permalink":"http://blog.renyimin.com/2017/04/11/rabbitmq/2017-04-11-rabbitmq-04/","excerpt":"","text":"Queue的Exclusive排他队列需要注意: 由于排他队列只在当前连接中有效, 所以应该放在消费者中创建, 否则, 如果在生产者中创建的话, 连接完成, 则队列也会被删除了(和queue的持久化无关); 排他队列通过消费者创建之后, 其他生产者也可以给该排他队列发送消息, 但是注意在其他生产者代码中不要再次声明与排他队列同名的队列了, 否则会报错; 排他队列通过消费者创建之后, 其他消费者即使在代码中没有创建与排他队列同名的队列, 它也无法绑定该排他队列, 会报错; 从以上其实可以看出: 排他队列通过消费者创建 可以有多个生产者(生产这种不能再次创建与排他队列同名的队列) 其他消费者不能获取排他队列的消费权限, 启动会报错 (即使在消费者中没有创建与排他队列同名的队列) 所以排他队列的消费者是私有的, 只能有一个消费者 ( 生产者(多) --- 队列(一) --- 消费者(一)) Consumer的Exclusive 如果队列设置为排他, 则消费者必然只能有一个, 此时, 消费者的 exclusive 设置true/false貌似没什么关系, 但一般队列如果设置为exclusive,则该消费者也设置为exclusive; 如果消费者中创建的队列设置为非排他队列, 而消费者设置为exclusive, 此时 别的生产者在发布消息时, 代码中依然可以包含创建同名队列 (因为队列是 非排他 的) 此时, 只有该消费者可以消费队列中的消息, 其他消费者如果也是消费该队列, 则启动会报错 消费者创建排他队列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;class Consumer extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer1&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; // 创建connection(TCP)连接 $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 声明交换机 $channel-&gt;exchange_declare(&apos;test&apos;, &apos;direct&apos;, false, true, false); // 声明队列 // 这里声明的是持久的, 排他的队列 $channel-&gt;queue_declare(&apos;hello&apos;, false, true, true, false); // 绑定队列和交换机 $channel-&gt;queue_bind(&apos;hello&apos;, &apos;test&apos;, &apos;hello&apos;); // 创建 接收消息的回调函数 $backCall = function ($message) &#123; echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; echo $message-&gt;delivery_info[&apos;delivery_tag&apos;]; echo &quot;\\n--------\\n&quot;; // 消息ack应答 $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; // 队列都排他了, 消费者必然也排他, 这里设置true/false都一样, 因此也设置为true $channel-&gt;basic_consume(&apos;hello&apos;,&apos;&apos;,false, false, true, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 消费者设置非排他队列(消费排他)如果消费者要设置非排他队列, 只是想消费排他, 只用设置basic_consume()的exclusive属性为true即可! 小结两种模式: 前者队列排他, 后者队列不排他 前者关闭消费者连接队列不删除, 后者关闭消费者连接队列会被删除 (不管队列是否是持久的) 前者队列只有一个消费者, 后者队列也只有一个消费者","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"CGI协议, CGI程序, FastCGI协议, FastCGI程序, PHP-FPM","slug":"PHP/2017-04-11-fpm","date":"2017-04-11T08:40:32.000Z","updated":"2018-05-09T12:15:57.000Z","comments":true,"path":"2017/04/11/PHP/2017-04-11-fpm/","link":"","permalink":"http://blog.renyimin.com/2017/04/11/PHP/2017-04-11-fpm/","excerpt":"","text":"网上很多相关这些概念的文章, 但不是不权威, 就是自相矛盾, 总之就是越看越糊涂; ThinkPHP官方讨论区的一篇文章争议就比较大, 有时间可以参阅一下 综合来看, 这篇文章比较权威 CGI协议: 只是一个协议 FastCGI协议: 也只是一个协议(不过是CGI协议升级后的协议) FastCGI程序: 实现了FastCGI协议的程序为FastCGI程序; php-cgi程序: 在php中的sapi/cgi/php-cgi就实现了FastCGI协议, 虽然它的名字取名为php-cgi, 但其实是支持FacsCGI协议的; PHP-FPM: 不但实现了FastCGI协议, 而且自己具有进程管理功能!","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"03. Exchange Type","slug":"rabbitmq/2017-04-10-rabbitmq-03","date":"2017-04-10T09:35:32.000Z","updated":"2018-03-14T01:14:40.000Z","comments":true,"path":"2017/04/10/rabbitmq/2017-04-10-rabbitmq-03/","link":"","permalink":"http://blog.renyimin.com/2017/04/10/rabbitmq/2017-04-10-rabbitmq-03/","excerpt":"","text":"RabbitMQ常用的Exchange Type有 fanout direct topic headers这四种(不过headers类型并不太实用,而且性能比较差,几乎再也用不到了) fanoutfanout类型的Exchange路由规则非常简单, 它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中 (忽略routingkey); directdirect类型的Exchange路由规则也很简单，它会把消息路由到那些Binding key与Routing key完全匹配的Queue中 ; topic前面讲到direct类型的Exchange路由规则是完全匹配binding key与routing key; 但这种严格的匹配方式在很多情况下不能满足实际业务需求。topic类型的Exchange在匹配规则上进行了扩展, 它与direct类型的Exchage相似, 也是将消息路由到binding key与routing key相匹配的Queue中, 但这里的匹配规则有些不同, 它约定: routing key为一个句点号.分隔的字符串(我们将被句点号.分隔开的每一段独立的字符串称为一个单词,如’stock.usd.nyse’、’nyse.vmw’、’quick.orange.rabbit’) binding key与routing key一样也是句点号’.’分隔的字符串 binding key中(注意不是routingkey)可以存在两种特殊字符 * 与 #, 用于做模糊匹配 其中 * 用于匹配一个单词 #” 用于匹配多个单词(可以是零个) 代码可参考本人github","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"02.php-amqplib库实例, 详细剖析各参数属性","slug":"rabbitmq/2017-04-10-rabbitmq-02","date":"2017-04-10T04:21:07.000Z","updated":"2018-03-14T01:12:30.000Z","comments":true,"path":"2017/04/10/rabbitmq/2017-04-10-rabbitmq-02/","link":"","permalink":"http://blog.renyimin.com/2017/04/10/rabbitmq/2017-04-10-rabbitmq-02/","excerpt":"","text":"接下来尝试写一个完整的基础版本的 生产者--消费者 demo, 并详细介绍一下常见属性的作用 (可以说非常详尽了);在编写中, 你会发现php-amqplib 中实现的属性还是比较多的, 不过有些属性的实现 rabbitmq不一定支持, 具体可参考; 基础准备 Mac安装rabbitmq比较简单, 自行搜索; Laravel中通过php-amqplib包来使用RabbitMQ; composer安装扩展包 composer require php-amqplib/php-amqplib 2.6.* 如下引入即可使用 12use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage; 更多细节查看php-qmqp 用法; 核心步骤 创建connection(TCP)连接 基于connection连接创建channle信道接下来就是通过channel信道与rabbit-server交互,向rabbit-server发出一些请求 设置信道消费投递策略这一步可根据需要选择有还是没有(通过使用basic_qos()的prefetch_count参数控制) 声明exchange这一步可以省略但是不推荐, 如果省略, 将会使用默认的名为(AMQP default) 的 隐式 exchange, direct 且 持久化)默认交换被隐式绑定到每个队列，其中路由密钥等于队列名称 无法显式绑定或取消绑定默认交换 它也不能被删除 声明queue 绑定队列 注意: 以上步骤最好在生产者和消费者中都实现, 因为你可能会在使用生产者发布消息之前启动消费者，所以也需要在使用消息之前确保队列已存在 接下来, 可以决定代码作为生产者来发送消息, 或者作为消费者来处理消息 生产者 (各参数详解)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206&lt;?phpnamespace App\\Http\\Controllers\\Test;use App\\Http\\Controllers\\Controller;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;use PhpAmqpLib\\Wire\\AMQPTable;class RabbitController extends Controller&#123; public function producer() &#123; // 1. 创建connection(TCP)连接 $connection = new AMQPStreamConnection( &apos;localhost&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;, // 默认使用的就是 / 这个vhost &apos;/&apos;, // 下面的这九个参数基本不会用到, 以后用到再做补充 false, &apos;AMQPLAIN&apos;, null, &apos;en_US&apos;, 3.0, 3.0, null, false, 0 ); // 2. 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 3. 通过信道, 向rabbit-server申请创建交换机 // 可以给交换机设置一些额外的信息 (会在浏览器界面上 每个交换机详情下的&quot;details&quot;中显示出来) // 注意: arguments需要是AMQPTable对象; // 另外, 第一次创建的时候设置好了额外信息, 如果想修改额外信息, 则需要删除交换机重新创建才行 // 因为在创建交换机时, 如果发现已经有该交换机, 是不会去重新创建的 $arguments = new AMQPTable([ &apos;arguments1&apos; =&gt; &apos;想写什么信息都行&apos;, &apos;arguments2&apos; =&gt; [ &apos;想写什么信息都行, 比如声明是那条业务线的&apos;, &apos;想写什么信息都行, 比如连接信息....&apos;, ] ] ); $res = $channel-&gt;exchange_declare( &apos;test&apos;, &apos;direct&apos;, // 如果passive设置为true: // rabbit-server会查看有没有名为test的exchange, 如果有就把名字什么的信息告诉你;没有就会直接报错 // 这个参数比较鸡肋, 不过倒是可以用来检查exchange是否存在 false, // 将exchange设置为持久的, 持久交换机在rabbit-server重启后会存在, 非持久的则会被清除 // 在使用中推荐设置为 true true, // 如果auto_delete设置为true, 当所有队列完成使用后, exchange将会被删除 // 注意, 没有队列绑定不算, 只有在绑定了队列,并且之后队列又被全部删除(队列也有该属性), 才会自动删除自己 (和durable无关) // 默认为true, 我们最好使用false false, // 如果设置为true, 表示将该路由设置为内部路由, 生产者将无法直接使用该交换机进行消息发布(发布就会报错) // 因为交换机还可以再绑定交换机 (可以参考:http://www.rabbitmq.com/e2e.html) false, // 默认是false, 则这个函数会有响应结果; 如果为true的话, 则没有响应结果, $res为null; false, // $arguments // ticket 先不解释 ); // var_dump($res); // 和nowait的设置有关 // 可以给队列设置一些额外的信息 (会在浏览器界面上&quot;queues&quot;下的&quot;details&quot;中显示出来) // 注意: arguments需要是AMQPTable对象; // 如果要设置优先级队列的话, 是需要在arguments参数中设置一个名为 x-max-priority 的元素来保证的 // 另外, 第一次创建的时候设置好了额外信息, 如果想修改额外信息, 则需要删除队列重新创建才行 // 因为在创建队列时, 如果发现已经有了该队列, 是不会去重新创建的 $arguments = new AMQPTable([ &apos;arguments1&apos; =&gt; &apos;想写什么信息都行&apos;, &apos;arguments2&apos; =&gt; [ &apos;想写什么信息都行, 比如声明是那条业务线的&apos;, &apos;想写什么信息都行, 比如连接信息....&apos;, ] ] ); // 4. 通过信道, 向rabbit-server申请创建队列 $res = $channel-&gt;queue_declare( // 队列名(后面来如果不显示绑定exchange与queue的话, 则默认将queue绑定到名为 (AMQP default) 的默认隐式交换机 (direct并且持久) // bindingkey与queue同名, 也为hello &apos;hello&apos;, // 如果为true: rabbit-server会查看有没有名为hello的queue, 如果有就把名字什么的信息告诉你; 如果没有就直接报错。(这个参数比较鸡肋, 不过倒是可以用来检查queue是否存在) // 而false就是没有则创建, 有就什么也不做 false, // true: 将queue设置为持久的, 持久队列在rabbit-server重启后会存在, 非持久的则会被清除 // 在使用中推荐为true true, // 如果设置为true, 则创建的为`排他队列` // 如果一个队列被声明为排他队列, 该队列仅对首次声明它的连接可见, 并在连接断开时自动删除。也就是说, 如果你在生产者中创建排他队列, 则连接结束, 队列就没了, 所以你可能一直看不到创建的队列; // 另外需要注意三点: // 1.排他队列是基于连接可见的, 同一连接的不同信道是可以同时访问同一个连接创建的排他队列的 // 2.首次，如果一个连接已经声明了一个排他队列, 其他连接是不允许建立同名的 排他/普通队列的 (这点和普通队列不同, 普通队列多次创建也不会报错) // 3.即使该队列是持久化的,一旦连接关闭或者客户端退出,该排他队列都会被自动删除的 // 所以, 排他队列只能由消费者创建, 而且这种队列适用于只有一个消费者消费消息的场景 false, // 自动删除(如果启用,那么队列将会在所有的消费者停止使用之后自动删除掉自身, 注意: 没有消费者不算, 只有在有了消费之后, 所有的消费者又断开后, 就会自动删除自己, 和durable无关) false, // 如果为false, 则这个函数会有响应结果; 如果为true的话, 则没有响应结果,$res为null false, // 传递给队列的一些详细信息 (可以在图形控制页 &quot;queues&quot;&gt;&quot;details&quot;中看到效果) $arguments //sticket 先不解释 ); // var_dump($res); // 和nowait的设置有关 // 5. 将queue与exchange使用 bindingkey 进行绑定 // 绑定的时候也可以设置一些额外绑定信息 $arguments = new AMQPTable([ &apos;arguments1&apos; =&gt; &apos;想写什么信息都行&apos;, &apos;arguments2&apos; =&gt; [ &apos;想写什么信息都行, 比如声明是那条业务线的&apos;, &apos;想写什么信息都行, 比如连接信息....&apos;, ] ] ); $res = $channel-&gt;queue_bind( &apos;hello&apos;, &apos;test&apos;, &apos;hello&apos;, false, $arguments //ticket 先不解释 ); // var_dump($res); //和nowait的设置有关 // 6. 一切准备就绪, 接下来该准备消息, 发布消息了 // 创建消息对象 $msg = new AMQPMessage( // 消息内容, 使用时可以发送json &apos;Hello World!&apos;, // 第二个数组参数稍微复杂 (可以参考:https://github.com/php-amqplib/php-amqplib/blob/b7b677d046a9735e0ad940d649feaf38d58c866c/doc/AMQPMessage.md) [ // 持久化标志: 如果你要持久化消息, 则需要设置如下配置项 &apos;delivery_mode&apos; =&gt; AMQPMessage::DELIVERY_MODE_PERSISTENT, // MIME content type &apos;content_type&apos; =&gt; &apos;text/plain&apos;, // 消息优先级: 0-9 // priority, // correlation_id：关联RPC的请求与响应; 服务端发送消息时的请求标识; // 消费者拿到correlation_id后会原封不动地返回给服务端; // 客户端用自己的correlation_id与服务端返回的id进行对比, 是我的，就接收; // &apos;correlation_id&apos; =&gt; &apos;testId&apos;, // 客户端设定 远程过程调用完, 服务端回复消息到哪个队列 (这个队列也需要提前声明) // &apos;reply_to&apos; =&gt; &apos;queue2&apos; // headers 参数为 AMQPTable 类型 // 另外还有 如下一些可以供你使用的属性参数(可以自己拿来看具体想怎么用) // message-id // user_id // app_id // type // timestamp ] ); $res = $channel-&gt;basic_publish( // 发送的消息对象 $msg, // 选择的exchange路由 &apos;test&apos;, // routingkey 用于和 bingkey 进行匹配 &apos;hello&apos;, // 当mandatory设置为true时，如果exchange根据自身类型和消息routeKey无法找到一个符合条件的queue, 那么会调用`basic.return`方法将消息返回给生产者； // 生产者需要使用set_return_listener来监听返回, 而且生产者也要做成像消费者那样是阻塞的,并且命令行启动, 否则接收不到 (所以综合来看, 这个选项目前来说, 没啥用) //当mandatory设置为false时, 出现上述情形broker会直接将消息扔掉 // immediate, ticket 以后再说 false ); var_dump($res); // 如果在上面发布消息时, 设置了mandastory属性为true, 生产者要阻塞监听回调, 如下// $returnListener = function ($replyCode, $replyText, $exchange, $routingKey, $message) &#123;// echo &quot;return: &quot;,// $replyCode, &quot;\\n&quot;,// $replyText, &quot;\\n&quot;,// $exchange, &quot;\\n&quot;,// $routingKey, &quot;\\n&quot;,// $message-&gt;body, &quot;\\n&quot;;// &#125;;// $channel-&gt;set_return_listener($returnListener);// while (true) &#123;// $channel-&gt;wait();// &#125; // 关闭信道 $channel-&gt;close(); // 关闭connection $connection-&gt;close(); &#125;&#125; 效果 消费者 (各参数详解)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596&lt;?phpnamespace App\\Console\\Commands;use Illuminate\\Console\\Command;use PhpAmqpLib\\Connection\\AMQPStreamConnection;use PhpAmqpLib\\Message\\AMQPMessage;class Consumer extends Command&#123; /** * The name and signature of the console command. * * @var string */ protected $signature = &apos;Consumer&apos;; /** * The console command description. * * @var string */ protected $description = &apos;测试消费者&apos;; public function __construct() &#123; parent::__construct(); &#125; /** * Execute the console command. * * @return mixed */ public function handle() &#123; // 创建connection(TCP)连接 $connection = new AMQPStreamConnection(&apos;127.0.0.1&apos;, 5672, &apos;guest&apos;, &apos;guest&apos;); // 基于TCP连接创建channel(信道, 一个connection可以创建很多信道) $channel = $connection-&gt;channel(); // 接下来将通过信道和rabbit-server进行交互 // 声明交换机 $channel-&gt;exchange_declare(&apos;test&apos;, &apos;direct&apos;, false, true, false); // 声明队列 // 这里声明的是持久的, 排他的队列 $channel-&gt;queue_declare(&apos;hello&apos;, false, true, false, false); // 绑定队列和交换机 $channel-&gt;queue_bind(&apos;hello&apos;, &apos;test&apos;, &apos;hello&apos;); // 创建 接收消息的回调函数 $backCall = function ($message) &#123; // 消息体在 $message 的body 属性中 // $message的delivery_info属性中的信息可以参考: https://github.com/php-amqplib/php-amqplib/blob/b7b677d046a9735e0ad940d649feaf38d58c866c/doc/AMQPMessage.md // 基本如下:// $delivery_info = array(// &quot;channel&quot; =&gt; $this,// &quot;consumer_tag&quot; =&gt; $consumer_tag, // 请记住, delivery_tag必须有效, 因此大部分时间只需使用服务器提供的那个// &quot;delivery_tag&quot; =&gt; $delivery_tag,// &quot;redelivered&quot; =&gt; $redelivered,// &quot;exchange&quot; =&gt; $exchange,// &quot;routing_key&quot; =&gt; $routing_key// ); //如果你不想直接访问delivery_info数组，你也可以使用$msg-&gt;get(&apos;delivery_tag&apos;), 但请记住，这比通过键访问数组慢。 echo &quot;\\n--------\\n&quot;; echo $message-&gt;body; echo $message-&gt;delivery_info[&apos;delivery_tag&apos;]; echo &quot;\\n--------\\n&quot;; // 消息ack应答 // 服务器分配的和通道专用的交付标签(暂时不用考虑) $message-&gt;delivery_info[&apos;channel&apos;]-&gt;basic_ack($message-&gt;delivery_info[&apos;delivery_tag&apos;]); &#125;; $res = $channel-&gt;basic_consume( // 消费者监听的队列 &apos;hello&apos;, // 消费者标签(如果此字段为空，则服务器将生成唯一标记,暂时不用考虑) &apos;&apos;, // 其实没什么用, 疑问rabbitmq 忽略了它https://github.com/php-amqplib/php-amqplib/issues/550 true, // 如果设置, 服务器不会期望这条消息的ack响应, 也就是说, 当消息传送到客户端时, 服务器会假定传送成功并立即将其出列 // 此功能可能会提高性能, 但会以可靠性为代价, 如果客户在交付给应用程序之前死亡, 则消息可能会丢失 // 所以还是不开启得好 ，默认就是false: 必须得到确认后, 消息才会出队列 false, // 以独占的模式进行消费, 只有该消费者能够访问本队列 // 如果只是队列设置为exclusive属性的话, 则其他消费者也可以消费队列中的内容, true, false, $backCall); while (count($channel-&gt;callbacks)) &#123; $channel-&gt;wait(); &#125; &#125;&#125; 参考官方文档","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"01.相关概念点详解","slug":"rabbitmq/2017-04-10-rabbitmq-01","date":"2017-04-10T03:50:02.000Z","updated":"2018-05-13T05:56:45.000Z","comments":true,"path":"2017/04/10/rabbitmq/2017-04-10-rabbitmq-01/","link":"","permalink":"http://blog.renyimin.com/2017/04/10/rabbitmq/2017-04-10-rabbitmq-01/","excerpt":"","text":"背景RabbitMQ是一个由Erlang开发的AMQP的开源实现; 引用百度百科: AMQP 即Advanced Message Queuing Protocol, 一个提供统一消息服务的应用层标准高级消息队列协议, 是应用层协议的一个开放标准, 为面向消息的中间件设计。 基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。 高级RabbitMQ集群使用各种工具进行监控编写RabbitMQ插件 应用场景 应用解耦假设系统A完成一步操作, 需要调用系统B,C完成一部分后续操作, 此时, 系统A对用户的响应就依赖于系统B,C完成后续操作的时间;而如果后续操作只是做一些及时性要求不是特别高的操作, 就没必要让用户等待这段操作的时间;此时就需要对系统间的调用进行异步解耦 系统调用中断如果系统B,C出现中断, 系统A中的操作已经完成(无法回滚), 那么整个流程就会不完整;此时就需要系统A的后续流程能够被保留重试; 流量削峰另外, 消息队列还可以用来缓解流量高峰, 对于高并发涌入的大量操作, 系统无法立即做出响应时, 可以将其丢入队列中, 以暂时缓解系统压力, 系统后续在进行逐一处理; … AMDQ协议就可以解决以上的问题, 而RabbitMQ实现了AMQP 基本概念先看一张整体架构图 RabbitMQ Server也叫broker server, 他的角色就是维护一条从Producer到Consumer的路线, 保证数据能够按照指定的方式进行传输;虽然这个保证也不是100%的保证, 但是对于普通的应用来说这已经足够了;(当然对于商业系统来说，可以再做一层数据一致性的guard, 就可以彻底保证系统的一致性了) vhost每个 virtual host 本质上都是一个RabbitMQ Server, 拥有它自己的 exchagne, queue, 和 bindings rule, consumer 等等; 这保证了你可以在多个不同的Application中使用RabbitMQ, 可以为每个应用定义一个vhost; RabbitMQ默认使用的是 / 这个vhost ; connectionconnection 其实就是一个TCP的连接, Producer 和 Consumer 都是通过TCP连接到RabbitMQ Server的, 后面会看到, 程序的起始就是建立这个TCP连接Connection是RabbitMQ的socket链接, 它封装了socket协议相关部分逻辑 channel channel 是我们与RabbitMQ打交道的最重要的一个接口, 我们大部分的业务操作是在channel这个接口中完成的 包括 定义exchange, 定义queue, 绑定exchange与queue、发布消息 等; channel 是虚拟连接, 它建立在上述的TCP连接 connection 中, 数据流动都是在channel中进行的 程序起始第一步就是建立TCP连接 connection, 第二步就是建立信道 channel; 为什么使用Channel, 而不是直接使用TCP连接？ 对于OS来说, 建立和关闭TCP连接是有代价的, 频繁的建立关闭TCP连接对于系统的性能有很大的影响, 而且TCP的连接数也有限制, 这也限制了系统处理高并发的能力; 但是，在TCP连接中建立channel是没有上述代价的, 对于 producer 或者 consumer 来说, 可以并发的使用多个channel进行 “publish” 或者 “receive”; publisher/producerproducer负责创建消息, 然后发布消息到broker server; 注意: producer永远不可能直接将消息丢到队列中; exchange(交换器) exchange是RabbitMQ的内部对象, 用于将消息路由到 queue; producer将消息发送到broker server后, broker server并不知道, 接下来消息需要发送给哪个queue, 所以broker server也不会直接将消息投递到某个队列中, 而是需要通过exchange交换机来判断消息该去哪里; 在RabbitMQ中, 生产者将消息直接投递到Queue中, 这种事情永远都不会发生; 实际的情况是, 生产者发送消息时, 会指定发送到RabbitMQ内部的哪个 exchange, 因为exchange可以绑定很多队列, 所以还需要指定发送给exchange上绑定的哪些队列(通过routingKey); exchange接收到消息之后, 通过匹配routingkey与bindingKey, 然后将消息路由到一个或多个与其绑定的queue中; 消息被exchange接收以后, 如果没有匹配的queue, 则消息会被丢弃! RabbitMQ常用的 Exchange Type 有 direct (是默认exchange类型)使用这种类型, 会将 生产者发送消息时设置的routing key 与 queue和exchange绑定时的bindingkey 进行匹配, 如果 routing key 和 binding key 完全一样, 那么Message就会被传递到相应的queue中;(在使用这个类型的Exchange时, 可以不必指定routing key的名字, 在此类型下创建的queue有一个默认的routing key, 这个routing key一般和queue同名) topic和上面一样, 只不过在匹配时, 可以使用 模式匹配, 比如 生产者发送消息时设置的routing key为ab*, 则可以与bingkey为ab开头的所有的queue匹配; fanout:使用这种类型的Exchange, 会忽略routing key的存在, 直接将message广播到所有与其绑定的queue中;每个队列中都会有该条消息; (可以做订阅发布) headers (类型并不太实用, 而且性能比较差, 几乎再也用不到了) 默认情况下, 如果不声明exchange 默认交换(AMQP default)被隐式绑定到每个队列, 其类型是direct, 也是持久化的, routingkey等于队列名称; 无法显式绑定或取消绑定默认交换, 它也不能被删除; queue(队列) queue 是RabbitMQ的 内部对象, 用于存储消息; RabbitMQ中的消息都只能存储在Queue中, 生产者生产消息并通过Exchange(交换机)最终投递到Queue中。 注意: 同一个vhost下, 不能声明同名队列, 即使是配置参数不同, 也会报错! consumer(消费者, 订阅者)多个消费者可以订阅同一个Queue, 这时Queue中的消息会被平均分摊给多个消费者进行处理, 而不是每个消费者都收到所有的消息并处理; prefetch_count正常情况下, 如果有多个消费者同时订阅同一个Queue中的消息, Queue中的消息会被平摊给多个消费者;但是, 如果每个消息的处理时间不同, 就有可能会导致某些消费者一直在忙, 而另外一些消费者很快就处理完手头工作并一直空闲的情况; 模拟场景此时, 可以通过设置prefetchCount来限制queue每次发送给每个消费者的消息数, 比如我们设置prefetchCount=1, 则queue每次给每个消费者发送一条消息, 消费者处理完这条消息后Queue会再给该消费者发送一条消息; ack消息确认 Message acknowledgment 在实际应用中, 可能会发生消费者收到queue中的消息, 但没有处理完成就宕机(或出现其他意外)的情况, 这种情况下就可能会导致消息丢失; 为了避免这种情况发生, 可以要求消费者在消费完消息后发送一个回执给RabbitMQ, RabbitMQ收到消息回执(Message acknowledgment)后才将该消息从queue中移除; 如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开, 则RabbitMQ会将该消息发送给其他消费者(如果存在多个消费者)进行处理; 这里不存在timeout概念, 一个消费者处理消息时间再长也不会导致该消息被发送给其他消费者; 如果开发人员在处理完业务逻辑后, 忘记发送回执给RabbitMQ, 这将会导致严重的bug——Queue中堆积的消息会越来越多; 消费者重启后会重复消费这些消息并重复执行业务逻辑; 消息确认中 no-ack 属性的设置为false: 此时为手动应答, 在这种情况下, 要求consumer在处理完接收到的 Basic.Deliver,Content-Header,Content-Body之后才回复Ack, 而这个Ack是AMQP协议中的Basic.Ack, 此Ack的回复是和业务处理相关的, 所以具体的回复时间应该要取决于业务处理的耗时; 如果是自动应答, consumer会在接收到Basic.Deliver,Content-Header,Content-Body之后, 立即回复Ack, 而这个Ack是TCP协议中的Ack, 此Ack的回复不关心consumer是否对接收到的数据进行了处理，当然也不关心处理数据所需要的耗时; 消息拒绝reject: 既然RabbitMQ提供了ACK某一个消息的命令, 当然也提供了Reject某一个消息的命令 当客户端发生错误, 调用basic.reject命令拒绝某一个消息时, 可以设置一个requeue的属性: 如果为true, 则消息服务器会重传该消息给下一个订阅者; 如果为false, 则会直接删除该消息; 当然,也可以通过ack,让消息服务器直接删除该消息并且不会重传。 RoutingKey, BindingKey真实情况下, 参数名都是RoutingKey，并没有BindingKey这个参数，为了区别 生产者发送消息时 和 Exchange-Queue绑定时 的概念, 我们才说RoutingKey和BindingKey; bindingKey: 在绑定(Binding)Exchange与Queue的时候, 一般会指定一个RoutingKey, 为了区分下面的RoutingKey, 此时这个RoutingKey叫BindingKey; routingKey: 消费者将消息发送给Exchange时, 一般会指定一个RoutingKey, 当BindingKey与RoutingKey相匹配时，生产者发送的消息才将会被路由到对应的Queue中; 在bind多个Queue到同一个Exchange的时候, 这些Binding允许使用相同的BindingKey (多重绑定); BindingKey并不是在所有情况下都生效, 它依赖于Exchange Type, 比如fanout类型的Exchange就会无视绑定时的BindingKey, 会直接将消息路由到所有绑定到该fanout-Exchange的Queues。 RabbitMQ中 用户 和 vhost 之间的关系: 可以创建用户并且给用户分配vhost作用域, 用户和vhost作用域之间是多对多的分配关系 可以在连接Rabbit服务的时候指定用户和作用域, 如下 :12// 主机地址, 端口, Rabbit用户, 密码, 分配给Rabbit用户的vhost域$connection = new AMQPStreamConnection(&apos;localhost&apos;, &apos;5672&apos;, &apos;tt&apos;, &apos;tt&apos;, &apos;test&apos;);","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"Trait","slug":"PHP/2017-03-20-Trait","date":"2017-03-20T11:36:23.000Z","updated":"2018-03-15T04:37:21.000Z","comments":true,"path":"2017/03/20/PHP/2017-03-20-Trait/","link":"","permalink":"http://blog.renyimin.com/2017/03/20/PHP/2017-03-20-Trait/","excerpt":"","text":"简介 php以前一直都是单继承的语言, 无法同时从两个基类中继承属性和方法; 为了解决这个问题, 自PHP5.4.0起, PHP实现了一种代码复用的方法, 称为trait; 它使开发人员能够自由地在不同层次结构内独立的类中复用 method, Trait 和 Class 组合的语义定义了一种减少复杂性的方式, 避免传统多继承 和 Mixin 类相关典型问题; 用法: 通过在类中使用 use 关键字, 声明要组合的Trait名称, 具体的Trait的声明使用Trait关键词, Trait不能实例化; 注意 使用trait的 本类和其基类中, 不能出现与trait中同名的属性, 可以使用同名方法; 一个类中可以插入多个trait, 通过 use关键字声明列出多个trait, to那个过逗号分隔; 冲突: 如果两个 trait 都插入了一个同名的方法，如果没有明确解决冲突将会产生一个致命错误; 为了解决多个 trait 在同一个类中的命名冲突, 需要使用 insteadof 操作符来明确指定使用冲突方法中的哪一个; 以上方式仅允许排除掉其它方法, as 操作符可以为某个方法引入别名, 注意, as 操作符不会对方法进行重命名, 也不会影响其方法; as 操作 还可以调整方法的访问控制 Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?phptrait A &#123; public function smallTalk() &#123; echo &apos;a&apos;; &#125; public function bigTalk() &#123; echo &apos;A&apos;; &#125;&#125;trait B &#123; public function smallTalk() &#123; echo &apos;b&apos;; &#125; private function bigTalk() &#123; echo &apos;B&apos;; &#125;&#125;class Talker &#123; use A, B &#123; B::smallTalk insteadof A; A::bigTalk insteadof B; &#125;&#125;$test = new Talker();$test-&gt;bigTalk(); // A中的bigTalk A$test-&gt;smallTalk(); // B中的smallTalk bclass Aliased_Talker &#123; use A, B &#123; B::smallTalk insteadof A; A::bigTalk insteadof B; // 本来上面已经定义的是, 使用A中的 bigTalk; // 但是下面又想使用B中的bigTalk，这样就需要为B重的bigTalk设置别名 并且下面还为B中的 bigTalk 冲洗你设置了访问权限和别名 B::bigTalk as public talk; &#125;&#125;$test = new Aliased_Talker();$test-&gt;smallTalk(); // b$test-&gt;bigTalk(); // A$test-&gt;talk(); // B","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"密码保护","slug":"PHP/2016-11-25-bcrypt","date":"2016-11-25T13:25:03.000Z","updated":"2018-03-14T02:05:30.000Z","comments":true,"path":"2016/11/25/PHP/2016-11-25-bcrypt/","link":"","permalink":"http://blog.renyimin.com/2016/11/25/PHP/2016-11-25-bcrypt/","excerpt":"","text":"关于密码存储的最佳实践是计算密码的哈希值,而不是加密用户的密码; 加密和哈希不是一回事, 加密是双向算法, 加密的数据可以解密, 而哈希是单向算法, 哈希后的数据不能再还原成原始值, 而且相同的数据得到的哈希值始终相同; 在数据库中存储用户的密码, 要先计算密码的哈希值, 然后在数据库中存储密码的哈希值, 如果黑客攻入数据库, 只能看到无意义的密码哈希值, 需要花费大量的时间和资源才能破解; 哈希算法有很多种(如md5、SHA1、bcrypt 和 scrypt), 有些算法速度很快, 用于验证数据完整性; 有些算法的速度则很慢, 旨在提高安全性; 生成密码和存储密码时要使用速度慢、安全性高的算法; 目前, 最安全的算法当属bcrypt，与md5和SHA1不同, bcrypt故意设计得很慢, bcrypt会自动加盐(salt), 防止潜在的彩虹表攻击, bcrypt算法会花费大量时间反复处理数据, 生成特别安全的哈希值。 在这个过程中, 处理数据的次数叫工作因子, 工作因子的值越高, 破解密码所需的时间越长, 安全性越好; bcrypt算法永不过时, 如果计算机运算速度变快了, 我们只需提高工作因子的值; 在Laravel中可以通过Hash门面来完成密码的生成, 检测, 及工作因子变更后密码的重新生成; 参考:Laravel文档Laravel文档","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"03. OAuth2.0的CSRF攻击","slug":"oauth/2016-10-22-OAuth-03","date":"2016-10-25T11:27:36.000Z","updated":"2018-03-14T01:33:36.000Z","comments":true,"path":"2016/10/25/oauth/2016-10-22-OAuth-03/","link":"","permalink":"http://blog.renyimin.com/2016/10/25/oauth/2016-10-22-OAuth-03/","excerpt":"","text":"场景假设有如下几个角色: 受害者 lant ; 攻击者 rymuscle ; 第三方客户端 liangren网 ; OAuth2服务提供平台 QQ ; 攻击流程 攻击者rymuscle 登录 liangren网 并且选择使用自己的 qq账号登录 ; liangren网 将 攻击者rymuscle 重定向到 qq的授权页, qq的授权页 向 攻击者rymuscle 显示 是否授权liangren网访问 ; 攻击者rymuscle 在点击”同意授权”之后, 截获 qq服务器返回的含有 Authorization Code 参数的HTTP响应; 然后 攻击者rymuscle 精心构造一个Web页面, 它会触发 liangren网 向 qq授权服务器 发起令牌申请的请求, 而这个请求中的Authorization Code参数正是上一步 攻击者rymuscle 截获到自己的code ; 攻击者rymuscle将这个Web页面放到互联网上, 等待或者诱骗受害者来访问 ; 假设受害者 lant 无意中访问了 攻击者rymuscle 准备的这个Web页面后, 令牌申请流程在 受害者lant 的浏览器里被顺利触发, laingren网 从 qq认证服务器 那里获取到access_token, 但是这个token以及通过它进一步获取到的用户信息却都是攻击者rymuscle的 ; 也就是最终 liangren网 将 攻击者rymuscle 的 qq 账号同 lant在liangren网的账号关联了起来 从此以后, lant只要没有察觉到自己最初使用的是别人的qq账号登录的, 那么lant在站点上所做的一切操作, rymuscle也可以使用qq登录进行查看; 攻击的关键点OAuth2的认证流程是分为好几步来完成的, 在标准oauth中, 第三方应用在收到一个GET请求时, 除了能知道当前用户的cookie, 以及URL中的Authorization Code之外, 难以分辨出这个请求到底是用户本人的意愿, 还是攻击者利用用户的身份伪造出来的请求;于是乎, 攻击者就能使用移花接木的手段, 提前准备一个含有自己的Authorization Code的请求, 并让受害者的浏览器来接着完成后续的令牌申请流程 ; 攻击难点 及 解决方案 尽管这个攻击既巧妙又隐蔽, 但是要成功进行这样的CSRF攻击也是比较困难的 整个攻击必须在短时间内完成, 因为OAuth2提供者颁发的Authorization Code有效期很短, OAuth2官方推荐的时间是不大于10分钟, 而一旦Authorization Code过期那么后续的攻击也就不能进行下去了; 一个Authorization Code只能被使用一次, 如果OAuth2提供者收到重复的Authorization Code, 它会拒绝当前的令牌申请请求, 不止如此, 根据OAuth2官方推荐, 它还可以把和这个已经使用过的Authorization Code相关联的access_token全部撤销掉, 进一步降低安全风险; 其实貌似只要做到Authorization Code只能被使用一次, 就可以防止csrf在此处的攻击了, 因为 rymuscle 在攻击的时候, 一旦获得 Authorization Code, 第三方站点服务器就会使用 Authorization Code 去申请access_token, 然后只要标记 Authorization Code 为已经使用, 那么 受害者lant 即使点击 攻击者rymuscle 构造好的链接也没用, 因为连接中的 Authorization Code 已经被标记为使用过了; 所以不一定非要使用下面的state参数来进行防御 比如微信公众平台的OAuth授权: state参数就是可选的 新浪开放平台的OAuth授权, state参数也是可选的 当然, 他们不一定做的是和此处一样的防御方案, 但明显不依赖于state参数来解决问题; state参数防御: 要防止这样的攻击其实很容易, 作为第三方应用的开发者, 只需在OAuth认证过程中加入 state 参数, 并验证它的参数值即可; 在将用户重定向到OAuth2的Authorization Endpoint去的时候, 为用户生成一个随机的字符串, 并作为state参数加入到URL中 ; 在收到OAuth2服务提供者返回的Authorization Code请求的时候, 验证接收到的state参数值, 如果是正确合法的请求, 那么此时接受到的参数值应该和上一步提到的为该用户生成的state参数值完全一致, 否则就是异常请求; 但需要注意 state参数 需要具备下面几个特性: 不可预测性: 足够的随机, 使得攻击者难以猜到正确的参数值 ;如果你每次生成的state都被放在一起, 比如一个库/缓存中存在很多state;那么问题就是攻击者还是可以拿着自己的code再加上一个state, 来构造一个链接欺骗用户来点击;(假设state正好就在你的库/缓存中); 重点是关联性: state参数值可以和当前用户会话(user session)相互关联的所以应该让state和具体的用户关联起来, 虽然用户还没有登录, 但是也可以让state放到session中 ;然后攻击者要猜测出来一个state的话, 即便是已经生成过了, 但是也得正好攻击的是这个用户; 唯一性: 每个用户每次请求生成的state参数值都是唯一的 ; 时效性: state参数一旦被使用则立即失效 ; 参考 蚂蚁金服开放平台: 其实可以结合以上各种方法一起来进行防御! 参考 移花接木参考 state参数漏洞参考 阮一峰","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"02. 授权码模式","slug":"oauth/2016-10-21-OAuth-02","date":"2016-10-21T07:27:36.000Z","updated":"2018-03-12T09:16:01.000Z","comments":true,"path":"2016/10/21/oauth/2016-10-21-OAuth-02/","link":"","permalink":"http://blog.renyimin.com/2016/10/21/oauth/2016-10-21-OAuth-02/","excerpt":"","text":"前言之前已经简单了解了授权码模式流程, 具体如何开发实现? 开发流程可参考 腾讯开放平台 使用Authorization_Code获取Access_Token 新浪Authorization_Code授权机制 微信网页Authorization_Code授权机制","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"01. OAuth(Open Authorization)开放式授权协议","slug":"oauth/2016-10-21-OAuth-01","date":"2016-10-21T03:05:08.000Z","updated":"2018-04-25T07:58:43.000Z","comments":true,"path":"2016/10/21/oauth/2016-10-21-OAuth-01/","link":"","permalink":"http://blog.renyimin.com/2016/10/21/oauth/2016-10-21-OAuth-01/","excerpt":"","text":"背景OAuth是一个关于授权(authorization)的开放网络标准, 目前的版本是2.0版 第三方站点传统授权的弊端聊起OAuth, 可能最常见的就是很多第三方站点接入的 QQ登录, 微信登录, 新浪微博登录, Github登录等; OAuth之前的 传统授权 比较 简单, 直接, 暴力, 一般是用户直接将自己 资源服务器(qq,微信,Github) 的账号和密码给客户端第三方站点, 这种做的法弊端太多: 如果用户对所有的第三方站点都提供自己 资源服务器 的 账号+密码, 那将会存在严重安全隐患; 用户无法设定第三方站点的权利范围; 用户想收回第三方站点的权利不太方便;用户只有修改密码, 才能收回赋予第三方网站的权力, 否则第三方网站将会永久拥有用户授权;但是用户直接修改密码后, 又会使得所有获得用户授权的第三方应用程序全部失效;虽然一些良心第三方可以设置 用户账号 和 资源服务器账号 的绑定和解绑, 但不一定所有第三方都会给你做 (csdn就做了与用户的QQ,微博等资源服务器的解绑功能) 而OAuth的授权不会让 第三方站点 触及到用户在 资源服务器 上的帐号信息(如用户名与密码), 即, 第三方站点无需使用用户资源服务器上的账号与密码, 就可以获得该用户在 资源服务器 上的资源, 因此 OAuth 相比于直接使用账号密码是安全的; 内部微服务系统间的授权 其实对于内部系统之间接口互调时的授权, 直接使用 账号+密码(AppID+AppSecret) 和 使用 token 没有什么太大区别, 都是内部系统, 也不会容易出现账号和密码泄露的问题; 和每次请求发送token一样, 每次请求需要发送当前服务的 账号+密码(AppID+AppSecret), 都可以加scope参数来做权限设置; (注意这种权限是针对服务的, 不是针对服务端的用户的) 仍然存在的问题: 每次请求都直接发送账号和密码还是不太安全, 外一就有泄露了呢?所以最好使用具有时效性的token, 即使被泄露, 也不是永久泄露, 相对于永久不变或者很久才变更一次的密码来说, token的 时效性 带来的安全提升要更大一些; 内部系统用户与系统的授权 典型的就是当前很多公司做的APP开发, 客户端与后台做数据交互的场景; 腾讯code: http://wiki.open.qq.com/wiki/website/%E4%BD%BF%E7%94%A8Authorization_Code%E8%8E%B7%E5%8F%96Access_Token 场景: 公司对接第三方登录, 你需要了解OAuth协议; 公司内部做授权中心(SSO(Single Sign On)单点登录, ) 几个名词定义1.第三方应用程序: 又称 客户端(client) 比如你的站点为了实现让用户快捷登录, 快速引流的目的, 需要对接QQ, 微博等第三方登录, 虽然对你的站点来说, QQ, 微博这些都是第三方站点; 但是从全局上来说, 服务提供商开发的认证服务器是要给第三方的站点授权的, 你的站点才是第三方站点, 是要用户 QQ,微博 这些资源服务器上的授权, 之后, 你的站点才能具有 使用用户QQ微博账号登录, 并且获取QQ头像….权利; 2.服务提供商: 也就是拥有资源服务器, 认证服务器的厂商 (比如QQ, 微博) 3.认证服务器: 即服务提供商专门用来处理认证的服务器; (比如QQ, 微博的认证服务器) 4.资源服务器: 即服务提供商存放用户生成的资源的服务器(它与认证服务器,可以是同一台服务器,也可以是不同的服务器) OAuth2.0四种授权类型OAuth2.0协议定义了用于获得授权的”四种主要授权类型” 授权码模式Authorization code1.授权码模式是功能最完整、流程最严密的授权模式(标准的Server授权模式, 非常适合Server端的Web应用); 2.它的特点是: 通过客户端的后台服务器 与 服务提供商的认证服务器 进行互动; 3.场景: 比如, 公司需要对接 QQ, 微博, 微信(网页授权) 等第三方登录授权; 4.授权流程图 5.流程解析 (A) 用户 访问 第三方客户端(比如点击了第三方客户端上的QQ登录), 则第三方客户端会将用户导向 服务提供商的认证服务器; (B) 用户选择是否给予 第三方客户端 授权; (C) 假设用户给予授权, 服务提供商的认证服务器 会将用户导向 第三方客户端 事先指定的 重定向URI(redirection URI), 同时附上一个授权码; (D) 第三方客户端 收到授权码, 附上早先的重定向URI, 向认证服务器申请令牌, 这一步是在第三方客户端的后台服务器上完成的, 对用户不可见; (E) 服务提供商的认证服务器 核对了 授权码 和 重定向URI, 确认无误后, 向客户端发送访问 令牌(access token) 和 更新令牌(refresh token); 隐式授权模式Implicit Grant1.也叫简化模式, 该模式不通过 第三方客户端 的服务器, 而是直接在浏览器中向 服务提供商的认证服务器 申请令牌, 跳过了授权码这个步骤, 因此得名; 2.它的特点是: 用户是以单个用户的名义, 所有步骤在浏览器中完成, 令牌对访问者是可见的; 3.场景: 密码模式Resource Owner Password Credentials1.用户向客户端提供自己的用户名和密码, 客户端使用这些信息向 “服务商提供商” 索要授权 ; 2.它的特点是: 客户端仍然是以单个用户的名义向服务提供商进行认证, 这种模式要求用户提供用户名和密码来交换访问令牌 access_token ; 在这种模式中, 用户必须把自己的密码给第三方, 但是第三方不得储存密码, 这通常用在用户对客户端高度信任的情况下, 比如客户端也是系统的一部分; 第三方站点, 认证服务器, 用户都是一家, 用户对第三方站点足够信任，才能采取这种模式来实现 (比如前后端分离的token授权模式) 3.场景: 比如公司做前后端分离, 貌似就可以使用这种密码模式来生成token给前端使用; 公司如果有多套内部后台系统, 开发人员和公司管理员可能就要准备多套账号, 比较麻烦, 为了解决这个问题, 可以做一个账号中心系统, 用户在登录各个系统后台的时候, 会先跳转到用户中心进行登录, 一旦登录成功之后, 就会给用户分发一个access_token, 用来在各个系统间作为登录认证 (这也实现了SSO单点登录); 4.流程图: 客户端模式Client Credentials1.客户端模式指第三方客户端以站点的名义, 而不是以单个用户的名义, 向服务提供商进行认证; 2.场景 服务器 不提供像用户数据这样的重要资源, 仅仅是一些开放的功能性API; (例如微信公众平台, Google Storage或Amazon S3 等开放平台提供的基础服务接口) 比如: 微信公众平台的开放接口其实就是使用这种方式, 因为我们的微信公众号是作为一个客户端来调用微信基础的功能性服务的 (而微信的网页授权采用的是授权码模式, 因为针对的是每个具体的用户进行授权的);(如果公司实现了一套基础服务的Api(都是些基础功能接口, 并不涉及用户数据这种重要资源), 就可以通过这种方式提供给内部其他系统通过认证的方式来调用; 公司如果实力强悍的话, 也可以将公司开发的基础服务Api公开出来 供外部其他第三方站点服务器 来调用) 3.流程图: 资料微信公众平台阮一峰博文博文","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"05.WebSocket - 实战","slug":"WebSocket/2017-10-29-websocket-05","date":"2016-10-19T14:09:10.000Z","updated":"2018-03-16T09:49:21.000Z","comments":true,"path":"2016/10/19/WebSocket/2017-10-29-websocket-05/","link":"","permalink":"http://blog.renyimin.com/2016/10/19/WebSocket/2017-10-29-websocket-05/","excerpt":"","text":"ThinkPHP + WorkerMan + WebSocket 实现节本聊天室功能git仓库地址~~未完待续","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"04.WebSocket - 客户端API","slug":"WebSocket/2017-10-28-websocket-04","date":"2016-10-19T12:50:52.000Z","updated":"2018-03-16T09:46:43.000Z","comments":true,"path":"2016/10/19/WebSocket/2017-10-28-websocket-04/","link":"","permalink":"http://blog.renyimin.com/2016/10/19/WebSocket/2017-10-28-websocket-04/","excerpt":"","text":"WebSocket客户端API如下WebSocket 构造函数 WebSocket对象作为一个构造函数, 用于新建 WebSocket 实例, 执行 var ws = new WebSocket(&#39;ws://localhost:8080&#39;); 之后, 客户端就会与服务器进行连接。 ws.readyState: readyState属性返回实例对象的当前状态,共有四种: CONNECTING: 值为0，表示正在连接。 OPEN: 值为1，表示连接成功，可以通信了。 CLOSING: 值为2，表示连接正在关闭。 CLOSED: 值为3，表示连接已经关闭，或者打开连接失败。 下面是一个示例:1234567891011121314151617switch (ws.readyState) &#123; case WebSocket.CONNECTING: // do something break; case WebSocket.OPEN: // do something break; case WebSocket.CLOSING: // do something break; case WebSocket.CLOSED: // do something break; default: // this never happens break;&#125; ws.onopen: 实例对象的onopen属性 用于指定连接成功后的回调函数: 123ws.onopen = function () &#123; ws.send(&apos;Hello Server!&apos;);&#125; - 如果要指定多个回调函数，可以使用addEventListener方法。 123456 ws.addEventListener(&apos;open&apos;, function (event) &#123; ws.send(&apos;Hello Server!&apos;); &#125;); ``` 4. ws.onclose - 实例对象的onclose属性，用于指定连接关闭后的回调函数 ws.onclose = function(event) { var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event }; ws.addEventListener(&quot;close&quot;, function(event) { var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event }); 125. ws.onmessage - 实例对象的onmessage属性，用于指定收到服务器数据后的回调函数 ws.onmessage = function(event) { var data = event.data; // 处理数据 }; ws.addEventListener(&quot;message&quot;, function(event) { var data = event.data; // 处理数据 }); 1- 注意, 服务器数据可能是文本, 也可能是二进制数据(`blob`对象或`Arraybuffer`对象) ws.onmessage = function(event){ if(typeof event.data === String) { console.log(&quot;Received data string&quot;); } if(event.data instanceof ArrayBuffer){ var buffer = event.data; console.log(&quot;Received arraybuffer&quot;); } } 1- 除了动态判断收到的数据类型, 也可以使用`binaryType`属性, 显式指定收到的二进制数据类型 // 收到的是 blob 数据 ws.binaryType = &quot;blob&quot;; ws.onmessage = function(e) { console.log(e.data.size); }; // 收到的是 ArrayBuffer 数据 ws.binaryType = &quot;arraybuffer&quot;; ws.onmessage = function(e) { console.log(e.data.byteLength); }; 12346. webSocket.send(): 实例对象的send()方法用于向服务器发送数据。 - 发送文本的 `ws.send(&apos;your message&apos;);` - 发送 `Blob` 对象的例子 var file = document .querySelector(&apos;input[type=&quot;file&quot;]&apos;) .files[0]; ws.send(file); 1- 发送 `ArrayBuffer` 对象的例子 // Sending canvas ImageData as ArrayBuffer var img = canvas_context.getImageData(0, 0, 400, 320); var binary = new Uint8Array(img.data.length); for (var i = 0; i &lt; img.data.length; i++) { binary[i] = img.data[i]; } ws.send(binary.buffer); 127. webSocket.bufferedAmount - 实例对象的 `bufferedAmount`属性, 表示还有多少字节的二进制数据没有发送出去, 它可以用来判断发送是否结束 var data = new ArrayBuffer(10000000); ws.send(data); if (ws.bufferedAmount === 0) { // 发送完毕 } else { // 发送还没结束 } 128. webSocket.onerror - 实例对象的onerror属性, 用于指定报错时的回调函数 socket.onerror = function(event) { // handle error event }; socket.addEventListener(&quot;error&quot;, function(event) { // handle error event }); ``` 关闭连接: ws.close()","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"03.跨站点WebSocket劫持漏洞","slug":"WebSocket/2017-10-28-websocket-03","date":"2016-10-17T09:30:42.000Z","updated":"2018-03-05T03:39:56.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-03/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-03/","excerpt":"","text":"跨站点WebSocket劫持漏洞原理是劫持而非简单的伪造 为了创建全双工通信, 客户端需要基于HTTP进行握手切换到WebSocket协议, 正是这个升级协议的握手过程存在着潜在的问题。 示例: 客户端伪造 与服务端同顶级域名 的跨子域cookie 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; document.cookie = &quot;name=renyimin; domain=websocket.org&quot;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 客户端与websocket.org网站的Echo测试服务建立连接 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; var ws = new WebSocket(&quot;ws://echo.websocket.org&quot;); ws.onopen = function(evt) &#123; console.log(&quot;Connection open ...&quot;); ws.send(&quot;Hello WebSockets!&quot;); &#125;; ws.onmessage = function(evt) &#123; console.log( &quot;Received Message: &quot; + evt.data); ws.close(); &#125;; ws.onclose = function(evt) &#123; console.log(&quot;Connection closed.&quot;); &#125;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 此时, 握手(升级协议)阶段的请求头中就携带了伪造的cookie 上面握手阶段Get请求头中可以看到, Cookie头部把同顶级域名下的Cookie信息发送到服务器端。 WebSocket协议没有规定服务器在握手阶段应该如何认证客户端身份。服务器可以采用任何HTTP服务器的客户端身份认证机制, 譬如cookie, HTTP基础认证, TLS身份认证等。 而对于绝大多数Web应用来说, 客户端身份认证应该都是 SessionID 等 Cookie 或者 HTTP Auth 头部参数等。 所有的浏览器都会发送 Origin 请求头, 如果服务器端没有针对Origin头部进行验证可能会导致跨站点 WebSocket 劫持攻击。 假设某个用户已经在浏览器登录过一个目标站点, 接下来假设他被诱骗访问某个恶意网站, 一旦点击某个诱导性的按钮, 恶意网页在这个操作中植入一个WebSocket握手请求申请跟目标应用建立WebSocket连接 此时, 这个websocket握手请求将会携带浏览器cookie罐中之前存储的目标站点cookie; 请注意, Origin 和 Sec-WebSocket-Key 都是由浏览器自动生成, Cookie等身份认证参数也都是由浏览器自动上传到目标应用服务器端。如果服务器端疏于检查Origin, 该请求伪造的cookie可能就会成功握手切换到 WebSocket 协议，恶意网页就可以成功绕过身份认证连接到 WebSocket 服务器，进而窃取到服务器端发来的信息，抑或发送伪造信息到服务器端篡改服务器端数据。 此时, 熟悉cors跨域的你, 可能会认为服务器端没有指定Access-Control-Allow-Origin的话， 浏览器端的脚本是无法访问跨域资源的, 这确实也是 HTML5 带来的新特性之一。但是很不幸，CORS不适应于 WebSocket, WebSocket 没有明确规定跨域处理的方法。 CSRF主要是通过恶意网页悄悄发起数据修改请求, 而跨站点WebSocket伪造攻击不仅可以修改服务器数据, 还可以控制整个读取/修改双向沟通通道。正是因为这个原因, Christian 将这个漏洞命名为劫持(Hijacking), 而不是请求伪造(Request Forgery)。 检测跨站点 WebSocket 劫持漏洞 明白跨站点 WebSocket 劫持漏洞原理后, 检测该漏洞也就容易了, 先撇开cookie伪造, 首先检测服务端是否能拒绝不同源origin 简单来说就是针对WebSocket握手请求地址, 修改请求中的 Origin 为其他来源(本地可以随意配置虚拟域名的), 然后发送这个请求, 看看服务器是否能够成功返回101响应。 如果连接失败, 那么说明这个 WebSocket 是安全的, 因为它可以正确拒绝来自不同源(Origin)的连接请求。 如果连接成功, 就说明服务器端没有执行源检查, 为了严谨起见, 最好进一步测试是否可以发送 WebSocket 消息, 如果这个 WebSocket 连接能够发送/接受消息的话, 则完全证明跨站点 WebSocket 劫持漏洞的存在。 防范跨站点WebSocket劫持攻击服务端Origin检查 前文介绍了跨站点 WebSocket 劫持漏洞原理和检测, 如何防范这个漏洞? 这个漏洞的原理听起来略微复杂, 但幸运的是测试起来相对比较简单, 那么修复会不会也很简单? 很多读者会想到, 不就是在服务器代码中检查 Origin 参数嘛, 是的, 检查Origin很有必要，但不充分。 推荐在服务器端的代码中增加Origin检查, 如果客户端发来的 Origin 信息来自不同域, 建议服务器端拒绝这个请求, 发回 403 错误响应拒绝连接。 WebSocket令牌机制 仅仅检查 Origin 远远不够, 别忘记了, 如果 WebSocket 的客户端不是浏览器, 非浏览器的客户端发来的请求不会项浏览器那样自动设置Origin, 恶意客户端也就可能伪造 Origin 头信息。 所以更彻底的解决方案还是要借鉴CSRF的解决方案 – 令牌机制 服务器端为每个 WebSocket 客户端生成唯一的一次性 Token; 客户端将Token作为 WebSocket 连接 URL 的参数(譬如 ws://echo.websocket.org/?token＝randomOneTimeToken), 发送到服务器端进行 WebSocket 握手连接; 服务器端验证 Token 是否正确, 一旦正确则将这个 Token 标示为废弃不再重用, 同时确认 WebSocket 握手连接成功; 如果 Token 验证失败或者身份认证失败, 则返回 403 错误; 这个方案里的 Token 设计是关键 推荐的方案是为登录用户生成一个 Secure Random 存储在 Session 中，然后利用对称加密(譬如 AES GCM)加密这个Secure Random值作为令牌, 将加密后的令牌发送给客户端用来进行连接。 这样每个 Session 有一个唯一的随机数, 每个随机数可以通过对称加密生成若干份一次性令牌。用户即便通过不同终端通过 WebSocket 连接到服务器, 服务器可以在保障令牌唯一且一次性使用的前提下, 依然能将不同通道中的信息关联到同一用户中。 参考","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"02.WebSocket简单示例分析","slug":"WebSocket/2017-10-28-websocket-02","date":"2016-10-17T05:57:08.000Z","updated":"2018-03-16T09:47:40.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-02/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-02/","excerpt":"","text":"客户端简单示例1.下面通过 客户端与websocket.org网站的Echo测试服务建立连接 来进行学习 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; var ws = new WebSocket(&quot;wss://echo.websocket.org&quot;); ws.onopen = function(evt) &#123; console.log(&quot;Connection open ...&quot;); ws.send(&quot;Hello WebSockets!&quot;); &#125;; ws.onmessage = function(evt) &#123; console.log( &quot;Received Message: &quot; + evt.data); ws.close(); &#125;; ws.onclose = function(evt) &#123; console.log(&quot;Connection closed.&quot;); &#125;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 2.chrome抓包发现, 实际发送了两次请求, 因为WebSocket协议由两阶段组成: 握手阶段 和 数据传输阶段 握手(handshake)阶段 请求头信息 12345678910111213GET wss://echo.websocket.org/ HTTP/1.1Host: echo.websocket.orgConnection: UpgradePragma: no-cacheCache-Control: no-cacheUpgrade: websocketOrigin: http://www.js.comSec-WebSocket-Version: 13User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36Accept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9Sec-WebSocket-Key: VxdjZ3Z53lPcCrB3OiVwJQ==Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits 为了建立一个 WebSocket 连接, 在握手阶段, 客户端浏览器首先要向服务器发起一个HTTP请求, 这个请求和通常的HTTP请求不同, 包含了一些附加头信息, 其中附加头信息 Upgrade: WebSocket 表明这是一个申请协议升级的 HTTP 请求, 服务器端解析这些附加的头信息然后产生应答信息返回给客户端, 客户端和服务器端的WebSocket连接就建立起来了, 双方就可以通过这个连接通道自由的传递信息, 并且这个连接会持续存在直到客户端或者服务器端的某一方主动的关闭连接。 首部字段的核心是 Connection:Upgrade 和 Upgrade:websocket这两个首部字段, 相当于告诉服务器端:我要申请切换到 WebSocket 协议。 响应头信息 12345678910111213HTTP/1.1 101 Web Socket Protocol HandshakeAccess-Control-Allow-Credentials: trueAccess-Control-Allow-Headers: content-typeAccess-Control-Allow-Headers: authorizationAccess-Control-Allow-Headers: x-websocket-extensionsAccess-Control-Allow-Headers: x-websocket-versionAccess-Control-Allow-Headers: x-websocket-protocolAccess-Control-Allow-Origin: http://www.js.comConnection: UpgradeDate: Fri, 02 Mar 2018 03:01:59 GMTSec-WebSocket-Accept: GAz+a+FeezkKAeHM/HK/fgFI9Wo=Server: Kaazing GatewayUpgrade: websocket 一旦服务器端返回101响应, 即可完成WebSocket协议切换。服务器端即可以基于相同端口, 将通信协议从 http:// 或 https:// 切换到 ws://或 wss://。 协议切换完成后, 浏览器和服务器端即可以使用 WebSocket API 互相发送和收取文本和二进制消息。 这里要解释一些涉及 WebSocket 安全相关的重要头部参数: Sec-WebSocket-Key , Sec-WebSocket-Accept: 客户端负责生成一个Base64编码过的随机数字作为Sec-WebSocket-Key, 服务器则会将一个GUID和这个客户端的随机数一起生成一个散列Key作为 Sec-WebSocket-Accept 返回给客户端。这个工作机制可以用来避免缓存代理(caching proxy), 也可以用来避免请求重播(request replay)。 其他Sec－开头的WebSocket相关Header其实也是WebSocket设计者为了安全的特意设计, 以Sec-开头的Header可以避免被浏览器脚本读取到,这样攻击者就不能利用 XMLHttpRequest 伪造 WebSocket 请求来执行跨协议攻击，因为 XMLHttpRequest 接口不允许设置 Sec- 开头的 Header。 Origin: 作安全使用, 防止跨站攻, 浏览器一般会使用这个来标识原始域 小结 浏览器通过JavaScript向服务器发出建立WebSocket连接的请求, 连接建立以后, 客户端和服务器端就可以通过 TCP 连接直接交换数据, 此后服务端与客户端通过此TCP连接进行实时通信, 不再需要原HTTP协议的参与了; 因为 WebSocket 连接本质上就是一个TCP连接, 所以在数据传输的稳定性和数据传输量的大小方面, 和轮询以及 Comet 技术比较, 具有很大的性能优势; Websocket.org 网站对传统的轮询方式和 WebSocket 调用方式作了一个详细的测试和比较，将一个简单的 Web 应用分别用轮询方式和 WebSocket 方式来实现，在这里引用一下他们的测试结果图：","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"01.引出WebSocket","slug":"WebSocket/2017-10-28-websocket-01","date":"2016-10-17T02:40:16.000Z","updated":"2018-03-16T09:47:19.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-01/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-01/","excerpt":"","text":"前言 传统模式的 Web 系统使用http协议在服务器和客户端之间进行通信, 是以客户端发出请求,服务器端响应的方式进行工作的, 服务器是无法主动的推送数据给客户端的, 这也是为了安全起见; 既然已经有了HTTP协议，为什么还需要另一个协议? 答案很简单, 因为HTTP协议有一个缺陷: 通信只能由客户端发起。 由于HTTP协议做不到服务器主动向客户端推送信息, 这种单向请求的特点就注定了, 如果服务器有连续频繁的状态变化, 客户端要获知就非常麻烦。(我们只能使用”轮询”：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室) 而现代web开发中, 经常会面对一些对实时性要求比较高的实时Web应用, 比如: 即时通信系统, 即时报价系统, 在线游戏, 在线证券, 设备监控等, 保持客户端和服务器端的信息同步是 实时Web应用 的关键要素。 传统窘境简介在 WebSocket 规范出来之前, 开发人员想实现这些实时的Web应用, 不得不采用一些折衷的方案, 其中最常用的就是 轮询 (Polling), 基于客户端套接口的&quot;服务器推&quot;技术, Comet 技术, 而Comet技术实际上是轮询技术的改进, 又可细分为两种实现方式: 一种是长轮询机制, 一种称为流技术。 下面简单介绍一下 ajax短轮询(Polling) 和 Comet长轮询, 其他请自行搜索资料~~ 传统Ajax短轮训(Polling) 这是最早的一种实现实时 Web 应用的方案, 主要就是客户端(浏览器), 以一定的时间间隔向服务端发出请求http请求, 以频繁请求的方式来保持客户端和服务器端的同步 ; 这种方案最大的问题就是: 你无法给出客户端发送请求的适当时间间隔 如果客户端(浏览器)发送请求的间隔时间过小, 服务器端的数据可能并没有更新, 这样会带来很多无谓的网络传输, 这样既浪费了网络带宽, 又浪费了CPU的利用率 ; 如果客户端(浏览器)发送请求的间隔时间过大, 在服务器端的数据更新很快时, 又不能保证客户端获取数据的实时性 ; Comet最近几年,因为 AJAX 技术的普及,以及把 IFrame 嵌在”htmlfile”的 ActiveX 组件中可以解决 IE 的加载显示问题, 一些受欢迎的应用如 meebo，gmail+gtalk 在实现中使用了这些新技术, 同时”服务器推”在现实应用中确实存在很多需求。因为这些原因, 基于纯浏览器的”服务器推”技术开始受到较多关注, Alex Russell(Dojo Toolkit 的项目 Lead)称这种基于 HTTP 长连接、无须在浏览器端安装插件的”服务器推”技术为Comet。 基于 AJAX 的长轮询long-polling 基于客户端套接口的”服务器推”技术, 需要在浏览器端安装插件, 基于套接口传送信息, 或是使用 RMI、CORBA 进行远程调用 ; 使用 AJAX 实现”服务器推” 与 传统的 AJAX 应用不同之处在于: 服务器端会阻塞请求直到有数据传递或超时才返回; 客户端 JavaScript 响应处理函数会在处理完服务器返回的信息后,再次发出请求,重新建立连接; 当客户端处理接收的数据、重新建立连接时, 服务器端可能有新的数据到达, 这些信息会被服务器端保存直到客户端重新建立连接, 客户端会一次把当前服务器端所有的信息取回; 基于长轮询的服务器推模型如下: 相对于”短轮询”(poll), 这种长轮询方式也可以称为”拉”(pull), 这种方案基于 AJAX，具有以下一些优点: 请求异步发出; 无须安装插件; IE、Mozilla FireFox 都支持 AJAX; 综合这几种方案, 可以发现这些所谓的实时技术并不是真正的实时技术, 它们只是在用 Ajax 方式来模拟实时的效果, 在每次客户端和服务器端交互的时候都是一次HTTP的请求和应答的过程, 而每一次的 HTTP 请求和应答都带有完整的 HTTP 头信息, 这就增加了每次传输的数据量, 而且这些方案中客户端和服务器端的编程实现都比较复杂, 在实际的应用中, 为了模拟比较真实的实时效果, 开发人员往往需要构造两个 HTTP 连接来模拟客户端和服务器之间的双向通讯, 一个连接用来处理客户端到服务器端的数据传输, 一个连接用来处理服务器端到客户端的数据传输, 这不可避免地增加了编程实现的复杂度, 也增加了服务器端的负载, 制约了应用系统的扩展性。 WebSocket简介 WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，就像Socket一样。 WebSocket 是HTML5中定义的新协议, 它实现了真正的长连接, 实现了浏览器与服务器的全双工通信; HTML5 WebSocket设计出来的目的就是要取代 轮询 和 Comet 技术, 使客户端浏览器具备像 C/S 架构下桌面系统的实时通讯能力。 特点: 建立在 TCP 协议之上，服务器端的实现比较容易。 与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用HTTP协议，因此握手时不容易屏蔽，能通过各种HTTP代理服务器 数据格式比较轻量，性能开销小，通信高效 可以发送文本，也可以发送二进制数据 没有同源限制，客户端可以与任意服务器通信 协议标识符是ws(如果加密，则为wss),服务器网址就是 URL 协议由两阶段组成: 握手阶段(handshake) 和 数据传输阶段","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"07.\"Jsonp\" 对比 \"CORS简单/非简单请求\"","slug":"2016-09-18-sameoriginpolicy-07","date":"2016-09-18T13:20:16.000Z","updated":"2018-03-02T06:52:27.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-07/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-07/","excerpt":"","text":"Jsonp 对比 CORS简单/非简单请求都可以方便实现跨域; Jsonp 简单适用, 老式浏览器全部支持, 服务器端改动很小; 但是JSONP只能发GET请求; JSONP跨域发送Cookie的话, 只用设置 cookie的domain属性 为相同的顶级域名即可; CORS简单请求 服务端需要设置一些允许选项; 请求方法比较简单, 如 GET, POST, HEAD ; 跨子域共享cookie的话, 服务端和客户端都要对Credentials header属性进行设置; 另外, 跨子域的话, 如果想同时共享cookie的话, 服务端 Access-Control-Allow-Origin 不能设置为 *, 否则会提示 :123Failed to load http://test.test.com/index.php?sex=renyimin&amp;age=100: The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' when the request's credentials mode is 'include'. Origin 'http://www.test.com' is therefore not allowed access. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute. CORS非简单请求 服务端需要设置一些允许选项; 其他请求方法 PUT, DELETE…. 可以设置自定义header头 跨子域共享cookie方面 和 CORS简单请求一样","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"06.CORS方案 not-so-simple request","slug":"2016-09-18-sameoriginpolicy-06","date":"2016-09-18T12:10:16.000Z","updated":"2018-03-01T02:14:12.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-06/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-06/","excerpt":"","text":"预检请求 preflight 说明非简单请求是那种对服务器有特殊要求的请求, 比如请求方法是 PUT 或 DELETE, 或者 Content-Type 字段的类型是 application/json ; 非简单请求的CORS请求, 会在正式通信之前, 增加一次HTTP查询请求, 称为 &quot;预检&quot;请求(preflight) 浏览器先询问服务器, 当前网页所在的域名是否在服务器的许可名单之中, 以及可以使用哪些HTTP动词和头信息字段; 只有得到肯定答复, 浏览器才会发出正式的XMLHttpRequest请求, 否则就报错 ; 非简单请求会导致原先的一次请求变成两次, 第一次请求是 预检请求;“预检”请求用的请求方法是 OPTIONS，表示这个请求是用来询问的，头信息里面关键字段是Origin，表示请求来自哪个源 ; 非简单请求的例子1.服务器前端代码: www.test.com/index.html 本例子使用 PUT 来进行ajax请求, 满足 非简单请求 的条件 ; 另外, 本例还自定义了请求时的 header 首部字段, 也满足 非简单请求 的条件 ; 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; //设置cookie document.cookie = &quot;name=renyimin; domain=test.com&quot;; document.cookie = &quot;age=100; domain=test.com&quot;; document.cookie = &quot;gender=boy; domain=test.com&quot;; //跨子域带cookie和简单请求的设置方法一样 $.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); $(&quot;#btn&quot;).click(function() &#123; //序列化name/value var data = $(&quot;form&quot;).serializeArray(); $.ajax(&#123; //这里用PUT, 则为 `非简单` 请求 type: &apos;PUT&apos;, url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, data: data, //或者如果你自定义了一些请求时的 header 首部字段, 那么请求就也是 复杂请求 headers: &#123;&quot;custom-header-field&quot; : &quot;test&quot;&#125;, success: function (result) &#123; console.log(result); &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: test.test.com/index.php 123456789101112&lt;?php//服务器允许的 Originheader(\"Access-Control-Allow-Origin: http://www.test.com\");//服务器允许的 methodsheader(\"Access-Control-Allow-Methods: PUT, GET, POST\");//服务器允许设置的头部字段header(\"Access-Control-Allow-Headers: custom-header-field\");//跨子域带cookie和简单请求的设置方法一样header(\"Access-Control-Allow-Credentials: true\");var_dump($_COOKIE);var_dump($_SERVER['HTTP_CUSTOM_HEADER_FIELD']); 3.注意: 像上面例子的复杂跨域请求 必须: 首先和简单请求一样, 服务器端的 Access-Control-Allow-Origin 是必须设置的, 不然首先就跨不了域; 必须: 其次, 是使用了 get, post, head 之外方法的 复杂请求, 那么就必须在服务端有对应的 Access-Control-Allow-Method, 否则: 可选: 如果你自定义了 自定义首部字段 的 复杂请求, 那么也要在服务端有对应的 Access-Control-Allow-Headers, 否则: 4.另外需要关注的是: 如果在ajax跨域的情况下, 你设置了自定义的首部字段, 那么即使你的请求类型是get, post, head, 也会变成也是复杂请求, 所以仍然会发出预请求; 分析预检请求1.上面www.test.com/index.php代码进行ajax请求的时候, HTTP请求的方法是PUT, 所以浏览器会发现, 这是一个非简单请求, 就自动发出一个”预检”请求, 要求服务器确认可以这样请求 ; 2.所以请求应该是包括 预检请求和 真正的请求 两个请求的: 3.下面是这个”预检”请求的HTTP头信息 和 响应信息: 4.可以看到, “预检”请求用的HTTP请求方法是OPTIONS方法, 表示这个请求是用来进行询问的, 头信息里面, 关键字段是Origin, 表示请求来自哪个源; 除了Origin字段，”预检”请求的头信息包括两个特殊字段:(1)Access-Control-Request-Method ：该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT ;(2)Access-Control-Request-Headers：该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段, 上例是X-Custom-Header ; 另外可以看到Access-Control-Allow-Credentials 响应首部字段, 和简单请求一样, 都是为了做跨子域cookie共享; 分析预检响应1.从之前的图中还可以看到, 在预检请求的响应中, 服务器收到”预检”请求以后, 检查了 Origin,Access-Control-Request-Method,Access-Control-Request-Headers,Access-Control-Allow-Credentials 字段以后，确认允许跨源请求，就可以做出回应 ; 2.并且预检请求部分是不会真的发送数据的:而第二次真实请求就会有真正数据! 3.上面的HTTP回应中，关键的是 Access-Control-Allow-Origin 字段，表示 http://www.test.com 可以请求数据, 该字段也可以设为星号，表示同意任意跨源请求 ; 123Access-Control-Allow-Origin: http://www.test.com或者Access-Control-Allow-Origin: * 4.如果服务器否定了”预检”请求，会返回一个正常的HTTP回应, 浏览器会认定服务器不同意预检请求，因此触发一个错误，被 XMLHttpRequest 对象的 onerror 回调函数捕获;控制台会打印出如下的报错信息 ; 其他CORS相关字段分析 Access-Control-Allow-Methods: GET, POST, PUT 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 Access-Control-Allow-Headers: X-Custom-Header 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 Access-Control-Allow-Credentials: true 该字段与简单请求时的含义相同。 Access-Control-Max-Age: 1728000 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 最后注意, 复杂请求 跨子域带cookie和 简单请求 的设置方法一样;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"05.CORS方案 simple request","slug":"2016-09-18-sameoriginpolicy-05","date":"2016-09-18T04:45:07.000Z","updated":"2018-03-16T09:42:05.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-05/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-05/","excerpt":"","text":"CORS说明CORS 是一个W3C标准, 全称是 跨域资源共享(Cross-origin resource sharing), 通俗说就是我们所熟知的跨域请求; 在以前，跨域可以采用 代理、JSONP 等方式，而在Modern浏览器面前, 这些终将成为过去式, 因为有了CORS ; CORS允许浏览器向跨源服务器发出XMLHttpRequest请求, 也就是克服了AJAX只能同源使用的限制; 另外, CORS需要浏览器和服务器同时支持 (目前, 所有浏览器都支持该功能, IE浏览器不能低于IE10) ; 整个CORS通信过程都是浏览器自动完成, 不需要用户参与 对于开发者来说, CORS通信与同源的AJAX通信没有差别, 代码完全一样, 浏览器一旦发现AJAX的请求是跨源的, 就会自动添加一些附加的头信息, 有时还会多出一次附加的请求, 但用户不会有感觉; 之所以CORS通信与同源的AJAX通信的代码没有差别, 是因为: 其实实现CORS通信的关键是服务器, 只要服务器实现了CORS接口，就可以跨源通信 CORS的两类请求浏览器将CORS请求分成两类: 简单请求(simple request) 和 非简单请求(not-so-simple request) 以下情况会被归类为 非简单请求 : 以 GET, HEAD 或者 POST 以外的方法发起请求; 虽然使用 POST，但请求数据为 application/x-www-form-urlencoded, multipart/form-data 或者 text/plain 以外的数据类型, 比如说，用 POST 发送数据类型为 application/xml 或者 text/xml 的 XML 数据的请求 ; 使用自定义请求头（比如添加诸如 X-PINGOTHER） 简单请求代码案例跨域出错1.前端代码: www.test.com/index.html: 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function(k) &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;post&apos;, //这里用GET url: &apos;http://www.haha.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.跨域后端代码: www.haha.com/index.php : 12&lt;?phpecho json_encode(['name' =&gt; 'lant', 'age' =&gt; 100]); 3.由于出现跨域(而同源策略会限制Ajax跨域请求)所以会报错: 123Failed to load http://www.haha.com/index.php: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://www.test.com&apos; is therefore not allowed access. Access-Control-Allow-Origin响应头解决修改服务器后端代码: www.haha.com/index.php 如下, 发现可以正常进行 跨域ajax请求: 123&lt;?phpheader(&quot;Access-Control-Allow-Origin: http://www.test.com&quot;);echo json_encode([&apos;name&apos; =&gt; &apos;lant&apos;, &apos;age&apos; =&gt; 100]); 简单请求基本流程分析origin 请求首部字段1.对于简单请求，浏览器直接发出CORS请求, 具体来说, 就是在头信息之中, 自动增加一个 Origin 字段 ; 浏览器发现这次跨源AJAX请求是简单请求, 就自动在头信息之中, 添加一个Origin字段: 2.Origin请求首部字段 用来说明本次请求来自哪个源(协议 + 域名 + 端口), 服务器根据这个值, 决定是否同意这次请求; 3.如果Origin源不在服务器的许可范围内 服务器仍然会返回一个正常的HTTP回应, 不过浏览器会发现, 这个回应的头信息并没有包含 Access-Control-Allow-Origin 字段(详见下文), 就知道出错了, 从而抛出一个错误, 被XMLHttpRequest的onerror回调函数捕获; 注意, 这种错误无法通过状态码识别, 因为HTTP回应的状态码有可能是200 ; 4.如果Origin源在服务器设置的许可范围内服务器的响应可能会出现如下几个头信息字段(当然也不一定是所有都包含, 具体还得看服务器如何进行设置): 1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 服务器响应首部字段分析上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头: Access-Control-Allow-Origin服务器要设置ajax请求可以跨域, 该字段是必须的, 它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求; Access-Control-Allow-Credentials该字段可选, 它的值是一个布尔值，表示是否允许发送Cookie, 默认情况下，Cookie不包括在CORS请求之中; 如果设置为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器;注意, 这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。当然, 如果带了cookie, 那貌似也只能是跨子域了; Access-Control-Expose-Headers该字段可选, CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma;如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定;之前提到的 Access-Control-Expose-Headers: FooBar 就可以让getResponseHeader(‘FooBar’)获取额外返回的FooBar字段的值。 目标域响应首部具体设置Access-Control-Allow-Origin1.服务器在设置的时候, 其实就是通过header函数设置上面的三个选项;2.比如之前的例子中, 如果服务器只是简单的为了实现跨域, 直接设置如下选项即可: 123&lt;?phpheader(\"Access-Control-Allow-Origin: http://www.haha.com\");echo json_encode(['name' =&gt; 'lant', 'age' =&gt; 100]); Access-Control-Allow-Credentials注意配合withCredentials属性1.CORS请求默认不会在不同域之间共享Cookie和HTTP认证信息, 如果要共享Cookie: 一方面要 服务器同意指定Access-Control-Allow-Credentials字段 : 123Access-Control-Allow-Credentials: true//php中设置如下:header(\"Access-Control-Allow-Credentials: true\"); 另一方面, 开发者必须在AJAX请求中打开 withCredentials 属性 (这一点JSONP不用设置) 1234var xhr = new XMLHttpRequest();xhr.withCredentials = true;//jquery中设置withCredentials的代码如下:$.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); 当然, 也仅限于跨子域 2.需要以上3方面都做到才可以 否则，即使服务器同意发送Cookie，浏览器也不会发送 ; 但是, 如果省略 withCredentials 设置, 有的浏览器还是会一起发送Cookie, 这时, 可以显式关闭 withCredentials ; 3.需要注意的是: 如果要发送Cookie, Access-Control-Allow-Origin 就不能设为星号*, 必须指定明确的、与请求网页一致的域名 ; 同时，Cookie依然遵循同源政策，只有顶级域名一样的情况下, 的Cookie才会共享, 其他域名的Cookie并不会上传 ; 测试代码1.前端代码 (www.test.com/index.html): 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; //设置cookie document.cookie = &quot;name=renyimin; domain=test.com&quot;; document.cookie = &quot;age=100; domain=test.com&quot;; document.cookie = &quot;gender=boy; domain=test.com&quot;; //要在跨域请求服务器时在cors请求中包含cookie, 就需要开启withCredentials属性 $.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); $(&quot;#btn&quot;).click(function(k) &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码 (test.test.com/index.php): 12345&lt;?phpheader(\"Access-Control-Allow-Origin: http://www.test.com\");//服务器允许前端在跨子域cors请求时包含cookieheader(\"Access-Control-Allow-Credentials: true\");var_dump($_COOKIE); 3.效果: 成功实现跨域, 并且实现跨子域cookie共享! 小结 如果 www.test.com/index.html 中设置了 withCredentials 为 true , 那么在ajax请求的目标域test.test.com/index.php中必须设置: 1header(\"Access-Control-Allow-Credentials: true\"); 否则, 报错如下: 如果www.test.com/index.html中没有设置 withCredentials 属性,test.test.com/index.php也没设置 header(&quot;Access-Control-Allow-Credentials: true&quot;);那么即使www.test.com/index.html中设置了domain属性为 .test.com 的cookie键值对test.test.com/index.php 中也获取不到cookie; 如果想跨子域携带cookie, 则服务器端需要注意: Access-Control-Allow-Origin响应首部字段不能为*哦!!","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"04.JSONP方案","slug":"2016-09-17-sameoriginpolicy-04","date":"2016-09-17T11:27:31.000Z","updated":"2018-03-01T01:51:34.000Z","comments":true,"path":"2016/09/17/2016-09-17-sameoriginpolicy-04/","link":"","permalink":"http://blog.renyimin.com/2016/09/17/2016-09-17-sameoriginpolicy-04/","excerpt":"","text":"JSONPJSONP是服务器与客户端 跨源通信 的常用方法; 最大特点就是 简单适用, 老式浏览器全部支持, 服务器端改造非常小; 但问题是，JSONP只能发GET请求 ; 正常ajax请求1.服务器前端代码: www.test.com/index.html 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://www.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: www.test.com/index.php 12345678&lt;?php//参数$sex = isset($_GET['sex']) ? trim($_GET['sex']) : '';$age = isset($_GET['age']) ? trim($_GET['age']) : '';$data = [\"sex\" =&gt; $sex, \"age\" =&gt; $age];$res = json_encode($data, JSON_UNESCAPED_UNICODE); //json 数据echo $res; 3.结果: 前端ajax正常得到后端代码的响应!! 跨域Ajax请求案例1.服务器前端代码: www.test.com/index.html, 修改请求的后端连接为 test.test.com/index.php 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码不变 3.问题出现: 123Failed to load http://test.test.com/index.php?sex=%E7%94%B7&amp;age=100: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://www.test.com&apos; is therefore not allowed access. jsonp跨域方案1.服务器前端代码: www.test.com/index.html 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;jsonp&apos;, //类型 data: data, jsonp: &apos;callback&apos;, //jsonp回调参数，峰哥资料上说是必须的(经测试, 至少的jquery下是可以不带的) success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: test.test.com/index.php 12345678910&lt;?php//jsonp回调参数，必需$callback = isset($_GET['callback']) ? trim($_GET['callback']) : '';//参数$sex = isset($_GET['sex']) ? trim($_GET['sex']) : '';$age = isset($_GET['age']) ? trim($_GET['age']) : '';$data = [\"sex\" =&gt; $sex, \"age\" =&gt; $age];$res = json_encode($data,JSON_UNESCAPED_UNICODE); //json 数据echo '(' . $res . ')'; // 返回格式貌似需要注意 () 的使用 3.结果, 规避了同源策略限制导致的Ajax请求不能发送的问题!! 只支持GET在使用jsonp的时候, 当你把GET方法换成POST之后, 发现其实也可以进行正常的Ajax请求;仔细观察请求头, 发现你设置的POST请求方法最终还是被换成了 GET; JSONP跨子域cookie共享1.JSONP如果是像上面测试那样跨子域的话, 是可以同时考虑跨子域共享Cookie的; (但是如果是跨顶级域, 那就不能共享cookie了) 服务器前端代码: www.test.com/index.html, 增加对cookie的设置 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; document.cookie = &quot;name=renyimin; domain=test.com&quot;; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;POST&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;jsonp&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 服务器后端代码 test.test.com/index.php, 在跨子域的情况下,可以正常获取cookie 2.但是如果是跨顶级域名, 就别奢望了, 测试发现, 上述代码做如下两处改动: 设置cookie的代码如果改为: document.cookie = &quot;name=renyimin; domain=haha.com&quot;;; ajax的url改为跨顶级域名的url: ‘http://www.haha.com/index.php‘, 3.你会发现, ajax可以正常跨域, 但是cookie却不会被设置成功的! 4.小结: ajax使用jsonp跨子域的时候是可以轻松像上面设置 cookie的domain属性 来共享cookie的 ; 而下一篇介绍的ajax使用cors方案跨域的话, 即使设置了cookie的 document.domain , 也不能带上cookie, 还需要注意前端和后端的 withCredentials 头字段 ;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"03.规避同源策略限制 之 \"Ajax请求不能发送\"","slug":"2016-09-16-sameoriginpolicy-03","date":"2016-09-16T05:04:17.000Z","updated":"2018-03-01T12:16:58.000Z","comments":true,"path":"2016/09/16/2016-09-16-sameoriginpolicy-03/","link":"","permalink":"http://blog.renyimin.com/2016/09/16/2016-09-16-sameoriginpolicy-03/","excerpt":"","text":"同源政策规定, AJAX请求只能发给同源的网址, 否则就报错 ; 除了架设服务器代理(浏览器请求同源服务器，再由后者请求外部服务), 有三种方法规避这个限制 : JSONP CORS WebSocket 参考","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"02.规避同源策略限制 之 \"Cookie无法读取\"","slug":"2016-09-15-sameoriginpolicy-02","date":"2016-09-15T13:10:13.000Z","updated":"2018-03-01T12:08:45.000Z","comments":true,"path":"2016/09/15/2016-09-15-sameoriginpolicy-02/","link":"","permalink":"http://blog.renyimin.com/2016/09/15/2016-09-15-sameoriginpolicy-02/","excerpt":"","text":"回顾 之前学习同源策略基础知识的时候, 了解了同源策略的 三种行为 限制: Cookie、LocalStorage 和 IndexDB 无法读取 DOM 无法获得 AJAX 请求不能发送 注意:同源策略的限制, 并没有限制住CSRF攻击, 它并不会限制 B站点中嵌入的A站点超链接去读取A站点用户的cookie; 同源策略限制 – Cookie无法读取 准备跨域站点: www.test.com 和 test.tset.com www.test.com站点(http://www.test.com/setCookie.php)写入cookie 12&lt;?phpsetcookie(\"name\", 'renyimin'); www.test.com 站点(http://www.test.com/getCookie.php)尝试读取cookie 12&lt;?phpvar_dump($_COOKIE['name']); // 可以看到, 未跨域的情况下, 可以正常获取cookie test.test.com 站点(http://test.test.com/getCookie.php)尝试读取cookie 12&lt;?phpvar_dump($_COOKIE['name']); // 你会发现读取不到cookie, 因为已经跨域 (虽然一级域名一样, 但是由于二级域名不一致, 导致了跨域) 合理规避 Cookie无法读取的限制 虽然同源的那些限制是必要, 但是有些情况下, 我们可能需要去合理规避Cookie无法读取的限制的; 比如: 如果两个站点的顶级域名相同, 只是二级域名不同的话, 浏览器其实是允许你通过设置 document.domain 来共享 Cookie 的; 例子: 还是之前的例子, 如果www.test.com/setCookie.php代码改为: 12&lt;?phpsetcookie(&quot;name&quot;, &apos;renyimin&apos;, 0, &apos;&apos;, &apos;test.com&apos;); // 这样你会发现, www.test.com/getCookie.php 和 test.test.com/getCookie.php 都可以正常获取 www.test.com/setCookie.php 设置的cookie 小结合理规避同源策略因跨域导致的cookie无法读取的问题, 有个前提是, 跨域双方需在同顶级域下(即顶级域名一致的情况下);你在设置cookie时, 如果指定的domain参数和当前域名的顶级域名不一致, cookie 的设置就会失败;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"01.同源策略(Same origin policy)","slug":"2016-09-15-sameoriginpolicy-01","date":"2016-09-15T11:21:54.000Z","updated":"2018-03-01T12:08:03.000Z","comments":true,"path":"2016/09/15/2016-09-15-sameoriginpolicy-01/","link":"","permalink":"http://blog.renyimin.com/2016/09/15/2016-09-15-sameoriginpolicy-01/","excerpt":"","text":"同源策略1995年，同源政策由 Netscape 公司引入浏览器。目前, 所有浏览器都实行这个政策; 最初，它的含义是指，A站点设置的Cookie，B站点不能打开，除非这两个站点属 同源，所谓同源指的是 “三个相同”: 1.协议相同http://blog.renyimin.com 和 https://blog.renyimin.com 就不是同一个源 ； 2.域名完全相同http://blog.renyimin.com/test/index.php 和 http://blog.renyimin.com/welcome/index.html 就是同一个源;但是 http://www.renyimin.com/test/index.php 和 http://blog.renyimin.com/test/index.php 就不是同一个源 ；请注意：localhost和127.0.0.1虽然都指向本机, 但也不是同一个源 ; 3.端口相同http://www.renyimin.com:8080/test/index.php 和 http://www.renyimin.com:80/test/index.php 就不是同一个源 ; 同源策略目的为了保证用户信息的安全，防止恶意的网站窃取并利用数据; 比如用户登录一家银行网站后，又去浏览其他站点, 如果没有同源策略限制, 其他站点就也能读取银行网站的 Cookie, 这样的话: 如果 Cookie 包含用户的私密信息，泄漏给第三方站点就比较危险, 当然, Cookie中包含的敏感信息通常经过加密，很难将其反向破解, 但这并不意味着绝对安全; 虽然第三方无法通过解密获取cookie中的信息, 但它可以不解密,而是直接使用Cookie去骗取银行网站的信任; 由此可见，”同源政策” 是必需的，否则, 各站点的 Cookie 可以随便共享，那互联网就毫无安全可言了 ; 同源策略的限制随着互联网的发展, “同源政策”越来越严格, 目前, 如果非同源, 共有三种行为受到限制: Cookie、LocalStorage 和 IndexDB 无法读取 DOM 无法获得 AJAX 请求不能发送 同源策略和CSRF安全问题需要注意一点: 同源策略的限制, 并没有限制住CSRF攻击 上面同源策略的几个限制中, 并没有限制 从B站点中通过超链接去跳转到A站点时, A站点去读取自身的cookie, 这样就会存在问题: 假如你当前已经登录了邮箱，或bbs，同时你又访问了另外一个站点，假设这就是一个钓鱼网站(站点中伪装了很多诱导用户去点击的超链接, 而这些超链接都是其他站点的一些敏感操作, 就这样放好诱饵等待曾经登陆过那些站点的用户去点击, 等鱼儿上钩); 假设这个网站上面可能因为某个图片吸引你，你去点击一下，而这个点击正是去往你的bbs站点进行一个发帖操作，由于当前你的浏览器状态已经是登陆了bbs站点，此时你点击这个钓鱼网站的连接是就会使用你之前登陆bbs站点在浏览器cookie罐中保存的信息, 就和正常的请求一样。于是钓鱼站点就纯天然的利用了其他站点的登陆状态，让用户在不知情的情况下，帮你发帖或干其他事情; 这也就是我们通常所说的CSRF攻击, CSRF攻击的主要目的是让用户在不知情的情况下攻击自己已登录的一个系统; 引出跨域问题由于同源策略的这些限制都是为了安全考虑, 自然是必要的; 但是有时很不方便, 可能会导致合理的用途也受到影响, 接下来将详细介绍如何在需要的时候合理地去 规避”同源政策”的限制; 参考 阮一峰","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"13. CURLOPT_AUTOREFERER, CURLOPT_REFERER","slug":"PHP/2016-07-12-curl-13","date":"2016-07-12T13:07:24.000Z","updated":"2018-03-15T05:07:28.000Z","comments":true,"path":"2016/07/12/PHP/2016-07-12-curl-13/","link":"","permalink":"http://blog.renyimin.com/2016/07/12/PHP/2016-07-12-curl-13/","excerpt":"","text":"CURLOPT_AUTOREFERER默认为false设置为true, 则访问目标时, 浏览器会自动带着referrer头 (忽略你通过CURLOPT_REFERER手动设置的referer), 目标页可以通过 $_SERVER[&#39;HTTP_REFERER&#39;] 来获取来源;如果设置为false, 则不行, 则会检查你有没有通过CURLOPT_REFERER手动设置的referer, 如果有, 就使用手动设置的referer, 没有则目标页中无法获取referer; 在开发中, 使用curl时, 如果后端需要使用referer，一般会如下做检测: 123456789function vcurl($url, $post = &apos;&apos;, $cookie = &apos;&apos;, $cookiejar = &apos;&apos;, $referer = &apos;&apos;, $isThrowEx = 0)// ......if ($referer) &#123; curl_setopt($curl, CURLOPT_REFERER, $referer);&#125; else &#123; curl_setopt($curl, CURLOPT_AUTOREFERER, 1);&#125;//.... CURLOPT_REFERERDemo curl_demo.php 1234567891011121314151617181920212223242526&lt;?php$ch = curl_init();// 设置URLcurl_setopt($ch, CURLOPT_URL, &quot;www.test.com/curl_location.php&quot;);// 设置是否将响应结果存入变量, 1是存入, 0是直接echo出curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);// 启用时会将头文件的信息作为数据流输出 (也就是会输出文件头信息)//curl_setopt($ch,CURLOPT_HEADER, true);// 设置为true: 当你访问一个页面时, 如果页面头中有Location重定向, 则会进行递归访问, 并且重定向几次, 就递归访问几次; 否则则不进行递归访问;curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);// 上面递归访问的次数根据如下限制curl_setopt($ch,CURLOPT_MAXREDIRS, 4);// 如果设置为false,会查看是否有手动设置referer, 如果没有, 访问目标无法通过 $_SERVER[&apos;HTTP_REFERER&apos;] 获取来源// 设置为true, 则会忽略手动设置的referer, 浏览器会自动加上来源// 并且目标页无论location跳转多深, 貌似都会带着这个来源curl_setopt($ch, CURLOPT_AUTOREFERER, true);// 如果手动配置了来源, 则只有当CURLOPT_AUTOREFERER为false时,才会生效curl_setopt($ch, CURLOPT_REFERER, &apos;http://www.haha.com&apos;);// 执行，然后将响应结果存入$output变量$output = curl_exec($ch);echo $output;// 关闭这个curl会话资源curl_close($ch); 1.目标页 curl_location.php 代码: 123&lt;?php// 第一次跳转的目标地址header(&quot;Location:http://www.test.com/curl_location1.php&quot;); 2.curl_location1.php 12&lt;?phpheader(&quot;Location:http://www.test.com/curl_location2.php&quot;); 3.curl_location2.php 12&lt;?phpvar_dump($_SERVER); 最终会在跳转到本页面之后, 仍然可以获取到我们手动设置的 referer;","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"},{"name":"curl","slug":"curl","permalink":"http://blog.renyimin.com/tags/curl/"}]},{"title":"14. Curl利用cookie模拟登录","slug":"PHP/2016-07-12-curl-14","date":"2016-07-12T13:07:24.000Z","updated":"2018-03-15T05:07:32.000Z","comments":true,"path":"2016/07/12/PHP/2016-07-12-curl-14/","link":"","permalink":"http://blog.renyimin.com/2016/07/12/PHP/2016-07-12-curl-14/","excerpt":"","text":"CURLOPT_COOKIEJAR连接结束后, 比如, 调用 curl_close 后, 保存 cookie 信息的文件;比如, 模拟登录, 登录成功之后, 将cookie文件存放的位置! CURLOPT_COOKIE设定 HTTP 请求中”Cookie: “部分的内容。多个 cookie 用分号分隔，分号后带一个空格(例如: “fruit=apple; colour=red”)。 CURLOPT_COOKIEFILE登录成功, 之后进行请求时所引用的 cookie 文件; 包含cookie数据的文件名, cookie文件的格式可以是Netscape格式, 或者只是纯HTTP头部风格, 存入文件; 如果文件名是空的, 不会加载cookie, 但cookie的处理仍旧启用; 模拟 服务端登录代码: server_login.php 123456789&lt;?phpsession_start();if ($_POST[&apos;username&apos;] == &apos;renyimin&apos; &amp;&amp; $_POST[&apos;pwd&apos;] == &quot;renyimin&quot;) &#123; $_SESSION[&apos;username&apos;] = $_POST[&apos;username&apos;]; $_SESSION[&apos;pwd&apos;] = $_POST[&apos;pwd&apos;]; echo &quot;登录成功&quot;;&#125; else &#123; echo &quot;登录失败&quot;;&#125; 服务端首页代码: server_index.php 12345&lt;?phpsession_start();if ($_SESSION[&apos;username&apos;]) &#123; echo &quot;你获取数据了!&quot;;&#125; curl模拟登录代码: curl_login.php 1234567891011121314&lt;?php//模拟登录function login_post($url, $cookie, $post) &#123; $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_RETURNTRANSFER, 0); curl_setopt($curl, CURLOPT_COOKIEJAR, $cookie); curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, $post); curl_exec($curl); curl_close($curl);&#125;login_post(&apos;http://www.test.com/curl/server_login.php&apos;, dirname(__FILE__) . &apos;/cookie.txt&apos;, [&apos;username&apos; =&gt; &apos;renyimin&apos;, &apos;pwd&apos; =&gt; &apos;renyimin&apos;]); curl模拟利用cookie, 获取首页信息代码 123456789101112131415&lt;?php//登录成功后获取数据function get_content($url, $cookie) &#123; $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, $url); curl_setopt($ch, CURLOPT_HEADER, 0); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 0); // 直接使用cookie文件 curl_setopt($ch, CURLOPT_COOKIEFILE, $cookie); // 或者你也可以取出cookie中的信息直接传递 // curl_setopt($ch, CURLOPT_COOKIE, &quot;PHPSESSID=7bffe4cc3db027e14b5eb4e84f9e7275&quot;); curl_exec($ch); curl_close($ch);&#125;get_content(&apos;http://www.test.com/curl/server_index.php&apos;, dirname(__FILE__) . &apos;/cookie.txt&apos;); 成功生成cookie.txt文件, 并且访问server_index.php成功!","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"},{"name":"curl","slug":"curl","permalink":"http://blog.renyimin.com/tags/curl/"}]},{"title":"POST请求, 文件上传","slug":"PHP/2016-07-12-curl-02","date":"2016-07-12T12:36:17.000Z","updated":"2018-03-15T05:07:13.000Z","comments":true,"path":"2016/07/12/PHP/2016-07-12-curl-02/","link":"","permalink":"http://blog.renyimin.com/2016/07/12/PHP/2016-07-12-curl-02/","excerpt":"","text":"CURLOPT_POST设置为 TRUE 时, 会发送 POST 请求, 类型为: application/x-www-form-urlencoded, 是 HTML 表单提交时最常见的一种; CURLOPT_POSTFIELDS 全部数据使用HTTP协议中的 “POST” 操作来发送; 这个参数可以是 urlencoded 后的字符串, 类似’para1=val1&amp;para2=val2&amp;…’; 也可以使用一个以字段名为键值, 字段数据为值的数组; 如果value是一个数组, Content-Type头将会被设置成multipart/form-data 对于文件的上传 从 PHP 5.2.0 开始, 使用 @ 前缀传递文件时, value 必须是个数组, 如下: 1234$data = [ &apos;name&apos;=&gt;&apos;boy&apos;, &quot;myFile&quot;=&gt;&quot;@/Users/renyimin/Desktop/qqq.jpeg&quot;]; 从 PHP 5.5.0 开始, @ 前缀已被废弃，文件可通过 CURLFile 发送, 设置 CURLOPT_SAFE_UPLOAD为TRUE, 可禁用 @ 前缀发送文件, 以增加安全性 123$data[&apos;myFile&apos;]=new CURLFile(&apos;/Users/renyimin/Desktop/qqq.jpeg&apos;);// .....curl_setopt($ch, CURLOPT_SAFE_UPLOAD, true); 数组数据发送POST请求 curl.demo.php 12345678910111213141516&lt;?php$data = [ &apos;name&apos; =&gt; &apos;任益民&apos;, &quot;age&quot; =&gt; &quot;100&quot;, &quot;gender&quot; =&gt; &quot;男&quot;];$ch = curl_init();curl_setopt($ch, CURLOPT_URL, &quot;http://www.test.com/server.php&quot;);curl_setopt($ch, CURLOPT_POST, true);// http_build_query 之后, 服务端可以使用$_POST接收, 也可以使用 urldecode(file_get_contents(&apos;php://input&apos;));// 否则只能使用$_POST接收curl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query($data));curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);$output = curl_exec($ch);echo $output;curl_close($ch); server.php 12&lt;?phpvar_dump($_POST); json数据发送POST curl_demo.php : 注意需要设置HTTP头 1234567891011121314151617&lt;?php$data = [ &apos;name&apos; =&gt; &apos;任益民&apos;, &quot;age&quot; =&gt; &quot;100&quot;, &quot;gender&quot; =&gt; &quot;男&quot;];$data = json_encode($data, JSON_UNESCAPED_UNICODE);$ch = curl_init();curl_setopt($ch, CURLOPT_URL, &quot;http://www.test.com/server.php&quot;);curl_setopt($ch, CURLOPT_POST, true);// 重点curl_setopt($ch, CURLOPT_HTTPHEADER, [&apos;Content-Type: application/json&apos;, &apos;Content-Length:&apos; . strlen($data)]);curl_setopt($ch, CURLOPT_POSTFIELDS, $data);curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);$output = curl_exec($ch);echo $output;curl_close($ch); server.php 123&lt;?php$phpInput = file_get_contents(&apos;php://input&apos;);var_dump(json_decode($phpInput, true)); 文件上传参数为数组形式 uploadFile.php 1234567891011121314151617&lt;?php$data = [ &apos;name&apos; =&gt; &apos;任益民&apos;, &quot;age&quot; =&gt; &quot;100&quot;, &quot;gender&quot; =&gt; &quot;男&quot;];$data[&apos;myfile&apos;] = new CURLFile(&apos;/Users/renyimin/Desktop/qqq.jpeg&apos;);$ch = curl_init();curl_setopt($ch, CURLOPT_URL, &quot;http://www.test.com/server.php&quot;);// 开启上传安全curl_setopt($ch, CURLOPT_SAFE_UPLOAD, true);curl_setopt($ch, CURLOPT_POST, true);curl_setopt($ch, CURLOPT_POSTFIELDS, $data);curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);$output = curl_exec($ch);echo $output;curl_close($ch); server.php 12345678910&lt;?php// 也可以接收POST参数if($_FILES)&#123; $filename = $_FILES[&apos;myfile&apos;][&apos;name&apos;]; $tmpname = $_FILES[&apos;myfile&apos;][&apos;tmp_name&apos;]; //保存图片到当前脚本所在目录 if(move_uploaded_file($tmpname,dirname(__FILE__).&apos;/&apos;.$filename))&#123; echo (&apos;上传成功&apos;); &#125;&#125;","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"},{"name":"curl","slug":"curl","permalink":"http://blog.renyimin.com/tags/curl/"}]},{"title":"11. CURLOPT_HEADER, CURLOPT_FOLLOWLOCATION, CURLOPT_MAXREDIRS","slug":"PHP/2016-07-12-curl-11","date":"2016-07-12T12:36:17.000Z","updated":"2018-03-15T05:07:25.000Z","comments":true,"path":"2016/07/12/PHP/2016-07-12-curl-11/","link":"","permalink":"http://blog.renyimin.com/2016/07/12/PHP/2016-07-12-curl-11/","excerpt":"","text":"CURLOPT_HEADER 启用时会将头文件的信息作为数据流输出 比如, 当你使用curl访问某个url时, 如果该url页面进行了Location跳转, 则你的访问会失效, 会没有任何返回值; 此时, 你就可以设置 CURLOPT_HEADER 选项为true; 如果你最终对访问信息进行了输出, 你会看到输出内容中包含了页面的HTTP头信息, 而且发现其中有 Location跳转; CURLOPT_FOLLOWLOCATION 如果遇到Location跳转, 你可以设置 CURLOPT_FOLLOWLOCATION 选项为true, 来继续进行追踪访问; 如果接下来的页面中还有 Location跳转, 则会进行递归访问; 当然, 如果开启了CURLOPT_FOLLOWLOCATION, CURLOPT_HEADER的输出也会是递归每个页面的HTTP头信息进行输出; CURLOPT_MAXREDIRS 上面的Location跳转, 可以使用CURLOPT_MAXREDIRS参数来设置最多追踪几次跳转; 如果访问的目标页面接下来会进行4次location跳转, 而你设置的CURLOPT_MAXREDIRS为3, 则依然访问不到任何内容; Demo0.curl_demo.php 123456789101112131415161718&lt;?php$ch = curl_init();// 设置URLcurl_setopt($ch, CURLOPT_URL, &quot;www.test.com/curl_location.php&quot;);// 设置是否将响应结果存入变量, 1是存入, 0是直接echo出curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);// 启用时会将头文件的信息作为数据流输出 (也就是会输出文件头信息)curl_setopt($ch,CURLOPT_HEADER, true);// 设置为true: 当你访问一个页面时, 如果页面头中有Location重定向, 则会进行递归访问, 并且重定向几次, 就递归访问几次; 否则则不进行递归访问;curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);// 上面递归访问的次数根据如下限制curl_setopt($ch,CURLOPT_MAXREDIRS, 4);// 执行，然后将响应结果存入$output变量$output = curl_exec($ch);echo $output;// 关闭这个curl会话资源curl_close($ch); 1.目标页 curl_location.php 代码: 123&lt;?php// 第一次跳转的目标地址header(&quot;Location:http://www.test.com/curl_location1.php&quot;); 2.curl_location1.php 12&lt;?phpheader(&quot;Location:http://www.test.com/curl_location2.php&quot;); 3.curl_location2.php 12&lt;?phpheader(&quot;Location:http://www.test.com/curl_location3.php&quot;); 4.curl_location3.php 12&lt;?phpheader(&quot;Location:http://www.test.com/test.html&quot;); 5.最终test.html页 12345678910&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;test&lt;/body&gt;&lt;/html&gt; 效果","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"},{"name":"curl","slug":"curl","permalink":"http://blog.renyimin.com/tags/curl/"}]},{"title":"closure 闭包","slug":"PHP/2016-06-28-closure","date":"2016-06-28T08:40:32.000Z","updated":"2018-04-03T05:45:14.000Z","comments":true,"path":"2016/06/28/PHP/2016-06-28-closure/","link":"","permalink":"http://blog.renyimin.com/2016/06/28/PHP/2016-06-28-closure/","excerpt":"","text":"JavaScript的闭包 对于JavaScript的闭包, 建议可以参考MDN Web Doc 闭包是由函数以及创建该函数的词法环境组合而成, 这个环境包含了这个闭包创建时所能访问的所有局部变量; 闭包很有用, 因为它允许将函数与其所操作的某些数据(环境)关联起来; 这显然类似于面向对象编程, 在面向对象编程中, 对象允许我们将某些数据(对象的属性)与一个或者多个方法相关联; (通常你使用 只有一个方法的对象 的地方, 都可以使用闭包;) PHP中的闭包 闭包是一个Closure对象(代表匿名函数类) 无属性闭包 12345678910&lt;?php$test = function () &#123; echo &apos;This is a Closure&apos;;&#125;;echo &quot;&lt;pre/&gt;&quot;;var_dump($test);die;// 结果object(Closure)#1 (0) &#123;&#125; 含属性闭包 1234567891011121314151617181920212223242526&lt;?php$a = 100;$b = 200;$test = function ($c) use ($a, $b) &#123; echo $a . &apos;-&apos; . $b . &apos;-&apos; . $c;&#125;;echo &quot;&lt;pre/&gt;&quot;;var_dump($test);die;// 结果object(Closure)#1 (2) &#123; [&quot;static&quot;]=&gt; array(2) &#123; [&quot;a&quot;]=&gt; int(100) [&quot;b&quot;]=&gt; int(200) &#125; [&quot;parameter&quot;]=&gt; array(1) &#123; [&quot;$c&quot;]=&gt; string(10) &quot;&quot; &#125;&#125; PHP的闭包不会像JS中那样自动封装应用的状态; 在PHP中, 你必须手动调用闭包对象的 bindTo() 方法, 或者使用 use 关键字, 来手动把应用的状态附加到PHP闭包上; use关键字use的用法比较常见, 经常用于从 父作用域 继承变量; bindTo() bindTo()是个成员方法, 可以直接使用匿名函数来调用; ‘bindTo()’的第二个参数尤为重要, 其作用是指定绑定闭包的那个对象所属的PHP类, 这样, 闭包就可以在其他地方(比如类外部), 在闭包内部通过$this访问绑定闭包的对象中受保护和私有的成员变量; 主要用于将 匿名(回调)函数本身 绑定到类中; (这样, 就可以在类外部, 在这个匿名函数中, 使用类中的 public, protected, private 成员属性 及 静态属性) Laravel中路由绑定的实现其实使用了该技术 123456789101112131415161718192021222324252627282930313233343536373839&lt;?phpclass App &#123; protected $routes = []; protected $responseStatus = &apos;200 OK&apos;; protected $responseContentType = &apos;text/html&apos;; public $responseBody1 = &apos;name&apos;; protected $responseBody2 = &apos;age&apos;; private $responseBody3 = &apos;gender&apos;; private static $responseBody4 = &apos;address&apos;; public function addRoute($routePath, $routeCallback) &#123; $this-&gt;routes[$routePath] = $routeCallback-&gt;bindTo($this, __CLASS__); &#125; public function dispatch($currentPath) &#123; foreach ($this-&gt;routes as $routePath =&gt; $callback) &#123; if( $routePath === $currentPath) &#123; $callback(); &#125; &#125; header(&apos;HTTP/1.1 &apos; . $this-&gt;responseStatus); header(&apos;Content-Type: &apos; . $this-&gt;responseContentType); header(&apos;Content-Length: &apos; . strlen($this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4)); echo $this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4; &#125;&#125;$app = new App();// 如下, 在类外部的匿名函数中, 实现了对类内部属性的操作$app-&gt;addRoute(&apos;user/nonfu&apos;, function()&#123; $this-&gt;responseContentType = &apos;application/json;charset=utf8&apos;; $this-&gt;responseBody1 = &apos;&#123;&quot;name&quot;:&quot;LaravelAcademy&quot;&#125;&apos;; $this-&gt;responseBody2 = &apos;&#123;&quot;age&quot;:&quot;100&quot;&#125;&apos;; $this-&gt;responseBody3 = &apos;&#123;&quot;gender&quot;:&quot;male&quot;&#125;&apos;; self::$responseBody4 = &apos;&#123;&quot;address&quot;:&quot;beijing&quot;&#125;&apos;;&#125;);$app-&gt;dispatch(&apos;user/nonfu&apos;); bind bind 是个静态方法, 需要使用Closure来静态调用; 主要用于将 指定的匿名(回调)函数 以 静态或非静态 的方式绑定到 类; 还是上面的例子, 只用做很小的改动即可 123456789101112131415161718192021222324252627282930313233343536373839&lt;?phpclass App &#123; protected $routes = []; protected $responseStatus = &apos;200 OK&apos;; protected $responseContentType = &apos;text/html&apos;; public $responseBody1 = &apos;name&apos;; protected $responseBody2 = &apos;age&apos;; private $responseBody3 = &apos;gender&apos;; private static $responseBody4 = &apos;address&apos;; public function addRoute($routePath, $routeCallback) &#123; // 注意此处改动 $this-&gt;routes[$routePath] = Closure::bind($routeCallback, $this, __CLASS__); &#125; public function dispatch($currentPath) &#123; foreach ($this-&gt;routes as $routePath =&gt; $callback) &#123; if( $routePath === $currentPath) &#123; $callback(); &#125; &#125; header(&apos;HTTP/1.1 &apos; . $this-&gt;responseStatus); header(&apos;Content-Type: &apos; . $this-&gt;responseContentType); header(&apos;Content-Length: &apos; . strlen($this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4)); echo $this-&gt;responseBody1 . $this-&gt;responseBody2 . $this-&gt;responseBody3 . self::$responseBody4; &#125;&#125;$app = new App();$app-&gt;addRoute(&apos;user/nonfu&apos;, function()&#123; $this-&gt;responseContentType = &apos;application/json;charset=utf8&apos;; $this-&gt;responseBody1 = &apos;&#123;&quot;name&quot;:&quot;LaravelAcademy&quot;&#125;&apos;; $this-&gt;responseBody2 = &apos;&#123;&quot;age&quot;:&quot;100&quot;&#125;&apos;; $this-&gt;responseBody3 = &apos;&#123;&quot;gender&quot;:&quot;male&quot;&#125;&apos;; self::$responseBody4 = &apos;&#123;&quot;address&quot;:&quot;beijing&quot;&#125;&apos;;&#125;);$app-&gt;dispatch(&apos;user/nonfu&apos;); 再看一个例子 12345678910111213141516&lt;?phpclass A &#123; private static $sfoo = 1; private $ifoo = 2;&#125;$cl1 = static function() &#123; return A::$sfoo;&#125;;$cl2 = function() &#123; return $this-&gt;ifoo;&#125;;$bcl1 = Closure::bind($cl1, null, &apos;A&apos;);$bcl2 = Closure::bind($cl2, new A(), &apos;A&apos;);echo $bcl1(), &quot;\\n&quot;;echo $bcl2(), &quot;\\n&quot;; __invoke还有一个 __invoke 方法, 这是为了与其他实现了__invoke()魔术方法的对象保持一致性, 但调用匿名函数的过程与它无关, 如下:123456&lt;?php$greet = function ($name) &#123; return sprintf(&quot;Hello %s\\r\\n&quot;, $name);&#125;;echo $greet(&apos;LaravelAcademy.org&apos;); 之所以能调用$greet变量，是因为这个变量的值是一个闭包，而且闭包对象实现了invoke()魔术方法, 只要变量名后有(), PHP就会查找并调用invoke方法; ~~未完待续","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"10. Laravel-Facade","slug":"OOP/2016-05-28-OOP-10-Laravel-Facade","date":"2016-05-28T07:13:05.000Z","updated":"2018-03-15T11:03:35.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-10-Laravel-Facade/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-10-Laravel-Facade/","excerpt":"","text":"代码追踪 以获取配置项的 Illuminate\\Support\\Facades\\Config 追踪Facade, Illuminate\\Support\\Facades 123456789101112131415161718192021public static function __callStatic($method, $args) &#123; $instance = static::getFacadeRoot(); if (! $instance) &#123; throw new RuntimeException(&apos;A facade root has not been set.&apos;); &#125; switch (count($args)) &#123; case 0: return $instance-&gt;$method(); case 1: return $instance-&gt;$method($args[0]); case 2: return $instance-&gt;$method($args[0], $args[1]); case 3: return $instance-&gt;$method($args[0], $args[1], $args[2]); case 4: return $instance-&gt;$method($args[0], $args[1], $args[2], $args[3]); default: return call_user_func_array([$instance, $method], $args); &#125; &#125; 这也就是为什么使用Facade的类可以直接使用静态调用的方式来调用方法, 正是Facade中的 __callStatic 方法生效了!","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"09. Facade 门面","slug":"OOP/2016-05-28-OOP-09-Facade","date":"2016-05-28T06:30:09.000Z","updated":"2018-03-15T11:03:00.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-09-Facade/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-09-Facade/","excerpt":"","text":"外观模式 外观模式中, 一个子系统的外部与其内部的通信, 通过一个统一的外观类进行 外观类将客户类与子系统的内部复杂性分隔开, 使得客户类只需要与外观角色打交道, 而不需要与子系统内部的很多对象打交道; 在外观模式中, 那些需要交互的业务类被称为子系统(Subsystem); 外观模式: 为子系统中的一组接口提供一个统一的入口 外观模式又称为门面模式, 它是一种对象结构型模式; 外观模式是迪米特法则的一种具体实现, 通过引入一个新的外观角色可以降低原有系统的复杂度, 同时降低客户类与子系统的耦合度。 迪米特法则(Law of Demeter)又叫作最少知道原则(Least Knowledge Principle 简写LKP)就是说一个对象应当对其他对象有尽可能少的了解, 不和陌生人说话。英文简写为: LoD. 外观模式没有一个一般化的类图描述, 通常使用下图来表示外观模式 外观模式包含如下两个角色： Facade(外观角色): 在客户端可以调用它的方法, 在外观角色中可以知道相关的(一个或者多个)子系统的功能和责任; 在正常情况下, 它将所有从客户端发来的请求委派到相应的子系统去, 传递给相应的子系统对象处理。 SubSystem(子系统角色): 在软件系统中可以有一个或者多个子系统角色, 每一个子系统可以不是一个单独的类, 而是一个类的集合, 它实现子系统的功能; 每一个子系统都可以被客户端直接调用, 或者被外观角色调用, 它处理由外观类传过来的请求; 子系统并不知道外观的存在, 对于子系统而言, 外观角色仅仅是另外一个客户端而已。 外观模式的主要目的在于降低系统的复杂程度, 在面向对象软件系统中, 类与类之间的关系越多, 表示系统中类之间的耦合度太大, 这样的系统在维护和修改时都缺乏灵活性, 因为一个类的改动会导致多个类发生变化, 而外观模式的引入在很大程度上降低了类与类之间的耦合关系。 引入外观模式之后, 增加新的子系统或者移除子系统都非常方便, 客户类无须进行修改(或者极少的修改), 只需要在外观类中增加或移除对子系统的引用即可。 从这一点来说, 外观模式在一定程度上并不符合开闭原则, 增加新的子系统需要对原有系统进行一定的修改, 虽然这个修改工作量不大。 ~~未完待续","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"08. Observer 观察者模式","slug":"OOP/2016-05-28-OOP-08-Observer","date":"2016-05-28T03:13:17.000Z","updated":"2018-03-15T10:38:29.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-08-Observer/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-08-Observer/","excerpt":"","text":"前言 在软件系统中, 有些对象之间存在类着似 交通信号灯和汽车 之间的关系: 一个对象的状态或行为的变化, 将导致其他对象的状态或行为也发生改变, 它们之间将产生联动; 为了更好地描述对象之间存在的这种一对多(包括一对一)的联动, 观察者模式应运而生, 它定义了对象之间一种一对多的依赖关系, 让一个对象的改变能够影响其他对象; 观察者模式 观察者模式是使用频率最高的设计模式之一, 是一种对象行为型模式; 它用于建立一种对象与对象之间的依赖关系, 一个对象发生改变时将自动通知其他对象, 其他对象将相应作出反应; 观察者模式(Observer Pattern)定义： 对象之间的一种一对多依赖关系, 使得每当一个对象状态发生改变时, 其相关依赖对象皆得到通知并被自动更新; 别名包括发布-订阅(Publish/Subscribe)模式、模型-视图(Model/View)模式, 源-监听器(Source/Listener)模式 或 从属者(Dependents)模式; 描述了如何建立对象与对象之间的依赖关系, 以及如何构造满足这种需求的系统; 包含观察目标 和 观察者 两类对象, 一个目标可以有任意数目的与之相依赖的观察者, 一旦目标的状态发生改变, 所有的观察者都将得到通知;为了获得这个通知, 每个观察者都将监视观察目标的状态, 以使其状态与目标状态同步, 这种交互也称为发布-订阅(Publish-Subscribe)。 目标是通知的发布者, 它发出通知时并不需要知道谁是它的观察者, 可以有任意数目的观察者订阅它并接收通知; 观察者模式结构中通常包括 目标 和 观察者 两个继承层次结构，其结构如下图所示： 在观察者模式结构图中包含如下几个角色: Subject(抽象目标): 它是被观察的对象, 可以是接口，也可以是抽象类;在目标中定义了一个观察者集合, 一个目标可以接受任意数量的观察者来观察目标自己;目标提供一系列方法来增加和删除观察者对象, 同时它定义了通知方法 notify(); ConcreteSubject(具体目标): 通常它包含有经常发生改变的数据, 当它的状态发生改变时, 向它的各个观察者发出通知; Observer(抽象观察者): 观察者将对目标的改变做出反应, 该接口/抽象类 声明了更新数据的方法 update() ConcreteObserver(具体观察者):在具体观察者中维护一个指向具体目标对象的引用, 它存储了本观察者的有关状态, 这些状态需要和具体目标的状态保持一致;它实现了在抽象观察者Observer中定义的update()方法, 可以调用具体目标类的 attach() 方法将自己添加到目标类的集合中或通过 detach() 方法将自己从目标类的集合中删除; Demo 先看目标 12345678910111213141516171819202122232425262728293031323334353637383940414243interface Subject&#123; //注册方法, 用于向观察者集合中增加一个观察者 public function attach(Observer $observer); //注销方法, 用于在观察者集合中删除一个观察者 public function detach(Observer $observer); //通知所有注册过的观察者对象 public function notify();&#125;class ConcreteSubject&#123; // 定义一个观察者集合,用于存放所有观察者对象 private $observers = []; //注册方法, 用于向观察者集合中增加一个观察者 public function attach(Observer $observer) &#123; return array_push($this-&gt;observers, $observer); &#125; //注销方法, 用于在观察者集合中删除一个观察者 public function detach(Observer $observer) &#123; $index = array_search($observer, $this-&gt;observers); if ($index === FALSE || ! array_key_exists($index, $this-&gt;observers)) &#123; return FALSE; &#125; unset($this-&gt;observers[$index]); return TRUE; &#125; //通知所有观察者 public function notify() &#123; if (!is_array($this-&gt;observers)) return false; foreach ($this-&gt;observers as $observer) $observer-&gt;update(); return true; &#125;&#125; 观察者 1234567891011121314151617181920//抽象观察者角色interface Observer &#123; // 更新方法 public function update();&#125;class ConcreteObserver implements Observer &#123; //观察者的名称 private $name; public function __construct($name) &#123; $this-&gt;name = $name; &#125; //更新方法 public function update() &#123; echo 'Observer ', $this-&gt;name, ' has notified.&lt;br /&gt;'; &#125;&#125; 客户端操作 1234567891011121314151617//实例化一个'观察目标'$subject = new ConcreteSubject();//实例化一个观察者$observer1 = new ConcreteObserver('lant01');//'观察目标'添加第一个观察者$subject-&gt;attach($observer1);//'观察目标' 通知 已经观察了目标自己的观察者$subject-&gt;notify();echo '添加第二个观察者后, 再次通知: &lt;br/&gt;';$observer2 = new ConcreteObserver('lant02');$subject-&gt;attach($observer2);$subject-&gt;notify();echo '删除第一个观察者后, 再次通知: &lt;br/&gt;';$subject-&gt;detach($observer1);$subject-&gt;notify(); 结果: 123456Observer lant01 has notified.添加第二个观察者后, 再次通知:Observer lant01 has notified.Observer lant02 has notified.删除第一个观察者后, 再次通知:Observer lant02 has notified.","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"07. Prototype Pattern 原型模式","slug":"OOP/2016-05-28-OOP-07-Prototype","date":"2016-05-28T02:50:21.000Z","updated":"2018-03-15T10:23:59.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-07-Prototype/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-07-Prototype/","excerpt":"","text":"背景 假设一套OA自动化办公系统在使用过程中, 由于某些岗位每周工作存在重复性, 工作周报内容都大同小异, 但是现在系统每周默认创建的周报都是空白报表，用户只能通过重新输入或不断复制粘贴来填写重复的周报内容，极大降低了工作效率; 开发人员通过对问题进行仔细分析, 决定按照如下思路对工作周报模块进行重新设计和实现： 除了允许用户创建新周报外, 还允许用户将创建好的周报保存为模板; 用户在再次创建周报时, 可以创建全新的周报，还可以选择合适的模板复制生成一份相同的周报，然后对新生成的周报根据实际情况进行修改，产生新的周报; 只要按照如上两个步骤进行处理, 工作周报的创建效率将得以大大提高。 上面你的过程类似日常使用电脑的两个基本操作: 复制和粘贴(通过对已有对象的复制和粘贴，我们可以创建大量的相同对象)。 如何在一个面向对象系统中实现对象的复制和粘贴呢？ 原型模式 在使用原型模式时，我们需要首先创建一个原型对象，再通过复制这个原型对象来创建更多同类型的对象; 原型模式的定义如下： 原型模式是一种对象创建型模式, 工作原理很简单: 将一个原型对象传给那个要发动创建的对象, 这个要发动创建的对象通过原型对象拷贝自己来实现创建过程; 原型模式是一种另类的创建型模式, 创建克隆对象的工厂就是原型类自身, 工厂方法由克隆方法来实现; 由于在软件系统中我们经常会遇到需要创建多个相同或者相似对象的情况，因此原型模式在真实开发中的使用频率还是非常高的。 PHP中的拷贝有 深拷贝 和 浅拷贝 浅拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 而且对其他对象的引用仍然是指向原来的对象, 即浅拷贝只负责当前对象实例, 对引用的对象不做拷贝; 深拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 除了那些引用其他对象的变量, 那些引用其他对象的变量将指向一个被拷贝的新对象，而不再是原来那些被引用的对象。(即深拷贝把要拷贝的对象所引用的对象也拷贝了一次, 而这种对被引用到的对象拷贝叫做间接拷贝)。 原型模式结构图 Prototype(抽象原型类): 它是声明克隆方法的接口, 是所有具体原型类的父类, 可以是抽象类也可以是接口, 甚至还可以是具体实现类; ConcretePrototype(具体原型类): 它实现在抽象原型类中声明的克隆方法, 在克隆方法中返回自己的一个克隆对象; Client(客户类): 让一个原型对象克隆其自身, 从而创建一个新的对象, 在客户类中只需要直接实例化或通过工厂方法等方式创建一个原型对象，再通过调用该对象的克隆方法即可得到多个相同的对象。 由于客户类针对抽象原型类Prototype编程，因此用户可以根据需要选择具体原型类，系统具有较好的可扩展性，增加或更换具体原型类都很方便。原型模式的核心在于如何实现克隆方法 原型模式简单演示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?phpinterface Prototype&#123; //浅拷贝 public function shallowCopy(); //深拷贝 public function deepCopy();&#125;class ConcretePrototype implements Prototype&#123; private $_name; public function __construct($name) &#123; $this-&gt;_name = $name; &#125; public function setName($name) &#123; $this-&gt;_name = $name; &#125; public function getName() &#123; return $this-&gt;_name; &#125; //浅拷贝 public function shallowCopy() &#123; return clone $this; &#125; //深拷贝 public function deepCopy() &#123; $serialize_obj = serialize($this); $clone_obj = unserialize($serialize_obj); return $clone_obj; &#125;&#125;class Demo&#123; public $string;&#125;class UsePrototype&#123; public function shallow() &#123; $demo = new Demo(); $demo-&gt;string = \"susan\"; $object_shallow_first = new ConcretePrototype($demo); $object_shallow_second = $object_shallow_first-&gt;shallowCopy(); var_dump($object_shallow_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_shallow_second-&gt;getName()); echo '&lt;br/&gt;'; $demo-&gt;string = \"sacha\"; var_dump($object_shallow_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_shallow_second-&gt;getName()); echo '&lt;br/&gt;'; &#125; public function deep() &#123; $demo = new Demo(); $demo-&gt;string = \"Siri\"; $object_deep_first = new ConcretePrototype($demo); $object_deep_second = $object_deep_first-&gt;deepCopy(); var_dump($object_deep_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_deep_second-&gt;getName()); echo '&lt;br/&gt;'; $demo-&gt;string = \"Demo\"; var_dump($object_deep_first-&gt;getName()); echo '&lt;br/&gt;'; // 这里就可以看出是深拷贝了, 因为这里的demo对象没有跟着上面变, 而是拷贝时的间接深度拷贝的 var_dump($object_deep_second-&gt;getName()); echo '&lt;br/&gt;'; &#125;&#125;$up = new UsePrototype;$up-&gt;shallow();echo '&lt;hr&gt;';$up-&gt;deep(); 原型管理器的引入和实现原型管理器(Prototype Manager)是将多个原型对象存储在一个集合中, 供客户端使用;它是一个专门负责克隆对象的工厂, 其中定义了一个集合用于存储原型对象, 如果需要某个原型对象的一个克隆，可以通过复制集合中对应的原型对象来获得。在原型管理器中针对抽象原型类进行编程, 以便扩展。其结构如下图：","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"05. Factory Method 工厂方法模式","slug":"OOP/2016-05-27-OOP-05-Factory","date":"2016-05-27T07:37:21.000Z","updated":"2018-03-15T10:03:34.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-05-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-05-Factory/","excerpt":"","text":"前言工厂模式是最常用的一类创建型设计模式, 通常所说的工厂模式是指工厂方法模式, 它也是使用频率最高的工厂模式。 日志记录器的设计假设系统需要做一个日志记录器(Logger), 可以通过多种方式记录日志(文件, Mysql, MongoDB…), 用户可以通过修改配置文件灵活地更换日志记录方式;日志记录器还可能会进行一些较为复杂的初始化工作(比如连接数据库, 读取配置文件等), 这就会导致代码较长, 如果将它们都写在构造函数中, 会导致构造函数庞大, 不利于代码的修改和维护; 简单工厂模式的设计 Sunny公司开发人员最初使用简单工厂模式对日志记录器进行了设计，初始结构如下图所示: 在图中, LoggerFactory充当创建日志记录器的工厂, 提供了工厂方法 createLogger() 用于创建日志记录器, Logger是抽象日志记录器接口, 其子类为具体日志记录器。其中, 工厂类LoggerFactory代码片段如下所示: 123456789101112131415161718192021222324252627//为了突出设计重点, 代码进行了简化, 省略了具体日志记录器类的初始化代码。//日志记录器工厂 class LoggerFactory &#123; //静态工厂方法 public static function createLogger($type) &#123; if($type == 'db')) &#123; //连接数据库, 代码省略 //... //创建数据库日志记录器对象 $logger = new DatabaseLogger(); //初始化数据库日志记录器，代码省略 //... return logger; &#125; else if ($type == 'file') &#123; //创建日志文件 //... //创建文件日志记录器对象 Logger logger = new FileLogger(); //初始化文件日志记录器，代码省略 //... return logger; &#125; else &#123; return null; &#125; &#125; &#125; 正如之前学习的简单工厂模式, 存在问题: 工厂类过于庞大，包含了大量的if…else…代码，导致维护和测试难度增大; 系统扩展不灵活，如果增加新类型的日志记录器，必须修改静态工厂方法的业务逻辑，违反了“开闭原则”。 如何解决这两个问题, 这就是本文所介绍的 工厂方法模式 的动机之一。 工厂方法 在简单工厂模式中只提供一个工厂类, 该工厂类处于对产品类进行实例化的中心位置, 它需要知道每一个产品对象的创建细节, 并决定何时实例化哪一个产品类。 简单工厂模式最大的缺点是: 当有新产品要加入到系统中时, 必须修改工厂类, 需要在其中加入必要的业务逻辑, 这违背了 开闭原则; 此外, 在简单工厂模式中, 所有的产品都由同一个工厂创建, 工厂类职责较重, 业务逻辑较为复杂, 具体产品与工厂类之间的耦合度高, 严重影响了系统的灵活性和扩展性, 而工厂方法模式则可以很好地解决这一问题; 在工厂方法模式中, 不再提供一个统一的工厂类来创建所有的产品对象, 而是提供一个抽象工厂接口来声明一个抽象的工厂, 而由其子类 具体工厂 来实现工厂方法, 你需要针对不同的产品提供不同的工厂; 在工厂方法模式结构图中包含如下几个角色: Factory（抽象工厂）：在抽象工厂类中, 声明了工厂方法(Factory Method)，用于返回一个产品。抽象工厂是工厂方法模式的核心，所有工厂类都必须实现该接口。 ConcreteFactory（具体工厂）：它是抽象工厂类的子类，实现了抽象工厂中定义的工厂方法，并可由客户端调用，返回一个具体产品类的实例。 Product（抽象产品）：它是定义产品的接口。 ConcreteProduct（具体产品）：它实现了抽象产品接口，某种类型的具体产品由专门的具体工厂创建, 具体工厂和具体产品之间一一对应。 与简单工厂模式相比, 工厂方法模式最重要的区别是引入了抽象工厂角色，抽象工厂可以是接口，也可以是抽象类，其典型代码如下所示： 123interface Factory &#123; public Product factoryMethod(); &#125; 在抽象工厂中声明了工厂方法, 具体产品对象的创建由其子类负责, 客户端针对抽象工厂编程, 可在运行时再指定具体工厂类, 具体工厂类实现了工厂方法, 不同的具体工厂可以创建不同的具体产品，其典型代码如下所示：12345678class ConcreteFactory implements Factory &#123; public function factoryMethod() &#123; // 在实际使用时，具体工厂类在实现工厂方法时, 除了创建具体产品对象之外， // 还可以负责产品对象的初始化工作以及一些资源和环境配置工作，例如连接数据库、创建文件等。 return new ConcreteProduct(); &#125; &#125; - 在客户端代码中，只需关心工厂类即可，**不同的具体工厂可以创建不同的产品**，典型的客户端类代码片段如下所示： 1234Factory factory; factory = new ConcreteFactory(); //可通过配置文件来实现 (通过配置来决定你要具体使用哪个具体的工厂类) Product product; product = factory-&gt;factoryMethod(); 可以通过配置文件来存储具体工厂类ConcreteFactory的类名，更换新的具体工厂时无须修改源代码，系统扩展更为方便。 小结工厂方法模式是简单工厂模式的延伸, 它继承了简单工厂模式的优点, 同时还弥补了简单工厂模式的不足;工厂方法模式是使用频率最高的设计模式之一, 是很多开源框架和API类库的核心模式;","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"04. Simple Factory 简单工厂模式","slug":"OOP/2016-05-27-OOP-04-Factory","date":"2016-05-27T07:08:11.000Z","updated":"2018-03-15T09:39:34.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-04-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-04-Factory/","excerpt":"","text":"前言 假设公司开发的CRM系统可以显示饼状图的效果,原始设计方案如下 123456789101112131415161718192021222324class Client&#123; public $chartObject = null; public function __construct($type) &#123; switch ($chartType) &#123; case 'pie' : $this-&gt;chartObject = new PieChart(); break; case 'bar' : $this-&gt;chartObject = new BarChart(); break; default: //TODO break; &#125; &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125;&#125; 客户端代码通过调用 Client类 的构造函数来创建图表对象, 根据参数 type 可以得到不同类型的图表，然后再调用show()方法来显示相应的图表。 传统设计存在问题不难看出，Client类是一个巨大的类 Client类中包含很多 “if…else…” / “switch…case…”代码块，整个类的代码相当冗长, 阅读难度、维护难度和测试难度也越大, 而且大量条件语句的存在还将影响系统的性能，程序在执行过程中需要做大量的条件判断; Client类的职责过重, 它将各种图表对象的创建和使用集中在一个类中实现, 违反了单一职责原则, 不利于类的重用和维护; 当需要增加新类型的图表时，必须修改Client类的源代码，违反了开闭原则; 客户端只能通过new关键字来直接创建图像对象, 图像类与客户端Client类耦合度较高 (比如一旦类的名字或参数发生变更, 你也必须修改Client代码的源代码); 客户端在创建Chart对象之前可能还需要进行大量初始化设置, 例如设置柱状图的颜色、高度等, 如果在Client类的构造函数中没有提供一个默认设置, 那就只能由客户端来完成初始设置，这些代码在每次创建图像对象时都会出现, 导致代码的重复。 简单工厂模式 为了将图像对象的创建和使用分离, 使用简单工厂模式对图表库进行重构, 重构后的结构如下图所示： 在图中, Chart接口充当抽象产品类, 其子类PieChart和BarChart充当具体产品类, ChartFactory充当工厂类。 现在, 我们使用工厂类的 静态工厂方法 来创建产品对象, 如果需要更换产品, 虽然也需要更改Client源码, 但是只需修改传递给静态工厂方法中的参数即可, 例如将柱状图改为饼状图, 只需将代码 $chartObject = ChartFactory::getChart(&quot;bar&quot;); 改为：$chartObject = ChartFactory::getChart(&quot;pie&quot;); 改进 现在你会发现: 在创建具体图像对象时, 每更换一个图像对象, 都需要修改客户端代码中静态工厂方法的参数(虽然修改都很小), 这对于客户端而言, 还是违反了“开闭原则”; 有没有一种方法能够在不修改客户端代码的前提下更换具体产品对象呢？答案是肯定的，下面将介绍一种常用的实现方式: 可以将静态工厂方法的参数放到配置文件中, 在配置文件中配置即可, 这样客户端代码如下所示：由$chartObject = ChartFactory::getChart(&quot;bar&quot;);改为：$type = Config::get('chartType'); $chartObject = ChartFactory::getChart($type); 简单工厂模式总结 简单工厂模式提供了专门的工厂类用于创建对象, 将对象的创建和对象的使用分离开, 它作为一种最简单的工厂模式在软件开发中得到了较为广泛的应用; 主要优点 工厂类包含必要的判断逻辑, 可以决定在什么时候创建哪一个产品类的实例, 客户端可以免除直接创建产品对象的职责, 而仅仅消费产品, 简单工厂模式实现了对象创建和使用的分离; 客户端无须知道所创建的具体产品类的类名，只需要知道创建具体产品类所需要对应的参数即可, 对于一些复杂的类名, 通过简单工厂模式可以在一定程度减少使用者的记忆量; 通过引入配置文件, 可以在不修改任何客户端代码的情况下更换和增加新的具体产品类, 在一定程度上提高了系统的灵活性, 做到了 一定程度的”开放-封闭原则”, 下面会看到其实工厂类部分还是违反的。 主要缺点 由于工厂类集中了所有产品的创建逻辑, 职责过重, 一旦不能正常工作, 整个系统都要受到影响; 使用简单工厂模式势, 势必会增加系统中类的个数(引入了新的工厂类), 增加了系统的复杂度和理解难度; 系统扩展困难，一旦添加新产品就不得不修改工厂逻辑, 在产品类型较多时, 有可能造成工厂逻辑过于复杂, 不利于系统的扩展和维护。(此处还是违反“开放-封闭原则”) 简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。 适用场景, 在以下情况下可以考虑使用简单工厂模式: 工厂类负责创建的对象比较少, 由于创建的对象较少, 不会造成工厂方法中的业务逻辑太过复杂。 客户端只知道传入工厂类的参数, 对于如何创建对象并不关心。 总之, 仍未完全做到 “开放-封闭原则”","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"03. 引入工厂类","slug":"OOP/2016-05-27-OOP-03-Why-Factory","date":"2016-05-27T06:33:19.000Z","updated":"2018-03-15T09:33:04.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-03-Why-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-03-Why-Factory/","excerpt":"","text":"前言 工厂模式, 包括简单工厂模式、工厂方法模式 和 抽象工厂模式 通常, 在客户端代码中直接使用new关键字是最简单的一种创建对象的方式, 但是它的 灵活性较差 假设系统可以显示饼状图的效果, 原始设计方案如下图: 1234567891011121314151617181920212223242526272829303132&lt;?php class PieChart &#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125; &#125; class BarChart &#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125; &#125; //Client class Client &#123; public $chartObject = null; public function __construct() &#123; $this-&gt;chartObject = new PieChart(); &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125; &#125; 上例在client类的构造函数中创建了PieChart类型的对象，并在show方法中调用了chartObject对象的display()方法, 这段代码看上去并没有什么问题; 但事实上: Client类即负责 对象的创建, 又负责 对象的使用, 创建对象和使用对象的职责耦合在一起; 同时, 这样的设计会导致一个很严重的问题, 这种强耦合必然违背了 开闭原则如果在 Client 中希望能够使用另一个种类型的图像方案, 比如使用柱状图BarChart类，那就必须修改Client类的源代码, 违反了”开闭原则”。 引出工厂类 最常用的一种解决方法是将 chartObject 对象的创建职责从 Client 类中移除, 通过引入工厂类, 在 Client 类之外创建对象: 客户类就不会再涉及对象的创建, 只是对对象进行使用 而创建对象的工厂类自然也只是负责创建对象, 也不会涉及对象的使用 引入工厂类 ChartFactory 之后的结构如下图所示: 工厂类的引入将降低维护工作量： 如果需要添加或移除不同的图像类, 你只要去维护 ChartFactory 的代码, 就不会影响到Client的代码; 而且如果 Chart 抽象接口发生改变, 例如添加、移除方法或改变方法名, 只需要修改Client代码, 不会给ChartFactory带来任何影响; 此外, 将对象的创建和使用分离还有一个好处: 防止用来实例化一个类的代码在多个类中到处都是, 可以将有关创建的代码搬移到一个工厂类中; 有时候创建一个对象不只是简单调用其构造函数, 还需要设置一些参数, 可能还需要配置环境, 此时, 可以引入工厂类来封装对象的创建逻辑和客户代码的实例化/配置选项。","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"01. Singleton单例模式","slug":"OOP/2016-05-27-OOP-01-Singleton","date":"2016-05-27T03:13:34.000Z","updated":"2018-03-15T09:22:43.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-01-Singleton/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-01-Singleton/","excerpt":"","text":"TP3.2分析 框架底层的数据库模型层对单例的使用 虽然 数据库模型层 使用了单例, 但并非传统意义上所谓 三公一私 的单例;TP3.2创建数据库模型实例的过程大概为: Think\\Model -&gt; Think\\Db -&gt; Think\\Db\\Driver\\Mysql -&gt; Think\\Db\\DriverD()/M() 方法都可以调用 Think\\Model 这个模型类, 虽然 Think\\Model 层并未做到单例, 即 new Model(…) 实例出的对象为非单例,但其通过调用下层 Think\\Db 的 getInstance(), 然后简单结合一个 数据库对象池$_db(注册树模式) 来保证底层各不同数据库对象的单例性;而 Think\\Db 其内部与底层沟通的方法全是static型, 用户在顶层控制器中直接new也没意义; 只有在顶层控制器直接 new Think\\Db\\Driver\\Mysql 你会获得不同的数据库连接实例, 但一般也不会直接new底层!最下层 Think\\Db\\Driver 为抽象层 优点 提供了对唯一实例的受控访问 由于在系统内存中只存在一个对象，因此可以节约系统资源，对于一些需要频繁创建和销毁的对象单例模式无疑可以提高系统的性能; 注意事项序列化问题 通常我们都是遵循正常的”三私一公”来写单例, 但是可以看到如下代码会因为 序列化, 反序列化 而导致单例出问题 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?phpclass Singleton&#123; private static $instance = null; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public static function getInstance() &#123; if (null === self::$instance) self::$instance = new self(); return self::$instance; &#125;&#125;echo &apos;&lt;pre/&gt;&apos;;$test_1 = Singleton::getInstance();$test_2 = Singleton::getInstance();var_dump($test_1); //实例1var_dump($test_2); //实例1var_dump($test_1 === $test_2); // trueecho &apos;unserialize, serialize:---------------------------&lt;br/&gt;&apos;;$test_1 = unserialize(serialize($test_1));var_dump($test_1); //实例2var_dump(Singleton::getInstance()); //实例1var_dump( Singleton::getInstance() === $test_1); //falseecho &apos;unserialize, serialize:---------------------------&lt;br/&gt;&apos;;$test_3 = Singleton::getInstance();var_dump($test_3); //实例1$test_3 = unserialize(serialize($test_3));var_dump($test_3); //实例3var_dump(Singleton::getInstance()); //实例1var_dump( Singleton::getInstance() === $test_3); //false 鸟哥博文其实并不能完全解决, 看下面例子: 虽然每次反序列化后的所有实例都一致, 但是一旦碰到再次反序列化, 还是会出问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?phpclass Singleton&#123; private static $instance = null; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public function __wakeup() &#123; self::$instance = $this; &#125; public static function getInstance() &#123; if (null === self::$instance) self::$instance = new self(); return self::$instance; &#125; public function __destruct() &#123; &#125;&#125;echo '&lt;pre/&gt;';$test_1 = Singleton::getInstance(); $test_2 = Singleton::getInstance(); var_dump($test_1); //实例1var_dump($test_2); //实例1var_dump($test_1 === $test_2); //trueecho 'unserialize, serialize:---------------------------&lt;br/&gt;';$test_1 = unserialize(serialize($test_1));var_dump($test_1); //实例2var_dump(Singleton::getInstance()); //实例2var_dump( Singleton::getInstance() === $test_1); //trueecho 'unserialize, serialize:---------------------------&lt;br/&gt;';$test_3 = Singleton::getInstance();var_dump($test_3); //实例2$test_3 = unserialize(serialize($test_3));var_dump($test_3); //实例3var_dump(Singleton::getInstance()); //实例3var_dump( Singleton::getInstance() === $test_3); //true 可以看到还是出现了多个不同的实例!!! 单例继承问题另外, 单例模式出现继承关系时, 需要注意: PHP的 self 和 static 关键字的区别 子类需要 清空静态实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?phpclass Singleton&#123; protected static $instance = null; protected function __construct() &#123; &#125; protected function __clone() &#123; &#125; protected function __wakeup() &#123; static::$instance = $this; &#125; public static function getInstance() &#123; if (null === static::$instance) static::$instance = new static(); return static::$instance; &#125;&#125;echo &apos;&lt;pre/&gt;&apos;;$test_1 = Singleton::getInstance();$test_2 = Singleton::getInstance();var_dump($test_1);var_dump($test_2);var_dump($test_1 === $test_2);class Log extends Singleton&#123; // 注意: 每个继承单例的子类, 必须要做清空, 否则所有的实例都是上面的实例结果 protected static $instance = null; public function write() &#123; echo &apos;success write something&apos;; &#125;&#125;class Model extends Singleton&#123; // 注意: 每个继承单例的子类, 必须要做清空, 否则所有的实例都是上面的实例结果 protected static $instance = null; public function select() &#123; echo &apos;success select something&apos;; &#125;&#125;$log_1 = Log::getInstance();$log_2 = Log::getInstance();var_dump($log_1);var_dump($log_2);var_dump($log_1 === $log_2);$model_1 = Model::getInstance();$model_2 = Model::getInstance();var_dump($model_1);var_dump($model_2);var_dump($model_1 === $model_2);var_dump($model_1 === $test_2); // false","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"0. S.O.L.I.D","slug":"OOP/2016-05-27-OOP-00-SOLID","date":"2016-05-27T02:07:12.000Z","updated":"2018-03-15T09:22:11.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-00-SOLID/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-00-SOLID/","excerpt":"","text":"单一职责原则 SRPThe Single Responsibility Principle 单一职责原则是最简单的面向对象设计原则, 用于控制类的粒度大小; 此原则的核心就是 解耦 和 增强内聚性 单一职责原则定义为: 一个类或者模块应该有且只有一个被改变的原因; 如果一个类承担的职责过多(耦合度就越大), 它被复用的可能性就越小; 一个职责的变化可能会影响其他的职责, 这种耦合会导致脆弱的设计, 当发生变化时, 设计会遭受到意想不到的破坏(因此要将这些职责进行分离，将不同的职责封装在不同的类中); 开放封闭原则 OCPThe Open/Closed Principle 开放-封闭原则: 一个软件实体应当对扩展开放, 对修改关闭(即软件实体应尽量在不修改原有代码的情况下进行扩展); 如果一个软件系统设计符合开闭原则, 则可以非常方便地对系统进行扩展，而且在扩展时无须修改现有代码。随着软件规模越来越大，软件寿命越来越长，软件维护成本越来越高，设计满足开闭原则的软件系统也变得越来越重要。 此原则的核心是 对抽象编程, 而不对具体编程; 为了满足开闭原则，需要对系统进行抽象化设计, 抽象化是开闭原则的关键; 例子, 假设系统可以显示各种类型的图表, 如 饼状图 和 柱状图 等, 为了支持多种图表显示方式 原始设计方案如下 代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?phpclass PieChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public $chartObject = null; public function __construct() &#123; //TODO &#125; public function display($chartType) &#123; switch ($chartType) &#123; case 'pie' : $piechart = new PieChart(); $piechart-&gt;display(); break; case 'bar' : $barchart = new BarChart(); $barchart-&gt;display(); break; default: //TODO break; &#125; &#125;&#125; 问题: 现在如果需要增加一个折线图LineChart, 你就要需要修改ChartDisplay类的display()方法的源代码, 增加新的判断逻辑, 违反了开闭原则; 现对该系统进行重构, 使之符合开闭原则 引入抽象图表类AbstractChart, 并且让ChartDisplay针对抽象图表类进行编程(依赖抽象), 再在ChartDisplay的display()方法中调用具体chart对象的display()方法显示图表。 接下来, 只需要将LineChart作为AbstractChart的子类, 在客户端向ChartDisplay中注入一个LineChart对象即可, 无须修改现有类库的源代码 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?phpabstract class AbstractChart&#123; protected function display() &#123; &#125;&#125;class PieChart extends AbstractChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart extends AbstractChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public function __construct() &#123; //TODO &#125; public function display(AbstractChart $chart) &#123; $chart-&gt;display(); &#125;&#125;$cd = new ChartDisplay();$cd-&gt;display(new PieChart());$cd-&gt;display(new BarChart()); 里氏替换原则LSPThe Liskov Substitution Principle 所有引用基类的地方, 必须能透明地使用其子类对象; 子类可以实现父类的抽象方法, 但是不能覆盖父类的非抽象方法, 也就是子类可以扩展父类的功能, 但是不能改变父类原有的功能; 主要就是说, 如果依赖的类将来有可能被扩展, 你最好设计一个抽象父类或接口, 子类继承、实现父类; 里氏代换原则是实现开闭原则的重要方式之一, 由于使用基类对象的地方都可以使用子类对象, 因此在程序中尽量使用基类类型来对对象进行定义, 而在运行时再确定其子类类型, 用子类对象来替换父类对象。 接口分离原则ISPThe Interface Segregation Principle 该原则比较好理解: 不要定义过于臃肿的接口, 接口中不要有很多不相关的逻辑方法(否则一定也违背单一职责原则); 过于臃肿的接口可能会强迫用户去实现接口内部用户并不需要的方法, 换句话说, 使用 多个专门的接口 比使用 一个臃肿的总接口 要好很多; 如果你在类中实现的接口中有你不需要使用方法, 估计也是重写为空方法, 这其实已经违背了接口分离原则。 也就是说，一个接口或者类应该拥有尽可能少的行为, 就是少到恰好能完成它自身的职责, 这也是保证软件系统模块的粒度尽可能少, 以达到高度可重用的目的; 依赖反转原则DIPThe Dependency Inversion Principle要针对接口编程, 而不是针对实现编程 如果说开闭原则是面向对象设计的目标的话, 那么依赖倒转原则就是面向对象设计的主要实现机制之一, 它是系统抽象化的具体实现; 上层不用去定义自己要依赖哪个具体的类, 而是定义自己依赖哪个 抽象; 然后让底层代码根据上层的要求, 去实现相应的 抽象; 这样就变成了底层对上层的依赖, 底层代码需要去 实现 上层代码定义的抽象; 在实现依赖倒转原则时, 我们需要针对抽象层编程, 将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象发生依赖关系时，通过抽象来注入所依赖的对象; 常用的注入方式有三种，分别是：构造注入，设值注入(Setter注入) 和 接口注入。 构造注入是指通过构造函数来传入具体类的对象 设值注入是指通过Setter方法来传入具体类的对象 而接口注入是指通过在接口中声明的业务方法来传入具体类的对象 小结在大多数情况下, 开闭原则、里氏代换原则 和 依赖倒转原则 这三个设计原则会同时出现: - 开闭原则是目标 - 里氏代换原则是基础 - 依赖倒转原则是手段 它们相辅相成, 相互补充, 目标一致, 只是分析问题时所站角度不同而已","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"04. 授权码模式各阶段参数分析","slug":"oauth/2016-10-25-OAuth-04","date":"2016-05-25T12:10:12.000Z","updated":"2018-03-14T01:33:15.000Z","comments":true,"path":"2016/05/25/oauth/2016-10-25-OAuth-04/","link":"","permalink":"http://blog.renyimin.com/2016/05/25/oauth/2016-10-25-OAuth-04/","excerpt":"","text":"第一阶段: 第三方站点将导向授权页 第三方应用将用户导向授权页时, 传递的参数如下: response_type: 表示授权类型, 必选项, 此处由于采用的是授权码模式, 所以值固定为 “code” client_id/AppID: 表示客户端的ID, 必选项由于你可能会有多个站点需要对接OAuth授权服务器, 所以一般在授权服务平台登录之后, 是可以创建多个 应用 的 (不同的站点对接授权服务器中不同的应用);每个应用对应你的一个 第三方站点, 开放平台会为每个应用(第三方站点)生成相应的 AppID 和 AppSecret/AppKey, 主要用来验证应用的合法性; redirect_uri: 设定的重定向到第三方站点URI, 必选项 scope: 表示申请的权限范围, 可选项 state: 表示客户端的当前状态, 可以指定任意值, 认证服务器会原封不动地返回这个值 下面是各开放平台的参数对比: 新浪 QQ 微信 GitHub 蚂蚁金服 简单测试: redirect_uri在OAuth服务器中为第三方站点创建 应用 的时候, 设定的回调地址, 无论在认证服务器, 还是在第三方站点, 都会对其进行校验, 以防篡改; 新浪授权传递错误 redirect_uri 简书qq授权传递错误 redirect_uri state第三方站点会对state做校验给了一个新的弹框用来进行授权, 但是如果恶意用户复制出弹框中的url, 之后再修改state并刷新页面, 授权后发现: 从上面各平台也可以看到, 返回参数相对比较简单; 返回的code是和授权页登录的用户身份相关的; (后面的access_token也是通过code和用户身份关联起来的) 第二阶段: 通过Authorization Code获取Access Token 如果第三方站点的用户在第一阶段的授权页中选择对第三方站点授权, 那么就第三方站点就会收到授权服务器的Authorization Code, 进而进入本阶段;(每个用户在授权后, 第三方站点都需要到授权服务器上为用户获取一个access_token, 这个access_token就是以后第三方站点从授权服务器上获取用户信息的凭证了, 一般在获取到access_token令牌之后, 可以存储到session中) 本阶段, 我们在自己的第三方站点中就可以使用第一阶段的Authorization Code获取Access Token: 微信 qq 基本上入参就像QQ互联那样大概有5个 (需要对每个参数进行了解); 本阶段的返回参数比较有讲究, 一般为如下三个 access_token 授权令牌access_token一般在获取到之后, 第三方站点可以将其 保存到用户的session中 , 第三方站点之后要获取用户在授权服务器上的资源的时候, 就需要带上当前session中用户的access_token去获取; expires_in 该access token的有效期,单位为秒 (微信公众平台access_token有效期为2小时, qq互联平台为3个月 可以作为参考)设置access_token有效期也是为了定期修改access_token, 以提高安全性;(并且微信对获取access_token这个基础API是有限制的，每天最多请求2000次, 因为有效期为2小时, 每天2000次也足够了;) refresh_token 授权自动续期时使用 (微信公众平台refresh_token有效期为30天, qq互联平台具体不详, 可以作为参考) (可选)权限自动续期问题 注意微信公众平台: 1.若access_token已超时，那么进行refresh_token会获取一个新的access_token，新的超时时间, 并且一旦使用refresh_token来刷新access_token的话, refresh_token的过期时间也会更新(自动延期) ; 2.若access_token未超时，那么进行refresh_token不会改变access_token，但超时时间会刷新，相当于续期access_token ; 这里说的超时时间刷新, 指的自然是第三方站点和授权服务器上的超时时间都要更新了(只不过你如果只是做对接的话, 授权服务器这部分人家已经开发好了, 如果你是授权服务器也是自己开发的话, 那你就需要注意这里了) 其实第三方站点在受到授权服务器分配给当前用户的access_token之后, 假设说授权服务器返回access_token的过期时间为7200s(2小时), 那么第三方站点将access_token保存到用户session中, 并设置过期时间为6600s(中间可以有个10分钟的服务器时间差); 如果第三方站点在使用access_token为用户获取授权服务器中的资源时, 发现session中的access_token并没有过期, 那么请求后就需要为access_token续期(第三方站点和授权服务器上都要做续期) 如果第三方站点在使用access_token为用户获取授权服务器中的资源时, 发现session中的access_token过期, 则就需要使用refresh_token调用生成access_token的api接口重新生成access_token来进行续期； qq: 这样下来, 基本上第三方站点只有在大于refresh_token的过期时间都没有调用过授权服务器的话, 才需要用户重新登录; 第三阶段(比较简单): 第三方站点通过access_token获取授权平台资源服务器上的用户资源 获取access_token后，进行接口调用，有以下前提： access_token有效且未超时； 微信用户已授权给第三方应用帐号相应接口作用域(scope);也就是在授权服务器上还会使用access_token去检测对应的scope权限是否正确; 许多开放平台在申请完access_token令牌之后, 都提供了对应接口来获取用户相关信息QQ互联提供了相应的接口, 使用Access Token来获取用户的OpenID;新浪开放平台提供了相应的接口来获取access_token对应的用户信息;而微信公众平台: 在获取access_token的时候, 会同时返回openid表示微信用户在本公众号中的唯一标识;","categories":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/categories/OAuth2-0/"}],"tags":[{"name":"OAuth2.0","slug":"OAuth2-0","permalink":"http://blog.renyimin.com/tags/OAuth2-0/"}]},{"title":"22. 分布式","slug":"memcache/2016-03-12-memcache-22","date":"2016-03-12T13:30:31.000Z","updated":"2018-03-12T05:58:58.000Z","comments":true,"path":"2016/03/12/memcache/2016-03-12-memcache-22/","link":"","permalink":"http://blog.renyimin.com/2016/03/12/memcache/2016-03-12-memcache-22/","excerpt":"","text":"前言 memcached虽然称为分布式缓存服务器, 但服务器端并没有分布式功能。服务器端仅包括内存存储功能，其实现非常简单。至于其分布式特性，则是完全由客户端程序库实现的, 这种分布式是memcached的最大特点。 好在php语言已经在memcache/memcached扩展中实现了分布式算法, 可以适当的配置或使用正确的参数来使用算法; 理解Memcached的分布式 假设memcached服务器有node1～node3三台, 应用程序要保存键名为’tokyo’, ‘beijing’…等的数据 存储: 要向memcached服务中添加key, 首先就需要将key传给客户端程序，然后客户端实现的算法就会根据key来决定数据应该保存到哪台memcached服务器, 服务器选定后，则会在该服务器上它保存key及其值 获取: 获取保存的key时, 也要将要key传递给客户端程序, 客户端程序要通过与设置key时相同的算法, 根据key选择服务器。 使用的算法相同, 就能选中与保存key时相同的服务器, 然后发送get命令, 就能获得保存的值 这样分布式部署后, 就将一台memcached的工作平均分配给多个memcache来分担, 所存储的键值对就会分散, 即使一台memcached服务器发生故障无法连接, 也不会影响其他的缓存数据, 系统依然能继续运行。 分布式算法取模(取余)算法 这是一个比较简单的分布式算法, 计算方法比较简单: key%memcache节点数量 向上面这样, 取模的结果为几, 就把数据缓存到第几台memcached服务器上, 数据的分散性也相当优秀, 但也有其缺点, 那就是当添加或者移除服务器的时候, 缓存重组的代价相当巨大, 因为添加服务器之后, 余数就会产生剧变, 这样就无法获取与保存时相同的服务器, 从而影响缓存的命中率; 假设一个节点掉线, 就会导致其他节点也无法正常命中 并且通过运算可以得出, 服务器的节点数越大, 在出现故障时, 后果越严重, 结果就是你买越多的服务器, 花了越多的钱, 却造成了越大的后果; 取模算法很不适合用作memcached服务的分布式算法; 一致性hash算法 首先求出memcached服务器(节点)的哈希值并将其分配到0～2^32的圆(continuum)上; 一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值 然后采用同样的方法求出存储数据的键的哈希值, 并映射到相同的圆上; 然后从数据映射到的位置开始顺时针查找, 将数据保存到找到的第一个服务器(节点)上; 如果超过2^32仍然找不到服务器, 就会保存到第一台memcached服务器上 ; 如下图 从上图可以看出, 如果node3失效, 造成的结果就是node4到node3之间的键会跑到node1上去寻找, 那自然就找不到了, 然后mysql就会把这个key和值重新查询出来, 再次写入到node1中即可;这样就只影响了node4到node3之间的节点, 只有node4到node3之间的节点需要到数据库中重新查询并写入node1中, 其他缓存数据的查询都没有受到影响 ;当然, 添加节点也是一样的, 影响都很小 ; 一致性hash在增加/减少一台机器后, 命中率为: (N-1)/N, 也就是会有1/N节点未命中; 这样服务器越多, 在添加服务器的时候, 命中率就越高了 ; 虚拟节点 一致性hash的影响虽然已经比取模分布式算法小很多了, 但是仍有一个问题, 那就是我们希望在node3丢失之后, node4-node3之间的这些缓存数据能够平均分配到剩余所有节点上, 而不是全部压在node1节点上, 导致node1节点压力很大，这就引入了虚拟节点的概念; 虚拟节点(virtual node)其实就是实际节点(机器)在hash空间的复制品 一个实际节点(机器)对应了若干个”虚拟节点”, 这个对应个数也称为”复制个数” ; 之前由于只部署了node1,node2,node3,node4四个实际节点, 之前的key在机器上的分布很不均衡, 现在我们以2个副本(复制个数)为例, 这样整个hash环中就存在了8个虚拟节点这样, key就不只是分布到node1,node2,node3,node4四个节点中了, 而是会更加均匀地分布到node1-1,node1-2,node2-1,node2-2,node3-1,node3-2,node4-1,node4-2这八个节点中了 如下图, 蓝色服务器宕机之后, 原本在蓝色服务器上的key会均匀地丢给其他几个节点, 而其他阶段不会受影响 : php实现一致性hash123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?phpinterface hash&#123; public function _hash($str);&#125;interface distribution&#123; public function lookUp($key); public function addNode($node); public function removeNode($node);&#125;// 一致性hashclass Consistent implements hash, distribution&#123; // 设置一个节点会增加64个虚拟节点 protected $_mul = 64; // 虚拟节点的位置 protected $position = []; // 把字符串转换成32位无符号整数 public function _hash($node) &#123; return sprintf(&apos;%u&apos;, crc32($node)); &#125; // 添加虚拟节点 public function addNode($node) &#123; for ($i = 0; $i &lt; $this-&gt;_mul; $i++) &#123; $virtualNode = $node . &apos;-&apos; . $i; $this-&gt;position[$this-&gt;_hash($virtualNode)] = $node; &#125; // 然后排序虚拟节点 ksort($this-&gt;position); &#125; public function lookUp($key) &#123; $key = $this-&gt;_hash($key); // 先取圆环上最小的节点作为结果 $point = current($this-&gt;position); foreach ($this-&gt;position as $k =&gt; $val) &#123; if ($key &lt;= $k) &#123; $point = $k; break; &#125; &#125; // 返回key所应该存放的位置 return $point; &#125; public function getNodes() &#123; echo &apos;&lt;pre/&gt;&apos;; print_r($this-&gt;position); &#125; public function removeNode($node) &#123; foreach ($this-&gt;position as $k =&gt; $v) &#123; if ($v == $node) &#123; unset($this-&gt;position[$k]); &#125; &#125; &#125;&#125;$c = new Consistent();$c-&gt;addNode(&apos;a&apos;);$c-&gt;addNode(&apos;b&apos;);$c-&gt;addNode(&apos;c&apos;);$c-&gt;getNodes();echo $c-&gt;_hash(&apos;title&apos;), &apos;&lt;br/&gt;&apos;;echo $c-&gt;lookUp(&apos;title&apos;);$c-&gt;removeNode(&apos;b&apos;);$c-&gt;getNodes();/*$m = new Moder();$m-&gt;addNode(&apos;a&apos;);$m-&gt;addNode(&apos;b&apos;);$m-&gt;addNode(&apos;c&apos;);$m-&gt;getNodes();echo $m-&gt;lookup(&apos;a&apos;);echo $m-&gt;lookup(&apos;b&apos;);echo $m-&gt;lookup(&apos;c&apos;);*/ 使用memcache扩展的一致性算法可以在php.ini中配置 : 1234[Memcache] Memcache.allow_failover = 1 Memcache.hash_strategy =consistent Memcache.hash_function =crc32 也可以动态配置 : 12ini_set(&apos;memcache.hash_function&apos;,&apos;crc32&apos;);ini_set(&apos;memcache.hash_strategy&apos;,&apos;consistent&apos;); // standard为取模分布式算法","categories":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/categories/memcached/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/tags/memcached/"}]},{"title":"让Json更懂中文(JSON_UNESCAPED_UNICODE)","slug":"PHP/2016-03-11-json","date":"2016-03-11T03:43:54.000Z","updated":"2018-03-14T01:45:01.000Z","comments":true,"path":"2016/03/11/PHP/2016-03-11-json/","link":"","permalink":"http://blog.renyimin.com/2016/03/11/PHP/2016-03-11-json/","excerpt":"","text":"当json中有中文时 1234567$data = [ &apos;name&apos; =&gt; &apos;任益民&apos;, &apos;age&apos; =&gt; &apos;20&apos;, &apos;gender&apos; =&gt; &apos;男&apos;];return json_encode($data);// 结果返回 &#123;&quot;name&quot;:&quot;\\u4efb\\u76ca\\u6c11&quot;,&quot;age&quot;:&quot;20&quot;,&quot;gender&quot;:&quot;\\u7537&quot;&#125; 为了让json更懂中文, PHP5.4, Json新增了一个选项: JSON_UNESCAPED_UNICODE, 故名思议, 就是说, Json不要编码Unicode 1234567$data = [ &apos;name&apos; =&gt; &apos;任益民&apos;, &apos;age&apos; =&gt; &apos;20&apos;, &apos;gender&apos; =&gt; &apos;男&apos;];return json_encode($data, JSON_UNESCAPED_UNICODE);// 结果返回 &#123;&quot;name&quot;:&quot;任益民&quot;,&quot;age&quot;:&quot;20&quot;,&quot;gender&quot;:&quot;男&quot;&#125; 参考: 鸟哥","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"21. 过期数据删除机制","slug":"memcache/2016-03-10-memcache-21","date":"2016-03-10T11:21:31.000Z","updated":"2018-03-12T05:10:31.000Z","comments":true,"path":"2016/03/10/memcache/2016-03-10-memcache-21/","link":"","permalink":"http://blog.renyimin.com/2016/03/10/memcache/2016-03-10-memcache-21/","excerpt":"","text":"在memcached中, 当某个key过期, 或者被删除之后, 这个key并不会立即从内存中删除, 而是 当某个新的key去占用它的chunk时, 它的chunk就会被当成空的; 当你再次get这个key时, 才会进行删除; 上面这种机制叫 lazy expiration 惰性失效 LRU : memcache在设置新的key时, 或优先使用已经超时的key的chunk空间, 但即使如此, 也会发生空间不足的情况; 此时会使用 Leasr Recently Used(LRU 最近最少使用)机制来分配空间, 将最近未被使用的记录的空间分配给新的key; 注意: 即使某个key设置的是永久有效, 也一样有可能被踢出;","categories":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/categories/memcached/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/tags/memcached/"}]},{"title":"20. 简介slab allocator内存分配机制","slug":"memcache/2016-03-10-memcache-20","date":"2016-03-10T10:51:28.000Z","updated":"2018-03-12T05:01:07.000Z","comments":true,"path":"2016/03/10/memcache/2016-03-10-memcache-20/","link":"","permalink":"http://blog.renyimin.com/2016/03/10/memcache/2016-03-10-memcache-20/","excerpt":"","text":"在memcached中, 为了缓解 内存碎片化 的问题, 使用了 slab allocator 机制来管理内存; slab allocator的原理比较简单 按照预先规定的大小, 将分配的内存分割成特定长度的块(chunk) 并把尺寸相同的块分成组 (slab) 需要存内容时, memcache会根据内容的大小, 为其选择合理的slab 从原理可以看出, slab allocator 机制其实依然是无法完全解决内存碎片的问题的, 只是缓解了问题 因为slab中的一个个chunck的大小都是固定的, 如果预先分割的chunk之间大小差距过大, 则难免会有浪费; 增长因子Grow factor 应该观察缓存数据大小的变化规律, 设置合理的增长因子Grow factor, 默认是1.25倍, 可以根据自己网站缓存的大小来调整比例因子; 在启动memcached服务器的时候, 确实可以看到slab和chunck的概念可以看到每种chunck之间的比例差不多就是在1.25倍左右(所以增长因子别设置的太大了, 就好比, 小碗吃米太少, 然后拿了个桶又太大, 因此应该拿个稍微大的碗, 比例不要增长的太大) 可以在启动时, 通过 -f 参数来设置增长因子","categories":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/categories/memcached/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"http://blog.renyimin.com/tags/memcached/"}]},{"title":"Output Control 静态化原理","slug":"PHP/2015-07-15-output-control","date":"2015-08-21T11:51:23.000Z","updated":"2018-03-16T02:44:47.000Z","comments":true,"path":"2015/08/21/PHP/2015-07-15-output-control/","link":"","permalink":"http://blog.renyimin.com/2015/08/21/PHP/2015-07-15-output-control/","excerpt":"","text":"使用输出缓冲控制中的函数不需要安装任何扩展，它们是PHP核心的一部分; php.ini配置输出缓冲控制中的函数, 它们的行为受 php.ini 配置的影响 output_buffering 该选项设置为 On 时, 将在所有的脚本中使用输出控制; 如果要限制输出缓冲区的最大值, 可将该选项设定为指定的最大字节数(例如: output_buffering=4096); 从PHP 4.3.5 版开始，该选项在PHP-CLI下总是为Off; output_handler该选项可将脚本所有的输出, 重定向到一个函数; implicit_flush 默认为FALSE, 如将该选项改为TRUE, PHP将使输出层在每段信息块输出后, 自动刷新; 这等同于在每次使用print, echo, include 等函数之后, 自动调用了PHP中的 flush() 函数; 不在web环境中使用PHP时, 打开这个选项对程序执行的性能有严重的影响, 通常只推荐在调试时使用。在 CLI SAPI 的执行模式下, 该标记默认为 TRUE; Output Control 常用函数ob_startbool ob_start ([ callback $output_callback [, int $chunk_size [, bool $erase ]]] ) : 此函数将打开输出缓冲 (可以多次打开, 相当于嵌套输出缓冲区);当输出缓冲激活后, 脚本将不会输出内容(除http标头外), 相反需要输出的内容被存储在 内部缓冲区中;一般我们都是直接使用 ob_start() 直接打开内部输出缓冲区; 如果涉及参数部分, 到时再进行补充 ob_get_level int ob_get_level ( void ) : 返回输出缓冲机制的嵌套级别, 如果没有开启缓冲区, 或缓冲区已被关闭, 则返回零; Demo: 1234567891011121314&lt;?php&lt;?phpvar_dump(ob_get_level()); //0ob_start();ob_start();var_dump(ob_get_level()); //2ob_start();var_dump(ob_get_level()); //3ob_end_flush();var_dump(ob_get_level()); //2ob_end_flush();var_dump(ob_get_level()); //1ob_end_flush();var_dump(ob_get_level()); //0 ob_get_length int ob_get_length ( void ): 返回输出缓冲区内容的长度, 如果没有开启缓冲区, 或缓冲区已被关闭, 则返回FALSE; demo 12345&lt;?phpob_start();echo 123 . &apos;任益民&apos;;$a = ob_get_length();var_dump($a); // 12 ob_get_contentsstring ob_get_contents ( void ): 获取输出缓冲区的内容 (只是得到输出缓冲区的内容); 如果没有开启缓冲区, 或缓冲区已被关闭, 则返回FALSE; ob_end_flushbool ob_end_flush ( void ) : 冲刷出(送出)输出缓冲区内容, 并关闭缓冲;如果想进一步处理缓冲区中的内容, 必须在ob_end_flush()之前调用ob_get_contents() ob_end_cleanbool ob_end_clean ( void ) : 清空(擦除)缓冲区并关闭输出缓冲;如果想进一步处理缓冲区中的内容, 必须在ob_end_clean()之前调用ob_get_contents() ob_flushvoid ob_flush ( void ) : 冲刷出(送出)输出缓冲区中的内容, 与 ob_end_flush 相比, 它不会关闭缓冲区 ob_cleanvoid ob_clean ( void ) : 清空(擦掉)输出缓冲区, 与 ob_end_clean() 相比, 它不会关闭缓冲区 ob_get_cleanstring ob_get_clean ( void ): 得到当前缓冲区的内容， 并清空当前输出缓冲区, 并且会关闭缓冲区; (手册说错了, 已有人在手册指出) ob_get_clean() 实质上是一起执行了 ob_get_contents() 和 ob_end_clean(); 如果没有开启缓冲区, 或缓冲区已被关闭, 则返回FALSE; ob_get_flushstring ob_get_flush ( void ): 得到当前缓冲区的内容, 并直接刷出(送出)缓冲区, 最后关闭输出缓冲区;如果输出缓冲区不是活跃的(未开启), 则返回 FALSE; ob_implicit_flush ob_implicit_flush : 打开/关闭绝对(隐式)刷送, 之前在提到pnp.ini的配置时, 已经提到过; 绝对(隐式)刷送: 将导致在每次输出调用后有一次刷送操作, 以便不再需要对 flush() 的显式调用; 也就是, 我们之前使用的 ob_flush, ob_get_flush, ob_end_flush 函数只是将输出缓冲区的内容刷出PHP缓冲; 而要想立即刷到客户端浏览器, 则还需要使用 flush(); ob_flush 和 flush其实这两个函数的区别网上有很多说明, 不过可能会由于Apache, Nginx的配置影响, 在测试过程中会出现与手册描述不符的现象;个人感觉重点是领会即可; 页面静态化原理不使用ob函数进行静态化通过简单的模板编译案例来了解 调用模板编译类 assign() 方法, 把即将填充到模板中的变量assign到模板编译类中; 调用模板编译类 display() 方法, 指定模板的路径; display()方法调用模板编译类中的 complie() 编译方法, 并传入模板路径; complie()中file_get_contents()读取模板的内容 complie()获取assign进来的变量, 与模板内容进行替换整合, 得到编译后的内容; // 检查编译后的文件是否存在并做过期行检测, 如果编译后的静态文件没有过期, 直接返回静态文件, 如果过期, 则重新进行编译并生成编译文件 file_put_content() 最终将编译好的内容生成编译文件; 最后将文件路径返回给display()方法; 最后display()方法直接 include 该文件即可; demo 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?php// 简化编译类class EasyComplie&#123; // 模板路径 public $template_dir = &apos;&apos;; // 编译后的静态文件路径 public $compile_dir = &apos;&apos;; // 接收assign的变量 public $arr; public function compile($template) &#123; $templateFile = $this-&gt;template_dir . &apos;/&apos; . $template; $compileFile = $this-&gt;compile_dir . &apos;/&apos; . $template . &apos;.php&apos;; if (file_exists($compileFile) &amp;&amp; filemtime($templateFile) &lt; filemtime($compileFile)) &#123; return $compileFile; &#125; else &#123; //读取模板的内容 $con = file_get_contents($templateFile); // 编译, 比如 str_replace() 进行替换 $res = $con; // 假设拿到了替换后的内容 //保存, $compileFile 是编译文件的路径 file_put_contents($compileFile, $res); // 最终返回路径 return $compileFile; &#125; &#125; public function display($template) &#123; $path = $this-&gt;compile($template); include($path); &#125; public function assign($k, $v) &#123; $this-&gt;arr[$k] = $v; return $this-&gt;arr; &#125;&#125; 小结可以看到, 上面并没有使用ob系列函数, 就可以实现页面静态化; 使用ob缓存实现静态化 调用模板编译类 assign() 方法, 把即将填充到模板中的变量assign到模板编译类中; 调用模板编译类 display() 方法, 指定模板的路径; display()方法调用模板编译类中的 complie() 编译方法, 并传入模板路径; complie()中先开启ob_start(), 然后可以include()直接将模板内容放入输出缓存区 ob_get_contents 获取输出缓存区中的模板内容 complie()获取assign进来的变量, 与模板内容进行替换整合, 得到编译后的内容;// 检查编译后的文件是否存在并做过期行检测, 如果编译后的静态文件没有过期, 直接返回静态文件, 如果过期, 则重新进行编译并生成编译文件 file_put_content() 最终将编译好的内容生成编译文件; ob_end_clean() 清空并关闭输出缓冲; (或者直接都没有下面的步骤了, 直接ob_end_flush将内容输出并关闭缓冲) 最后将文件路径返回给display()方法; 最后display()方法直接 include 该文件即可; Demo: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?php// 简化编译类class EasyComplie&#123; // 模板路径 public $template_dir = &apos;&apos;; // 编译后的静态文件路径 public $compile_dir = &apos;&apos;; // 接收assign的变量 public $arr; public function compile($template) &#123; $templateFile = $this-&gt;template_dir . &apos;/&apos; . $template; $compileFile = $this-&gt;compile_dir . &apos;/&apos; . $template . &apos;.php&apos;; if (file_exists($compileFile) &amp;&amp; filemtime($templateFile) &lt; filemtime($compileFile)) &#123; return $compileFile; &#125; else &#123; ob_start(); include($templateFile); $con = ob_get_contents(); // 编译, 比如 str_replace() 进行替换 $res = $con; // 假设拿到了替换后的内容 //保存, $compileFile 是编译文件的路径 file_put_contents($compileFile, $res); // 接下来直接关闭并清空输出缓冲 ob_end_clean(); // 最终返回路径 return $compileFile; // 接下来直接关闭输出缓冲并直接进行输出 // ob_end_flush(); // flush(); &#125; &#125; public function display($template) &#123; $path = $this-&gt;compile($template); include($path); &#125; public function assign($k, $v) &#123; $this-&gt;arr[$k] = $v; return $this-&gt;arr; &#125;&#125;","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]}]}