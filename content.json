{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"05. MySQL5.7 源码编译安装","slug":"MySQL/2018-10-14-mysql-05","date":"2019-04-25T09:11:24.000Z","updated":"2019-04-25T09:11:24.000Z","comments":true,"path":"2019/04/25/MySQL/2018-10-14-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2019/04/25/MySQL/2018-10-14-mysql-05/","excerpt":"","text":"源码下载 mysql5.7.25 源码下载 因为 mysql5.7系列要求安装boost_1_59_0, 这里我们选择包含boost的版本 (相对于前者, 它内部已经准备好了boost, 不用你再去下载对应的boost版本) 依赖安装 参考 cmake: mysql使用cmake跨平台工具预编译源码, 用于设置mysql的编译参数 (如: 安装目录、数据存放目录、字符编码、排序规则等, 安装最新版本即可) make3.75 : mysql源代码是由C和C++语言编写, 在linux下使用make对源码进行编译和构建, 要求必须安装 make 3.75或以上版本 (yum默认都是安装最新版的软件) gcc4.4.6 : GCC是Linux下的C语言编译工具, mysql源码编译完全由C和C++编写, 要求必须安装GCC4.4.6或以上版本 Boost1.59.0 : mysql源码中用到了C++的Boost库, 要求必须安装boost1.59.0或以上版本 bison2.1 : Linux下C/C++语法分析器 ncurses : 字符终端处理库 所以在安装前, 需先安装相关的依赖库 1yum -y install gcc gcc-c++ ncurses ncurses-devel bison libgcrypt perl make cmake 编译、安装 相比之前的Mysql编译选项, CMake编译选项现在都是大写的, 参考 在编译时通常会设置的编译选项如下: (可参考 lnmp1.6 中的MySQL5.7编译脚本) 123456789101112131415161718192021222324cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql // 指定MySQL程序的安装目录, 默认 /usr/local/mysql-DSYSCONFDIR=/usr/local/mysql/etc // 初始化参数文件my.cnf的目录, 貌似没有默认值, 需要指定-DWITH_MYISAM_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_INNOBASE_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_PARTITION_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DWITH_FEDERATED_STORAGE_ENGINE=1 // 指定静态编译到mysql的存储引擎, InnoDB, MyISAM, MERGE, MEMORY和CSV引擎是必需的（始终编译到服务器中）, 无需显式安装。-DEXTRA_CHARSETS=all // 默认就是all-DDEFAULT_CHARSET=utf8mb4 // 指定服务器默认字符集, 默认 latin1-DDEFAULT_COLLATION=utf8mb4_general_ci // 指定服务器默认的校对规则, 默认 latin1_general_ci-DWITH_EMBEDDED_SERVER=1 -DENABLED_LOCAL_INFILE=1 // 指定是否允许本地执行 LOAD DATA INFILE, 默认OFF-DWITH_BOOST=/usr/local/src/mysql-5.7.25/boost // 不用指定 -DENABLE_DOWNLOADS=1# 以下选项, lnmp1.6 并没有指定-DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_BLACKHOLE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DMYSQL_TCP_PORT // 指定服务器默认的端口号, 默认 3306-DMYSQL_DATADIR=/usr/local/mysql/data // 数据文件目录 (这个没有默认值)-MYSQL_UNIX_ADDR // socket文件路径, 默认 /tmp/mysql.sock-DENABLE_DOWNLOADS=1 // 是否要下载可选的文件。例如, 启用此选项（设置为1）, cmake将下载谷歌所使用的测试套件运行单元测试。-DMYSQL_USER=mysql // 官网没找到该选项-DDOWNLOAD_BOOST // 不用指定, 因为本次选择的是 mysql-boost 版本的, 所以不用下载boost, 而且下载过程很容易被墙而下载失败 编译安装 123cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DSYSCONFDIR=/usr/local/mysql/etc -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DWITH_FEDERATED_STORAGE_ENGINE=1 -DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8mb4 -DDEFAULT_COLLATION=utf8mb4_general_ci -DWITH_EMBEDDED_SERVER=1 -DENABLED_LOCAL_INFILE=1 -DWITH_BOOST=/usr/local/src/mysql-5.7.25/boost -DMYSQL_DATADIR=/usr/local/mysql/datamake // 花费时间可能会比较长make install 已经生成 /usr/local/mysql my.cnf 首先, 在编译时, 我们通过 -DSYSCONFDIR 指定的 my.cnf 的路径是在 /usr/local/mysql/etc 下, 但是编译完成后, /usr/local/mysql/ 下并没有 etc 目录 这个选项只是指定将来 my.cnf 的位置, 不会帮我们创建目录和文件 (包括之前在编译时通过 -DMYSQL_DATADIR=/usr/local/mysql/data 指定的mysql的数据目录, 都是需要自己创建) 另外, mysql5.7.18之后, 貌似已经不在解压包的support-files目录中提供my-default.cnf文件, 参考 参考, 在Unix和类Unix系统上, MySQL程序按照指定的顺序从下表中显示的文件中读取启动选项（首先列出的文件首先读取, 后面读取的文件优先） (你会发现在centos7-minimal系统的/etc下默认就有my.cnf文件) 自己准备 my.cnf 文件 Mysql参数优化对于新手来讲, 是比较难懂的东西, 其实这个参数优化, 是个很复杂的东西, 对于不同的网站, 及其在线量, 访问量, 帖子数量, 网络情况, 以及机器硬件配置都有关系, 优化不可能一次性完成, 需要不断的观察以及调试, 才有可能得到最佳效果 下面是个my.cnf示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185[client] # 客户端设置，即客户端默认的连接参数default-character-set = utf8mb4 port = 3306 # 默认连接端口socket = /usr/local/mysql/mysql.sock # 用于本地连接的socket套接字 (注意client的socket要和mysqld是一样的，因为客户端和服务端通信靠的就是这个文件，肯定要一致)[mysql] # 服务端基本设置#auto-rehash # MySQL开启命令自动补全功能, mysql命令行工具自带这个功能, 但是默认是禁用的, 想启用其实很简单, 打开配置文件找到 no-auto-rehash, 用符号 # 将其注释, 另外增加auto-rehash即可 # 亲测, 感觉不咋实用[mysqld]user=mysql # ?? mysqld程序在启动后将在给定UNIX/Linux账户下执行 mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序socket = /usr/local/mysql/mysql.sock # 为MySQL客户端程序和服务器之间的本地通讯指定一个套接字文件 (编译时未指定socket位置, 就用默认的位置)port=3306 # MySQL监听端口basedir=/usr/local/mysql # 指定了安装 MySQL 的安装路径, 填写全路径可以解决相对路径所造成的问题datadir=/usr/local/mysql/data # 数据文件存放的目录tmpdir = /usr/local/mysql/tmp # ?? MySQL存放临时文件的目录symbolic-links=0 # ?? 是否支持符号链接, 即数据库或表可以存储在my.cnf中指定datadir之外的分区或目录, 为0不开启log-error=/usr/local/mysql/logs/mysql.log # 错误日志位置pid-file=/usr/local/mysql/mysqld.pidskip-name-resolve # ?? 禁用主机名解析 (待答疑)default-storage-engine = InnoDB # 默认的数据库引擎innodb-file-per-table=1 # ??innodb_force_recovery = 1 # ??group_concat_max_len = 10240 # ??#sql_mode = &apos;PIPES_AS_CONCAT,ANSI_QUOTES,IGNORE_SPACE,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_FIELD_OPTIONS,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;sql_mode = NO_ENGINE_SUBSTITUTION,NO_AUTO_CREATE_USER # sql_mode, 定义了mysql应该支持的sql语法，数据校验等 # NO_AUTO_CREATE_USER: 禁止GRANT创建密码为空的用户 # NO_ENGINE_SUBSTITUTION: 如果需要的存储引擎被禁用或未编译, 可以防止自动替换存储引擎expire_logs_days = 7 # ??memlock # ??### 字符集配置character-set-client-handshake = FALSEcharacter-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect=&apos;SET NAMES utf8mb4&apos; # ??### GTIDserver_id = 330759 # ?? 服务端ID，用来高可用时做区分binlog_format = row # ?? 为保证 GTID 复制的稳定, 行级日志binlog_rows_query_log_events=1 # 在row模式下, 开启该参数, 将把sql语句打印到binlog日志里面, 默认是0(off), 虽然将语句放入了binlog,但不会执行这个sql,就相当于注释一样.但对于dba来说,在查看binlog的时候,很有用处.binlog_row_image=&apos;minimal&apos; # 默认为full, 在binlog为row格式下, full将记录update前后所有字段的值, minimal时, 只记录更改字段的值和where字段的值, noblob时, 记录除了blob和text的所有字段的值, 如果update的blob或text字段,也只记录该字段更改后的值,更改前的不记录; # 大家都知道row格式下的binlog增长速度太快, 对存储空间, 主从传输都是一个不小的压力.因为每个event记录了所有更改数据前后的值,不管数据是否有改动都会记录.binlog_row_image的引入就大大减少了binlog记录的数据.在结合binlog_rows_query_log_events,对于dba日常维护binlog是完全没有压力的,而且节省了硬盘空间开销,减小I/O,减少了主从传输压力;gtid_mode = on # ?? 开启 gtid 功能# 保障 GTID 事务安全# 当启用 enforce_gtid_consistency 功能时# MySQL只允许能够保障事务安全, 并且能够被日志记录的SQL语句被执行# 像create table ... select 和 create temporarytable语句# 以及同时更新事务表和非事务表的SQL语句或事务都不允许执行enforce-gtid-consistency = true # 以下两条配置为主从切换, 数据库高可用的必须配置log_bin = mysql57-bin # 开启 binlog 日志功能log-slave-updates = on # ?? 开启从库更新 binlog 日志skip_slave_start=1 # ?? slave复制进程不随mysql启动而启动### 慢查询日志slow_query_log = 1 # 打开慢查询日志功能long_query_time = 2 # 超过2秒的查询记录下来log_queries_not_using_indexes = 0 # 记录下没有使用索引的查询slow_query_log_file =/usr/local/mysql/logs/slow.log # 慢查询日志文件#log=/data/logs/mysql57/all.log### 自动修复relay_log_info_repository = TABLE # 记录 relay.info 到数据表中master_info_repository = TABLE # 记录 master.info 到数据表中 relay_log_recovery = on # 启用 relaylog 的自动修复功能relay_log_purge = 1 # 在 SQL 线程执行完一个 relaylog 后自动删除### 数据安全性配置log_bin_trust_function_creators = on # wei关闭 master 创建 function 的功能sync_binlog = 1 # 每执行一个事务都强制写入磁盘explicit_defaults_for_timestamp=true # 设置 timestamp 的列值为 null, 不会被设置为 current timestamp # timestamp 列如果没有显式定义为 not null, 则支持null属性### 优化配置ft_min_word_len = 1 # 优化中文全文模糊索引lower_case_table_names = 1 # 默认库名表名保存为小写, 不区分大小写max_allowed_packet = 256M # 单条记录写入最大的大小限制, 过小可能会导致写入(导入)数据失败#rpl_semi_sync_master_enabled = 1 # ?? 半同步复制开启#rpl_semi_sync_slave_enabled = 1 #rpl_semi_sync_master_timeout = 1000 # ?? 半同步复制超时时间设置#rpl_semi_sync_master_wait_point = AFTER_SYNC # ?? 复制模式(保持系统默认)#rpl_semi_sync_master_wait_slave_count = 1 # 多线程复制 # ?? 后端只要有一台收到日志并写入 relaylog 就算成功slave_parallel_type = logical_clock # ?? 在MySQL5.7版本后可以利用逻辑时钟方式分配SQL多线程slave_parallel_workers = 4 # ?? 并行的SQL线程数量, 此参数只有设置 1&lt;N 的情况下才会才起N个线程进行SQL重做 # ?? 经过测试对比发现, 如果主库的连接线程为M, 只有M &lt; N的情况下, 备库的延迟才可以完全避免### 连接数限制max_connections = 1500 max_connect_errors = 200 # 验证密码超过20次拒绝连接# back_log值指出在mysql暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中# 也就是说, 如果MySql的连接数达到max_connections时, 新来的请求将会被存在堆栈中# 以等待某一连接释放资源, 该堆栈的数量即back_log, 如果等待连接的数量超过back_log# 将不被授予连接资源back_log = 500open_files_limit = 65535 # 服务器关闭交互式连接前等待活动的秒数interactive_timeout = 3600 # 服务器关闭非交互连接之前等待活动的秒数wait_timeout = 3600### 内存分配# 指定表高速缓存的大小。每当MySQL访问一个表时, 如果在表缓冲区中还有空间# 该表就被打开并放入其中, 这样可以更快地访问表内容table_open_cache = 1024 binlog_cache_size = 4M # 为每个session 分配的内存, 在事务过程中用来存储二进制日志的缓存tmp_table_size = 128M # 在内存的临时表最大大小# 创建内存表的最大大小(保持系统默认, 不允许创建过大的内存表)# 如果有需求当做缓存来用, 可以适当调大此值max_heap_table_size = 16M# 顺序读, 读入缓冲区大小设置# 全表扫描次数多的话, 可以调大此值read_buffer_size = 1M# 随机读, 读入缓冲区大小设置read_rnd_buffer_size = 8M# 高并发的情况下, 需要减小此值到64K-128Ksort_buffer_size = 1M# 每个查询最大的缓存大小是1M, 最大缓存64M 数据query_cache_size = 64Mquery_cache_limit = 1M# 提到 join 的效率join_buffer_size = 16M# 线程连接重复利用thread_cache_size = 64### InnoDB 优化## 内存利用方面的设置# 数据缓冲区innodb_buffer_pool_size=2G## 日志方面设置# 事务日志大小innodb_log_file_size = 256M# 日志缓冲区大小innodb_log_buffer_size = 4M# 事务在内存中的缓冲innodb_log_buffer_size = 3M# 主库保持系统默认, 事务立即写入磁盘, 不会丢失任何一个事务innodb_flush_log_at_trx_commit = 1# mysql 的数据文件设置, 初始100, 以10M 自动扩展#innodb_data_file_path = ibdata1:100M:autoextend# 为提高性能, MySQL可以以循环方式将日志文件写到多个文件innodb_log_files_in_group = 3##其他设置# 如果库里的表特别多的情况, 请增加此值#innodb_open_files = 800# 为每个 InnoDB 表分配单独的表空间innodb_file_per_table = 1# InnoDB 使用后台线程处理数据页上写 I/O（输入）请求的数量innodb_write_io_threads = 8# InnoDB 使用后台线程处理数据页上读 I/O（输出）请求的数量innodb_read_io_threads = 8# 启用单独的线程来回收无用的数据innodb_purge_threads = 1# 脏数据刷入磁盘(先保持系统默认, swap 过多使用时, 调小此值, 调小后, 与磁盘交互增多, 性能降低)innodb_max_dirty_pages_pct = 90 # 事务等待获取资源等待的最长时间innodb_lock_wait_timeout = 120 # 开启 InnoDB 严格检查模式, 不警告, 直接报错 1开启 0关闭innodb_strict_mode=1 # 允许列索引最大达到3072innodb_large_prefix = on[mysqldump]# 开启快速导出quickdefault-character-set = utf8mb4max_allowed_packet = 256M## include all files from the config directory#!includedir /etc/my.cnf.d https://www.cnblogs.com/pengineer/p/4845218.html 问题 貌似开启 skip-name-resolve模式 后, 启动mysqld的话, 会报警告 (貌似是因为初始数据库的时候, 会默认有一个 root | localhost 用户生成, 有该模式不支持反向dns, 但是默认却使用了localhost，所以会有警告) 123456789mysql&gt; select user,host from user;+---------------+-----------+| user | host |+---------------+-----------+| mysql.session | localhost || mysql.sys | localhost || root | localhost |+---------------+-----------+3 rows in set (0.00 sec) 参考: http://blog.itpub.net/14184018/viewspace-1061224/ 1234567891011121314152019-03-29T09:21:02.228170Z 0 [Note] Server hostname (bind-address): &apos;*&apos;; port: 33062019-03-29T09:21:02.228201Z 0 [Note] IPv6 is available.2019-03-29T09:21:02.228211Z 0 [Note] - &apos;::&apos; resolves to &apos;::&apos;;2019-03-29T09:21:02.228223Z 0 [Note] Server socket created on IP: &apos;::&apos;.2019-03-29T09:21:02.231618Z 0 [Warning] &apos;user&apos; entry &apos;mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231654Z 0 [Warning] &apos;user&apos; entry &apos;mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231665Z 0 [Warning] &apos;user&apos; entry &apos;lant@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231671Z 0 [Warning] &apos;user&apos; entry &apos;lant1@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231695Z 0 [Warning] &apos;db&apos; entry &apos;performance_schema mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231699Z 0 [Warning] &apos;db&apos; entry &apos;sys mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.231717Z 0 [Warning] &apos;proxies_priv&apos; entry &apos;@ root@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.232858Z 0 [Warning] &apos;tables_priv&apos; entry &apos;user mysql.session@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.232897Z 0 [Warning] &apos;tables_priv&apos; entry &apos;sys_config mysql.sys@localhost&apos; ignored in --skip-name-resolve mode.2019-03-29T09:21:02.242378Z 0 [Note] Event Scheduler: Loaded 0 events2019-03-29T09:21:02.242750Z 0 [Note] /usr/local/mysql/bin/mysqld: ready for connections. 编码相关https://blog.csdn.net/javandroid/article/details/81235387 运行用户 (-M: 不要自动建立用户的登入目录; -s: 指定用户登入后所使用的shell; -g: 指定用户所属的起始群组 ) 12groupadd mysqluseradd -s /sbin/nologin -M -g mysql mysql 创建所需目录 12cd /usr/local/mysql/mkdir logs tmp data 设置目录相关权限 1chown -R mysql:mysql /usr/local/mysql/ root密码忘记 mysqld --skip-grant-tables &amp; mysql -uroot -p 直接回车即可设置环境变量 环境变量设置 123echo &apos;PATH=/usr/local/mysql/bin:/usr/local/mysql/lib:$PATH&apos; &gt;&gt; /etc/profile // 设置环境变量, 并开机运行echo &apos;export PATH&apos; &gt;&gt; /etc/profile //把PATH设为全局变量source /etc/profile 此时就可以全局使用/usr/local/mysql/bin下的命令 1234567891011121314151617[root@lant mysql]# mysqld --help2019-03-28T04:11:28.230733Z 0 [ERROR] mysqld: option &apos;--init_connect&apos; requires an argumentmysqld Ver 5.7.25 for Linux on x86_64 (Source distribution)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Starts the MySQL database server.Usage: mysqld [OPTIONS]For more help options (several pages), use mysqld --verbose --help.2019-03-28T04:11:28.231753Z 0 [ERROR] Aborting[root@lant mysql]# 初始化数据库 会自动在/usr/local/mysql下生成data目录, 并且是mysql用户身份 1mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 开机启动 将mysql添加到systemctl系统服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445cp /usr/local/src/mysql-5.7.23/scripts/systemd/mysqld.service.in /usr/lib/systemd/system/mysqld.servicecd /usr/lib/systemd/system/vi mysqld.service[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/usr/local/mysql/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables // 这里没找到这个脚本, 就先注释掉了#ExecStartPre=/usr/local/mysql/bin/mysqld_pre_systemd# Start main serviceExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysql# Sets open_files_limitLimitNOFILE = 5000Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false 然后设置开机自动启动: 123456789systemctl daemon-reloadsystemctl start mysqldnetstat -anpt | grep 3306systemctl enable mysqld[root@lant mysql]# ps aux |grep mysqlmysql 2833 0.1 9.8 1768144 179708 ? Sl 02:40 0:00 /usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pidroot 2867 0.0 0.0 112704 972 pts/0 S+ 02:43 0:00 grep --color=auto mysql[root@lant mysql]# 设置密码 默认没有密码, 所以直接设置新密码后即可登录 12mysqladmin -u root -p password &quot;renyimin&quot;;mysql -uroot -p 外部客户端连接? 最后更新时间 2018/08/16 安装存储引擎 : https://www.cnblogs.com/wt645631686/p/8086682.html 编译过程中的问题 CMAKE 编译参数指定后, 是否在启动 mysql 时, 还可以动态修改配置? CMAKE 编译参数未指定的话, 是否在启动mysql时, 还可以动态修改配置? 存储引擎是以插件形式存在的, 如果编译时没有将某个存储引擎编译进来, 后续应该就不能用了https://www.cnblogs.com/hllnj2008/p/4043778.html 如果忘记编译进某个存储引擎 https://help.aliyun.com/knowledge_detail/41107.html?spm=5176.11065259.1996646101.searchclickresult.35c13f8dXRN0Rx 3","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"60. EXPLAIN -- Extra","slug":"MySQL/2019-04-15-mysql-60","date":"2019-04-15T08:53:49.000Z","updated":"2019-04-17T08:36:06.000Z","comments":true,"path":"2019/04/15/MySQL/2019-04-15-mysql-60/","link":"","permalink":"http://blog.renyimin.com/2019/04/15/MySQL/2019-04-15-mysql-60/","excerpt":"","text":"Extra列包含有关MySQL如何解析查询的其他信息, 该列显示的是不在其他列显示的额外信息; 此字段能够给出让我们深入理解执行计划进一步的细节信息, 比如是否使用ICP, MRR等 Using filesort 当 SQL 中包含 ORDER BY, 而且无法利用索引完成排序操作的时候, MySQL Query Optimizer 不得不选择相应的排序算法来实现; 数据较少时从内存排序, 否则从磁盘排序; Explain不会显示的告诉客户端用哪种排序 官方解释: “MySQL需要额外的一次传递, 以找出如何按排序顺序检索行, 通过根据联接类型浏览所有行并为所有匹配WHERE子句的行保存排序关键字和行的指针来完成排序。然后关键字被排序, 并按排序顺序检索行” 示例: 使用的排序字段并没有为其创建索引 1234567mysql&gt; explain select `name` from t3 where id in (1,2) order by name;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+| 1 | SIMPLE | t3 | NULL | range | PRIMARY | PRIMARY | 4 | NULL | 2 | 100.00 | Using where; Using filesort |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-----------------------------+1 row in set, 1 warning (0.00 sec) Using index 主键-覆盖索引 直接在主键索引上完成查询和所有数据的获取, 一般是只获取主键ID (主键 覆盖索引) 示例: 1234567mysql&gt; explain select id from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) 示例: 如果使用的是主键, 但是select的字段列表不止主键id, Extra 为 NULL 123456789101112131415mysql&gt; explain select id,goods_name from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) mysql&gt; explain select * from explain_goods where id=1;+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+---------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 小结: where条件使用了主键索引, 且 select 只有主键 Using where; Using index 二级索引-覆盖索引 直接在二级索引(覆盖索引)上获取全部数据 示例: 1234567891011121314mysql&gt; explain select id from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using where; Using index |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; explain select id,goods_name from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using where; Using index |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+--------------------------+ 小结: where条件用到了二级索引, 并且 select 做到覆盖索引 Using index condition 二级索引-非覆盖索引 where条件用到了二级索引, 但无法提供所有select字段: 先在二级索引上使用索引查找到主键ID, 然后在主键索引上通过主键ID进行查找 这里之所以需要去主键索引上查, 是因为 select 需要的数据, 二级索引不能完全提供 示例: 123456789101112131415mysql&gt; explain select * from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) mysql&gt; explain select id,goods_name,goods_status from explain_goods where goods_name=&quot;华为&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) 小结: where条件用到了二级索引, 只不过 select 未做到覆盖索引 注意: 如果是范围查找, 优化器会在索引存在的情况下，通过符合 RANGE 范围的条数和总数的比例来选择是使用索引还是进行全表遍历 1234567891011121314151617// 正常情况下mysql&gt; explain select stock,goods_status from explain_goods where stock&gt;1;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_stock | idx_stock | 4 | NULL | 2 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec)// 而如下一张实战的表, `car_id varchar(45)` 为单列普通索引, 表数据量比较大, mysql优化器最终变成了全表扫描 (全表扫描, 参考 type 列)mysql&gt; explain select * from cm_bid_history where car_id&gt;&quot;1000&quot;;+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history | ALL | car_id | NULL | NULL | NULL | 141048 | Using where |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------------+1 row in set (0.00 sec) 注意: 二级索引貌似为 字符类型 时, 才会是上述结果, 如果为int, Extra 可能会是 null 123456789101112131415mysql&gt; explain select stock,goods_name from explain_goods where stock=&quot;1&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_stock | idx_stock | 33 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) mysql&gt; explain select stock,goods_name from explain_goods where stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+| 1 | SIMPLE | explain_goods | ref | idx_stock | idx_stock | 4 | const | 1 | NULL |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-------+1 row in set (0.00 sec) Using index condition; Using where where 条件有二级索引, 也有 非索引字段时 12345678910111213141516mysql&gt; explain select * from explain_goods where goods_name=&quot;华为&quot; and stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition; Using where |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+1 row in set (0.00 sec) mysql&gt; explain select id from explain_goods where goods_name=&quot;华为&quot; and stock=1;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition; Using where |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+------------------------------------+1 row in set (0.00 sec) 小结: where 条件有二级索引, 也有 非索引字段时, 和覆盖不覆盖没有关系 Using where 未用到索引 普通where条件, 无索引 (全表扫描, 参考 type 列) 示例: 1234567891011121314151617mysql&gt; explain select * from explain_goods where stock=1;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 2 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; explain select * from explain_goods where goods_status=1;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 2 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) ………… 未完待续const row not found For a query such as SELECT … FROM tbl_name, the table was empty (类似于select … from tbl_name, 而表记录为空) no matching row in const table 表为空或者表中根据唯一键查询时没有匹配的行 示例: MySQL 5.7.25 1234567mysql&gt; explain select * from t1 where id= 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set, 1 warning (0.00 sec) 示例: MySQL 5.6.35 1234567mysql&gt; explain select `name` from t1 where id = 12;+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+1 row in set (0.00 sec) 问题问题参考: https://segmentfault.com/q/1010000004197413","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"57. EXPLAIN -- key_len 检测多列索引的使用情况","slug":"MySQL/2019-04-15-mysql-57","date":"2019-04-15T05:13:36.000Z","updated":"2019-04-15T05:57:44.000Z","comments":true,"path":"2019/04/15/MySQL/2019-04-15-mysql-57/","link":"","permalink":"http://blog.renyimin.com/2019/04/15/MySQL/2019-04-15-mysql-57/","excerpt":"","text":"key_len表示索引使用的字节数，根据这个值可以判断索引的使用情况, 特别是在组合索引的时候, 判断该索引有多少部分被使用到非常重要 在不损失精确性的情况下, 长度越短越好 在计算key_len时，下面是一些需要考虑的点: 参考 索引字段的附加信息: 可以分为变长和定长数据类型讨论, 当索引字段为定长数据类型时, 如 char, int, datetime 需要有是否为空的标记, 这个标记占用1个字节 (对于not null的字段来说,则不需要这1字节); 对于变长数据类型,比如varchar,除了是否为空的标记外,还需要有长度信息,需要占用 1-2 个字节 对于 char、varchar、blob、text 等字符集来说, key len的长度还和字符集有关, latin1一个字符占用1个字节, gbk一个字符占用2个字节, utf8一个字符占用3个字节 注意: key_len只指示了where中用于条件过滤时被选中的索引列，是不包含order by/group by这一部分被选中的索引列的例如, 有个联合索引idx(c1,c2,c3), 3列均是int not null, 那么下面的SQL执行计划中, key_len的值是8而不是12:select ... from tb where c1=? and c2=? order by c1; 示例表结构 123456789101112131415161718CREATE TABLE `explain_goods` ( `id` int(11) NOT NULL AUTO_INCREMENT, `goods_number` varchar(50) NOT NULL, `goods_name` varchar(100) NOT NULL, `goods_weight` int(11) NOT NULL, `goods_brand` varchar(50) NOT NULL, `goods_desc` text NOT NULL, `stock` int(11) NOT NULL, `goods_status` tinyint(1) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_goods_number` (`goods_number`) USING BTREE, KEY `idx_goods` (`goods_name`,`goods_brand`,`goods_weight`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;INSERT INTO `explain_goods` VALUES (1, &apos;xs335671&apos;, &apos;华为plus&apos;, 11, &apos;荣耀&apos;, &apos;国产最强&apos;, 110, 1, &apos;2019-04-15 13:16:11&apos;, &apos;2019-04-15 13:16:37&apos;);INSERT INTO `explain_goods` VALUES (2, &apos;cxf77890&apos;, &apos;华为Meta20&apos;, 13, &apos;Meta&apos;, &apos;相机莱卡&apos;, 76, 1, &apos;2019-04-15 13:17:15&apos;, &apos;2019-04-15 13:17:15&apos;); 示例1, 多列索引中的三个字段都被使用到时: goods_name varchar(100) NOT NULL : 100 3(utf8) = 300字节 2个字节(长度, 貌似无论varchar(M)的M是多少, 此处都是按照2个字节来计算长度, 而且一个varchar字段此处最大是 3255+2=767) goods_brand varchar(50) NOT NULL: 50 3(utf8) = 150 字节 2个字节(长度, 貌似无论varchar(M)的M是多少, 此处都是按照2个字节来计算长度, 而且一个varchar字段此处最大是 3255+2=767 ) goods_weight int(11) NOT NULL: 4个字节 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 458 | const,const,const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------------------+------+-----------------------+1 row in set (0.00 sec) 当使用到其中两列: 少了一个 “goods_weight int(11) NOT NULL”, 非空 int 列, 也就少了4个字节 (458-4=454) 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 454 | const,const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------------+------+-----------------------+1 row in set (0.00 sec) 当使用到其中一列: 又少了一个 “goods_brand varchar(50) NOT NULL,”, 非空 varchar 列, 也就少了 50*3+2 = 152 个字节 (454-152) 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot;;+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+| 1 | SIMPLE | explain_goods | ref | idx_goods | idx_goods | 302 | const | 1 | Using index condition |+----+-------------+---------------+------+---------------+-----------+---------+-------+------+-----------------------+1 row in set (0.00 sec) 如下: 当多列索引中某一列使用了范围查询, 则右边的列无法使用索引优化 少了 goods_weight int(11) NOT NULL 的 4个字节 1234567mysql&gt; explain select * from explain_goods where goods_name = &quot;华为Plus&quot; and goods_brand&gt;&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 454 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec) 少了 goods_brand varchar(50) NOT NULL 152 + goods_weight int(11) NOT NULL 的 4 = 156个字节 1234567mysql&gt; explain select * from explain_goods where goods_name &gt; &quot;华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 302 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec) 不是最左前缀匹配, 不会使用索引: 12345678910111213141516// 最左前缀：mysql&gt; explain select * from explain_goods where goods_name like &quot;华为Plus%&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+| 1 | SIMPLE | explain_goods | range | idx_goods | idx_goods | 458 | NULL | 1 | Using index condition |+----+-------------+---------------+-------+---------------+-----------+---------+------+------+-----------------------+1 row in set (0.00 sec)// 非最左前缀, 未使用索引:mysql&gt; explain select * from explain_goods where goods_name like &quot;%华为Plus&quot; and goods_brand=&quot;荣耀&quot; and goods_weight=3;+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+---------------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec)","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"55. EXPLAIN -- type 列","slug":"MySQL/2019-04-13-mysql-55","date":"2019-04-13T11:31:21.000Z","updated":"2019-04-15T05:11:09.000Z","comments":true,"path":"2019/04/13/MySQL/2019-04-13-mysql-55/","link":"","permalink":"http://blog.renyimin.com/2019/04/13/MySQL/2019-04-13-mysql-55/","excerpt":"","text":"type 列介绍这一列描述了MySQL是如何查找表中的行的, 下面的类型从最优到最差: system system 其实是 const 的特例, 当表只有一行记录(等于系统表)时会出现 (业务上几乎不会出现只有一条记录的表, 所以这种情况并不多见) system 貌似只能用于 MyISAM 和 Memory, InnoDB 模拟不出来 MyISAM: t2表只有一条id为1, name 为Lant2的记录 (如下两条查询, 不管用没用到索引, 由于只有一条记录, 所以 type 就是 system) 123456789101112131415mysql&gt; explain SELECT * from t2 where id=1;+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | t2 | system | PRIMARY | NULL | NULL | NULL | 1 | NULL |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+1 row in set (0.00 sec) mysql&gt; explain SELECT * from t2 where name=&quot;Lant2&quot;;+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | t2 | system | NULL | NULL | NULL | NULL | 1 | NULL |+----+-------------+-------+--------+---------------+------+---------+------+------+-------+1 row in set (0.00 sec) InnoDB示例: t2表只有一条id为1的记录, 查询不仅使用了主键索引, 查询的内容还做到了覆盖索引, 但发现 type列 结果是 const, 而不是 system 1234567mysql&gt; explain SELECT id from t2 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | t2 | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) const const 表示通过索引一次就找到了, 用于 primary key 或者 unique 索引, 因为只匹配一行数据, 所以很快 (MyISAM和InnoDB使用主键或唯一索引在where条件, 都可以模拟出来) 示例: (注意, 覆盖索引和非覆盖索引的Extra可不同) 1234567891011121314151617mysql&gt; explain SELECT id from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) mysql&gt; explain SELECT * from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) mysql&gt; eq_ref 读取本表表中和 关联表 表中的每行组合成的一行, 除 了 system 和 const 类型之外, 这是最好的联接类型。当连接使用索引的所有部分时, 索引是 主键 或 唯一非NULL索引 时, 将使用该值 (InnoDB 和 MyISAM 略有不同) InnoDB示例: 当使用主键或唯一索引, 并且查询的内容是覆盖索引, 会出现 eq_ref: 12345678mysql&gt; explain SELECT t1.id,t3.id FROM t1 join t3 on t1.id=t3.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| 1 | SIMPLE | t1 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | Using index |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+2 rows in set (0.00 sec) InnoDB示例: 当使用主键或唯一索引, 但查询的内容没有做到覆盖索引时, 就不会出现 eq_ref, 如下: 12345678910111213141516171819// 可以看到 t3 使用了 index, 而 t1 使用了 ALL 全表扫描mysql&gt; explain SELECT t1.id,t3.id,t1.name FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+| 1 | SIMPLE | t3 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------------------------------------------------+2 rows in set (0.00 sec)// 如下的话, 则都是全表扫描mysql&gt; explain SELECT * FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | NULL || 1 | SIMPLE | t3 | ALL | PRIMARY | NULL | NULL | NULL | 3 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------+---------------+------+---------+------+------+----------------------------------------------------+2 rows in set (0.00 sec) InnoDB示例: 需要注意的是, 当 InnoDB 数据量比较多的情况下, 有时即使没有做到覆盖所有, 也会出现 eq_ref 12// cm_bid_history 表中有10万数据explain SELECT * FROM cm_bid_history a join cm_bid_history as b on a.id=b.id; MyISAM示例 : 只要像官网说的那样, 用到了 主键 或唯一索引, 就会出现 eq_ref 12345678mysql&gt; explain SELECT * FROM t1,t3 where t3.id=t1.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 3 | NULL || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | NULL |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------+2 rows in set (0.00 sec) ref 这是一种索引访问, 它可能会找到多个符合条件的行, 因此它是查找和扫描的混合体; 此类索引访问只有当使用 非唯一性索引 或者 唯一性索引的非唯一性前缀 时才会发生; 不像 eq_ref 那样要求连接顺序, 也没有主键和唯一索引的要求, 只要使用相等条件检索时就可能出现, 常见于辅助索引的等值查找, 或者多列主键、唯一索引中, 使用第一个列之外的列作为等值查找也会出现, 总之, 返回数据不唯一的等值查找就可能出现 如 explain select num from goods where num=11; (num列是个普通索引) 1234567mysql&gt; explain select num from goods where num=11;+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | goods | ref | idx_num | idx_num | 5 | const | 1 | Using index |+----+-------------+-------+------+---------------+---------+---------+-------+------+-------------+1 row in set (0.00 sec) ref_ro_null 是ref之上的一个变体, 与ref方法类似, 只是增加了null值的比较, 实际用的不多 1234567mysql&gt; explain select id,num from goods where num=11 or num is null;+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+| 1 | SIMPLE | goods | ref_or_null | idx_num | idx_num | 5 | const | 2 | Using index condition |+----+-------------+-------+-------------+---------------+---------+---------+-------+------+-----------------------+1 row in set (0.00 sec) range 范围扫描是一个有限制的索引扫描, 它开始于索引里的某一点, 返回匹配这个值域的行; 这比全索引扫描好一些, 因为它用不着遍历全部索引; 显而易见的范围扫描是where子句里带有 BETWEEN 或 &gt;、&lt;、in 等的查询 MyISAM in() 示例: (goods设置了num做BTree索引) 1234567mysql&gt; explain select id,num from goods where num in (10,11);+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| 1 | SIMPLE | goods | range | idx_num | idx_num | 5 | NULL | 3 | Using index condition |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+1 row in set (0.00 sec) MyISAM or 示例: 1234567mysql&gt; explain select id,num from goods where num=10 or num=11;+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+| 1 | SIMPLE | goods | range | idx_num | idx_num | 5 | NULL | 3 | Using index condition |+----+-------------+-------+-------+---------------+---------+---------+------+------+-----------------------+1 row in set (0.00 sec) 注意: 当MySQL使用索引 用 in() 和 or 去查找一系列值时, 虽然都会显示为范围扫描, 但这两者其实是相当不同的访问类型, 在性能上有重要的差异 … index Full Index Scan, 和全表扫描一样, 不过index只遍历索引树, 通常比ALL快 (虽说都是读全表, 但是index是从索引中读取的, 而all是从硬盘中读取的) 如果在Extra列中看到 “Using index”, 说明MySQL正在使用覆盖索引, 它只扫描索引的数据, 比按索引次序全表扫描的开销要少很多; 示例: 1234567891011121314151617// InnoDBmysql&gt; explain select id from cm_bid_history;+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history | index | NULL | deleted | 1 | NULL | 142485 | Using index |+----+-------------+----------------+-------+---------------+---------+---------+------+--------+-------------+1 row in set (0.00 sec)// MyISAMmysql&gt; explain select id from cm_bid_history_copy1;+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+| 1 | SIMPLE | cm_bid_history_copy1 | index | NULL | PRIMARY | 4 | NULL | 142267 | Using index |+----+-------------+----------------------+-------+---------------+---------+---------+------+--------+-------------+1 row in set (0.00 sec) … ALL 全表扫描, 通常意味着Mysql必须扫描整张表, 从头到尾去找到需要的行 (也有例外, 例如在查询里使用了LIMIT, 或者在 Extra 列中显示 “Using distinct/not exists”) 示例: 1234567891011121314151617// InnoDBmysql&gt; explain select * from cm_bid_history;+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+| 1 | SIMPLE | cm_bid_history | ALL | NULL | NULL | NULL | NULL | 142485 | NULL |+----+-------------+----------------+------+---------------+------+---------+------+--------+-------+1 row in set (0.00 sec)// MyISAMmysql&gt; explain select * from cm_bid_history_copy1;+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+| 1 | SIMPLE | cm_bid_history_copy1 | ALL | NULL | NULL | NULL | NULL | 142267 | NULL |+----+-------------+----------------------+------+---------------+------+---------+------+--------+-------+1 row in set (0.00 sec) NULLNULL是最好的, 不用访问索引或表, 直接获得数据…","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"54. EXPLAIN -- select_type 列","slug":"MySQL/2019-04-11-mysql-54","date":"2019-04-11T03:21:53.000Z","updated":"2019-04-15T03:59:00.000Z","comments":true,"path":"2019/04/11/MySQL/2019-04-11-mysql-54/","link":"","permalink":"http://blog.renyimin.com/2019/04/11/MySQL/2019-04-11-mysql-54/","excerpt":"","text":"select_type列介绍 这一列显示了对应行是简单还是复杂SELECT, 如果是复杂SELECT, 则是三种复杂类型中的一种 上一篇已经介绍过: MySQL将select查询分为 简单 和 复杂 两种类型; 复杂类型又可以分成三大类: 简单子查询、所谓的派生表(在FROM子句中的子查询), 以及UNION查询 (DERIVED:派生, 衍生) select_type 列的常见值有: SIMPLE: 简单的select查询, 意味着查询中不包含 UNION 或 子查询 (比较常见: 参考EXPLAIN — id 列 简单查询示例) PRIMARY: 如果查询有任何复杂的子部分, 则最外层的查询会被标记为 PRIMARY (参考EXPLAIN — id 列 简单子查询示例) SUBQUERY: 在SELECT或WHERE列表中包含的子查询 (注意不是 FORM的子句) (参考EXPLAIN — id 列 简单子查询示例) DERIVED: 在FROM列表中包含的子查询被标记为DERIVED(衍生), MySQL会把结果放在临时表里 (参考EXPLAIN — id 列 派生表示例) UNION: 若第二个select出现在 UNION 之后, 则被标记为 UNION; 若UNION包含在FROM子句的子查询中, 外层SELECT将被标记为 DERIVED; (参考EXPLAIN — id 列 派生表示例) UNION RESULT: 用来从 UNION 的匿名临时表检索结果的SELECT被标记为 UNION RESULT (参考EXPLAIN — id 列 UNION查询示例) 除了上面的值, SUBQUERY 和 UNION 还可以被标记为 DEPENDENT 和 UNCACHEABLEDEPENDENT 意味着SELECT依赖于外层查询中发现的数据UNCACHEABLE 意味着SELECT中的某些特性组织结果被缓存与一个 Item_cache 中 DEPENDENT SUBQUERY: 参考P224关联子查询的优化 UNCACHEABLE SUBQUERY DEPENDENT UNION UNCACHEABLE UNION 测试表准备为了后面的测试工作, 接下来简单创建几张测试表 (表本身没有业务意义, 只是为了做测试) 测试接下来将会用几个示例来演示id列几种值的效果, 对于EXPLAIN结果, 我们先只关注 select_type 这一列 DEPENDENT SUBQUERY 参考 P224 关联子查询的优化 P223: MySQL5.6之前, 在子查询方面比较糟糕 如果是 select * from a where a.id in (1,2,3), 速度不会慢 但是如果写为 select * from a where a.id in (select id from b where b.id in (1,2,3)), mysql不会将 select id from b where b.id in (1,2,3) 这个内容查询出来, 放到in里 如果用explain分析, 则会看到 DEPENDENT SUBQUERY, mysql会将外层的结果先查询出来, 然后逐条执行子查询, 如果外层结果集很大, 这个就会很慢(可以参考下高性能mysql的查询性能优化章节, 有详细描述) 一般出现这种情况, 是需要进行优化的 123456789101112131415161718192021222324mysql&gt; show variables like &quot;version&quot;;+---------------+--------+| Variable_name | Value |+---------------+--------+| version | 5.5.62 |+---------------+--------+1 row in set (0.00 sec)mysql&gt; EXPLAIN SELECT customer_id FROM explain_order WHERE id = ( SELECT goods_id FROM explain_goods WHERE goods_number = &quot;693822310030&quot;);+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | explain_order | ALL | NULL | NULL | NULL | NULL | 1 | Using where || 2 | DEPENDENT SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+2 rows in set (0.00 sec)mysql&gt; EXPLAIN SELECT customer_id FROM explain_order WHERE id in ( SELECT goods_id FROM explain_goods WHERE goods_number = &quot;693822310030&quot;);+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | explain_order | ALL | NULL | NULL | NULL | NULL | 1 | Using where || 2 | DEPENDENT SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables |+----+--------------------+---------------+------+---------------+------+---------+------+------+-----------------------------------------------------+2 rows in set (0.00 sec) 不过, 在 5.6+版本, 已经对上面的子查询做了很大的改进, 一般不会再出现 DEPENDENT SUBQUERY (但不是一定不出现, 如果出现需要进一步分析), 而是 SUBQUERY","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"","slug":"MySQL/50-bak","date":"2019-04-10T08:15:46.000Z","updated":"2019-04-10T08:15:46.000Z","comments":true,"path":"2019/04/10/MySQL/50-bak/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL/50-bak/","excerpt":"","text":"bak准备环境1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB, DEFAULT CHARSET = utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`))ENGINE = InnoDB,DEFAULT CHARSET = utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) select_type select_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 (最常见的查询类别就是 SIMPLE 了) PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 如果使用了UNION查询, 那么EXPLAIN 输出结果类似如下: 123456789101112mysql&gt; EXPLAIN ( SELECT * FROM user_info WHERE id IN ( 1, 2, 3 ) ) UNION( SELECT * FROM user_info WHERE id IN ( 3, 4, 5 ) );+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+3 rows in set (0.00 sec) mysql&gt; type type 字段比较重要, 它提供了判断查询是否高效的重要依据依据; 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等; type 常用的取值有: system: 表中只有一条数据, 这个类型是特殊的 const 类型; ?? const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据(const 查询速度非常快, 因为它仅仅读取一次即可) eq_ref: 此类型通常出现在多表的join查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果, 并且查询的比较操作通常是 =, 查询效率较高, 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using where; Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | NULL |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询, 例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5;+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+2 rows in set (0.00 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录; 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL (没用到索引), 并且 key_len 字段是此次查询中使用到的索引的最长的那个 1234567mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+1 row in set (0.00 sec) 下面对比, 都使用了范围查询, 但是一个可以使用索引范围查询, 另一个不能使用索引 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 另外, 可参考 P185: in语句虽然有时候 type结果也是range (不过, 对于真正的范围查询, 确实是无法使用范围列后面的其他索引了, 但是对于”多个等值条件查询”则没有这个限制) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过ALL类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据, 即 做的是覆盖索引, 当是这种情况时, Extra 字段会显示 Using index 下面的例子中, 查询的 name 字段恰好是一个索引(做到了覆盖索引), 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据;因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index; 1234567mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 下面不但使用了全索引扫描, 而且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;nihao&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) 但是, 如果不使用索引的话, 下面type就是ALL, 表示使用了全表扫描, 并且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age=10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 下面 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一, 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. type小结type 类型的性能比较 : 通常来说, 不同的 type 类型的性能关系如: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的; 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快; 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了; possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引;注意: 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到;(MySQL 在查询时具体使用了哪些索引, 由 key 字段决定) key此字段是 MySQL 在当前查询时所真正使用到的索引 rowsrows 也是一个重要的字段, MySQL 查询优化器根据统计信息, 估算SQL要查找到结果集需要到表中扫描读取的数据行数(上面的例子可以看到, 基本上使用到了索引的话, 真正扫描的行数都很少); 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好 ExtraExplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 比如下面, 使用索引扫描做排序 和 不使用索引扫描做排序 的效果: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY name;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY age;+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using filesort |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+1 row in set (0.00 sec) Using index 与 Using index condition “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 比如下面, 第一个做到了覆盖索引扫描, 后面两个都没做到 mysql&gt; EXPLAIN SELECT name FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index | +----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name,age FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT * FROM user_info where name=&#39;haha&#39;; +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ | 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition | +----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+ 1 row in set (0.00 sec) Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.","categories":[],"tags":[]},{"title":"53. EXPLAIN -- id 列","slug":"MySQL/2019-04-10-mysql-53","date":"2019-04-10T06:03:18.000Z","updated":"2019-04-15T03:55:56.000Z","comments":true,"path":"2019/04/10/MySQL/2019-04-10-mysql-53/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL/2019-04-10-mysql-53/","excerpt":"","text":"id 列介绍 EXPLAIN 的执行结果会返回一行或多行信息, 显示出执行计划中的的每一部分和执行顺序, 简单点说, 就是会显示出查询中执行表及其顺序, id列就是表示查询中 ‘表’ 的执行顺序 (‘表’的意思在这里比较广义, 可以是一个子查询, 一个UNION结果等等;) 如果查询是两个表的联接, 那么输出中将有两行, 别名表会单算为一个表 (如果把一个表与自己联接, 输出中也会有两行); MySQL将select查询分为 简单 和 复杂 两种类型; 复杂类型又可以分成三大类: 简单子查询、所谓的派生表(在FROM子句中的子查询), 以及UNION查询 (DERIVED:派生, 衍生) id列一般会有如下几种值: 简单查询: id 编号相同, 执行顺序由上至下 简单子查询: id的序号会递增, id值越大优先级越高, 越先被执行 派生表(FROM子句中的子查询): id 有相同的, 也有不同的, 相同的id可以认为是同一组, 从上往下顺序执行; id值大的, 优先级越高 UNION查询: id 为null, 当引用其他查询结果做union时, 该值为null, 且table列的值为 union(m,n), 意思是把id为m和n的查询结果做union 测试表准备为了后面的测试工作, 接下来简单创建几张测试表 (表本身没有业务意义, 只是为了做测试) t1表 12345CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; t2表 12345CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; t3表 12345CREATE TABLE `t3` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; 测试接下来将会用几个示例来演示id列几种值的效果, 对于EXPLAIN结果, 我们先只关注 id 这一列 简单查询 并行的表查询, id都是1 123456789mysql&gt; EXPLAIN SELECT * FROM t1, t2, t3 WHERE t1.id = t2.id and t1.id=t3.id and t1.name=&quot;test&quot;;+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | Using where || 1 | SIMPLE | t2 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | NULL || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | NULL |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) 简单子查询 id会递增 123456789mysql&gt; EXPLAIN SELECT * FROM t1 WHERE id=(SELECT id FROM t2 WHERE id = (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE noticed after reading const tables || 2 | SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 3 | SUBQUERY | t3 | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------------------------------------------+3 rows in set (0.00 sec) in 非简单子查询?? 注意, 这里只是把上面语句的 = 改为 in, 却发现 id 没有递增 123456789mysql&gt; EXPLAIN SELECT * FROM t1 WHERE id in (SELECT id FROM t2 WHERE id in (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+| 1 | SIMPLE | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | NULL || 1 | SIMPLE | t2 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | explain_test.t1.id | 1 | Using where |+----+-------------+-------+--------+---------------+---------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) 派生表(FROM子句中的子查询) 下面 &lt;derived2&gt; 和 t2 都是1, 是因为 t2 和 是并行的一组, 而 &lt;derived2&gt;(derived是衍生的意思, 2是t2这一行的id值) 是衍生临时表 123456789mysql&gt; EXPLAIN SELECT * FROM (SELECT * FROM t2 WHERE name = &quot;test&quot;) as tmp2, t1 where t1.id=tmp2.id;+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+| 1 | PRIMARY | t1 | ALL | PRIMARY | NULL | NULL | NULL | 1 | NULL || 1 | PRIMARY | &lt;derived2&gt; | ref | &lt;auto_key0&gt; | &lt;auto_key0&gt; | 4 | explain_test.t1.id | 2 | NULL || 2 | DERIVED | t2 | ALL | NULL | NULL | NULL | NULL | 1 | Using where |+----+-------------+------------+------+---------------+-------------+---------+--------------------+------+-------------+3 rows in set (0.00 sec) UNION查询 id 为null 123456789mysql&gt; explain select 1 union all select 1;+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || 2 | UNION | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+------+---------------+------+---------+------+------+-----------------+3 rows in set (0.00 sec) 疑问 in 非简单子查询?? (通过 explain extended + show warnings 分析, mysql的优化语句为 join 结构)","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"52. EXPLAIN","slug":"MySQL/2019-04-10-mysql-52","date":"2019-04-10T05:20:29.000Z","updated":"2019-04-17T06:53:38.000Z","comments":true,"path":"2019/04/10/MySQL/2019-04-10-mysql-52/","link":"","permalink":"http://blog.renyimin.com/2019/04/10/MySQL/2019-04-10-mysql-52/","excerpt":"","text":"ExplainMySQL中的 explain 命令, 其主要功能是用来分析 select 语句的运行效果, 例如它可以获取select语句使用的索引情况、排序情况等等 语法: EXPLAIN [EXTENDED] SELECT select_options EXPLAIN 只能解释select查询, 并不会对存储程序调用和INSERT、UPDATE、DELETE或其他语句做解释 EXPLAIN 结果的表头如下MySQL 5.6.35123+----+-------------+--------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+------+-------+ MySQL 5.7.25 (多了两列)123+----+-------------+--------+------------+-------+---------------+-----+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+--------+------------+-------+---------------+-----+---------+------+------+----------+-------+ 测试准备 几张简陋的测试表 1234567891011121314151617CREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8;CREATE TABLE `t3` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 几张简陋的测试表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253DROP TABLE IF EXISTS `explain_customer`;CREATE TABLE `explain_customer` ( `id` int(11) NOT NULL AUTO_INCREMENT, `customer_name` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_age` tinyint(3) NOT NULL DEFAULT &apos;0&apos;, `gender` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `identity_card_type` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `identity_card_no` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_phone` varchar(20) NOT NULL DEFAULT &apos;&apos;, `customer_level` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `customer_email` varchar(50) NOT NULL DEFAULT &apos;&apos;, `status` int(11) NOT NULL DEFAULT &apos;0&apos;, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_unique_identity_card_no` (`identity_card_no`) USING BTREE COMMENT &apos;身份证号唯一&apos;, UNIQUE KEY `idx_unique_phone` (`customer_phone`) USING BTREE COMMENT &apos;手机号唯一&apos;, KEY `idx_unique_custome` (`customer_name`,`gender`,`customer_level`) USING BTREE COMMENT &apos;cms常用查询条件&apos;) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `explain_goods` ( `id` int(11) NOT NULL AUTO_INCREMENT, `goods_number` varchar(50) NOT NULL, `goods_name` varchar(100) NOT NULL, `goods_weight` int(11) NOT NULL, `goods_brand` varchar(50) NOT NULL, `stock` int(11) NOT NULL, `goods_status` tinyint(1) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_goods_number` (`goods_number`) USING BTREE, KEY `idx_goods` (`goods_name`,`goods_brand`,`goods_weight`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;CREATE TABLE `explain_order` ( `id` int(11) NOT NULL AUTO_INCREMENT, `customer_id` int(11) NOT NULL, `seller_id` int(11) NOT NULL, `goods_id` int(11) NOT NULL, `order_number` varchar(50) NOT NULL, `order_status` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `pay_status` tinyint(1) NOT NULL DEFAULT &apos;0&apos;, `seller_status` tinyint(1) NOT NULL COMMENT &apos;卖家备货状态&apos;, `delivery_type` tinyint(1) NOT NULL COMMENT &apos;配送类型&apos;, `delivery_id` int(11) NOT NULL COMMENT &apos;配送方id&apos;, `order_amount` decimal(12,2) NOT NULL, `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `idx_order_number` (`order_number`) USING BTREE, KEY `idx_ocreated_at` (`created_at`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8; EXPLAIN EXTENDED其实除了 explain 以外, 其变种 explain extended 命令也非常有用, 它能够在原本explain的基础上额外的提供一些查询优化的信息, 这些信息可以通过mysql的 show warnings 命令得到, 执行分为两步: explain extended 你的SQL; show warnings; EXPLAIN EXTENDED 命令的表头如下: MySQL 5.6.35 (多了 filtered 列) 123+----+-------------+-------+------+---------------+-----+---------+------+------+----------+--------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------+---------------+-----+---------+------+------+----------+---------+ MySQL 5.7.25 (相对 EXPLAIN 来说, 表头没有变化) 123+----+-------------+-------+------------+--------+---------------+-----+---------+-----+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+-----+---------+-----+------+----------+-------+ 从 explain extend 的输出中, 你可以看到sql的执行方式, 对于分析sql还是很有帮助的 explain extended 除了能够告诉我们mysql的查询优化能做什么, 同时也能告诉我们mysql的查询优化做不了什么 比如 mysql的查询优化器不能将 id&gt;5 和 id&gt;6 这两个查询条件优化合并成一个 id&gt;6 (参考)1234567891011121314mysql&gt; EXPLAIN EXTENDED SELECT * FROM t1 WHERE id &gt; 5 AND id &gt; 6;+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | SIMPLE | t1 | range | PRIMARY | PRIMARY | 4 | NULL | 1 | 100.00 | Using where |+----+-------------+-------+-------+---------------+---------+---------+------+------+----------+-------------+1 row in set (0.00 sec)mysql&gt; show warnings;+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Note | 1003 | /* select#1 */ select `explain_test`.`t1`.`id` AS `id`,`explain_test`.`t1`.`name` AS `name` from `explain_test`.`t1` where ((`explain_test`.`t1`.`id` &gt; 5) and (`explain_test`.`t1`.`id` &gt; 6)) |+-------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 注意: 从 EXPLAIN extended + show warnings 得到 优化以后 的查询语句可能还不是最终优化执行的sql, 或者说explain extended看到的信息还不足以说明mysql最终对查询语句优化的结果 (?? https://www.jianshu.com/p/7656b114e783) EXPLAIN EXTENDED 示例如下在一个 简单的子查询中, 使用 = 和 使用 in 的结果截然不同 使用 = 时, 可以看到, 3条执行记录的id是递增的, 是有两个子句的执行, 最后才执行了外层的查询 (表中无数据) 12345678910111213141516171819// 当表中没有数据时mysql&gt; EXPLAIN extended SELECT * FROM t1 WHERE id = (SELECT id FROM t2 WHERE id = (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 2 | SUBQUERY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table || 3 | SUBQUERY | t3 | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+3 rows in set, 2 warnings (0.00 sec)mysql&gt; show warnings;+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Warning | 1681 | &apos;EXTENDED&apos; is deprecated and will be removed in a future release. || Note | 1003 | /* select#1 */ select NULL AS `id`,NULL AS `name` from `test`.`t1` where multiple equal((/* select#2 */ select NULL from `test`.`t2` where multiple equal((/* select#3 */ select `test`.`t3`.`id` from `test`.`t3` where (`test`.`t3`.`name` = &apos;Lant3&apos;)), NULL)), NULL) |+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) 而在使用 in 时, 可以看到, id 列的值都是1, 即为并行执行的, 使用 show warnings 查看后发现, MySQL将语句优化成了 join 结构 123456789101112131415161718mysql&gt; EXPLAIN extended SELECT * FROM t1 WHERE id in (SELECT id FROM t2 WHERE id in (SELECT id FROM t3 WHERE name=&quot;Lant3&quot;));+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+| 1 | SIMPLE | t1 | NULL | ALL | PRIMARY | NULL | NULL | NULL | 1 | 100.00 | NULL || 1 | SIMPLE | t2 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | 100.00 | Using index || 1 | SIMPLE | t3 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | 100.00 | Using where |+----+-------------+-------+------------+--------+---------------+---------+---------+------------+------+----------+-------------+3 rows in set, 2 warnings (0.00 sec)mysql&gt; show warnings;+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Warning | 1681 | &apos;EXTENDED&apos; is deprecated and will be removed in a future release. || Note | 1003 | /* select#1 */ select `test`.`t1`.`id` AS `id`,`test`.`t1`.`name` AS `name` from `test`.`t3` join `test`.`t2` join `test`.`t1` where ((`test`.`t2`.`id` = `test`.`t1`.`id`) and (`test`.`t3`.`id` = `test`.`t1`.`id`) and (`test`.`t3`.`name` = &apos;Lant3&apos;)) |+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec) EXPLAIN PARTITIONSEXPLAIN 结果列说明跳转到 id 列跳转到 select_type列table 列 显示这一行显示了对应行正在访问哪个表, 在通常情况下, 它相当明了: 它就是那个表, 或是该表的别名(如果sql中定义了别名) 当FROM子句中有子查询或者有UNION时, table列会变得复杂得多, 在这些场景下, 确实没有一个 “表” 可以参考到, 因为 MySQL 创建的匿名临时表仅在查询执行过程中存在 当在FROM子句中有子查询时, table列是 的形式, 其中 N 是子查询的id 当有 UNION 时, UNION RESULT 的table列包含一个参与UNION的 id 列表 ( ) 跳转到 type 列 这一列显示了 MySQL 是决定如何查找表中的行, 下面是最重要的访问方法, 依次从最差到最优 possible_keys, key possible_keys: 显示可能应用在这张表中的索引, 一个或多个; 查询涉及到的字段上若存在索引, 则该索引将被列出, 但不一定被查询实际使用 key : 实际使用的索引, 如果为NULL, 表示没有使用索引; 如果查询中使用了覆盖索引, 则该索引仅出现在key列表中 跳转到 key_lenref 显示索引的哪一列被使用了, 如果可能的话, 是一个常数 (哪些列或常量被用于查找索引列上的值) 示例 12345678mysql&gt; explain SELECT t1.id,t3.id FROM t1 join t3 on t1.id=t3.id;+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+| 1 | SIMPLE | t1 | index | PRIMARY | PRIMARY | 4 | NULL | 3 | Using index || 1 | SIMPLE | t3 | eq_ref | PRIMARY | PRIMARY | 4 | test.t1.id | 1 | Using index |+----+-------------+-------+--------+---------------+---------+---------+------------+------+-------------+2 rows in set (0.00 sec) 示例 1234567mysql&gt; explain SELECT * from t1 where id=1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | t1 | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) rows 根据表统计信息及索引选用情况, 大致估算出找到所需的记录所需要读取的行数, 这是MySQL认为它要检查的行数, 而不是结果集里的行数; 对于InnoDB表, 此数字是估计值, 可能并不总是准确的 … 跳转到 Extra 列","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"01. 第一章 MySQL 体系结构和存储引擎 概述","slug":"mysql技术内幕/2019-04-03-01","date":"2019-04-03T07:48:16.000Z","updated":"2019-04-08T10:01:20.000Z","comments":true,"path":"2019/04/03/mysql技术内幕/2019-04-03-01/","link":"","permalink":"http://blog.renyimin.com/2019/04/03/mysql技术内幕/2019-04-03-01/","excerpt":"","text":"概述 数据库 和 实例 两个术语区分 从概念上讲, 数据库是文件的集合, 是依照某种数据模型组织起来并存放于二级存储器中的数据集合 而数据库实例是程序, 是位于用户与操作系统之间的一层数据管理软件, 用户对数据库数据的任何操作, 包括数据库定义、数据查询、数据维护、数据库运行控制等都是在数据库实例下进行的 应用程序只有通过 数据库实例 才能和 数据库打交道 MySQL是单进程多线程架构的数据库, MySQL数据库实例在系统上的表现就是一个进程; 当启动MySQL实例时, MySQL数据会去读取配置文件, 根据配置文件的参数来启动数据库实例; 而且MySQL数据库可以没有配置文件, 在这种情况下, MySQL会按照编译时的默认参数设置启动实例 命令 mysql --help | grep my.cnf 可以查看当MySQL数据库实例启动时, 会在哪些位置查找配置文件: 12345[root@lant ~]# mysql --help | grep my.cnforder of preference, my.cnf, $MYSQL_TCP_PORT,/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf// (书中的是 `/etc/my.cnf /etc/mysql/my.cnf /usr/local/mysql/etc/my.cnf ~/.my.cnf`)[root@lant ~]# 上述便是MySQL数据库读取配置文件的顺序, 所以如果多个配置文件中有相同的参数, 则以最后一个配置文件中的参数为准; Linux下, 配置文件一般放在 /etc/my.cnf (Windows平台下, 配置文件的后缀可能是 .cnf, 也可能是 .ini) MySQL体系结构概述 MySQL的数据库体系结构图: 上图可以发现, MySQL由以下几部分组成: 连接池组件 管理服务和工具组件 SQL接口组件 查询分析器组件 优化器组件 缓冲(Cache)组件 插件式存储引擎 物理文件 MySQL数据库区别于其他数据库的最重要一个特点就是其 插件式的表存储引擎; 需要注意的是: 存储引擎是基于表的而不是基于数据库的 (每个存储引擎都有各自的特点, 同一个库中, 可以根据具体应用的特点, 使用不同的存储引擎表) MySQL存储引擎 概述 由于MySQL数据库的开源特性, 存储引擎可以分为 MySQL官方存储引擎 和 第三方存储引擎 (大名鼎鼎的InnoDB存储引擎最早是第三方存储引擎, 后被Oracle收购, 其应用极其广泛, 甚至是MySQL数据库 OLTP 应用中使用最广泛的存储引擎) 可以通过 SHOW ENGINES; 来查看当前使用的MySQL数据库所支持的存储引擎; InnoDB 存储引擎 从MySQL5.5.8版本开始, InnoDB是默认的存储引擎; (从MySQL4.1开始, 可以将InnoDB存储引擎的表单独放到一个独立的ibd文件中) 其特点是: 行锁设计 支持外键 支持全文索引 (MySQL5.6版本开始支持InnoDB引擎的全文索引, 语法层面上大多数兼容之前MyISAM的全文索引模式 ) 使用 next-key locking 避免幻读(phantom)的产生 InnoDB引擎采用的是聚簇(clustered)方式, 因此每张表的存储都是按主键顺序进行存放, 如果没有显示地在表定义时指定主键, InnoDB存储引擎会为每一行生成一个6字节的ROWID, 并以此为主键; MyISAM存储引擎 在MySQL5.5.8之前, MyISAM是默认的存储引擎(除Windows版本外), 该存储引擎不支持事务, 是 表锁设计, 不支持外键, 支持全文索引 MyISAM存储引擎的表由 .frm, .MYD 和 .MYI 组成 .frm 表结构文件 .MYD 表数据文件 .MYI 表索引文件 在MySQL5.0之前, MyISAM默认支持的表达小为4GB, 如果需要支持大于4GB的MyISAM表时, 则需要定制 MAX_ROWS 和 AVG_ROW_LENGTH 属性; 从MySQL5.0版本开始, MyISAM 默认支持256TB的单标数据, 这足够满足一般应用需求 注意: 对于MyISAM存储引擎表, MySQL数据库只缓存其索引文件, 数据文件的缓存交由操作系统本身来完成 其他存储引擎 NDB 存储引擎 MEMORY 存储引擎 ARCHIVE 存储引擎 FEDERATED 存储引擎 BLACKHOLE 存储引擎 Maria 存储引擎","categories":[{"name":"MySQL技术内幕 InnoDB存储引擎","slug":"MySQL技术内幕-InnoDB存储引擎","permalink":"http://blog.renyimin.com/categories/MySQL技术内幕-InnoDB存储引擎/"}],"tags":[{"name":"MySQL技术内幕 InnoDB存储引擎","slug":"MySQL技术内幕-InnoDB存储引擎","permalink":"http://blog.renyimin.com/tags/MySQL技术内幕-InnoDB存储引擎/"}]},{"title":"32. InnoDB 存储引擎","slug":"MySQL/2019-03-07-mysql-32","date":"2019-04-02T11:31:07.000Z","updated":"2019-04-22T11:18:29.000Z","comments":true,"path":"2019/04/02/MySQL/2019-03-07-mysql-32/","link":"","permalink":"http://blog.renyimin.com/2019/04/02/MySQL/2019-03-07-mysql-32/","excerpt":"","text":"概述 InnoDB存储引擎是事务安全的MySQL存储引擎, 该存储引是第一个完整支持ACID事务的MySQL存储引擎(BDB不算, 已弃用), 通常来说, 它是 在线事务处理(OLTP)应用 中, 核心表的首选存储引擎; (注意: 存储引擎是基于表的而不是基于数据库的, 也就是同一个库中, 你可以创建并使用不同存储引擎的表) 从MySQL5.5.8版本开始, InnoDB是默认的存储引擎(之前的版本InnoDB仅在Windows下座位默认的存储引擎); (从MySQL4.1开始, 可以将InnoDB存储引擎的表单独放到一个独立的ibd文件中) InnoDB其特点是 行锁设计, 支持MVCC, 支持外键, 提供一致性非锁定读, 同时被设计用来最有效地利用以及使用内存和CPU； 另外, 从MySQL5.1版本时, MySQL数据库允许存储引擎以插件的方式动态进行加载; 查看当前MySQL数据库的InnoDB版本 1234567mysql&gt; show variables like &quot;%innodb_version%&quot;;+----------------+--------+| Variable_name | Value |+----------------+--------+| innodb_version | 5.7.25 |+----------------+--------+1 row in set (0.00 sec) InnoDB逻辑存储结构 从逻辑存储结构来看, InnoDB存储引擎下的数据都被逻辑地放在一个空间中, 称为 表空间 (tablespace); 表空间是InnoDB存储引擎逻辑结构的最高层, 所有的数据都存放在表空间中, 默认, InnoDB存储引擎只有一个共享表空间ibdata1, 即所有数据都存放在这个表空间内; 如果用户启用了参数 innodb_file_per_table, 则每张表内的数据可以单独放到一个表空间内; 不过需要注意的是, 每张表的表空间年内存放的只是数据, 索引, 和插入缓冲Bitmap页, 其他类的数据, 如 回滚(undo)信息, 插入缓冲索引页, 系统事务信息, 二次写缓冲(Double write buffer)等还是存放在原来的共享表空间内; 表空间是InnoDB存储引擎逻辑结构的最高层, 它由 段(segment), 区(extent), 页(page)(一些文档中也称为块(block)) 组成; InnoDB存储引擎的逻辑存储结构大致如下: 段(segment) 上图中显示了 tablespace由segment组成, segment 由 extend 组成, extend由page组成, page由row组成; 常见的段有 数据段, 索引段, 回滚段等; 在InnoDB存储引擎中, 对段的管理都是由引擎自身所完成的, DBA不能也没有必要对其进行控制; 前面已经介绍过了InnoDB存储引擎表是索引组织的(index organized), 因此数据即索引, 索引即数据 数据段: B+树的叶节点 索引段: B+树的非叶节点 回滚段: 即 rollback segment, 管理undo log segment 区(extend) 区是由连续的页组成, 在任何情况下, 每个区的大小都为1M, 为了保证区中页的连续性, InnoDB一次从磁盘申请4~5个区, 默认情况下, Innodb中, 页大小为16K, 即一个区中有64个连续的页; … 页(Page) 页是InnoDB磁盘管理的最小单位, 默认每个页的大小为16KB; 在InnoDB存储引擎中, 常见的页类型有: 数据页(B-Tree Node) undo页(undo Log Page) 系统页(System Page) 事务数据页(Transaction system Page) 插入缓冲位图页(Insert Buffer Bitmap) 插入缓冲空闲列表页 (Insert Buffer Free List) 未压缩的二进制大对象页 (Uncompressed BLOB Page) 压缩的二进制大对象页 (compressed BLOB Page) 行 InnoDB体系架构 InnoDB存储引擎是多线程的模型, 因此其后台有多个不同的后台线程, 负责处理不同的任务 Master Thread: 是一个非常核心的后台线程, 主要负责将缓冲池中的数据异步刷新到磁盘, 保证数据的一致性, 包括脏页的刷新、合并插入缓冲(INSERT BUFFER)、UNDO页的回收等, 后面会详细介绍; IO Thread: InnoDB存储引擎中大量使用了AIO(Async IO)来处理写IO请求, 这样可以极大提高数据库的性能而 IO Thread 的工作主要是负责这些IO请求的回调(call back)处理 可以通过 SHOW ENGINE INNODB STATUS 来观察InnoDB中的 IO Thread: 可以看到 IO Thread 0 为 insert buffer thread, IO Thread 1 为 log thread; 之后就是根据参数 innodb_read_io_threads 及 innodb_write_io_threads 来设置的读写线程, 并且读线程的ID总是小于写线程;从InnoDB 1.X 开始, read thread 和 write thread 分别增大到了4个 12345678910111213mysql&gt; show variables like &quot;innodb%io_threads&quot;\\G;*************************** 1. row ***************************Variable_name: innodb_read_io_threads Value: 4*************************** 2. row ***************************Variable_name: innodb_write_io_threads Value: 42 rows in set (0.00 sec)ERROR:No query specifiedmysql&gt; Pure Thread事务被提价后, 其所使用的undolog可能不再需要,","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"10. char(M) 和 varchar(M)","slug":"MySQL/2017-08-23-mysql-10","date":"2019-03-30T10:51:28.000Z","updated":"2019-03-30T09:07:28.000Z","comments":true,"path":"2019/03/30/MySQL/2017-08-23-mysql-10/","link":"","permalink":"http://blog.renyimin.com/2019/03/30/MySQL/2017-08-23-mysql-10/","excerpt":"","text":"char(M) char(M) 中的 M 用于指定该字段可存储的 字符(注意不是字节) 的最大个数 M 最大可以设置到 255 , 即 char(M) 最大可以存储 255个字符 注意: 在任何字符集下, M的最大值都是 255, why? 因为不管在什么字符集下, 对于char(M), MySQL都会首先给该字段的分配 M*3(utf8) 个字节, 那这个字段的所有数据占用的空间都是定长的字节数了 所以char(M)的存储空间就是这么定长的, 并且你永远不可能超过MySQL分配给 char(M) 的字节数, 因为在这之前你已经被M所限制的字符个数限制住了 varchar(M) varchar(M) 中的 M 也是用于指定该字段可存储的 字符(注意不是字节) 的最大个数 不过 varchar(M) 中的 M 和 字符编码有关, 由于 varchar最大长度是65535个字节, 所以: 在utf-8编码下, M值不应该超过 21845 (因为M超过255后, 会有两个字节用于记录M的大小, 所以虽然M的最大值为 21845, 但实际能存储的最大字符数为: 65535-2 / 3 = 21844 ) 在GBK编码下, M值也不应该超过 32767 (最大有效字符能存储, …. 65535-2 / 2 = 32766) 在utfbbm4编码下, M值也不应该超过 16383 (最大有效字符能存储, … 65535-2 / 4 = 16383) 另外, 由于varchar(M)是不定长的, 因此, 每次在读取数据的时候需要知道这个数据有多少个字符, 这样才能算出总字节, 从而完整地把varchar(M)字段的数据读出来, 所以varchar事实上还会提供一个值来记录每个数据的字符长度, 这个值会占用 1-2 个字节的空间 为什么该值是 1-2 个字节, 不是固定的字节? 因为1个字节可以表示的最大数值255, 而如果varchar(M)的M指定的字符数超过了255, 一个字节就不够了, 那MySQL就需要两个字节来记录字符的长度 varchar 坑来了 表中只要有字段没有设置为 not null 属性, 每行记录就会浪费一个字节来记录null字段(具体也不知道为什么要记录), 只有所有的字段都为not null的时候, 才不会浪费一个字节来记录null 表中一旦有了varchar(M)字段之后, 表中的每行记录都自动有了每行记录的总长度, 也是65535个字节, 所以, varchar(M)的M一般不可能达到最大值 验证 如下, 你觉着如下的varchar(M), M最大可设置为多少? 推算: 1234表中一旦有了varchar(M)字段之后, 表中的每行记录都自动有了每行记录的总长度, 也是65535个字节65535(行记录的总字节数) - 4*3(3个int字段) - 10*3+1(varchar(10)字段, 1个字节记录长度) = 6549265492/3 = 21830.67所以最大可以设置为21830 设置为 varchar(21831) 报错, 设置 varchar(21830) 就ok了 由于表中只要有字段没有设置为 not null 属性, 每行记录就会浪费一个字节来记录null字段, 此时如果设置一个字段为null, 会发现又不够了 text (可忽略) 每个BLOB和TEXT列, 账户只占其中的5至9个字节, BLOB和TEXT类型需要 1、2、3或者4个字节来记录列值的长度，取决于该类型的最大可能的长度 varchar(21830) —&gt; varchar(21826) 少了4个utf8字符, 即12字节： 再增一个text字段, varchar(21826) —&gt; varchar(21823) 少了3个utf8字符, 即9字节: 小结在建表选择字段时, 当使用varchar(M)的M值较大时, 需要注意以上内容","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"100. GUID 的生成","slug":"MySQL/2019-03-27-guid","date":"2019-03-27T06:29:01.000Z","updated":"2019-03-27T08:34:28.000Z","comments":true,"path":"2019/03/27/MySQL/2019-03-27-guid/","link":"","permalink":"http://blog.renyimin.com/2019/03/27/MySQL/2019-03-27-guid/","excerpt":"","text":"binlog相关 全局GID的应用场景 …等等… 为什么分布式系统需要用到ID生成系统?https://www.jianshu.com/p/6ff45e309e65https://blog.csdn.net/linzhiqiang0316/article/details/80425437 在复杂分布式系统中, 往往需要对大量的数据和消息进行唯一标识 如在美团点评的金融、支付、餐饮、酒店、猫眼电影等产品的系统中, 数据日渐增长, 对数据库的分库分表后需要有一个唯一ID来标识一条数据或消息, 数据库的自增ID显然不能满足需求；特别一点的如订单、骑手、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。 概括下来, 业务系统对ID号的要求有哪些呢？ 链接：https://www.jianshu.com/p/9d7ebe37215ehttp://www.sohu.com/a/211681078_467759 GUID 与 UUID http://cenalulu.github.io/mysql/guid-generate/http://www.jianshu.com/p/9d7ebe37215ehttps://blog.csdn.net/xyznol/article/details/81262868","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"50. MyISAM InnoDB 区别","slug":"MySQL/2019-03-16-mysql-50","date":"2019-03-16T08:30:56.000Z","updated":"2019-04-25T07:11:38.000Z","comments":true,"path":"2019/03/16/MySQL/2019-03-16-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2019/03/16/MySQL/2019-03-16-mysql-50/","excerpt":"","text":"InnoDB支持事务, MyISAM不支持, 对于InnoDB每一条SQL语言都默认封装成事务, 自动提交, 这样会影响速度, 所以最好把多条SQL语言放在begin和commit之间, 组成一个事务; InnoDB支持外键, 而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败; InnoDB是聚集索引, 数据文件是和索引绑在一起的, 必须要有主键, 通过主键索引效率很高。但是辅助索引需要两次查询, 先查询到主键, 然后再通过主键查询到数据。因此, 主键不应该过大, 因为主键太大, 其他索引也都会很大。而MyISAM是非聚集索引, 数据文件是分离的, 索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB不保存表的具体行数, 执行 select count(*) from table 时需要全表扫描。而MyISAM用一个变量保存了整个表的行数, 执行上述语句时只需要读出该变量即可, 速度很快; Innodb不支持全文索引, 而MyISAM支持全文索引, 查询效率上MyISAM要高;","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"35. The Binary Log 基础","slug":"MySQL/2019-03-13-mysql-35","date":"2019-03-13T14:48:49.000Z","updated":"2019-04-03T03:34:54.000Z","comments":true,"path":"2019/03/13/MySQL/2019-03-13-mysql-35/","link":"","permalink":"http://blog.renyimin.com/2019/03/13/MySQL/2019-03-13-mysql-35/","excerpt":"","text":"概述 MySQL的二进制日志 binlog 可以说是MySQL最重要的日志了, 它记录了对MySQL数据库执行更改的所有操作(同时, 还包括了更新操作的时间和其他信息;), 但是不包括 SELECT 和 SHOW 这类操作(这类操作对数据库本身并没有修改); 但是, binlog默认并没有开启, 所以直接运行 show master status\\G; 会是如下结果: 1234567mysql&gt; show master status\\G;Empty set (0.00 sec)ERROR:No query specifiedmysql&gt; 开启 binlog 会影响性能, 但是性能的损失十分有限, 官方测试表明, 开启binlog会使性能下降1%, 但考虑到它带来的强大功能, 这些性能损失是绝对可以且应该被接受的; binlog 主要有以下几种作用: 数据恢复: 某些数据的恢复需要binlog (例如, 在一个数据库全备文件回复后, 用户可以通过二进制日志进行 point-int-time 的恢复) 复制: 原理与恢复类似, 通过复制和执行二进制日志使一台远程的MySQL服务器(一般称为slave或standby)与一台MySQL服务器(一般称为master或primary)进行实时同步; 审计: 用户可以通过二进制日志中的信息来进行审计, 判断是否对数据库进行注入攻击; 查看binlog的相关配置项, 可以执行: (注意, 貌似在my.cnf中配置项的命名是-,_分隔单词都行, 而如下查到的是_) 12show global variables like &quot;%binlog%&quot;\\G; // 可以看到, 默认 binlog 是关闭状态 如下是常用的两个用来查看binlog状态的命令: 1234567891011121314151617181920212223mysql&gt; show master status\\G;*************************** 1. row *************************** File: mysql57-bin.000005 Position: 1865 Binlog_Do_DB: Binlog_Ignore_DB:Executed_Gtid_Set:1 row in set (0.00 sec)ERROR:No query specifiedmysql&gt; show binlog events in &apos;mysql57-bin.000005&apos;\\G;*************************** 1. row *************************** Log_name: mysql57-bin.000005 Pos: 4 Event_type: Format_desc Server_id: 100End_log_pos: 123 Info: Server ver: 5.7.25-log, Binlog ver: 4*************************** 2. row ***************************...... 查看 binlog 相关配置选项 log-bin[=name] : 如果不指定name, 则默认二进制文件名为主机名, 后缀名为二进制日志的序列号, 所在路径为数据库所在目录(datadir) 要开启 binlog, 可以简单配置如下两项即可开启 123[mysqld]server_id = 100log_bin=mysql57-bin 生成的 binlog 文件如下: 123456789101112root@lant mysql]# pwd/var/lib/mysql[root@lant mysql]# lsauto.cnf ibtmp1 mysql.sockca-key.pem mysql mysql.sock.lockca.pem mysql57-bin.000001 performance_schemaclient-cert.pem private_key.pem client-key.pem public_key.pem ib_buffer_pool server-cert.pemibdata1 server-key.pem ib_logfile0 sys ib_logfile1 mysql57-bin.index test[root@lant mysql]# mysql57-bin.index 为二进制的索引文件, 用来存储过往产生的二进制日志序号, 一般不建议修改该文件; 开启后 show master status\\G; 即可看到效果1 binlog_rows_query_log_events, binlog_row_image: 注意, MySQL默认是不会把你 执行的修改sql语句 记录到binlog中的, mysql 5.6.2 引入了两个参数 binlog_rows_query_log_events 和 binlog_row_image, 对于使用 row 格式的binlog, 个人觉得很有用binlog-format=STATEMENT 当设置为该配置时, 即使update的更新为0, 也会记录该语句; 而如果设置为Row, 则如果update更新为0, 不会记录; binlog_rows_query_log_events=1(默认是0(off)), 开启该参数后, 会把你执行的修改相关的sql语句打印到binlog日志里面 (如果日志格式为 STATEMENT, 即便你的修改语句影响行数为0, 也会被记录)对于dba来说, 在查看binlog的时候,很有用处 binlog_row_image=&#39;minimal默认为full, 在binlog为row格式下, full将记录update前后所有字段的值minimal时,只记录更改字段的值和where字段的值noblob时,记录除了blob和text的所有字段的值,如果update的blob或text字段,也只记录该字段更改后的值,更改前的不记录; binlog_row_image的引入就大大减少了binlog记录的数据, 再结合 binlog_rows_query_log_events, 对于dba日常维护binlog是完全没有压力的, 而且节省了硬盘空间开销, 减小I/O, 减少了主从传输压力; 开启 binlog_rows_query_log_events 后: 有些更新操作, 本身并没有导致数据库发生变化, 但仍然可能会写入二进制日志 (当配置了 binlog-format=STATEMENT 时, 即便 update/delete 影响行数为0时, binlog也会记录该sql语句) 12345678910 mysql&gt; show variables like &apos;binlog_rows_query_log_events&apos;; +------------------------------+-------+ | Variable_name | Value | +------------------------------+-------+ | binlog_rows_query_log_events | ON | +------------------------------+-------+ 1 row in set (0.01 sec) ``` 3. 什么时候会生成新的 binlog 文件? 于binlog文件的生成规则, 大概有如下三种情况: 每重启一次, 便会重新生成一个binlog文件; 还有一种情况就是运行了 FLUSH LOGS 命令也会重建一个; 还有一种情况就是当这个binlog文件的大小到了设定的值后, 就会重新生成一个新的(max_binlog_size: binlog日志每达到设定大小后, 会使用新的bin log日志, 从MySQL5.0开始的默认值是1Gmysql&gt; show global variables like “%max_binlog_size%”\\G;* 1. row *Variable_name: max_binlog_size Value: 1073741824 1 row in set (0.00 sec) ERROR:No query specified mysql&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283844. `binlog_cache_size` : 当使用事务时, 所有未提交的二进制日志会被记录到一个缓存中区, 等该事务提交时, 直接将缓冲中的二进制日志写入binlog中, 而该缓冲的大小由 `binlog_cache_size` 决定, 默认大小为32k - 此外, binlog_cache_size 是基于会话(session)的, 即, 当一个线程开始一个事务时, MySQL会自动分配一个大小为 binlog_cache_size 的缓存, 因此该值的设置需要相当小心, 不能设置过大; - 当一个事务的记录大于设定的 binlog_cache_size 时, MySQL会把缓存中的日志写入一个临时文件中, 因此该值又不能设置的太小 5. `binlog_cache_use`, `binlog_cache_disk_use`： 通过 `show global status` 命令可以查看 `binlog_cache_use`, `binlog_cache_disk_use` 的状态, 可以判断当前 binlog_cache_size 的设置是否合适 - `binlog_cache_use` 记录了使用缓冲写二进制日志的次数 - `binlog_cache_disk_use` 记录了使用临时文件写二进制日志的次数 (如果该值为0, 表示当前binlog_cache_size设置的缓冲大小对于当前系统完全够用, 暂时没有必要增加binlog_cache_size的值) 6. `sync_binlog=[N]` : 默认情况下, 二进制日志并不是在每次写的时候都会同步到磁盘(可以理解为缓冲写), 因此, 当数据库所在操作系统发生宕机时, 可能会有最后一部分数据没有写入二进制日志文件中, 这就会给 恢复 和 复制 带来问题 - 参数 `sync_binlog=[N]` 表示每写缓冲多少次就同步到磁盘, 如果N设置为1, 表示采用同步写磁盘的方式来写二进制日志, 这时写操作不使用操作系统的缓冲来写二进制日志 - Mysql中默认 `sync_binlog=0`, 即不作任何强制性的磁盘刷新指令, 这时性能是最好的, 但风险也是最大的, 一旦系统绷Crash, 在文件系统缓存中的所有Binlog信息都会丢失 (发现MySQL5.7默认该值是1) 如果使用InnoDB存储引擎进行复制, 并且想得到最大的高可用性, 建议将该值设置为ON, 不过该值设置为ON, 确实会对数据库的IO带来一定的影响;7. `innodb_support_xa`: 但是, 即便是 sync_binlog 设置为1, 还是会有一种情况导致问题的发生 - 当使用InnoDB存储引擎时, 在一个事务发出COMMIT动作之前, 由于sync_binlog为1, 因此会将二进制日志立即写入磁盘 - 如果这时已经写入了二进制日志, 但是提交还没有发生, 突然宕机, 那么下次MySQL启动时, 由于commit操作并没有发生, 所以这个事务会被回滚掉; 但是二进制日志已经记录了该事物信息, 不能被回滚; 这个问题可以通过将参数 `innodb_support_xa` 设置为1来解决 - innodb_support_xa 可以开关InnoDB的xa两段式事务提交, 可参考[此文](http://blog.itpub.net/15498/viewspace-2153760/)8. `binlog-do-db` 和 `binlog-ignore-db` 表示需要写入或忽略写入哪些库的日志, 默认为空, 表示需要同步所有库的日志到二进制日志 如果当前数据库是复制中的slave角色, 则它不会将从master取得并执行的二进制日志写入自己的二进制日志文件中去, 如果要写入, 需要设置 `log-slave-update`, 如果需要搭建 `master=&gt;slave=&gt;slave` 架构的复制, 则必须设置该参数; 9. `binlog_format` 参数十分重要, 是MySQL5.1引入的, 可以设置如下几个值: - `STATEMENT` : STATEMENT和之前的MySQL版本一样, 二进制日志记录的是日志的逻辑SQL语句 - `ROW` : 在ROW下, 二进制日志记录的不再是简单的SQL语句, 而是记录表的 行更改情况 **从MySQL5.1开始, 如果设置了binlog_format为ROW, 则可以将InnoDB的事务隔离级别设置为 READ COMMITTED, 以获得更高的并发性!** - `MIXED` : MySQL默认会再用satement格式进行二进制日志文件的记录, 但是在一些情况下会使用ROW格式, 可能的情况有: 表的存储引擎为NDB, 这时对表的DML操作都会以ROW格式记录 使用了UUID(), USER(), CURRENT_USER(), FOUND_ROWS(), ROW_COUNT() 等不确定函数 使用了 INSERT DELAY语句 使用了用户自定义函数(UDF) 使用了临时表## binlog与redo log1. 在MySQL中, 二进制日志 binlog 是server层的, 主要用来做主从复制和即时点恢复时使用的, 而事务日志 redo-log 是InnoDB存储引擎层的, 用来保证事务安全; 2. 那binlog和redo-log之间有什么关系? 为什么MySQL有binlog了还有redo-log? 因为MySQL体系结构的原因, MySQL是多存储引擎的, 不管使用那种存储引擎, 都会有binlog, 而不一定有redo log; 简单的说，binlog是MySQL Server层的, redo log是InnoDB层的3. 事务提交先写 binlog 还是 redo log? 如何保证这两部分的日志做到顺序一致性?3. MySQL Binary log在MySQL 5.1版本后推出主要用于主备复制的搭建, 回顾下MySQL在开启/关闭Binary Log功能的情况下是如何工作的: - 首先看一下什么是CrashSafe？CrashSafe指MySQL服务器宕机重启后，能够保证： - MySQL没有开启Binary log的情况下？ – 所有已经提交的事务的数据仍然存在。 – 所有没有提交的事务的数据自动回滚。 Innodb通过Redo Log和Undo Log可以保证以上两点。为了保证严格的CrashSafe，必须要在每个事务提交的时候，将Redo Log写入硬件存储。这样做会牺牲一些性能，但是可靠性最好。为了平衡两者，InnoDB提供了一个 `innodb_flush_log_at_trx_commit` 系统变量，用户可以根据应用的需求自行调整。 innodb_flush_log_at_trx_commit = 0|1|2 0 – 每N秒将Redo Log Buffer的记录写入Redo Log文件，并且将文件刷入硬件存储1次。N由innodb_flush_log_at_timeout控制。 1 – 每个事务提交时，将记录从Redo Log Buffer写入Redo Log文件，并且将文件刷入硬件存储。 2 – 每个事务提交时，仅将记录从Redo Log Buffer写入Redo Log文件。Redo Log何时刷入硬件存储由操作系统和innodb_flush_log_at_timeout决定。这个选项可以保证在MySQL宕机，而操作系统正常工作时，数据的完整性。 通过redo日志将所有已经在存储引擎内部提交的事务应用redo log恢复，所有已经prepare但是没有commit的transactions将会应用undo log做rollback。然后客户端连接时就能看到已经提交的数据存在数据库内，未提交被回滚地数据需要重新执行。 2. 3. 1. https://blog.csdn.net/zbszhangbosen/article/details/9132833## https://www.cnblogs.com/danhuangpai/p/9456990.html1. MySQL的二进制日志 binlog 可以说是MySQL最重要的日志了, 它记录了所有的DDL和DML(除了数据查询语句)语句, 以事件形式记录, 还包含语句所执行的消耗的时间, MySQL的二进制日志是事务安全型的。 一般来说开启二进制日志大概会有1%的性能损耗(参见MySQL官方中文手册 5.1.24版)。二进制有两个最重要的使用场景: 其一：MySQL Replication在Master端开启binlog, Mster把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 其二：自然就是数据恢复了, 通过使用mysqlbinlog工具来使恢复数据。 二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件, 二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。 https://www.cnblogs.com/kevingrace/p/6065088.html1. binlog 默认是关闭的 mysql&gt; show global variables like “%log_bin%”;+————————————————-+———-+| Variable_name | Value |+————————————————-+———-+| log_bin | OFF || log_bin_basename | || log_bin_index | || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF |+————————————————-+———-+5 rows in set (0.01 sec)``` https://blog.csdn.net/a375015762/article/details/80152964 binlog 打开瞧瞧binlog 实现闪回https://blog.51cto.com/lisea/1949859http://www.17bianji.com/lsqh/39027.html binlog 解析工具binlog 可视化binlog日志文件的格式为二进制, 不能像错误日志文件, 慢查询日志文件那样用 cat, head, tail 等命令来查看, 要查看二进制日志文件的内容, 必须通过MySQL提供的工具 mysqlbinlog https://blog.csdn.net/enmotech/article/details/80175420https://yq.aliyun.com/articles/338423 阿里 canal","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"33. 事务, redolog, undolog","slug":"MySQL/2019-03-10-mysql-33","date":"2019-03-10T14:48:49.000Z","updated":"2019-04-08T13:45:36.000Z","comments":true,"path":"2019/03/10/MySQL/2019-03-10-mysql-33/","link":"","permalink":"http://blog.renyimin.com/2019/03/10/MySQL/2019-03-10-mysql-33/","excerpt":"","text":"https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html redo 概述 redo log 也叫重做日志, 用来实现事务的持久性, 即 ACID 中的 D, 其由两部分组成: 一是 内存中的 重做日志缓冲(redo log buffer), 其是易失的 二是 重做日志文件(redo log file), 其是持久的 InnoDB存储引擎通过 Force Log at Commit 机制实现事务的持久性, 即当事务提交(COMMIT)时, 必须先将该事务的所有日志写入到磁盘上的 重做日志文件 进行持久化 注意: 这里的重做日志, 不是特指 redo log, 这里说的重做日志是包含了 redo log, undo log 两部分 为了确保事务在提交时, 其所有日志都能写入到 重做日志文件 中, 在每次将 log buffer 中的日志写入日志文件的过程中都会调用一次操作系统的 fsync 操作(即 fsync系统调用) 在 InnoDB存储引擎的配置中参数 innodb_flush_method 通常设置为 O_DIRECT, 这也是官方文档所推荐的设置值。DBA或开发人员 知道该参数是文件打开的一个标识，启用后文件的写入将绕过操作系统缓存, 直接写文件。其在InnoDB存储引擎中的表现为对于写入到数据表空间将绕过操作 系统缓存。这样设置通常不会有更好的性能，但是数据库已经有自己的缓存系统，这样的设置可以确定数据库系统对于内存的使用。 123456789mysql&gt; show global variables like &quot;%innodb_flush_method%&quot;;+---------------------+-------+| Variable_name | Value |+---------------------+-------+| innodb_flush_method | |+---------------------+-------+1 row in set (0.00 sec)mysql&gt; 也就是说, 从 redo log buffer 将日志刷到磁盘的 redo log file 的过程大致如下: MySQL 支持用户自定义在commit事务时, 如何将 log buffer 中的日志刷到 磁盘的 log file 中, 参数 innodb_flush_log_at_trx_commit 就是用来控制重做日志刷新到磁盘的策略 innodb_flush_log_at_trx_commit 有3种值: 0, 1, 2 (默认为1) 1234567mysql&gt; show variables like &quot;%innodb_flush_log_at_trx_commit%&quot;;+--------------------------------+-------+| Variable_name | Value |+--------------------------------+-------+| innodb_flush_log_at_trx_commit | 1 |+--------------------------------+-------+1 row in set (0.01 sec) 当设置为1时, 事务每次提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file on disk中, 这种方式即使系统崩溃也不会丢失任何数据, 但是因为每次提交都写入磁盘, IO的性能较差 当设置为0时, 事务提交时不会将log buffer中日志写入到os buffer, 而是每秒写入os buffer并调用fsync()写入到log file on disk中, 也就是说设置为0时, 是(大约)每秒刷新写入到磁盘中的, 当系统崩溃, 会丢失1秒钟的数据 当设置为2时, 每次提交都仅写入到os buffer, 然后是每秒调用fsync()将os buffer中的日志写入到log file on disk, 在这个设置下, 当 MySQL数据库发生宕机而操作系统不发生宕机时, 并不会导致事务的丢失而当操作系统宕机时, 重启数据库后会丢失未从文件系统缓存刷新到重做日志文件那部分事务 innodb_flush_log_at_trx_commit 策略大致如下: 注意, 有一个变量 innodb_flush_log_at_timeout 的值为1秒, 该变量表示的是刷日志的频率 注意: 在主从复制结构中, 要保证事务的持久性和一致性, 需要对日志相关变量设置为如下: 如果启用了二进制日志, 则设置 sync_binlog=1, 即每提交一次事务同步写到磁盘中 总是设置 innodb_flush_log_at_trx_commit=1, 即每提交一次事务都写到磁盘中 上述两项变量的设置保证了 每次提交事务都写入 二进制日志 和 事务日志, 并在提交时将它们刷新到磁盘中 log block Innodb存储引擎中, redo log 是以 块 为单位进行存储的, 每个块占512字节, 这称为 redo log block, 所以不管是log buffer中还是os buffer中以及redo log file on disk中, 都是这样以512字节的块存储的; 每个redo log block由3部分组成：日志块头、日志块尾和日志主体。其中日志块头占用12字节，日志块尾占用8字节，所以每个redo log block的日志主体部分只有512-12-8=492字节 redo 与 binlog undolog redolog 记录了事务的行为, 可以很好地通过其对页进行 “重做” 操作, 但是事务有时还需要进行回滚操作, 这时就需要 undo; 因此在对数据库进行修改时, InnoDB存储引擎不但会产生redo, 还会产生一定量的 undo 这样, 如果用户执行的事务由于某种原因失败了, 或者用户执行了 ROLLBACK 语句请求回滚, 就可以利用这些 undo 信息将数据回滚到修改之前的样子; redo存放在重做日志文件中, 与redo不同, undo存放在数据库内部的一个特殊段(segment)中, 这个段称为 undo 段 (undo segment) undo段位于共享表空间内 用户通常对undo有这样的误解:undo用于将数据库物理地恢复到执行语句或事务之前的样子, 但事实并非如此undo是逻辑日志, 因此只是将数据库逻辑地恢复到原来的样子, 所有修改都被逻辑地取消了, 但是数据结构和页本身在回滚之后可能大不相同; 这是因为在多用户并发系统中, 可能会有数十、数百甚至数千个并发事务。数据库的主要任务就是协调对数据记录的并发访问。比如, 一个事务在修改当前一个页中某几条记录, 同时还有别的事务在对同一个页中另几条记录进行修改, 因此,不能将一个页回滚到事务开始的样子, 因为这样会影响其他事务正在进行的工作;例如, 用户执行了一个INSERT10W条记录的事务, 这个事务会导致分配一个新的段, 即表空间会增大, 在用户执行 ROLLBACK时, 会将插入的事务进行回滚, 但是表空间的大小并不会因此而收缩, 因此, 当 InnoDB存储引擎回滚时, 它实际上做的是与先前相反的工作, 对于每个INSERT, InnoDB存储引擎会完成一个DELETE; 对于每个DELETE, InnoDB存储引擎会执行一个 INSERT; 对于每个UPDATE, InnoDB存储引擎会执行一个相反的UPDATE, 将修改前的行放回去;除了回滚操作, undo的另一个作用是MVCC, 即在 InnoDB存储引擎中MVCC的实现是通过undo来完成, 当用户读取一行记录时, 若该记录已经被其他事务占用, 当前事务可以通过undo读取之前的行版本信息, 以此实现非锁定读取最后也是最为重要的一点是, undo log会产生redo log, 也就是undo log的产生会伴随着 redo log的产生, 这是因为undo log也需要持久性的保护","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"46. 高性能索引 -- 覆盖索引","slug":"MySQL/2019-03-09-mysql-46","date":"2019-03-09T03:12:09.000Z","updated":"2019-04-25T09:06:26.000Z","comments":true,"path":"2019/03/09/MySQL/2019-03-09-mysql-46/","link":"","permalink":"http://blog.renyimin.com/2019/03/09/MySQL/2019-03-09-mysql-46/","excerpt":"","text":"概述 如果一个索引包含(或者说覆盖)所有需要查询的字段的值, 我们就称该索引为 “覆盖索引”; （其实覆盖索引是针对二级索引的, 聚簇索引的查找已经算是回表了） 覆盖索引对于InnoDB表特别有用, InnoDB的二级索引在叶子节点中保存了行的主键值, 但如果二级索引能够做到覆盖索引(即除了主键还保存了你所需要查询的列值), 那么只需要遍历一次B-Tree(可以直接在二级索引中找到数据), 可以避免 回表 对主键索引的二次查询; (其他更多参考 P171) 如果索引不能覆盖查询所需的全部列, 那就不得不每扫描一次索引记录, 就回表查询一次对应的行 select * .... : 因为查询从表中选择了所有的列, 而一般你不会创建覆盖了所有列的二级索引, 所以这种局域肯定不会用到覆盖索引; 疑问 既然InnoDB表有聚簇索引, 那么即便你的二级索引做不到覆盖索引, 要做二次扫描, 由于聚簇索引的叶子节点中包含了所有的数据行字段值, 那么最终也不会导致扫表啊, 为什么会扫表?","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"45. 高性能索引 -- 聚簇索引","slug":"MySQL/2019-03-05-mysql-45","date":"2019-03-05T12:07:57.000Z","updated":"2019-04-25T08:49:23.000Z","comments":true,"path":"2019/03/05/MySQL/2019-03-05-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2019/03/05/MySQL/2019-03-05-mysql-45/","excerpt":"","text":"“索引组织表” 第一篇 重点: InnoDB存储引擎的表是 索引组织表, 即 表中的数据本身就是按照主键顺序存放, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 同时叶子结点存放的即为表的行纪录数据(聚集索引的叶子结点也称为数据页); 聚集索引的这个特性也决定了索引组织表中数据也是索引的一部分, 和 B+Tree 数据结构一样, 每个数据页之间都通过一个双向链表来进行链接; 从 表存储文件 看 “索引组织表” 对于 InnoDB 表, 可以通过 innodb_file_per_table 选项 开启 独立表空间, 此时用户不用将所有数据都存放于默认的表空间(ibdataX文件)中, 而是会产生单独的 .ibd 独立表空间文件可以看到: 这些独立的表空间文件中存储了 innodb表的数据、索引 等信息 (而InnoDB的数据本身就是按照B+Tree组织的, 所以说该文件中其实包含了 数据+聚簇索引 和 二级索引) 但是对于 MyISAM 来说, 其数据表文件分为 .frm(存储表的结构), .MYD(存储数据), .MYI(存储索引)可以明显的看到, MyISAM 的数据和索引是分开存储的 (其 主键索引 和 普通索引 的索引策略没有本质区别) 小结: InnoDB是聚集索引, 数据文件是和索引绑在一起的, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 因此通过主键索引效率很高; 但是辅助索引需要两次查询, 先通过二级索引查询到主键, 然后再通过主键查询到数据; 而MyISAM是非聚集索引, 数据文件和索引是分离的; “索引组织表” 第二篇 InnoDB存储引擎表中, 每张表都有个主键(Primary Key), 因为 表中的数据本身就是按照主键顺序存放, 聚集索引就是按照每张表的主键构造一颗 B+Tree, 所以如果在创建表时没有显示地定义主键, 则InnoDB存储引擎会按如下方式选择或创建主键: 首先判断表中是否有 非空的唯一索引(unique not null), 如果有, 则该列即为主键 如果不符合上述条件, InnoDB存储引擎会自动创建一个6字节大小的指针 (_rowid) 当表中有多个非空唯一索引时, InnoDB存储引擎会选择建表时第一个定义的非空索引为主键 (这里要注意的是, 主键的选择根据的是定义索引的顺序, 而不是建表时列的顺序) 例: 下面创建一张表t, 有a,b,c,d 4个列, b,c,d上都有唯一索引, 不同的是b列允许为NULL, 由于没有显示地定义主键, 因此会选择非空的唯一索引为主键, 并且顺序是第一个定义的索引, 即 字段 d 12345678910111213141516171819202122232425262728293031323334mysql&gt; CREATE TABLE t ( -&gt; a int not null, -&gt; b int null, -&gt; c int not null, -&gt; d int not null, -&gt; UNIQUE KEY(b), -&gt; UNIQUE KEY(d), -&gt; UNIQUE KEY(c) -&gt; );Query OK, 0 rows affected (0.02 sec)// 插入几条测试数据mysql&gt; insert into t select 1,2,3,4;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into t select 5,6,7,8;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into t select 9,10,11,12;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0// _rowid 可以显示表的主键, 因此通过下面的查询语句可以找到表t的主键 (可以看到是d列的值, d被选为主键了)mysql&gt; select a,b,c,d,_rowid FROM t;+---+------+----+----+--------+| a | b | c | d | _rowid |+---+------+----+----+--------+| 1 | 2 | 3 | 4 | 4 || 5 | 6 | 7 | 8 | 8 || 9 | 10 | 11 | 12 | 12 |+---+------+----+----+--------+3 rows in set (0.00 sec) _rowid 可以显示表的主键, 但是只能用于查看单个列为主键的情况, 对于多列组成的主键就无能为力了: 1234567891011121314151617mysql&gt; create table a ( -&gt; a INT, -&gt; b INT, -&gt; PRIMARY KEY(a,b) -&gt; );Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into a select 1,2;Query OK, 1 row affected (0.01 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into a select 3,4;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; select a,_rowid FROM a;ERROR 1054 (42S22): Unknown column &apos;_rowid&apos; in &apos;field list&apos; MyISAM 与 InnoDB 索引对比 主键索引+覆盖索引 MyISAM 从 主键索引 找到B+Tree中对应的叶子节点 找到数据的物理行指针, 然后寻址找到数据 InnoDB 从 聚簇索引 找到B+Tree中对应的叶子节点 直接就找到了数据 看上去 InnoDB是直接找到了数据, 而 MyISAM 还需要寻址, 但其实 MyISAM 还是要快一些 ??? 普通索引+覆盖索引 MyISAM 从普通索引中就可以取到具体数据 而InnoDB 从二级索引中就可以取到数据的 看起来是一样快的 如果索引生效, 但没做到覆盖索引: MyISAM 从普通索引中 就可以取到具体数据的物理行位置, 然后通过 物理行号 再回表, 到具体数据行 而InnoDB 需要两次索引扫描, 先从二级索引中可以取到数据的主键, 然后 通过 主键 在聚簇索引上找到具体数据行 如果索引未生效, 走全表扫描 MyISAM 的数据分布比较简单, 按照数据的插入顺序存储在磁盘上, 扫表比较快 InnoDB 表的数据本身就是聚集的, 也就是说, 表本身就是聚集索引, 全表扫描, 扫的就是当然就是聚集索引本身 正常情况下, 我们可以定义一个代理键作为主键, 这种主键的数据应该和应用无关, 最简单的方法是使用 AUTO_INCREMENT 自增列, 这样可以保证数据行是按顺序写入的最好避免随机的(不连续且值的分布范围非常大)的聚簇索引, 特别是对于I/O密集型的应用, 这样会导致聚簇索引的插入变得完全随机, 这是最坏情况, 数据没有任何聚集特性 做全表扫描时, InnoDB 会按主键顺序扫描页面和行, 如果主键页表没有碎片(存储主键和行的页表), 全表扫描是相当快, 因为读取顺序接近物理存储顺序, 但如果主键碎片化变大, 随机性高, 则性能会下降; 执行 select count(*) from table 时需要全表扫描 InnoDB不保存表的具体行数 而MyISAM用一个变量保存了整个表的行数, 执行上述语句时只需要读出该变量即可, 速度很快 聚簇索引 在MySQL中, InnoDB使用的是聚簇索引, 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上, 若使用 “where id = 14” 这样的条件查找主键, 即做到了 主键 覆盖索引, 则按照B+树的检索算法即可查找到对应的叶节点, 之后获得行数据; 若对Name普通索引列进行条件搜索, 则需要两个步骤: 第一步在辅助索引B+树中检索Name, 到达其叶子节点获取对应的主键; 第二步使用主键在主索引B+树种再执行一次B+树检索操作, 最终到达叶子节点即可获取整行数据 MyISM使用的是非聚簇索引, 非聚簇索引的两棵B+树看上去没什么不同, 节点的结构完全一致。表数据存储在独立的地方, 这两颗B+树的叶子节点都使用一个 地址 指向真正的表数据, 对于表数据来说, 这两个键没有任何差别, 由于索引树是独立的, 通过辅助键检索无需访问主键的索引树; 下图这张抽象图描述了两者保存数据和索引的区别 聚簇索引的特点: 一个表只能有一个聚簇索引 (不过覆盖索引可以模拟多个聚簇索引的情况) 有一点和MyISAM不同的是: InnoDB 聚簇索引的每一个叶子节点中都包含了主键值, 事务ID, 用于事务和MVCC的回滚指针 以及 表中剩余的所有列; 而 MyISAM 索引的叶子节点只有你创建索引时选择的列的值(也可以做到覆盖索引) 还有一点和MyISAM不同的是: InnoDB的 二级(辅助)索引 和 聚簇索引 有很大的不同, InnoDB 二级(辅助)索引 的叶子节点中存储的 不是物理 行指针, 而是主键值, 通过该主键值指向聚簇索引中的主键而对 MyISAM 存储引擎来说, 其 主键索引 和 二级(辅助)索引 在结构上没有什么不同 一些缺点: 二级索引(非聚簇索引) 可能会比想象的要稍大一些, 因为在二级索引的叶子节点包含了引用行的主键列 更新聚簇索引列的代价很高, 因为会强制Innodb将每个被更新的行移动到新的位置 基于聚簇索引的表在插入新行, 或者逐渐被更新导致需要移动行时, 可能面临 “页分裂” 的问题, 页分裂会导致表占用更多的磁盘空间 … 参考 P163 小结: 所以, MyISAM 用索引检索数据时, 无论是使用的 主键索引 还是 普通索引, 只会访问一次索引即可拿到叶子节点的物理行指针; 而InnoDB在使用 二级(非聚簇)索引 时, 需要访问可能两次索引查找, 而不是一次, 除非你做到了覆盖索引 或者 你用到了主键索引; 另外需要知道的是: 在MySQL目前内建的存储引擎中, 不支持 手动选择一个列作为聚簇索引 (InnoDB 如果没有定义主键, MySQL 会隐式定义一个主键来作为聚簇索引) 覆盖索引可以模拟多个聚簇索引的情况?? 聚簇索引可能对性能有帮助, 但也可能导致严重的性能问题 (尤其是将表的存储引擎从InnoDB改成其他引擎, 或者反过来时, 需要仔细考虑聚簇索引) ?? 在InnoDB中按主键顺序插入行… 未完待续 参考:http://blog.haohtml.com/archives/17372","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"44. 高性能索引 -- 多列索引","slug":"MySQL/2019-03-05-mysql-44","date":"2019-03-05T06:12:31.000Z","updated":"2019-04-22T12:10:29.000Z","comments":true,"path":"2019/03/05/MySQL/2019-03-05-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2019/03/05/MySQL/2019-03-05-mysql-44/","excerpt":"","text":"合并索引 技术 首先, 你需要了解的是 “为多个列各自创建独立的索引 在大部分情况下并不能提高MySQL的性能” 尽管MySQL5.0+引入了一种叫 “索引合并”(index merge) 的技术, 在一定程度上可以使用表上的多个单列索引来定位指定的行 示例: explain_goods 创建了 stock, goods_weight 两个独立索引 1234567891011121314151617// 当表中有数据时, 确实使用了 索引合并 技术mysql&gt; explain select id from explain_goods where goods_weight=10 or stock=11; +----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+| 1 | SIMPLE | explain_goods | index_merge | idx_stock,idx_goods_weight | idx_goods_weight,idx_stock | 4,4 | NULL | 2 | Using union(idx_goods_weight,idx_stock); Using where |+----+-------------+---------------+-------------+----------------------------+----------------------------+---------+------+------+------------------------------------------------------+1 row in set (0.00 sec) // 注意一个例外情况: 当表中没有数据时, 发现MySQL优化器走了全表扫描mysql&gt; explain select id from explain_goods_copy1 where goods_weight=10 or stock=11; +----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+| 1 | SIMPLE | explain_goods_copy1 | ALL | idx_stock,idx_goods_weight | NULL | NULL | NULL | 1 | Using where |+----+-------------+---------------------+------+----------------------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 这种算法有三个变种: or条件的联合, and条件的相交, 组合前两种情况的联合及相交 多列索引的顺序 本节适用于 B-Tree 索引 多列 B-Tree 索引的顺序至关重要 当不需要考虑排序和分组时, 通常将选择性最高的列放在前面 (这样设计索引, 能够最快地过滤出处需要的行, 对于在where条件中只使用了索引部分前缀列的查询来说选择性也更好)","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"43. 高性能索引 -- 前缀索引 & 索引选择性","slug":"MySQL/2019-03-03-mysql-43","date":"2019-03-03T12:07:57.000Z","updated":"2019-04-22T12:08:07.000Z","comments":true,"path":"2019/03/03/MySQL/2019-03-03-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2019/03/03/MySQL/2019-03-03-mysql-43/","excerpt":"","text":"什么时候要用前缀索引? 有时候需要为某个很长的字符列创建索引, 这会让索引变得大且慢, 一个策略是前面提到过的模拟哈希索引, 但有时候这样却不那么合适 其实对于很长的字符列, 如果要创建索引, 我们可以尝试为该列开始的部分字符创建索引, 这样就可以大大节约索引空间, 从而提高索引效率, 但可能会降低索引的选择性; 什么是索引的选择性? 索引的选择性是指 不重复的索引值的数量 和 数据表的记录总数(n) 的比值 (范围从 1/n 到 1之间) 索引的选择性越高, 则查询效率越高; 因为选择性高的索引可以让MySQL在查找时过滤更多的行, 唯一索引的选择性是1, 这是最好的索引选择性, 性能也是最好的; 一般情况下, 某个列的前缀索引的选择性都是比较高的, 足以满足查询性能, 对于 BLOB, TEXT 或 很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 要注意的是: 我们要选择足够长的前缀字符, 以保证较高的选择性, 但同时又不能太长(以便于节约空间) 如何选择合适的前缀长度? 那么 当你为一个 较长的字符列 创建索引时, 如何决定前缀的合适长度呢? (我们的目的是选择足够长的前缀, 从而提高前缀索引的选择性, 提高查询性能) 准备一张简单的城市表, 里面存放了美国的一些地方名 前缀长度 — 统计观察 尝试 根据各城市名分组 根据各组统计的次数 倒序排名: 可以看到, 完整列的选择性也并非是1, 我们可以选择合适的前缀, 让选择性逐渐接近完整列的 12345678910111213141516mysql&gt; select count(*) as cnt, city_name from test_city group by city_name order by cnt desc limit 10;+-----+-----------+| cnt | city_name |+-----+-----------+| 4 | SantaRosa || 3 | Auburn || 3 | Norfolk || 3 | Roanoke || 3 | Richfield || 3 | Olympia || 3 | Danville || 3 | Arlington || 3 | Provo || 3 | Plano |+-----+-----------+10 rows in set (0.01 sec) 尝试取长度为3的前缀, 继续上述统计: 12345678910111213141516mysql&gt; select count(*) as cnt, left(city_name, 3) as pref from test_city group by pref order by cnt desc limit 10;+-----+------+| cnt | pref |+-----+------+| 19 | New || 17 | Nor || 16 | For || 16 | San || 13 | Cha || 11 | Sal || 10 | Roc || 9 | Gra || 9 | Ken || 9 | Bel |+-----+------+10 rows in set (0.01 sec) 尝试增加前缀长度, 让前缀选择性接近完整列的选择性 前缀长度 — 选择性计算 计算合适前缀长度的另外一个方法就是计算完整列的选择性, 并使前缀的选择性接近于完整列的选择性 下面显示了如何计算完整列的选择性: 1234567mysql&gt; SELECT COUNT(DISTINCT city_name ) / COUNT( * ) FROM test_city;+-----------------------------------------+| COUNT(DISTINCT city_name ) / COUNT( * ) |+-----------------------------------------+| 0.4821 |+-----------------------------------------+1 row in set (0.00 sec) 上面看来, 对 city_name 这列来说, 完整列的选择性是 0.4821, 不算高, 但如果要对该列建索引, 那么选择的前缀要保证选择性能接近 0.4821 即可 下面给出了如何在同一个查询中计算不同前缀长度的选择性: 12345678910111213141516171819mysql&gt; SELECT COUNT( DISTINCT city_name ) / COUNT( * ) as t0, COUNT( DISTINCT left(city_name, 4) ) / COUNT( * ) as t1, COUNT( DISTINCT left(city_name, 5) ) / COUNT( * ) as t2, COUNT( DISTINCT left(city_name, 6) ) / COUNT( * ) as t3, COUNT( DISTINCT left(city_name, 7) ) / COUNT( * ) as t4, COUNT( DISTINCT left(city_name, 8) ) / COUNT( * ) as t5, COUNT( DISTINCT left(city_name, 9) ) / COUNT( * ) as t6, COUNT( DISTINCT left(city_name, 10) ) / COUNT( * ) as t7, COUNT( DISTINCT left(city_name, 11) ) / COUNT( * ) as t8FROM test_city;+--------+--------+--------+--------+--------+--------+--------+--------+--------+| t0 | t1 | t2 | t3 | t4 | t5 | t6 | t7 | t8 |+--------+--------+--------+--------+--------+--------+--------+--------+--------+| 0.4821 | 0.4055 | 0.4351 | 0.4597 | 0.4729 | 0.4760 | 0.4791 | 0.4811 | 0.4811 |+--------+--------+--------+--------+--------+--------+--------+--------+--------+1 row in set (0.01 sec) 可以看到, 前缀长度到达 10 的时候, 选择性的提升已经很小了, 因此 10 可以算是一个差不多合理的长度了 注意: 前缀索引是一种能似索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 也无法使用前缀索引做覆盖索引","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"42. 高性能索引 -- 独立的列","slug":"MySQL/2019-03-03-mysql-42","date":"2019-03-03T11:05:16.000Z","updated":"2019-04-22T12:05:42.000Z","comments":true,"path":"2019/03/03/MySQL/2019-03-03-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2019/03/03/MySQL/2019-03-03-mysql-42/","excerpt":"","text":"本篇非常简单 独立的列 是指索引列不能是表达式的一部分, 也不能是函数的参数; 示例: 很容易看出 where 中的表达式其实等价于 actor_id=4, 但MySQL却无法自动解析这个方程式 1mysql&gt; select actor_id FROM actor where actor_id+1=5; (就像 mysql的查询优化器不能将 id&gt;5 和 id&gt;6 这两个查询条件优化合并成一个 id&gt;6 一样, MySQL优化器有时候没有我们想的那么自然, 但的确非常有用)","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"41. 高性能索引","slug":"MySQL/2019-03-02-mysql-41","date":"2019-03-02T13:51:21.000Z","updated":"2019-04-23T02:56:07.000Z","comments":true,"path":"2019/03/02/MySQL/2019-03-02-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2019/03/02/MySQL/2019-03-02-mysql-41/","excerpt":"","text":"独立的列前缀索引 &amp; 索引选择性多列索引聚簇索引覆盖索引使用索引扫描做排序压缩(前缀压缩)索引冗余和重复索引未使用的索引索引和锁","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"40. 索引基础","slug":"MySQL/2019-03-02-mysql-40","date":"2019-03-02T06:30:25.000Z","updated":"2019-04-17T08:17:17.000Z","comments":true,"path":"2019/03/02/MySQL/2019-03-02-mysql-40/","link":"","permalink":"http://blog.renyimin.com/2019/03/02/MySQL/2019-03-02-mysql-40/","excerpt":"","text":"概述 索引是存储引擎用于快速找到记录的一种数据结构, 这也是它最基本的功能 索引有很多种类型, 可以为不同的场景提供更好的性能, MySQL支持的常见索引类型有: B-Tree索引 (事实上其升级版B+Tree更常被采用) 哈希索引 全文索引 空间数据索引(R-Tree) … 其他类型的索引 MySQL中, 索引是在存储引擎层而不是服务器层实现的, 所以并没有统一的索引标准, 这就导致了 不同存储引擎的索引的工作方式可能不一样 也不是所有的存储引擎都支持所有类型的索引 即使多个存储引擎支持同一种类型的索引, 其底层实现也可能不同 索引对于良好的性能非常关键, 尤其是当表中的数据量越来越大时, 索引对性能的影响愈发重要; 它是对查询性能优化最有效的手段了, 能轻易将查询性能提高几个数量级; 索引可以让服务器快速地定位到表的指定位置, 但这并不是索引的唯一作用, 根据创建索引的数据结构的不同, 索引还有一些其他的附加作用, 如 最常见的B-Tree(实际应用比较多的是其升级版B+Tree), 是按照顺序存储数据结构的, 所以MySQL可以用来做 ORDER BY 和 GROUP BY 操作, 而且B-Tree会将相关的列值都存储在一起, 这样由于索引中存储了实际的列值, 所以某些查询只使用索引就能够完成全部查询 总结下来索引有以下三个优点: 索引大大减少了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机I/O编为顺序I/O 索引虽好, 但我们要学会创建最优的索引, 不恰当的索引反而会导致性能的下降 ? 索引并不总是最好的工具, 总的来说, 只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时, 索引才是有效的 对于非常小的表, 大部分情况下简单的全表扫描更高效 对于中到大型的表, 索引就非常有效 但是对于特大型的表, 建立和使用索引的代价将随之增长, 这种情况下需要一种技术可以直接区分出查询需要的一组数据, 而不是一条记录一条记录的匹配, 可以考虑分区技术; 不恰当的索引 数据量小且负载较低时, 不恰当的索引对性能的影响可能还不明显, 但当数据量逐渐增大时, 会导致性能急剧下降; 待补充 索引可以包含 一个 或 多个列: 如果如果索引包含多个列, 索引中列的顺序也非常重要, 因为MySQL只能高效地使用索引的 最左前缀列 创建一个包含两个列的索引 和 创建两个只包含一个列的索引 是大不相同的 B-Tree索引基础概述 当人们讨论索引时, 如果没有特别指明类型, 多半说的是B-Tree索引, 它使用B-Tree数据结构来存储数据, 但实际上很多存储引擎使用的是升级版的B+Tree (即每一个叶子节点都包含了指向下一个叶子节点的指针, 从而方便了叶子节点的范围遍历) 所以在MySQL中我们讨论B-Tree时, 其实说的就是B+Tree, 它是目前关系型数据库系统中查找最为常用和最为有效的索引, 很多地方使用术语 B-Tree, 是因为MySQL 官方很多地方都是用的 B-Tree/BTree(其实 B-Tree 中间的 - 不是 减, 只是个分隔词, 参考旧金山大学数据结构可视化平台) , 比如创建表索引时, 指定的索引类型就是BTREE, 而没有B+TREE B-Tree 索引列是顺序组织存储的, 所以很适合查找范围数据; 另外, 其每一个叶子页到根的距离都是相同的; 叶子页 比较特别, 他们的指针指向的是被索引的数据, 而不是其他的节点页(不同引擎的”指针”类型不同); 下图展示了 B-Tree 索引的抽象表示(从技术上来说是B+Tree), 大致反映了InnoDB索引是如何工作的, MyISAM使用的结构有所不同, 但基本思想类似: 可以看出B+Tree索引能够加快访问数据的速度, 是因为存储引擎不再需要进行全表扫描来获取需要的数据, 而是直接从索引树中进行搜索 最后, 你可以去旧金山大学数据结构可视化平台尝试构建一棵自己的B+TREE MySQL大多数存储引擎都支持这种索引(除了Archive), 不同的存储引擎以不同的方式使用B-Tree索引, 性能也各有不同, 各有优劣, 如: MyISAM 使用前缀压缩技术使得索引更小; 而 InnoDB 则按照原数据格式进行存储 MyISAM 索引通过数据的物理位置引用被索引的行; 而 InnoDB 则根据主键引用被索引的行 案例 假设有如下数据表: 1234567891011121314151617CREATE TABLE `People` ( `last_name` varchar(50) NOT NULL, `first_name` varchar(50) NOT NULL, `dob` date NOT NULL, `gender` enum(&apos;m&apos;,&apos;f&apos;) NOT NULL, KEY `idx_test` (`last_name`,`first_name`,`dob`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Christian&apos;, &apos;1958-12-07&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Debbie&apos;, &apos;1990-03-18&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Akroyd&apos;, &apos;Kirsten&apos;, &apos;1978-11-02&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Cuba&apos;, &apos;1960-01-01&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Kim&apos;, &apos;1930-07-12&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Allen&apos;, &apos;Meryl&apos;, &apos;1980-12-12&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Julia&apos;, &apos;2000-05-16&apos;, &apos;f&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Viven&apos;, &apos;1976-12-08&apos;, &apos;m&apos;);INSERT INTO `People` VALUES (&apos;Barrymore&apos;, &apos;Viven&apos;, &apos;1979-01-24&apos;, &apos;m&apos;); 如下显示了该索引是如何组织数据的存储的: 注意: 索引对多个列的值进行排序的依据是 定义索引时列的顺序 (如上图中, 最后两个条目, 两个人的姓和名都一样, 则根据他们的出生日期来进行排列顺序) B-Tree 使用限制 B-Tree 索引对如下类型的查询有效 全值匹配: 指的是where条件中有序地包含了索引中的所有列 (需要注意的是, 如果你的查询条件做到了全值匹配, 那么即使你查询条件的顺序不是依照左前缀原则, MySQL也会做优化) 匹配最左列: 可以只匹配 多列索引 的第一列, 比如查找 所有姓为 Allen 的人 匹配最左列的前缀: 可以匹配 索引 中第一列的值的开头部分, 比如, 查找 姓以’J’开头 的人 ( like ‘J%’) 匹配最左列范围值: 查找 ‘姓在Allen和Barrymore’ 之间的人 精确匹配第一列 并 范围匹配第二列: 查找 ‘姓为Allen 并且 名字以K开头’ 的人 只访问索引的查询: 这种查询只需要访问索引, 而无需访问数据行, 后面会单独讨论这种 覆盖索引B-Tree索引中存储了实际的列值, 所以某些查询(满足覆盖索引时)可能只使用索引就能够完成查询工作了 B-Tree 索引的有效排序: 由于索引树中的节点是有序的, 所以除了按值查找之外, 索引还可以用于查询中的 ORDER BY 操作(按顺序查找); 一般来说, 如果 B-Tree 可以按照某种方式查询到值, 那么也可以按照这种方式用于排序 B-Tree索引的限制: 如果查询条件不是从索引的最左列开始写, 则无法使用索引 如People表的索引, 无法用于 ‘查找名字为Bill的人’, 也无法用于 ‘查找生日为1960-01-01的人’, 因为这两列都不是最左数据列 (不满足匹配最左列)也无法用于查找 ‘姓以某个字母结尾的人’ (你建索引时指定的列顺序, 列的值内容 都要符合最左前缀才能利用到索引) (不满足匹配最左列的前缀) 查询条件不能跳过索引中的列: 比如, 查询 ‘姓为Smith 并且 生日为1960-01-01’ 只能只用到索引的第一列 如果查询条件中有某个列是范围查询, 则其右边的所有列都无法使用索引来优化查找 如 where last_name=&#39;Smith&#39; and first_name LIKE &#39;J%&#39; AND dob=&#39;1976-12-23&#39; 这个查询只能使用索引的前两列所以, 如果范围查询的列的值结果有限, 比如数据表中只有2个人是 ‘名字以J开头’ 的, 那你也别用范围查找了, 直接用多个等于条件来代替就行: where last_name=&#39;Smith&#39; and (first_name=&#39;Jack&#39; or first_name=&#39;Jieke&#39; ) AND dob=&#39;1976-12-23&#39; 哈希索引概述 hash index 是基于哈希表实现, 只有精确匹配索引所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code), 哈希码是一个较小的值, 并且不同键值的行计算出来的哈希码也不一样, 哈希索引将所有的哈希码存储在索引中, 同时在哈希表中保存指向每个数据行的指针; 哈希索引自身只需要存储对应的哈希值, 所以索引的结构之分紧促, 查找的速度非常快 哈希索引的缺陷: 由于哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快, 所以大部分情况下这一点对性能的影响并不明显; 哈希索引数据并不是按照索引值顺序存储的, 所以也就无法用于排序 哈希索引页不支持部分索引列匹配查找, 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 哈希索引只支持等值比较查询, 包括 =, in(), 不支持任何范围查询(如 &gt; 等) 访问哈希索引的数据非常快, 除非有很多哈希冲突 (当出现哈希冲突时, 存储引擎必须遍历链表中所有的行指针, 逐行进行比较, 直到找到所有符合条件的行) 如果哈希索引冲突很多的话, 一些索引维护操作的代价也会很高, 例如在某个选择性很低(哈希冲突很多)的列上创建哈希索引, 那么当从表中删除一行时, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的引用, 冲突越多, 代价越大; 因为这些限制, 哈希索引只适用于某些特定的场合。而一旦适合哈希索引, 则它带来的性能提升将非常显著。举个例子, 在数据仓库应用中有一种经典的 “星型” schema, 需要关联很多查询表, 哈希索引就非常适合查找表的需求。 InnoDB引擎有一个特殊的功能叫做 **自适应哈希索引(adaptive hash index), 当InnoDB注意到某些索引值被使用得非常频繁时, 它会在内存中基于B-Tree索引之上再创建一个哈希索引, 这样就让B-Tree索引也具有哈希索引的一些优点, 比如快速的哈希查找; 这是一个完全自动的、内部的行为, 用户无法控制或者配置, 不过如果有必要, 完全可以关闭该功能; InnoDB — HASH索引陷阱 首先, 在翻阅《高性能MySQL》一书时, P146页提到 “在MySQL中, 只有Memory存储引擎显示支持哈希索引”; 而 《MySQL技术内幕 InnoDB存储引擎》一书却在P183提到 InnoDB是支持哈希索引的; 那么 InnoDB 到底支不支持Hash索引呢? 参考官方手册 发现手册中说的是InnoDB不支持Hash索引, 但是同时给一段特殊的提示 ‘InnoDB utilizes hash indexes internally for its Adaptive Hash Index feature’ 其实官方的描述和《MySQL技术内幕 InnoDB存储引擎》中表述的类似 InnoDB存储引擎 是支持hash索引的, 不过, 我们必须启用; 如下可以查看hash的开启状态 (默认就是开启的) 1234567mysql&gt; show variables like &apos;%ap%hash_index&apos;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| innodb_adaptive_hash_index | ON |+----------------------------+-------+1 row in set (0.00 sec) hash索引的创建由InnoDB存储引擎根据表的使用情况自动优化为表生成哈希索引, 我们干预不了 平时遇到的假象 既然我们无法干预是否在一张表中生成哈希索引, 为什么在 Navicat 中为InnoDB表创建索引时, 可以指定索引类型为 HASH索引, 而且MySql 并不会报错? 而且你通过 SHOW CREATE TABLE 查看该索引也是 Hash123456CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) USING HASH) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 真相: 事实上并非如此, 我们都被 MySql 给骗了, 我们使用 SHOW INDEXES FROM 语句对该表索引进行检索, 发现其实用的是BTREE 12345678mysql&gt; SHOW INDEXES FROM t2;+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| t2 | 0 | PRIMARY | 1 | id | A | 0 | NULL | NULL | | BTREE | | || t2 | 1 | idx_name | 1 | name | A | 0 | NULL | NULL | | BTREE | | |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+2 rows in set (0.00 sec) 虽然InnoDB并不支持 Hash 索引, 但 InnoDB 有另一种实现方法: 自适应哈希索引 InnoDB 存储引擎会监控对表上索引的查找, 如果观察到建立哈希索引可以带来速度的提升, 则建立哈希索引 可以通过 SHOW ENGINE INNODB STATUS 来查看当前自适应哈希索引的使用状况 创建自定义hash索引 如果存储引擎不支持哈希索引, 你可以模拟像InnoDB一样创建哈希索引, 这可以享受一些哈希索引的便利, 例如只需要很小的索引就可以为超长的键创建索引, 思路很简单: 在B-Tree基础上创建一个伪哈希索引, 这和真正的哈希索引不是一回事, 因为还是使用B-Tree进行查找, 但是它使用哈希值而不是键本身进行索引查找; 你需要做的就是在查询的WHERE子句中手动指定使用哈希函数; 下面是一个实例, 例如需要存储大量的URL, 并需要根据URL进行搜索查找如果使用B-Tree来存储URL, 存储的内容就会很大, 因为URL本身都很长, 正常情况下会有如下查询: 1mysql&gt; select id from url where url=&quot;http://www.mysql.com&quot;; 若删除原来的URL列上的索引, 而新增一个被索引的url_crc列, 使用CRC32做哈希, 就可以使用下面的方式查询: 1mysql&gt; select id from url where url=&quot;http://www.mysql.com&quot; and url_crc=CRC32(&quot;http://www.mysql.com&quot;); 这样做的性能会非常高, 因为MySQL优化器会使用这个选择性很高而体积很小的基于url_crc列的索引来完成查找, 即使有多个记录有相同的索引值, 查找仍然很快, 只需要根据哈希值做快速的整数比较就能找到索引条目, 然后一一比较返回对应的行; 这样实现的缺陷是需要维护哈希值, 可以手动维护, 也可以使用触发器实现 下面的案例演示了触发器如何在插入和更新时维护url_crc列, 首先创建如下表: ?? 数据空间索引全文索引其他索引类别","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"25","slug":"MySQL/2019-02-21-mysql-25","date":"2019-02-21T03:31:07.000Z","updated":"2019-04-25T09:03:55.000Z","comments":true,"path":"2019/02/21/MySQL/2019-02-21-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2019/02/21/MySQL/2019-02-21-mysql-25/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"01.","slug":"大并发/01","date":"2019-02-19T13:04:27.000Z","updated":"2019-02-19T13:06:47.000Z","comments":true,"path":"2019/02/19/大并发/01/","link":"","permalink":"http://blog.renyimin.com/2019/02/19/大并发/01/","excerpt":"","text":"JDKPERL","categories":[],"tags":[]},{"title":"01. JDBC","slug":"JavaWeb/JDBC/2019-01-30-01","date":"2019-01-30T07:25:09.000Z","updated":"2019-02-01T06:57:09.000Z","comments":true,"path":"2019/01/30/JavaWeb/JDBC/2019-01-30-01/","link":"","permalink":"http://blog.renyimin.com/2019/01/30/JavaWeb/JDBC/2019-01-30-01/","excerpt":"","text":"类似于PHP中的PDO JDBC（Java Database Connectivity）, 即Java数据库连接, 是一种用于执行SQL语句的Java API, 可以为多种关系数据库提供同一访问, 它由一组用Java语言编写的类和接口组成。JDBC提供了一种基准, 根据这种基准可以构建更高级的工具和接口, 使数据库开发人员能够编写数据库应用程序。总而言之, JDBC做了三件事: 与数据库建立连接 发送操作数据库的语句 处理结果 Bean Bean(豆子) JavaBeanPOJO","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01.","slug":"JavaWeb/2019-01-30-01","date":"2019-01-30T07:25:09.000Z","updated":"2019-02-02T07:26:43.000Z","comments":true,"path":"2019/01/30/JavaWeb/2019-01-30-01/","link":"","permalink":"http://blog.renyimin.com/2019/01/30/JavaWeb/2019-01-30-01/","excerpt":"","text":"Servlet 本质上就是运行在服务器上的Java类 Servlet容器 Bean Bean(豆子) JavaBeanPOJO","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"50. NIO","slug":"JavaSE/2019-01-21-50","date":"2019-01-21T02:21:33.000Z","updated":"2019-02-01T02:21:37.000Z","comments":true,"path":"2019/01/21/JavaSE/2019-01-21-50/","link":"","permalink":"http://blog.renyimin.com/2019/01/21/JavaSE/2019-01-21-50/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01. redis","slug":"redis/2019-01-26-01","date":"2019-01-16T02:29:26.000Z","updated":"2019-03-15T03:08:45.000Z","comments":true,"path":"2019/01/16/redis/2019-01-26-01/","link":"","permalink":"http://blog.renyimin.com/2019/01/16/redis/2019-01-26-01/","excerpt":"","text":"LRU和AOF持久化文件 SYNC 和 BGSAVEhttp://chenzhenianqing.com/articles/1069.html redis的持久化主要是为了做灾难恢复, 如果redis的持久化做好, 备份和恢复方案做到企业级, 即使redis故障, 也可以通过备份数据, 快速恢复并对外提供服务 危险命令的禁用Redis中线上使用keys *命令，也是非常危险的。因此线上的Redis必须考虑禁用一些危险的命令，或者尽量避免谁都可以使用这些命令，Redis没有完整的管理系统，但是也提供了一些方案https://blog.csdn.net/qq_36101933/article/details/82893540 redis库名每个数据库对外都是一个从0开始的递增数字命名，Redis默认支持16个数据库（可以通过配置文件支持更多，无上限），可以通过配置databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据库，不过可以随时使用SELECT命令更换数据库，如要选择1号数据库然而这些以数字命名的数据库又与我们理解的数据库有所区别。首先Redis不支持自定义数据库的名字，每个数据库都以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。另外Redis也不支持为每个数据库设置不同的访问密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库之间并不是完全隔离的，比如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。综上所述，这些数据库更像是一种命名空间，而不适宜存储不同应用程序的数据。比如可以使用0号数据库存储某个应用生产环境中的数据，使用1号数据库存储测试环境中的数据，但不适宜使用0号数据库存储A应用的数据而使用1号数据库B应用的数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用的内在只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。 Redis白名单设置 阿里云数据库 Redis 设置 ECS-IP白名单 redis的持久化 持久化的意义主要是在于 故障恢复 redis版本选择 redis 3.2 迁移到4.x注意事项 和 redis 4.0的主要特性 redis的阻塞?Redis 节点故障后，主备切换的数据一致性 主从, 集群, 分片 网络分区, 崩溃问题? redis 高并发+高性能+高可用(99.99% 4个9), qps极限, 压测? 如何用复杂的缓存架构去支撑高并发, 将缓存架构做成高可用机会，也可以学到高可用系统架构构建的技术 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 通过主从, 读写分离, 来实现redis支撑高并发 QPS10万+ 日志分析? redis事务问题? WATCH, 重试? redis锁问题? 云数据库Redis版（ApsaraDB for Redis） 架构? Redis LRU 算法? lua脚本? keylord : https://protonail.com/ 主从同步机制是? 同步时, 也额外需要大量内存? 123456789101112131415161718192021222324252627真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？（1）如何让redis集群支撑几十万QPS高并发+99.99%高可用+TB级海量数据+企业级数据备份与恢复？：redis企业级集群架构（2）如何支撑高性能以及高并发到极致？同时给缓存架构最后的安全保护层？：(nginx+lua)+redis+ehcache的三级缓存架构（3）高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？：企业级的完美的数据库+缓存双写一致性解决方案（4）如何解决大value缓存的全量更新效率低下问题？：缓存维度化拆分解决方案（5）如何将缓存命中率提升到极致？：双层nginx部署架构，以及lua脚本实现的一致性hash流量分发策略（6）如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？：基于zookeeper分布式锁的缓存并发重建解决方案（7）如何解决高并发场景下，缓存冷启动MySQL瞬间被打死的问题？：基于storm实时统计热数据的分布式快速缓存预热解决方案（8）如何解决热点缓存导致单机器负载瞬间超高？：基于storm的实时热点发现，以及毫秒级的实时热点缓存负载均衡降级（9）如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？：基于hystrix的高可用缓存服务，资源隔离+限流+降级+熔断+超时控制（10）如何应用分布式系统中的高可用服务的高阶技术？：基于hystrix的容错+多级降级+手动降级+生产环境参数优化经验+可视化运维与监控（11）如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？：独家的事前+事中+事后三层次完美解决方案（12）如何解决高并发场景下的缓存穿透问题？避免给MySQL带来过大的压力？：缓存穿透解决方案（13）如何解决高并发场景下的缓存失效问题？避免给redis集群带来过大的压力？：缓存失效解决方案","categories":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/tags/redis/"}]},{"title":"02. redis","slug":"redis/2019-01-26-02","date":"2019-01-16T02:29:26.000Z","updated":"2019-03-05T05:24:34.000Z","comments":true,"path":"2019/01/16/redis/2019-01-26-02/","link":"","permalink":"http://blog.renyimin.com/2019/01/16/redis/2019-01-26-02/","excerpt":"","text":"https://carlosfu.iteye.com/blog/2254154 如何发觉redis被阻塞","categories":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://blog.renyimin.com/tags/redis/"}]},{"title":"42","slug":"JavaSE/2019-01-12-42","date":"2019-01-12T09:32:11.000Z","updated":"2019-02-02T07:24:38.000Z","comments":true,"path":"2019/01/12/JavaSE/2019-01-12-42/","link":"","permalink":"http://blog.renyimin.com/2019/01/12/JavaSE/2019-01-12-42/","excerpt":"","text":"线程安全synchronized 关键字同步代码块同步监视器 同步方法","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"41","slug":"JavaSE/2019-01-11-41","date":"2019-01-11T13:15:08.000Z","updated":"2019-02-02T07:22:30.000Z","comments":true,"path":"2019/01/11/JavaSE/2019-01-11-41/","link":"","permalink":"http://blog.renyimin.com/2019/01/11/JavaSE/2019-01-11-41/","excerpt":"","text":"线程安全synchronized 关键字同步代码块同步方法","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"40. 多线程","slug":"JavaSE/2019-01-10-40","date":"2019-01-10T13:15:08.000Z","updated":"2019-02-02T06:47:13.000Z","comments":true,"path":"2019/01/10/JavaSE/2019-01-10-40/","link":"","permalink":"http://blog.renyimin.com/2019/01/10/JavaSE/2019-01-10-40/","excerpt":"","text":"什么是多线程? 首先, 对于 进程 的概念, 如果还不理解的话, 可以自行查看 “Windows任务管理器” 中的列表, 没错, 你完全可以将运行在内存中的exe文件理解成进程 (进程是操作系统管理的基本运行单元) 线程 可以理解成是在进程中独立运行的子任务 (比如, QQ.exe 运行时, 就有很多子任务在同时运行, 如 发送表情, 下载文件, 传输数据, 与好友视频线… 等, 每一项任务都有各自对应的线程在后台默默地运行) 使用多线程的好处: CPU通过在多个任务间不停地快速切换, 给读者以 “同一时间内运行多个不同种类任务” 的感觉; 提高CPU的利用率 对于单任务系统, 假设有两个互相独立的任务1和2需要执行, 假设任务1正在等待远程服务器返回数据, 以便进行后期的处理, 如果任务1在10秒后才得到远程服务器的数据, 那就意味着任务1在10秒后才能运行, 这期间CPU将一直处于等待状态; 而悲催的是, 任务2可能只需要执行1秒即可完成, 但它也必须等待任务1运行结束后才可以运行; 这就造成了任务2有非常长的等待时间单任务的这种排序执行的缺点, 就造成了CPU利用率大幅的降低! 而对于多任务系统, CPU完全可以在任务1和任务2之间来回切换, 这样就使得任务2不必等到10秒后再运行, 系统的运行效率将大大提高多线程技术正是使用这种异步机制, 从而提升了CPU利用率! 缺点, 切换可能浪费时间 运行你的第一个线程 一个进程在运行时, 至少会有一个线程在运行, 比如在Java中调用 public static void main() 的线程(由JVM创建) 1234567public class test &#123; public static void main(String[] args) &#123; // 在控制台输出的 main 和 main() 方法的名字可没啥关系 // 在控制台输出的 main 其实是一个名叫main的线程 (就是该线程在执行main()方法中的代码) System.out.println(Thread.currentThread().getName()); // main &#125;&#125; 上面已经清楚地看到了Java中你遇到的第一个线程, 下面看看如何在Java中使用多线程技术 Java中使用多线程 在Java的JDK开发包中, 已经自带了对多线程技术的支持, 可以很方便地进行多线程编程; 实现多线程编程的方式主要有两种 继承 Thread 类 (其实该类也实现了 Runnable 接口 ) 实现 Runnable 接口 为什么有两种多线程编程? 使用 继承Thread类 的方式创建新线程时, 最大的局限性就是Java并不支持多继承, 你的类继承了Thread类的话, 就继承不了其他的类了 所以为了支持多继承, 你可以通过 实现 Runnable 接口 的方式, 一边实现一边继承 不过这两种方式创建的线程在工作时的性质是一样的, 没有本质区别 继承Thread类 第一个多线程类: 1234567891011121314151617181920212223242526272829303132/** * @Author:YiminRen * @Date: 2019/2/1 下午3:58 *//** * 创建第一个自定义的线程类 * 此线程类继承自Thread * 并且需要重写run方法 (线程需要执行的任务就写在run方法中) */public class MyThread extends Thread &#123; @Override public void run() &#123; super.run(); System.out.println(&quot;My First Thread Class&quot;); &#125; /** * Thread.java类的start()方法用来通知 &quot;线程规划器&quot; 此线程已经准备就绪, 等待调用此线程对象的 run() 方法 * 这个过程其实就是让系统安排一个时间来调用Thread中的run()方法, 也就是使线程得到运行, 启动线程, **具有异步执行的效果** * 注意: 如果你调用代码 thread.run(), 这就不是异步执行, 而是同步, 此线程对象并不会交给 &quot;线程规划器&quot; 来进行处理, 而是由 main 主线程来调用 run() 方法 * * @param args */ public static void main(String[] args) &#123; MyThread myThread = new MyThread(); myThread.start(); // thread.run() // myThread.start(); System.out.println(&quot;main()方法 运行结束!&quot;); &#125;&#125; 多运行几次, 发现运行结果可能会如下: 12main()方法 运行结束!My First Thread Class 运行结果说明: 在使用多线程技术时, 代码的运行结果与执行顺序(调用顺序)是无关的, 线程作为一个子任务, CPU以不确定的方式(随机的时间)来调用线程中的run()方法 注意: 如果多次调用线程类的 start() 方法, 会报异常 Exception in thread &quot;main&quot; java.lang.IllegalThreadStateException 实现 Runnable 接口 如果要创建的线程类已经有一个父类了, 这时就不能再继承自 Thread 类了 (Java的单继承), 就要实现 Runnable 接口来应对这种情况 123456789101112131415161718public class MyRunnable implements Runnable&#123; public void run() &#123; System.out.println(&quot;运行中!&quot;); &#125; /** * 实现了Runnable接口的线程类如何被使用? * 仍需使用Thread类 * Thread类的构造函数支持传入一个 Runnable 接口的对象 * @param args */ public static void main(String[] args) &#123; Runnable runnable = new MyRunnable(); Thread thread = new Thread(runnable); thread.start(); System.out.println(&quot;运行结束!&quot;); &#125;&#125; 正是为了避免 继承Thread 将开发多线程时 单继承的局限性, 我们使用了 实现Runnable接口 的方式来实现多线程技术 另外, 由于 Thread.java 类本身也实现了 Runnable 接口, 因此 Thread 类的构造函数还可以传入一个 Thread 类的对象, 这样就相当于将一个Thread对象的run()方法交给了其他线程进行调用 1234567891011121314151617181920212223242526public class MyRunnable implements Runnable &#123; public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;运行中!&quot;); &#125; public static void main(String[] args) &#123; MyThread1 myThread1 = new MyThread1(); Thread thread2 = new Thread(myThread1); thread2.start(); MyThread1 myThread2 = new MyThread1(); Thread thread3 = new Thread(myThread2); thread3.start(); System.out.println(Thread.currentThread().getName() + &quot;运行结束!&quot;); &#125;&#125;class MyThread1 extends Thread&#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;运行中!&quot;); &#125;&#125; 上述例子比较奇葩的一点是: 线程名的编号 跳过了1 和 2 原因因为 传递给Thread的 并不是实现了 Runnable 的线程类, 而是Thread的子类对象 而Thread子类在创建对象时, 其默认空参的构造函数会调用父类Thread的构造函数, 再此处会进行一次++操作123Thread-1运行中!main运行结束!Thread-3运行中! 如下, 可能会好理解一些 123456789101112131415161718192021222324public class MyRunnable implements Runnable &#123; public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;运行中!&quot;); &#125; public static void main(String[] args) &#123; MyThread1 myThread1 = new MyThread1(); myThread1.start(); // 相当于将一个Thread对象的run()方法交给了其他线程进行调用 Thread thread2 = new Thread(myThread1); thread2.start(); System.out.println(Thread.currentThread().getName() + &quot;运行结束!&quot;); &#125;&#125;class MyThread1 extends Thread&#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;运行中!&quot;); &#125;&#125; 实例变量与线程安全自定义线程类中的实例变量, 针对其他线程可以分为 共享 和 非共享 两种类型, 这在多个线程之间进行交互是一个很重要的技术点; 多线程不共享数据: 1234567891011121314151617181920212223242526272829303132public class UnShareDataThread extends Thread &#123; private int count = 5; public UnShareDataThread(String name) &#123; super(); this.setName(name); // 设置线程名称 &#125; @Override public void run() &#123; super.run(); while(count &gt; 0) &#123; count--; System.out.println(&quot;由&quot; + this.currentThread().getName() + &quot;计算 count=&quot; + count); &#125; &#125; /** * 下面一共创建了3个线程 * 每个线程都有自己各自的count, 自己减少自己的count变量的值 * 这样的情况就是变量不共享 (并不存在&quot;多个&quot;线程访问&quot;同一个实例变量&quot;的情况, 而是多个线程访问多个实例变量) * @param args */ public static void main(String[] args) &#123; UnShareDataThread unShareDataThread1 = new UnShareDataThread(&quot;A&quot;); UnShareDataThread unShareDataThread2 = new UnShareDataThread(&quot;B&quot;); UnShareDataThread unShareDataThread3 = new UnShareDataThread(&quot;C&quot;); unShareDataThread1.start(); unShareDataThread2.start(); unShareDataThread3.start(); &#125;&#125; 要实现共享, 可以将上面的count属性改为static来让三个线程对象共用, 不过还有如下方式 多线程共享数据: 出现 非线程安全问题 1234567891011121314151617181920212223242526272829public class SharedDataThread extends Thread &#123; private int count = 5; @Override public void run() &#123; super.run(); count--; System.out.println(&quot;由&quot; + this.currentThread().getName() + &quot;计算 count=&quot; + count); &#125; /** * 虽然多个线程在跑 * 但其实是启动了多个线程来处理同一个线程任务 (多线程处理同一任务) * 非线程安全主要就是指, 多个线程同时对一个对象中的同一个实例变量进行操作时会出现值不同步的问题, 进而影响程序的执行流程 * @param args */ public static void main(String[] args) &#123; SharedDataThread sharedDataThread = new SharedDataThread(); Thread a = new Thread(sharedDataThread, &quot;A&quot;); Thread b = new Thread(sharedDataThread, &quot;B&quot;); Thread c = new Thread(sharedDataThread, &quot;C&quot;); Thread d = new Thread(sharedDataThread, &quot;D&quot;); Thread e = new Thread(sharedDataThread, &quot;E&quot;); a.start(); b.start(); c.start(); d.start(); e.start(); &#125;&#125; 多线程共享数据的结果出现了 “非线程安全问题” 12345由A计算 count=2由C计算 count=2由D计算 count=0由B计算 count=2由E计算 count=0 非线程安全主要就是指, 多个线程同时对一个对象中的同一个实例变量进行操作时会出现值不同步的问题, 进而影响程序的执行流程 其实上面的示例就是典型的 高并发超卖场景, 此时就需要使多个线程之间进行同步, 也就是 按照顺序排队进行 的方式对商品数进行递减; 要解决 “非线程安全问题”, 只需要在线程类的 run() 前加上关键字 synchronized, 使多个线程在执行 run() 时, 是以排队的方式进行处理 synchronized 关键字可以在任意对象和方法上加锁, 而加锁的这段代码称为 互斥区 或者 临界区 当一个线程在调用run前, 会判断run方法有没有被上锁, 如果被上锁, 说明有其他线程正在调用run方法, 必须等其他线程对run方法调用结束后, 自己才可以执行run方法 这样就实现了多个线程排队调用run方法的目的 当一个线程想要执行 被synchronized修饰为同步方法 中的代码时, 线程首先会尝试去拿这把锁, 如果能够拿到这把锁, 那么这个线程就可以执行方法里面的代码; 如果拿不到这把锁, 直到能够拿到为止(同时可能会有多个线程同时去争抢这把锁) 1234567891011121314151617181920212223242526272829public class SharedDataThread extends Thread &#123; private int count = 5; @Override synchronized public void run() &#123; super.run(); count--; System.out.println(&quot;由&quot; + this.currentThread().getName() + &quot;计算 count=&quot; + count); &#125; /** * 虽然多个线程在跑 * 但其实是启动了多个线程来处理同一个线程任务 (多线程处理同一任务) * @param args */ public static void main(String[] args) &#123; SharedDataThread sharedDataThread = new SharedDataThread(); Thread a = new Thread(sharedDataThread, &quot;A&quot;); Thread b = new Thread(sharedDataThread, &quot;B&quot;); Thread c = new Thread(sharedDataThread, &quot;C&quot;); Thread d = new Thread(sharedDataThread, &quot;D&quot;); Thread e = new Thread(sharedDataThread, &quot;E&quot;); a.start(); b.start(); c.start(); d.start(); e.start(); &#125;&#125; 当然, 非线程安全问题并不只是 “多个线程调用同一个对象的同一个实例变量” 才会出现, “多个线程调用同一个类的静态类变量也会出现” 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class LoginServlet &#123; private static String usernameRed; private static String passwordRef; public static void doPost(String username, String password) &#123; try &#123; usernameRed = username; if (username.equals(&quot;a&quot;)) &#123; Thread.sleep(5000); &#125; passwordRef = password; System.out.println(&quot;username=&quot; + usernameRed + &quot; password=&quot; + passwordRef); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; ALogin a = new ALogin(); BLogin b = new BLogin(); a.start(); b.start(); /** * 上面这种执行顺序最容易构造出的结果是: 原因比较简单, 不多说了 * username=b password=bb * username=b password=aa * * 其实理论上, 由于两个线程的执行是随机的, 上面这种编写代码的顺序也有可能出现下面的结果, 但可能性貌似很小: * 即, A线程还没执行到 usernameRed = username; , B线程就直接执行完了, 自然会输出 username=b password=bb * 紧接着, B线程走完, 输出 username=a password=aa * * 当然也有可能出现下面说的结果 */ /** * 虽然说线程的执行顺序和代码编写顺序无关, 是随机执行的, 但是如果写成下面这样, 更容易构造出后面的结果: * b.start(); * a.start(); * 可能出现结果: * username=a password=bb * username=a password=aa * 原因: * 线程B上去直接就执行到了输出语句前, 但是还没等输出语句执行, 线程A执行到了usernameRed=username, 虽然立刻sleep了, 但是毕竟usernameRed值已经赋上了 * 所以线程B打印出的就是 username=a password=bb * 接下来, 线程A睡了5秒后, 自然打印出的是 username=a password=aa */ &#125;&#125;class ALogin extends Thread &#123; @Override public void run() &#123; LoginServlet.doPost(&quot;a&quot;, &quot;aa&quot;); &#125;&#125;class BLogin extends Thread &#123; @Override public void run() &#123; LoginServlet.doPost(&quot;b&quot;, &quot;bb&quot;); &#125;&#125; 解决这个 “非线程安全” 的方法也是使用 synchronized 关键字: synchronized public static void doPost(String username, String password) { Thread.currentThread 与 this 代码 123456789101112131415161718192021222324252627282930313233public class CurrentThread01 extends Thread &#123; public CurrentThread01() &#123; System.out.println(&quot;CountOperate --- begin&quot;); System.out.println(&quot;Thread.currentThread().getName()=&quot; + Thread.currentThread().getName()); System.out.println(&quot;this.getName()=&quot; + this.getName()); System.out.println(&quot;CountOperate --- end&quot;); &#125; @Override public void run() &#123; System.out.println(); System.out.println(&quot;run --- begin&quot;); System.out.println(&quot;Thread.currentThread().getName()=&quot; + Thread.currentThread().getName()); System.out.println(&quot;this.getName()=&quot; + this.getName()); System.out.println(&quot;run --- end&quot;); &#125; /** * 为什么在new对象的时候,执行构造方法时 Thread.currentThread().getName()与this.getName()的结果不同呢. * Thread.currentThread()是指向当前线程的,由于我们是在main方法内new对象执行构造方法的,所以结果值是 main * * this.getName(); 是指当前对象所调用的getName()方法.当前对象是CountOperate对象,他本身是没有getName()方法的,从而执行该对象的父类Thread的getName()方法 * * @param args */ public static void main(String[] args) &#123; CurrentThread01 currentThread = new CurrentThread01(); currentThread.start(); Thread t1 = new Thread(currentThread); t1.start(); &#125;&#125; 结果: 1234567891011121314CountOperate --- beginThread.currentThread().getName()=mainthis.getName()=Thread-0CountOperate --- endrun --- beginThread.currentThread().getName()=Thread-0this.getName()=Thread-0run --- endrun --- beginThread.currentThread().getName()=Thread-1this.getName()=Thread-0run --- end stop() 与 interrup() 停止一个线程可以使用 Thread.stop(), 但是最好不要用它, 虽然它可以停止一个正在运行的线程, 但是这个方法是不安全的(unsafe), 而且是已被弃用作废的(deprecated) 大多数停止一个线程的操作使用 Thread.interrup(), 但这个方法不糊终止一个正在运行的线程, 还需要加入一个判断才可以完成线程的停止;参考: http://ifeve.com/java-concurrency-thread/《Java多线程编程核心技术》","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"30. bean serverlet","slug":"JavaSE/2019-01-02-30","date":"2019-01-02T11:31:25.000Z","updated":"2019-02-18T08:57:23.000Z","comments":true,"path":"2019/01/02/JavaSE/2019-01-02-30/","link":"","permalink":"http://blog.renyimin.com/2019/01/02/JavaSE/2019-01-02-30/","excerpt":"","text":"https://www.cnblogs.com/GoForMyDream/p/7746370.html","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"22. IO","slug":"JavaSE/2018-12-30-22","date":"2018-12-29T02:31:25.000Z","updated":"2019-02-01T06:45:12.000Z","comments":true,"path":"2018/12/29/JavaSE/2018-12-30-22/","link":"","permalink":"http://blog.renyimin.com/2018/12/29/JavaSE/2018-12-30-22/","excerpt":"","text":"概述 Java中IO流的继承体系图 Java中的IO流可以进行如下分类: 按处理的数据类型可分为 字节流(byte) 与 字符流(char) 按流的流向可分为 输入流(in) 与 输出流(out) 按流的功能可分为 节点流(Node) 和 过滤流(Filter) 字节流 vs 字符流: 字节流: 一般适用于处理字节数据(诸如图片、视频)数据传输的根本是2进制形式, 而传输过程中是以8位1个字节为基本单位, 所以字节流是最基本的流, 一般的数据传输都可以使用字节流, InputStream 和 OutputStream 分别为字节输入流和字节输出流的基类(抽象类) 字符流: 一般适用于处理字符数据(诸如文本文件)传输时还是以字节作为基本单位, 但会把对应的字节自动转换为字符, 方便文本处理, 因此, 涉及到文本的读写时最好使用字符流, Reader 和 Writer 分别为字符输入流和字符输出流的基类(抽象类) InputStream此抽象类是表示字节输入流的所有类的超类 (实现了此类的子类必须实现抽象方法 read() ) FileInputStream FileInputStream 流被称为文件字节输入流, 指对文件数据以字节的形式进行读取操作(如读取图片视频等) 根据其构造方法, 它可以: 通过打开 File类对象 代表的实际文件的链接 来创建FileInputStream流对象若File类对象的所代表的文件不存在、不是文件是目录、或者其他原因不能打开的话, 则会抛出FileNotFoundException 通过指定的字符串参数来创建File类对象, 而后再与File对象所代表的实际路径建立链接创建FileInputStream流对象 但二者并没有严格的功能划分, 因为有 转换流 的存在, 使得对于数据的处理变得更加灵活","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"21. IO -- File类","slug":"JavaSE/2018-12-29-21","date":"2018-12-29T02:31:25.000Z","updated":"2019-02-01T06:42:52.000Z","comments":true,"path":"2018/12/29/JavaSE/2018-12-29-21/","link":"","permalink":"http://blog.renyimin.com/2018/12/29/JavaSE/2018-12-29-21/","excerpt":"","text":"文件","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"20. lambda 表达式","slug":"JavaSE/2018-12-28-20","date":"2018-12-28T13:52:26.000Z","updated":"2019-02-01T06:45:50.000Z","comments":true,"path":"2018/12/28/JavaSE/2018-12-28-20/","link":"","permalink":"http://blog.renyimin.com/2018/12/28/JavaSE/2018-12-28-20/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"17","slug":"JavaSE/2018-12-20-17","date":"2018-12-20T11:50:26.000Z","updated":"2019-02-01T07:00:24.000Z","comments":true,"path":"2018/12/20/JavaSE/2018-12-20-17/","link":"","permalink":"http://blog.renyimin.com/2018/12/20/JavaSE/2018-12-20-17/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"16. Java 反射","slug":"JavaSE/2018-12-19-16","date":"2018-12-19T05:43:51.000Z","updated":"2019-02-01T06:45:48.000Z","comments":true,"path":"2018/12/19/JavaSE/2018-12-19-16/","link":"","permalink":"http://blog.renyimin.com/2018/12/19/JavaSE/2018-12-19-16/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"14. Generics 泛型","slug":"JavaSE/2018-12-18-14","date":"2018-12-18T05:43:51.000Z","updated":"2019-01-31T02:11:30.000Z","comments":true,"path":"2018/12/18/JavaSE/2018-12-18-14/","link":"","permalink":"http://blog.renyimin.com/2018/12/18/JavaSE/2018-12-18-14/","excerpt":"","text":"泛型(Generics): 参数化类型, 将 ‘类型’ 定义成 ‘参数’, 然后在调用时传入具体的类型; 泛型是 JDK5 中引入的一个新特性, 在java中有很重要的地位, 在面向对象编程及各种设计模式中有非常广泛的应用泛型的本质是为了将类型 参数化, 在泛型使用过程中, ‘数据类型’ 被指定为一个参数, 这种参数类型可以用在 类、接口 和 方法中, 分别被称为 泛型类、泛型接口、泛型方法 集合类对泛型的使用 之前学过的集合类, 很多都使用了 泛型, 比如 ArrayList 类: 下面构造了三个List, 分别盛装String、Integer和Double这就是ArrayList的过人之处: 即各种类型的变量都可以组装成对应的List, 而不必针对每个类型分别实现一个构建ArrayList的类 123ArrayList&lt;String&gt; strList = new ArrayList&lt;String&gt;(); ArrayList&lt;Integer&gt; intList = new ArrayList&lt;Integer&gt;(); ArrayList&lt;Double&gt; doubleList = new ArrayList&lt;Double&gt;(); 当然, ArrayList也可以不用泛型, 如果在实例化时没有使用泛型的话, 那么默认的泛型是 Object类, 即能够传入任意类型的实例 集合使用泛型的好处: 通过上述案例, 可以看到, 使用泛型, 减少了对大量相同结构的类的创建; 同时, 在不使用泛型时, 默认又可以传递任意类型的实例, 非常方便 另外, 泛型提供了 编译时类型安全检测机制, 允许我们在编译时检测到非法的类型:在使用集合时, 如果不使用泛型, 而是往集合中存放了多个类型不同的对象, 在读取时, 可能会面临类型转换的错误: Exception in thread &quot;main&quot; java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer为了避免在集合中随意存放不同的类型, 导致在遍历时类型转换可能的错误, 所以集合类使用了泛型 123456789// 如果没有使用泛型, 任何Object及其子类的对象都可以添加进集合中List list = new ArrayList();list.add(100);list.add(100);list.add(&quot;AA&quot;);for (int i = 0; i &lt; list.size(); i++) &#123; int score = (Integer)list.get(i); System.out.println(score);&#125; 强制转换时, 不会出错, 因为编译器也不知道你传进去的是什么; 而如果使用泛型, 在编译阶段即可检查到错误: 泛型类 泛型类的定义: 在类名后面加一对尖括号 &lt;&gt;, 括号里是一个大写字母, 叫 预设类型 (可以是任何大写字母) 示例: 泛型类的定义和使用 1234567891011121314151617181920212223242526272829class GenericTest&lt;T&gt; &#123; private T key; public T getKey() &#123; return key; &#125; public GenericTest(T key) &#123; this.key = key; &#125; public static void main(String[] args) &#123; /** * 这里与普通构造类实例的不同之点在于, 普通类构造函数是这样的：GenericTest g = new GenericTest() ; * 而泛型类的构造则需要在类名后添加上&lt;String&gt;, 即一对尖括号, 中间写上要传入的类型 * 因为类定义为 class GenericTest&lt;T&gt;, 所以在使用的时候也要在 GenericTest 后加上类型来定义 T 代表的意义 * 然后在 getKey() 时就没有什么特殊的了, 直接调用即可 * 从上面的使用时, 明显可以看出泛型的作用, 在构造泛型类的实例的时候: 尖括号中, 你传进去的是什么, T就代表什么类型 */ // 传入的 类型 实参 需与泛型的 类型参数 类型相同, 即为Integer GenericTest&lt;Integer&gt; genericInteger = new GenericTest&lt;Integer&gt;(123456); //传入的实参类型需与泛型的类型参数类型相同, 即为String. GenericTest&lt;String&gt; genericString = new GenericTest&lt;String&gt;(&quot;test&quot;); System.out.println(&quot;key is &quot; + genericInteger.getKey()); System.out.println(&quot;key is &quot; + genericString.getKey()); &#125;&#125; 很显然, PHP是弱类型, 所以不存在需要根据某种数据类型来定义特定的类, 所以也就不需要泛型类 泛型变量 命名规范: 为了简化命名, 通常只用一个字母来命名 泛型变量(预设类型), 常用字母如下: E: 代表Element (在集合中使用, 因为集合中存放的是元素, ArrayList 用的就是 E) T: Type (表示派生自Object类的任何类) K V: 分别代表键值中的Key和Value N: Number(数值类型) ?: 表示不确定的类型 多泛型变量定义: 上面在定义泛型类时, 只定义了一个泛型变量 class GenericTest&lt;T&gt;, 如果需要传进去多个泛型, 只用 class GenericTest&lt;T,U&gt;、class GenericTest&lt;T,U,A,B,C&gt; 即可 (多个类型参数之间用逗号分隔) 泛型接口在接口上定义泛型与在类中定义泛型是一样的, 与泛型类的定义一样, 也是在接口名后加尖括号&lt;&gt;这里用如下两种方法来使用泛型接口: 非泛型类实现泛型接口 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839import org.junit.Test;interface Info&lt;T&gt;&#123; // 在接口上定义泛型 public T getVar() ; // 定义抽象方法, 抽象方法的返回值就是泛型类型 public void setVar(T x);&#125;class InfoImpl implements Info&lt;String&gt;&#123; // 定义泛型接口的子类 private String var ; // 定义属性 public InfoImpl(String var)&#123; // 通过构造方法设置属性内容 this.setVar(var) ; &#125; /** * 由于在定义类时, 实现了泛型接口Info, 并指明了参数类型为 String, 接口中的 T 也就都是String类型 * 因此重写时, 参数类型也需要是String类型 * * 使用IDE自动生成的话, 会也我们直接生成String类型的重写函数 * @param var */ @Override public void setVar(String var)&#123; this.var = var ; &#125; @Override public String getVar()&#123; return this.var ; &#125;&#125;public class GenericTest &#123; @Test public void test() &#123; // 使用时比较简单 InfoImpl i = new InfoImpl(&quot;test&quot;); System.out.println(i.getVar()) ; &#125;&#125; 注意: InfoImpl 只是实现了泛型接口, 它本身并不是一个泛型类, 因为它的类名后没有 &lt;T&gt; 泛型类实现泛型接口 在方法1中, 在定义类实现接口时, 我们直接把 Info 接口的 类型参数 给填充成了 String 类型: class InfoImpl implements Info&lt;String&gt;; 但我们的类其实是可以构造成泛型类的 在下面构造泛型类 InfoImpl 时, 同样是实现泛型接口 Info, 但我们并没有填充泛型接口的类型参数, 而是把泛型类InfoImpl的泛型变量T传给了泛型接口Info, 即, 接口 和 泛型类使用的都是同一个泛型变量 然后在使用时, 就是构造一个泛型类的实例的过程, 使用过程也不变 使用泛型类来继承泛型接口的作用就是让用户来定义接口所使用的变量类型, 而不是像方法1中那样, 在类中写死 代码如下: 12345678910111213141516171819202122232425262728293031import org.junit.Test;public class GenericTest &#123; @Test public void test() &#123; Info&lt;String&gt; i = new InfoImpl&lt;String&gt;(&quot;test&quot;); System.out.println(i.getVar()) ; &#125;&#125;interface Info&lt;T&gt;&#123; public T getVar(); public void setVar(T x);&#125;class InfoImpl&lt;T&gt; implements Info&lt;T&gt;&#123; private T var; public InfoImpl(T var)&#123; this.setVar(var) ; &#125; @Override public void setVar(T var)&#123; this.var = var ; &#125; @Override public T getVar()&#123; return this.var ; &#125;&#125; 多泛型变量泛型类继承泛型接口 最后, 构造一个多个泛型变量的类, 并继承自Info接口 代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import org.junit.Test;import java.util.ArrayList;import java.util.List;public class GenericTest &#123; @Test public void test() &#123; InfoImpl&lt;Integer,String,Double&gt; i = new InfoImpl&lt;Integer,String,Double&gt;(12.5); System.out.println(i.getVar()) ; &#125;&#125;/** * 泛型变量的标识符, 在传递时, 和实参-&gt;形参类似 * 实参和形参名字可以不同 (正如本例中, 泛型类InfoImpl的泛型变量U 和 泛型接口Info的泛型变量I) * @param &lt;I&gt; */interface Info&lt;I&gt;&#123; public I getVar(); public void setVar(I x);&#125;/** * T, K 两个泛型变量是泛型类InfoImpl自己使用 * 而泛型变量 U 是泛型类填充给泛型接口的, 所以Info接口所使用的类型就是由泛型变量U来决定的 * @param &lt;T&gt; * @param &lt;K&gt; * @param &lt;U&gt; */class InfoImpl&lt;T,K,U&gt; implements Info&lt;U&gt;&#123; private U var ; private T x; // T, K 两个泛型变量是 泛型类自己使用 private K y; public InfoImpl(U var)&#123; // 通过构造方法设置属性内容 this.setVar(var) ; &#125; @Override public void setVar(U var)&#123; this.var = var ; &#125; @Override public U getVar()&#123; return this.var ; &#125;&#125; 泛型方法 定义及使用: 上面我们讲解了类和接口的泛型使用, 下面我们再说说, 怎么单独在一个函数里使用泛型, 比如我们在新建一个普通的类 StaticFans, 然后在其中定义了两个 泛型方法: 123456789101112131415161718192021222324252627import org.junit.Test;public class GenericTest &#123; @Test public void test() &#123; //静态方法 StaticFans.StaticMethod(&quot;adfdsa&quot;);//使用方法一 StaticFans.&lt;String&gt;StaticMethod(&quot;adfdsa&quot;);//使用方法二 //常规方法 StaticFans staticFans = new StaticFans(); staticFans.OtherMethod(new Integer(123));//使用方法一 staticFans.&lt;Integer&gt;OtherMethod(new Integer(123));//使用方法二 &#125;&#125;class StaticFans &#123; //静态方法 public static &lt;T&gt; void StaticMethod(T a) &#123; System.out.println(&quot;StaticMethod: &quot; + a.toString()); &#125; //普通方法 public &lt;T&gt; void OtherMethod(T a) &#123; System.out.println(&quot;OtherMethod: &quot; + a.toString()); &#125;&#125; 上面分别是 静态 泛型方法 和 常规 泛型方法 的定义方法, 与以往方法的唯一不同点就是在返回值前加上 &lt;T&gt; 来指明这个函数是一个 泛型方法, 使用的泛型变量名为 T 注意: 使用了泛型变量 || 返回值是泛型变量的方法, 并不算是泛型方法, 泛型方法是需要在方法返回值前加上 &lt;T&gt; 来定义的 静态泛型方法的使用 静态泛型方法的使用有两种方式: 12StaticFans.StaticMethod(&quot;adfdsa&quot;);//使用方法一 StaticFans.&lt;String&gt;StaticMethod(&quot;adfdsa&quot;);//使用方法二 推荐本方法 从结果看的话, 这两种方法的结果是完全一样的, 但它们还有些区别: 方法一, 可以像普通方法一样, 直接传值, 任何值都可以(但必须是派生自Object类的类型, 比如String,Integer等), 函数会在内部根据传进去的参数来识别当前T的类别, 但尽量不要使用这种隐式的传递方式, 代码不利于阅读和维护, 因为从外观根本看不出来你调用的是一个 泛型方法; 方法二, 与方法一不同的地方在于, 在调用方法前加了一个 来指定传给 的值, 如果加了这个 来指定参数的值的话, 那 StaticMethod() 函数里所有用到的T类型也就是强制指定了是String类型, 传递过来的参数如果不是String那么编译器就会报错, 这是我们建议使用的方式 普通泛型方法的使用 普通泛型方法的使用也有这两种方式: 123StaticFans staticFans = new StaticFans(); staticFans.OtherMethod(new Integer(123));//使用方法一 staticFans.&lt;Integer&gt;OtherMethod(new Integer(123));//使用方法二 可以看到, 与平常一样, 先创建类的实例, 然后调用 泛型方法 方法一, 隐式传递了T的类型, 与上面一样, 不建议这么做 方法二, 显示将T赋值为Integer类型, 这样 OtherMethod(T a) 传递过来的参数如果不是Integer那么编译器就会报错 Static 与 泛型方法 可以看到, 泛型方法可以被 static 关键字修饰为静态方法, 因为泛型方法的泛型变量是方法自己的局部变量, 并不是通过泛型类传递的; 而泛型类中 使用了泛型变量 || 返回值是泛型变量的方法 不能被 static 关键字修饰, 因为泛型变量是需要类被实例化之后才能确定具体类型值; 进阶—返回值中存在泛型 上面我们的函数中, 返回值都是void, 但现实中不可能都是void, 有时, 我们需要将泛型变量返回, 比如下面这个函数: 1234public static &lt;T&gt; List&lt;T&gt; parseArray(String response, Class&lt;T&gt; object)&#123; List&lt;T&gt; modelList = JSON.parseArray(response, object); return modelList; &#125; 函数返回值是 List 类型, 至于传入参数 Class&lt;T&gt; object 的意义, 后面会讲 这里也就是想通过这个例子来告诉大家, 泛型变量其实跟 String,Integer, Double等等的类的使用上没有任何区别, T只是一个符号, 可以代表String,Integer, Double……这些类型 在泛型函数使用时, 直接把T看成String,Integer, Double……中的任一个来写代码就可以了, 唯一不同的是, 要在函数定义的中在返回值前加上标识泛型; Class类传递 使用 Class 传递泛型类Class对象 : 有时，我们会遇到一个情况，比如，我们在使用JSON解析字符串的时候，代码一般是这样的 1234public static List&lt;SuccessModel&gt; parseArray(String response)&#123; List&lt;SuccessModel&gt; modelList = JSON.parseArray(response, SuccessModel.class); return modelList; &#125; 其中SuccessModel是自定义的解析类，代码如下，其实大家不用管SuccessModel的定义，只考虑上面的那段代码就行了。写出来SuccessModel的代码，只是不想大家感到迷惑，其实，这里只是fastJson的基本用法而已。这段代码的意义就是根据SuccessModel解析出List的数组。 泛型数组摘自: https://blog.csdn.net/harvic880925/article/details/49872903# 泛型与继承 泛型注意事项 前后泛型的类型要一致 (即便前后的类型有继承关系也不可以) : 12// 即便Student类继承自Person类, 也不会通过编译List&lt;Person&gt; list = new ArrayList&lt;Student&gt;(); - 在向集合中添加元素时, 可以添加泛型类型的子类型: 12List&lt;Person&gt; list = new ArrayList&lt;Person&gt;();list.add(new Student()); - 在jdk7推出了一个新特性, 泛型的菱形语法, 即 后面的泛型类型可以省略 (因为编译器可以从前面(List)推断出推断出类型参数, 所以后面的ArrayList之后可以不用写泛型参数, 只用一对空着的尖括号就可以): 12345678// 在Java SE 7之前, 声明泛型对象的代码如下:List&lt;Person&gt; list = new ArrayList&lt;Person&gt;();// 而在Java 7中, 可以使用如下代码:List&lt;Person&gt; list = new ArrayList&lt;&gt;();list.add(new Student());// 如下也行, 但是会有警告List&lt;Person&gt; list = new ArrayList();list.add(new Student()); - **泛型只支持对象类型**, 如下是错误示例: 1List&lt;int&gt; list = new ArrayList&lt;int&gt;(); ??注意: 不能对确切的泛型类型使用 instanceof 操作, 如下面的操作是非法的, 编译时会出错 类型擦除?? 泛型是 Java 1.5 版本才引进的概念, 在这之前是没有泛型的概念的, 但显然, 泛型代码能够很好地和之前版本的代码很好地兼容。 这是因为, 泛型信息只存在于代码编译阶段, 在进入 JVM 之前, 与泛型相关的信息会被擦除掉, 专业术语叫做类型擦除; 通俗地讲, 泛型类和普通类在 java 虚拟机内是没有什么特别的地方, 对于下面的代码, 打印的结果为 true, 是因为 List 和 List 在 jvm 中的 Class 都是 List.class, 泛型信息被擦除了 1234List&lt;String&gt; l1 = new ArrayList&lt;String&gt;();List&lt;Integer&gt; l2 = new ArrayList&lt;Integer&gt;();System.out.println(l1.getClass() == l2.getClass()); 如果使用泛型, 编译器会在编译阶段就能够帮我们发现类似这样的问题 泛型通配符 无边界? 和 Object 不一样么? Object 不接受子类 类型边界 extends 可以实现接口 既继承父类又实现接口 注意 &amp; 符号, 类在前, 接口在后 super 固定上边界通配符","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"13. 遍历","slug":"JavaSE/2018-12-16-13","date":"2018-12-16T12:58:09.000Z","updated":"2019-01-30T06:50:27.000Z","comments":true,"path":"2018/12/16/JavaSE/2018-12-16-13/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/JavaSE/2018-12-16-13/","excerpt":"","text":"https://www.cnblogs.com/leskang/p/6031282.html 集合类的通用遍历方式, 用迭代器迭代: 1234Iterator it = list.iterator();while(it.hasNext()) &#123; Object obj = it.next();&#125; 通过 Map.entrySet 使用 iterator 遍历key和value 12345Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator();while (it.hasNext()) &#123; Map.Entry&lt;Integer, String&gt; entry = it.next(); System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue());&#125;","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"05. 版本部分回退","slug":"git/2018-12-16-05","date":"2018-12-16T09:10:06.000Z","updated":"2018-12-16T09:17:36.000Z","comments":true,"path":"2018/12/16/git/2018-12-16-05/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/git/2018-12-16-05/","excerpt":"","text":"场景 假设你正在A分支上进行开发, 此时对 1,2,3 三份文件做了修改, 然后直接 git add ., git commit -m &quot;&quot; 进行了提交, 但是提交后发现, 文件3 是不需要修改的, 你需要将该文件恢复之前版本 如果直接进行版本回退, 1,2,3 这三个文件的改动都会被回退, 如果改动内容比较少, 你可以这样做; 但是如果这个功能对三个文件进行的改动非常大, 你总不能重来一遍吧, 即便是可以通过编辑器本身的history功能可以找到文件的修改版本, 但这种经方法还是比较low 所以你需要的是能够将某次commit中的部分文件进行版本回退的功能","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"04. git stash","slug":"git/2018-12-16-04","date":"2018-12-16T08:53:02.000Z","updated":"2018-12-16T09:09:54.000Z","comments":true,"path":"2018/12/16/git/2018-12-16-04/","link":"","permalink":"http://blog.renyimin.com/2018/12/16/git/2018-12-16-04/","excerpt":"","text":"场景 在使用git时, 经常会出现 你正在A分支进行开发, 但是突然有个小需求需要紧急切换到B分支进行开发, 遇到这种情况, 我以前的做法通常如下: 把A分支上上榜为完成开发的代码直接 commit 然后checkout到B分支进行开发 上面方式存在的问题是: 本来我每次commit, 都希望是一次比较完整的功能点提交, 而上述操作可能会出现, 某个功能块是尚未完成就直接进行了一次无意义的commit, 显得比较low了, 此时就应该使用 git stash 了 先将A分支目前的工作现场保暂存起来 然后checkout到B分支进行开发工作 B分支上的工作开发完成后, 在checkout到A分支, 将之前暂存的开发内容恢复到工作区即可 git rebase git stash 可以用来保存工作现场 git stash list 可以用来查看保存的工作现场列表 恢复工作现场: git stash apply : 恢复时不删除stash中的内容, 需要通过 git stash drop stash@{0} 手动删除例 git stash apply stash@{0} git stash pop : 恢复的同时也将stash内容删除掉","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"12. JCF - Java集合框架","slug":"JavaSE/2018-12-15-12","date":"2018-12-15T03:17:23.000Z","updated":"2019-01-30T06:33:04.000Z","comments":true,"path":"2018/12/15/JavaSE/2018-12-15-12/","link":"","permalink":"http://blog.renyimin.com/2018/12/15/JavaSE/2018-12-15-12/","excerpt":"","text":"JCF JCF(Java Collection Framework, Java集合框架) 是由一组精心设计的接口、类和隐含在其中的数据结构及算法所组成, 通过它们可以对Java对象进行存储、获取、操作和转换等功能; 所有的集合类都位于 java.util 包下, 后来为了处理多线程环境下的并发安全问题, java5还在 java.util.concurrent 包下提供了一些多线程支持的集合类 (Java集合 也叫 Java容器) 常见的Java集合框架图 点击查看Java 集合框架图 Java集合框架大体包含两种类型的容器: Collection 和 Map, 它们是Java集合框架的根接口 Collection: 一个独立元素的序列, 这些元素会遵循一定的规则 Map: 一组成对的 “键值对” 对象, 允许你使用键来查找值 Collection接口 Collection是最基本的集合接口, Collection 接口下又有 3 种子接口 List、Set 和 Queue, 再下面是一些抽象类, 最后是具体实现类,常用的有 ArrayList、LinkedList、HashSet、LinkedHashSet、HashMap、LinkedHashMap 等等 Collection中的几个主要实现类 12345678910111213141516|Collection | ├List // 有序, 可重复| │--├ArrayList| │--├LinkedList | │--└Vector | │ └Stack | ├Set // 无序, 不可重复| │--├HashSet | │--├TreeSet | │--└LinkedSet | │| ├Queue| │--├PriorityQueue| │--├Deque| │ ├ArrayDeque| │ └LinkedList List接口 实现 List 接口的类: 可以用来存放 有序, 可重复 的一组对象, 能够通过索引(元素在List中位置, 类似于数组的下标)来访问List中的元素, 第一个元素的索引为0 List接口下常用的实现类有: ArrayList, LinkedList, Vector及其子类Stack 12345├List │--├ArrayList // 底层用 数组 实现│--├LinkedList // 底层用 双向循环链表 实现│--└Vector (已过时)│ └Stack List接口是有序集合, 所以与Set接口相比, 增加了一些与索引位置相关的操作; ArrayList 底层使用的是数组, 它封装了一个动态增长的、允许再分配的 Object[] 数组 (允许包括 null 在内的所有元素) 由于 Arraylist 底层使用的是数组, 所以其 读取数据效率高,插入删除特定位置效率低 ArrayList 不是线程同步的, 即线程不安全 Vector Vector是一种老的动态数组, 与ArrayList一样都是基于数组实现, 它是线程同步的, 效率很低, 一般不赞成使用 其实 ArrayList 和 Vector 在用法上完全相同, 但由于Vector是一个古老的集合(从jdk1.0就有了), 那时候java还没有提供系统的集合框架, 所以在Vector里提供了一些方法名很长的方法(例如:addElement(Object obj)), 实际上这个方法和add(Object obj)没什么区别; 从jdk1.2以后, Java提供了系统的集合框架, 就将Vector改为实现List接口, 作为List的实现之一, 从而导致Vector里有一些重复的方法; Vector里有一些功能重复的方法, 这些方法中方法名更短的是属于后来新增的方法, 更长的是原先vector的方法, 而后来ArrayList是作为List的主要实现类, 看过的Java思想编程中也提到了Vector有很多缺点, 尽量少用Vector实现类 Vector的子类 Stack: Stack类表示后进先出(LIFO)的对象堆栈; 由于 Vector是通过数组实现的, 这就意味着, Stack也是通过数组实现的, 而非链表 与 ArrayList 的差异 Vector可以设置增长因子, 而ArrayList不可以 (类允许设置默认的增长长度, 默认扩容方式为原来的2倍) Vector的方法都是同步的(Synchronized), 是线程安全的(thread-safe), 而ArrayList的方法不是, 由于线程的同步必然要影响性能, 因此, ArrayList的性能比Vector好 LinkedList LinkedList 底层使用的是双向循环链表数据结构(插入, 删除效率特别高) LinkedList 实现了List接口, 允许 null 元素, 此外LinkedList提供额外的get, remove, insert方法在LinkedList的首部或尾部, 这些操作使LinkedList可被用作堆栈(stack), 队列(queue)或双向队列(deque) 当数据特别多, 而且经常需要插入删除元素时建议选用 LinkedList; 一般程序只用Arraylist就够用了, 因为一般数据量都不会蛮大, Arraylist是使用最多的集合类 注意: LinkedList没有同步方法, 如果多个线程同时访问一个List, 则必须自己实现访问同步, 一种解决方法是在创建List时构造一个同步的List: List list = Collections.synchronizedList(new LinkedList(…)); 与 ArrayList 的差异 Arraylist 底层使用的是数组(存读数据效率高, 插入删除特定位置效率低), LinkedList底层使用的是双向循环链表数据结构(插入, 删除效率特别高) 在各种Lists中, 最好的做法是以ArrayList作为缺省选择; 当插入、删除频繁时, 使用LinkedList(); Queue接口 用于模拟 队列 这种数据结构, 实现 FIFO 等数据结构; 通常, 队列不允许随机访问队列中的元素; Queue 接口并未定义阻塞队列的方法, 但这在方法并发编程中是很常见的, 因此Queue的子接口 BlockingQueue接口 定义了那些等待元素出现或等待队列中有可用空间的方法, 这些方法扩展了此接口; Queue 实现通常不允许插入 null 元素, 尽管某些实现(如 LinkedList)并不禁止插入 null, 即使在允许 null 的实现中, 也不应该将 null 插入到 Queue 中, 因为 null 也用作 poll 方法的一个特殊返回值, 表明队列不包含元素; Deque接口 Deque 是 Queue 的子接口, 提供了 LIFO 堆栈操作更完整和更一致的集合, Queue是一种队列形式, 而Deque则是双向队列,它支持从两个端点方向检索和插入元素 Deque接口是一种比 Stack和Vector 更为丰富的抽象数据形式, 因为它同时实现了以上两者, 应该优先使用此集合, 而非Stack类 Vector类 和 Stack类 这两个都是jdk1.0的过时API, 应该避免使用 jdk1.5新增了很多多线程情况下使用的集合类, 位于java.util.concurrent 如果你说, Vector是同步的, 你要在多线程使用, 那你应该使用 java.util.concurrent.CopyOnWriteArrayList 等, 而不是Vector 如果你要使用Stack做类似的业务, 那么非线程的你可以选择linkedList, 多线程情况你可以选择 java.util.concurrent.ConcurrentLinkedDeque 或者java.util.concurrent.ConcurrentLinkedQueue 同Queue一样, Deque的实现也可以划分成 通用实现 和 并发实现, 通用实现主要有两个实现类 ArrayDeque 和 LinkedList ArrayDeque 是个可变数组, 它是在Java 6之后新添加的, 而LinkedList是一种链表结构的list LinkedList要比ArrayDeque更加灵活, 因为它也实现了List接口的所有操作, 并且可以插入null元素, 这在ArrayDeque中是不允许的 从效率来看, ArrayDeque要比LinkedList在两端增删元素上更为高效, 因为没有在节点创建删除上的开销 最适合使用LinkedList的情况是迭代队列时删除当前迭代的元素此外LinkedList可能是在遍历元素时最差的数据结构, 并且 LinkedList 也占用更多的内存(因为LinkedList是通过链表连接其整个队列) 总体ArrayDeque要比LinkedList更优越, 在大队列的测试上有3倍与LinkedList的性能, 最好的是给ArrayDeque一个较大的初始化大小, 以避免底层数组扩容时数据拷贝的开销 LinkedBlockingDeque是Deque的并发实现, 在队列为空的时候, 它的takeFirst, takeLast会阻塞等待队列处于可用状态 多线程情况下, 应尽量使用java.util.concurrent包下的类; 摘自: https://www.cnblogs.com/devin-ou/p/7989451.html Set接口 Set 接口存储一组 唯一, 无序的对象 Set接口下我们通常使用的实现类有: HashSet, TreeSet, LinkedSet 1234├Set│--├HashSet │--├TreeSet │--└LinkedSet HashSet HashSet是Set接口的典型实现, HashSet使用HASH算法来存储集合中的元素, 因此具有良好的存取和查找性能 HashSet不允许出现重复元素, 不保证集合中元素的顺序, 允许包含值为 null 的元素,但最多只能一个(当然了, 元素不能重复) HashSet不是同步的, 多线程访问同一步HashSet对象时, 需要手工同步 内部使用 HashMap 来存储数据, 数据存储在 HashMap 的 key中 HashSet集合判断两个元素相等的标准 通过 hashCode() 检查 HashSet 对应位置是否有值没有值, 则将元素对象直接存入有值, 需要使用 equals() 检测两个对象是否相等, 如果相等则不放入 如果 HashSet 对应位置 有值, 但是 equals() 不相等?正常情况下, 我们要保证 hashCode() 和 equals() 是同步的 (即, hashcode()相同时, 对应位置的元素与当前元素通过 equals() 的结果也应该是相同的) 如果要将自定义的类的对象, 添加到HashSet集合中, 需要为自定义类提供 equals() 和 hashCode() 两个方法的重写, 否则, 属性相同的两个对象会存多个到HashSet中 ((像String等类已经提供并重写了这两个方法, 所以你可以直接往set中存储字符串)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.*;public class hset &#123; public static void main(String[] args) &#123; Set hashSet = new HashSet(); hashSet.add(&quot;AAA&quot;); hashSet.add(&quot;BBB&quot;); hashSet.add(123); hashSet.add(new Person(&quot;rym&quot;, 100)); hashSet.add(new Person(&quot;rym&quot;, 100)); Person p = new Person(&quot;rym&quot;, 100); hashSet.add(p); hashSet.add(p); // 如果Person类中缺少任何一个方法, hashSet结果都是 6 个元素 System.out.println(hashSet); &#125;&#125;class Person&#123; private String name; private Integer age; public Person(String name, Integer age) &#123; this.name = name; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Person person = (Person) o; return Objects.equals(name, person.name) &amp;&amp; Objects.equals(age, person.age); &#125; @Override public int hashCode() &#123; return Objects.hash(name, age); &#125;&#125; HashSet几个重要的方法 add(E e) : HashSet的确定性(也可以理解为唯一性), 是通过 HashMap 的 put方法 来保证的, 往HashMap中put数据时, 如果key是一样的, 只会替换key对应的value, 不会新插入一条数据; 所以往HashSet中add相同的元素没有什么用, 这里的相同是通过equals方法保证的, 具体的在HashMap中细说 123public boolean add(E var1) &#123; return this.map.put(var1, PRESENT) == null;&#125; remove(Object o) : 简单粗暴, 从HashMap中移除一条数据 123public boolean remove(Object var1) &#123; return this.map.remove(var1) == PRESENT;&#125; contains(Object o) iterator() 其他的方法诸如: size()、isEmpty()、contains()、clear() 等都完全委托给了HashMap, 需要注意的是: HashSet没有提供set、get等方法; LinkedHashSet LinkedHashSet 继承自 HashSet, 内部使用的是 LinkHashMap (使得LinkedHashSet中的元素顺序是可以保证的, 即 遍历序 和 插入序 一致, HashSet 和 HashMap 都不保证顺序, Linkxxx 能使用链表保证顺序) 相比 HashSet, LinkedHashSet 在迭代访问Set中的全部元素时, 性能比HashSet好(HashSet每个元素都需要用hashcode映射, 而LinkedHashed直接可以通过链表指针遍历即可), 但是插入时性能稍微逊色于HashSet(因为LinkedHashSet还要维护链表来保证遍历顺序和插入顺序一致) 和 HashSet 一样需要依赖 hashCode() 和 equals() 方法保证元素的唯一性 LinkedHashSet 非线程安全 TreeSet TreeSet 是 Set 的一个子类, TreeSet集合可以用来对 对象元素 进行排序, 同样它也可以保证元素的唯一性 TreeSet集合 添加元素报错: 如下, 会报类型转换异常 Exception in thread &quot;main&quot; java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String 123Set treeSet = new TreeSet();treeSet.add(123);treeSet.add(&quot;AAA&quot;); 自定义类会报异常: Exception in thread &quot;main&quot; java.lang.ClassCastException: Person cannot be cast to java.lang.Comparable 1234Set treeSet = new TreeSet();treeSet.add(new Person(&quot;renyimin&quot;, 100));treeSet.add(new Student(&quot;renyimin&quot;, 100));System.out.println(treeSet); 上述原因在于我们没有告诉TreeSet如何来对元素进行比较, 如果不指定, 就会抛出这个异常 TreeSet的内部操作的底层数据是 TreeMap (只是我们操作的是 TreeMap的key) 在对大量信息进行检索的时候, TreeSet 比 ArrayList 更有效率, 能保证在log(n)的时间内完成 ( ?? 不对啊, arraylist的随机读取不是O(1)么?? 其实这句话的意思是, 当你不知道索引时, 查找一个元素, TreeSet自然是O(log(n)), 而ArrayList在不指定索引时, 如果没有搜索算法, 自然是log(n)) TreeSet 是用树形结构来存储信息的, 每个节点都会保存一下指针对象, 分别指向父节点, 左分支, 右分支, 相比较而言, ArrayList就是一个含有元素的简单数组了, 正因为如此, 它占的内存也要比ArrayList多一些; 向 TreeSet 插入元素也比 ArrayList 要快一些, 因为当元素插入到ArrayList的任意位置时, 平均每次要移动一半的列表, 需要O(n)的时间, 而TreeSet深度遍历查询花费的实施只需要O(log(n)) 自然顺序(Comparable) 对象所对应的类必须实现 java.lang.Comparable 接口, 并重写 compareTo(Object obj) compareTo() 方法的返回值说明: 在此方法中指明用自定义类的哪个属性进行排序, 当向TreeSet中添加元素时, 首先按照 compareTo() 进行比较, 一旦返回0, 虽然可能只是compareTo()方法中比较的这个元素相同, 但程序会认为这两个对象是相同的, 后一个对象就不会添加进来 12345public int compareTo(Person o) &#123; return 0; //当compareTo方法返回0的时候集合中只有一个元素 return 1; //当compareTo方法返回正数的时候集合会怎么存就怎么取 return -1; //当compareTo方法返回负数的时候集合会倒序存储&#125; 为什么返回0, 只会存一个元素, 返回-1会倒序存储, 返回1会怎么存就怎么取呢? 原因在于TreeSet底层其实是一个二叉树结构, 且每插入一个新元素(第一个除外)都会调用compareTo()方法去和上一个插入的元素作比较, 并按二叉树的结构进行排列: 如果将compareTo()返回值写死为0, 元素值每次比较, 都认为是相同的元素, 这时就不再向TreeSet中插入除第一个外的新元素, 所以TreeSet中就只存在插入的第一个元素 如果将compareTo()返回值写死为1, 元素值每次比较, 都认为新插入的元素比上一个元素大, 于是二叉树存储时, 会存在根的右侧, 读取时就是正序排列的 如果将compareTo()返回值写死为-1, 元素值每次比较, 都认为新插入的元素比上一个元素小, 于是二叉树存储时, 会存在根的左侧, 读取时就是倒序序排列的 compareTo() 、hashCode()、equals() 三者要保持一致 与HashSet不同, TreeSet插入元素时的判断标准其实只需要实现Comparable接口的compareTo()方法就可以了, 但是一般情况了, 推荐同时重写实现equals()这个方法, 原因在于 我们无法确认后面 会不会用到equals()方法, 比如可能会要把这个类的实例加入到HashSet中? 这是可能出现的; 另外, 我们当然不希望出现compareTo()方法得到的结果为0, 但是equals()方法得到的却是flase这样的奇怪情况出现, 所以一般实现compareTo()时会同时重写equals()来保证两个方法的结果一致, 不产生冲突。 示例: 按照年龄来排序:12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.*;public class hset &#123; public static void main(String[] args) &#123; Set treeSet = new TreeSet(); // 两个age相同, name差为负数, 所以结果会倒序 Person p1 = new Person(&quot;rym&quot;, 100); Person p2 = new Person(&quot;aym&quot;, 100); treeSet.add(p1); treeSet.add(p2); System.out.println(p1); System.out.println(p2); System.out.println(); System.out.println(treeSet); &#125;&#125;class Person implements Comparable&lt;Person&gt;&#123; private String name; private Integer age; public Person(String name, Integer age) &#123; this.name = name; this.age = age; &#125; @Override public int compareTo(Person o) &#123; int num = this.age - o.age; // 年龄是比较的主要条件 return num == 0 ? this.name.compareTo(o.name) : num; //姓名是比较的次要条件 &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Person person = (Person) o; return Objects.equals(name, person.name) &amp;&amp; Objects.equals(age, person.age); &#125;&#125; 定制排序 ?? 类需要实现 Comparator接口 需要重写 compare() 方法, 指明是按照哪个属性进行排序的 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import org.junit.Test;import java.util.*;public class hset &#123; public static void main(String[] args) &#123; //1.创建一个实现了Comparator接口类的对象 Comparator com = new Comparator() &#123; //2.向TreeSet中添加Person类的对象，在此compare()方法中，指明是按照Customer的哪个属性进行排序的 @Override public int compare(Object o1, Object o2) &#123; if(o1 instanceof Person &amp;&amp; o2 instanceof Person)&#123; Person p1 = (Person)o1; Person p2 = (Person)o2; int i = p1.getAge().compareTo(p2.getAge()); if(i == 0)&#123; return p1.getName().compareTo(p2.getName()); &#125; return i; &#125; return 0; &#125; &#125;; //3.将此对象作为形参传递给TreeSet的构造器中 TreeSet&lt;Person&gt; set = new TreeSet&lt;Person&gt;(com); //4.向TreeSet中添加Comparator接口中的compare方法中涉及的类的对象 Person p1 = new Person(&quot;rym&quot;, 100); Person p2 = new Person(&quot;aym&quot;, 100); set.add(p1); set.add(p2); System.out.println(p1); System.out.println(p2); System.out.println(set); &#125;&#125;class Person implements Comparable&lt;Person&gt;&#123; private String name; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; private Integer age; public Person(String name, Integer age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public int compareTo(Person o) &#123; int num = this.age - o.age; // 年龄是比较的主要条件 return num == 0 ? this.name.compareTo(o.name) : num; //姓名是比较的次要条件 &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Person person = (Person) o; return Objects.equals(name, person.name) &amp;&amp; Objects.equals(age, person.age); &#125;&#125; Map 接口 用Key来搜索的专家, 使用键值对存储; Map会维护与Key有关联的值, 两个Key可以引用相同的对象, 但Key不能重复, 典型的Key是String类型, 但也可以是任何对象; Java 自带了各种 Map 类, 这些 Map 类可归为三种类型 通用Map: 用于在应用程序中管理映射, 通常在 java.util 程序包中实现HashMap、Hashtable、Properties、LinkedHashMap、IdentityHashMap、TreeMap、WeakHashMap、ConcurrentHashMap 专用Map: 通常我们不必亲自创建此类Map, 而是通过某些其他类对其进行访问java.util.jar.Attributes、javax.print.attribute.standard.PrinterStateReasons、java.security.Provider、java.awt.RenderingHints、javax.swing.UIDefaults 自行实现Map: 一个用于帮助我们实现自己的Map类的抽象类AbstractMap 类型区别 HashMap: 最常用的Map, 它根据键的HashCode值存储数据, 根据键可以直接获取它的值, 具有很快的访问速度; HashMap最多只允许一条记录的键为Null(多条会覆盖), 允许多条记录的值为 Null, 非同步 TreeMap: 能够把它保存的记录根据键(key)排序, 默认是按升序排序, 也可以指定排序的比较器, 当用Iterator 遍历TreeMap时, 得到的记录是排过序的; TreeMap不允许key的值为null, 非同步 Hashtable: 与 HashMap类似, 不同的是:key和value的值均不允许为null; 它支持线程的同步, 即任一时刻只有一个线程能写Hashtable, 因此也导致了Hashtale在写入时会比较慢 LinkedHashMap: 保存了记录的插入顺序, 在用Iterator遍历LinkedHashMap时, 先得到的记录肯定是先插入的, 在遍历的时候会比HashMap慢, key和value均允许为空, 非同步的 摘自: https://baike.xsoftlab.net/view/250.html#paragraph_1 小知识点 Java容器里只能放对象, 对于基本类型(int, long, float, double等), 需要将其包装成对象类型后(Integer, Long, Float, Double等)才能放到容器里, 很多时候拆包装和解包装能够自动完成, 这虽然会导致额外的性能和空间开销, 但简化了设计和编程; 集合与数组 数组: 大小固定, 只能存储相同数据类型的数据; 数组存储的元素可以是同一类基本类型, 也可以是同一类对象; 集合: 大小可动态扩展, 可以存储不同类型的数据; 集合里只能保存对象;在编程中, 常常需要集中存放多个数据, 虽然数组是个不错的选择, 但数组需要你事先明确知道你将要保存的对象的数量, 数组在初始化时就会指定长度, 并且之后这个数组长度是不可变的;而如果我们需要保存动态增长的数据, 就无法再使用数组了, 此时, java的集合类就是一个很好的设计方案了","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01. Intellij IDEA 搭建Spring Boot项目","slug":"JavaEE/2019-01-28-01","date":"2018-12-15T03:17:23.000Z","updated":"2019-02-19T08:56:36.000Z","comments":true,"path":"2018/12/15/JavaEE/2019-01-28-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/15/JavaEE/2019-01-28-01/","excerpt":"","text":"https://www.cnblogs.com/Leo_wl/p/4459274.htmlhttps://blog.csdn.net/wuyinlei/article/details/79227962","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01. Intellij IDEA 搭建Spring MVC项目","slug":"JavaEE/2018-12-10-01","date":"2018-12-15T03:17:23.000Z","updated":"2019-02-19T09:21:33.000Z","comments":true,"path":"2018/12/15/JavaEE/2018-12-10-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/15/JavaEE/2018-12-10-01/","excerpt":"","text":"https://www.cnblogs.com/wormday/p/8435617.html","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"10. 异常","slug":"JavaSE/2018-12-14-10","date":"2018-12-14T13:58:36.000Z","updated":"2019-02-01T05:55:08.000Z","comments":true,"path":"2018/12/14/JavaSE/2018-12-14-10/","link":"","permalink":"http://blog.renyimin.com/2018/12/14/JavaSE/2018-12-14-10/","excerpt":"","text":"","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"03. git push 时出现 \"Merge branch 'master' of ...\"","slug":"git/2018-12-14-03","date":"2018-12-14T06:16:39.000Z","updated":"2018-12-19T02:16:32.000Z","comments":true,"path":"2018/12/14/git/2018-12-14-03/","link":"","permalink":"http://blog.renyimin.com/2018/12/14/git/2018-12-14-03/","excerpt":"","text":"当我们在使用git进行协同开发时, 假设 develop 为大家的一条协同分支: 当你在本地进行开发完后, 会将自己的分支合并到develop并 git push 到远端 如果此时在你之前并没有其他人对develop进行过commit并push到远端, 你是可以直接push成功的, 但这种情况一般不多, 毕竟多人协同开发现在很普遍 与上面相对的是, 在你push之前, 很可能已经有其他人对develop进行过commit并push到远端, 所以你经常会出现 push 不成功的问题, 如下: 此时一般常见做法是 先 git pull, 然后再 git push, 但在 git pull 时, 其实还分两种情况 一种是远端和本地产生了冲突:此时你需要在本地解决冲突, 并重新进行 commit, push最终的分支结构如下: 另外一种是远端虽然有改动, 但是和本地没有冲突此时虽然没有提示冲突, 但依然push失败, 当你执行pull时, 会直接弹出如下编辑框如果你wq保存编辑页, 你会发现git pull自动进行了ff自动合并最终的分支结构如下: 可以发现, 当 git pull 时, 会将远端与本地进行一次 git merge, 此时 可能会无冲突自动完成 fast-farward 合并 (此时会出现一个 Merge branch &#39;master&#39; of ... commit点) 也可能需要解决冲突再手动提交 (此时是自己手动打上的 commit点) 如何在 git push 时避免出现 Merge branch &#39;master&#39; of ... ? 方案一: (比较low的做法)在 git pull 弹出编辑页面时, 对commit点的日志内容进行修改 ; 或者先 git fetch 然后 git merge, 在本地编写commit点日志信息 (简单来说 git pull = git fetch + git merge)上面的做法并没有解决实际问题, 还会有一个合并时的commit点 方案二: (比较巧) 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature 方案二测试: 两个分支的初始基点一致 两个分支分别进行两次提交, 构造冲突: 然后在dev分支rebase来合并master分支内容 (master可以先pull, 这是不会有冲突的) 查看两分支 最后再在master分支merge去合并dev分支的内容, 此时自然也不会有冲突了 最终的分支都是直线向上 出现类似: “Merge remote-tracking branch ‘remotes/origin/develop’” : ?? git pull 和 git pull origin master ?? 参考: https://www.cnblogs.com/Sinte-Beuve/p/9195018.html https://git-scm.com/book/en/v2/Git-Branching-Remote-Branches","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"09. 常用类","slug":"JavaSE/2018-12-13-09","date":"2018-12-13T11:15:29.000Z","updated":"2019-02-18T14:25:05.000Z","comments":true,"path":"2018/12/13/JavaSE/2018-12-13-09/","link":"","permalink":"http://blog.renyimin.com/2018/12/13/JavaSE/2018-12-13-09/","excerpt":"","text":"java.util.Date 比较老, jdk1.0就被设计出来 Date类的核心都是围绕着 private transient long fastTime; 这个long类型的变量 现在该类中大部分方法都被注解了@Deprecated, 已经不再推荐使用了; 这里只推荐使用下面两个构造方法来构造Date对象 一个是默认无参构造器(内部调用本地函数获取系统当前时间计算与标准时间的毫秒差值, 并赋值给 fastTime 属性) 12345678910public Date() &#123; this(System.currentTimeMillis());&#125;// 可以使用如下代码获取当前的毫秒时间戳Date d = new Date();System.out.println(d.getTime());// 也可以自己手动获取系统当前时间计算与标准时间的毫秒差值long t = System.currentTimeMillis();System.out.println(t); 另一个则需要手动传入一个毫秒值构造Date对象 123public Date(long date) &#123; fastTime = date;&#125; 抽象的java.util.Calendar 但由于Date类不支持国际化等原因, 现在其大部分方法被注解, 不再推荐使用; 而处理年月日的这种转换则完全交给了 Calendar类处理, 所以Calendar目前是日期时间处理中的核心类, 源码如下: 1234567891011// 和Date一样封装了毫秒属性protected long time;protected int fields[];// 在Calendar的内部封装了17个静态常量, 这些常量将会作为索引用来检索fields属性// 例如：fields[YEAR]将返回当前毫秒值对应的日期时间的年份部分, fields[MONTH]将返回的是月份部分的值等等。至于这些值是哪里来的, 等我们介绍到后续源码的时候再说明, 此处只需要理解这些常量的作用即可public final static int ERA = 0;public final static int YEAR = 1;public final static int MONTH = 2;public final static int WEEK_OF_YEAR = 3;.........public final static int DST_OFFSET = 16; 该类是抽象类, 主要有四个方法用于创建Calendar实例, 其实内部调用的是同一个方法, 只是传入的参数不同 12345678910111213141516171819public static Calendar getInstance()&#123; return createCalendar(TimeZone.getDefault(), Locale.getDefault(Locale.Category.FORMAT));&#125;public static Calendar getInstance(TimeZone zone)&#123; return createCalendar(zone, Locale.getDefault(Locale.Category.FORMAT));&#125;public static Calendar getInstance(Locale aLocale)&#123; return createCalendar(TimeZone.getDefault(), aLocale);&#125;public static Calendar getInstance(TimeZone zone, Locale aLocale)&#123; return createCalendar(zone, aLocale);&#125; 创建一个 Calendar 实例需要两个参数: 一个是TimeZone时区, 另一个是Locale语言国家, 如果我们没有指定时区和国家语言, 那么将会默认使用本机系统信息 接下来看下如何通过获取到Calendar实例完成对日期时间进行计算 获取和设置 代表毫秒的time属性: 1234public final Date getTime() &#123; return new Date(getTimeInMillis());&#125;public void setTimeInMillis(long millis)&#123;&#125; 也有获取上述介绍的17种属性的方法: 1234567public int get(int field)&#123; // complete方法就是调用了本地函数完成对fields属性中没有值的元素赋值 // 调用internalGet方法其实就是调用的fields[field], 为我们返回指定属性的结果值 complete(); return internalGet(field);&#125; 例子: 123456Calendar calendar = Calendar.getInstance();System.out.println(calendar.get(Calendar.YEAR)); // 2018// 需要注意一点的是, month属性是从0开始的, 也就是0表示一月, 11表示12月, 星期也是一样System.out.println(calendar.get(Calendar.MONTH)); // 11// 上述中的AM_PM表示的是上下午的概念, 上午为0, 下午为1System.out.println(calendar.get(Calendar.AM_PM)); // 1 抽象的java.text.DateFormat DateFormat是一个抽象类, 该类主要用于实现Date对象和字符串之间相互转换, 涉及到两个转换的方法: 1234//将Date类型转换为String类型public final String format(Date date)//将String类型转换Date类型public Date parse(String source) 除此之外, DateFormat还提供了四个静态常量, 代表着四种不同的风格。不同的风格输出信息的内容详尽程度不同, 默认的风格是 MEDIUM(折中) 12345public static final int FULL = 0;public static final int LONG = 1;public static final int MEDIUM = 2;public static final int SHORT = 3;public static final int DEFAULT = MEDIUM; 该类是抽象类, 一样需要使用静态工厂获取实例对象 1234567891011public final static DateFormat getTimeInstance()public final static DateFormat getTimeInstance(int style)public final static DateFormat getTimeInstance(int style,Locale aLocale)public final static DateFormat getDateInstance()public final static DateFormat getDateInstance(int style)public final static DateFormat getDateInstance(int style,Locale aLocale)public final static DateFormat getDateTimeInstance()public final static DateFormat getDateTimeInstance(int dateStyle,int timeStyle)public final static DateFormat getDateTimeInstance(int dateStyle, int timeStyle, Locale aLocale) 很明显, 有三种不同的方式来获取DateFormat实例, 每种方式有三个重载, getDateInstance用来处理日期, getTimeInstance用来处理时间, getDateTimeInstance既可以处理日期, 也可以处理时间。我们通过一个例子看看他们之间的区别: 123456789101112public static void main(String[] args) &#123; Calendar c = Calendar.getInstance(); // 无论是上述的哪一种工厂方法, 在他们内部都调用的是同一个函数 `get` System.out.println(DateFormat.getDateInstance().format(c.getTime())); System.out.println(DateFormat.getTimeInstance().format(c.getTime())); System.out.println(DateFormat.getDateTimeInstance().format(c.getTime()));&#125;// 结果:2019-2-1821:52:06 2019-2-18 21:52:06 SimpleDateFormat SimpleDateFormat是DateFormat的一个优秀的实现类, 它增强了一个重要的性质。它允许自定义格式输出模板。构造SimpleDateFormat实例的时候, 可以传入一个pattern作为输出模板 12345Calendar c = Calendar.getInstance();SimpleDateFormat sm = new SimpleDateFormat(&quot;yyyy年MM月dd日 E HH时mm分ss秒&quot;);System.out.println(sm.format(c.getTime()));// 输出结果:2019年02月18日 星期一 21时57分52秒 上述的代码中, 字符串 yyyy年MM月dd日 E HH时mm分ss秒 就是一个模板pattern, 其中: yyyy 表示使用四位数字输出年份 MM 表示使用两位数字表示月份 dd 表示使用两位数字表示日 E 表示星期几 HH 表示使用两位数字表示小时（24以内） mm 和 ss 分别表示分钟和秒数","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"02. 选择 merge 还是 rebase?","slug":"git/2018-12-12-02","date":"2018-12-12T09:01:23.000Z","updated":"2019-03-11T13:09:38.000Z","comments":true,"path":"2018/12/12/git/2018-12-12-02/","link":"","permalink":"http://blog.renyimin.com/2018/12/12/git/2018-12-12-02/","excerpt":"","text":"git rebase 介绍 rebase: 变基, 即改变分支的根基 从某种程度来说, git rebase 和 git merge 都可以完成类似的合并工作, 但实际上两者有着本质的不同 假设你的项目有 mywork, origin 两个分支, 如果你想让 mywork 的分支历史看起来像没有经过任何合并一样, 你可以用 git rebase (虽然 git merge 也可以实现, 但是一旦合并遇到冲突时, 还是会出现分叉) 如下命令: 会把你的 origin 分支从 mywork 切出来后的每个提交(commit)取消掉, 并且把它们临时保存为补丁(patch)(这些补丁放到”.git/rebase”目录中), 然后把 origin 分支更新到最新的 mywork 分支, 最后把保存的这些补丁应用到 mywork 分支上。 12$ git checkout mywork$ git rebase origin 具体如下图: 尝试主分支没有commit时 git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 (不修改同一行,先不构造冲突) git log --graph 查看dev日志 git rebase 合并 发现效果和之前的 git merge fast-forward无冲突时 貌似没什么区别 主分支有commit且出现冲突时 接着上面的例子, master 和 dev 分别修改同一行内容, 构造冲突 切换到 master 分支, git rebase 合并dev分支 如上 git rebase 在出现冲突时, Git会停止rebase并会让你去解决冲突, 在解决完冲突后, 用 git add 命令去更新这些内容的索引(index), 然后, 你无需执行 git-commit, 只要执行: git rebase --continue （当然, 无冲突时, 直接 git rebase 就直接完成了合并） 发现好像之前master上的那次提交的信息(时间, 日志等信息)都在, 但是取而代之的是一个新的commit_id, rebase会修改 根基之后 的提交历史 主分支有commit但无冲突 经过测试, 即便无冲突, master分支的新commit也会丢失 测试如下: master, dev两个分支基点相同 master, dev两个分支各自做改动, 但是改动并不不冲突 发现master进行 git rebase dev 直接就合并成功了, 虽然没有冲突, 但是master的commit历史确实还是被改掉了!! 小结 不要在master和其他协作分支上使用 git rebase, 它会修改提交历史; 当使用git做合并操作时, 如果没有冲突, 则 git merge(fast-forward) 和 git rebase 效果一样都是直线, 而git merge --no-ff会出现分叉和新提交点; 当使用git做合并操作时, 如果出现冲突, 则 git merge(fast-forward) 和 git merge --no-ff 一样都会出现分叉和新提交点, git rebase则不同, 它是直线, 但会修改历史提交点; rebase 使用技巧 git rebase 由于会改变历史提交点, 所以一定不能用在协作分支上 使用rebase既可以保证分支直线美观, 也可以保证在git push时不出现类似 “Merge remote-tracking branch…” 问题 通常如下来使用, 下文在解决 类似 “Merge remote-tracking branch…” 问题时, 会有详细过程: 1234567891011git checkout mastergit pull# 主要是此处, 将协作分支master rebase 到自己的dev分支git checkout devgit rebase mastergit checkout mastergit merge devgit push origin feature","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"01. git merge 对比 fast-forward 与 --no-ff","slug":"git/2018-12-12-01","date":"2018-12-12T05:32:54.000Z","updated":"2018-12-14T06:17:17.000Z","comments":true,"path":"2018/12/12/git/2018-12-12-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/12/git/2018-12-12-01/","excerpt":"","text":"git merge 介绍 在使用 git merge 进行分支合并时, 可以直接使用如下两种方式 git merge (fast-forward) git merge --no-ff 这两者的区别如下: 但是需要注意的是 git merge 的一个特例: 当合并出现冲突时, 其实是无法执行快速合并的,需要解决冲突后再进行一次提交, 所以效果和 git merge --no-ff 是一样的, 也就是还会出现分叉 (可以参考此文) fast-forward git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 Fast-forward 合并 最终可以看到, master分支只是简单地将dev分支的那次提交合进自己的分支内 (不会在graph图中保留dev分支线) 即便是在dev分支做了多次提交, master分支也只是简单地将dev分支的多次提交合进自己的分支内 (不会保留dev分支) —no-ff git init 初始化一个仓库, 在默认的 master分支创建一个index.php文件并提交 (新建分支的话, 必须要保证初始分支的仓库中是有内容的) (下面做了两次提交) git log --graph 查看master日志 新建并切换到 dev 分支, 对index.php文件做改动并提交 git log --graph 查看dev日志 切换到master分支进行 —no-ff 合并 (相比 FastForward, 这里还会弹出编辑界面, 允许你对本次合并进行说明) 最终可以看到: master分支并没有将dev上的两次提交合到自己的分支上 而是在graph图中保留了dev分支线 并且将这次合并当做一次dev向master的提交, 在master上生成一个新的commit 此时master的分支的commit点已经比dev上的commit要更新一步 此时如果不把master的提交分支合并到dev, 而是在dev上继续做两次提交, 然后再在master上进行 —no-ff 合并, 效果如下: 红色框表示—no-ff合并时所进行的新commit及备注日志 现在如果把master的提交合并到dev 如果在master切出新的分支, 然后再新分支上进行提交, 再回到master进行 —no-ff 合并, 效果依然如下图: ff 冲突导致的例外 重新创建本地仓库, 对master进行两次提交; 新建并切换到dev分支, 并修改文件: 切换回master分支, 修改dev上修改的同一行内容, 构造冲突: 此时, 即便是在master上使用 fast-forward 合并dev, 由于有冲突, 还是会进行一次提交:","categories":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.renyimin.com/tags/git/"}]},{"title":"08. 面向对象特征 多态 (Java对比PHP)","slug":"JavaSE/2018-12-11-08","date":"2018-12-11T09:05:19.000Z","updated":"2019-01-26T04:18:22.000Z","comments":true,"path":"2018/12/11/JavaSE/2018-12-11-08/","link":"","permalink":"http://blog.renyimin.com/2018/12/11/JavaSE/2018-12-11-08/","excerpt":"","text":"重载VS重写 重载: 发生在同一个类中, 指有多个方法的名称相同, 但是参数列表不同, 所谓参数列表不同是指如下几条 参数的个数不同 参数的类型不同 参数的多类型顺序不同 重载与方法的返回值类型无关, 与参数名称无关 重写的要求: 发生在父子类中 方法名、参数列表必须相同 返回值范围小于等于父类 抛出的异常范围小于等于父类 访问修饰符范围大于等于父类 如果父类方法访问修饰符为 private 则子类就不能重写该方法 多态 封装: 是指隐藏对象的实现细节, 仅对外提供公共访问方式, 提高了代码重用性和健壮性 继承: 提高了代码的复用性, 让类与类之间产生了关系, 给第三个特征”多态”提供了前提 多态: 其实方法的 重写 和 重载 也算是一种多态, 但是我们讨论面向对象多态时, 更多指的是 子类对象的多态性 在Java中有两种形式可以实现多态 继承基类 (基类可以是 普通类 也可以是 抽象类) 实现接口 多态具有 向上转型 与 向下转型 两种转型的概念 多态特点(Java VS PHP) 多态性只针对方法, 不针对属性 对于方法: 子类对象向上转型后, 只能调用子类重写的父类方法, 如果在向上转型后调用子类特有的方法, Java 在 编译阶段 就会报错, 你需要需要强制向下转型才可以; 对于属性: 子类对象向上转型后, 和方法不同, 即便是子类重写了父类的属性, 你在调用属性时, Java编译器不会认为你调用的是子类重写的属性, 如果你需要显示指明用子类的属性, 需要强制向下转型; PHP会自动向下转型 对于方法: 子类对象向上转型后, 在IDE编码阶段, 如果调用了子类特有的方法, IDE会识别不到, 但在运行时会自动向下转型, 所以最终还是会调用到; 对于属性: 子类对象向上转型后, 即便是子类重写了父类的属性, 在IDE编码阶段, 如果调用了同名属性时, IDE也会识别不到, 但在运行时会自动向下转型, 所以最终调用的是子类的属性; 向上/下转型 (Java VS PHP) 向上转型: 在面向对象编程中, 为了提高扩展性, 你应该面向抽象(普通基类, 抽象基类 或 接口)编程, 而不是去直接面对具体类型进行编程 当具体的子类对象传递给一个父类引用时, 便是向上转型, 多态本身就是向上转型的过程; php示例: 12345678910111213141516171819202122232425262728293031323334353637383940&lt;?phpabstract class Car&#123; public $name = &quot;汽车草图&quot;; public function start() &#123; var_dump(&quot;汽车启动了&quot;); &#125;&#125;class BMW extends Car&#123; public $name = &quot;宝马车&quot;; public function start() &#123; var_dump(&quot;BMW启动了&quot;); &#125; public function getCarBrand() &#123; var_dump(__CLASS__); &#125;&#125;class Driver&#123; // 向上转型, 面向抽象编程 public function drive(Car $car) &#123; $car-&gt;start(); // BMW启动了 $car-&gt;getCarBrand(); // BMW (由于该方法是具体子类对象特有的, 所以在运行前并不被识别, 但是运行时确实可以访问到, 因为运行时PHP自动做了向下转型) $car-&gt;name; // 宝马车 (php竟然自己做了向下转型) &#125;&#125;$car = new BMW();$driver = new Driver();$driver-&gt;drive($car); 分析: 对于成员方法来说, 由于 Car中并没有 getCarBrand方法, 所以在编码阶段, IDE不能识别该方法, 但是由于PHP在 运行时 最终会做 自动向下转型, 所以最终可以找到子类特有的方法的 对于成员属性来说, 由于 多态性并不适用于属性, 所以, 在向上转型后, 其实应该和Java一样看到的是父类的属性(如下图), 但PHP比较特殊, 它自动做了向下转型, 所以最终结果其实是子类的属性 Java示例: 123456789101112131415161718192021222324252627282930313233343536373839404142abstract class Car&#123; public String name = &quot;汽车草图&quot;; public void start() &#123; System.out.println(&quot;汽车启动了&quot;); &#125;&#125;class BMW extends Car&#123; public String name = &quot;宝马车&quot;; @Override public void start() &#123; System.out.println(&quot;BMW启动了&quot;); &#125; public void getCarBrand() &#123; System.out.println(&quot;BMW&quot;); &#125;&#125;class Driver&#123; public void drive(Car car) &#123; car.start(); // BMW启动了 ((BMW)car).getCarBrand(); // BMW 需要向下转型 System.out.println(car.name); // 汽车草图 System.out.println(((BMW)car).name); // 宝马车 (属性不参与多态性) &#125;&#125;public class Upcast &#123; public static void main(String[] args) &#123; BMW car = new BMW(); Driver driver = new Driver(); driver.drive(car); &#125;&#125; 分析: 程序在 编译阶段 , 编译器就检查到Car没有getCarBrand方法, 因此编译无法通过 子类在向上转型后, 只能调用其重写父类的方法, 要想使用子类特有的方法, 还需要手动向下转型 (PHP不同, 自动向下转型了) Java中, 子类对象的多态性并不适用于成员属性, 如下, 即便是子类和父类都有共同的属性, 向上转型后, 调用的却是父类的属性, 要想调用子类重写的属性, 还得向下转型 (PHP不同, 自动向下转型了) 向下转型: 一个已经向上转型的子类对象可以使用强制类型转换, 将父类引用转为子类引用, 这个过程是向下转型; 注意: 如果是直接创建父类对象, 是无法向下转型的!(向下转型的前提是首先进行了向上转型) 什么时候使用向下转型 当你面向抽象编程时, 某个方法依赖的可能是抽象类型而不是具体类型, 这样的话, 在程序运行时, 具体的子类被传递进来时, 可能就需要使用向下转型来使用子类特有的一些方法或属性(向下转型的前提是首先进行了向上转型) 很明显在PHP中, 这种弱类型的向下转型是隐式自动的 小结多态 向上转型 Java 可以调用的是: 子类重写的方法, 父类的属性; 要调用子类特有方法和子类同名属性, 都需要向下转型; PHP 可以调用的是: 子类重写的方法, 子类属性; (只能在IDE编码阶段体会到多态向上转型后, Java中的特点);","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"07. 面向对象基础知识","slug":"JavaSE/2018-12-09-07","date":"2018-12-09T03:21:37.000Z","updated":"2019-02-18T08:51:03.000Z","comments":true,"path":"2018/12/09/JavaSE/2018-12-09-07/","link":"","permalink":"http://blog.renyimin.com/2018/12/09/JavaSE/2018-12-09-07/","excerpt":"","text":"四种访问权限修饰符 访问权限 本类 本包的类 子类 非子类的外包类 private 是 否 否 否 default 是 是 否 否 protected 是 是 是 否 public 是 是 是 是 default : 这个缺省情况比较特殊, php貌似没有 另外, Java 可以直接创建包, 很容易实现自动加载 (php中, 虽然很多框架都自动提供了自动加载; 但自己的练习项目, 如果想使用自动加载, 需要使用composer或者自己实现, 和Java相比还是比较鸡肋) 外部类的访问控制只能是: public 、default 这两种 抽象方法只有一个限制: 不能用 private 来修饰, 也即抽象方法不能是私有的, 否则, 子类就无法继承实现抽象方法 接口成员的访问权限, 接口由于其的特殊性, 所有成员的访问权限都规定得死死的, 下面是接口成员的访问权限: 1234变量 : public static final抽象方法 : public abstract静态方法 : public static, JDK1.8后才支持内部类、内部接口 : public static 也因为所有的一切都默认强制规定好了, 所以我们在用的时候, 并不一定需要完整写出所有的修饰符, 编译器会帮我们完成的, 也就是, 可以少写修饰符, 但不能写错修饰符 匿名对象即没有名字的对象: 当对象对方法仅进行一次调用的时候, 就可以简化成使用匿名对象调用 new Car().run(); 构造器 在Java中, 函数名与类名相同, 正因如此, 子类无法重写构造函数, 但是可以重载 (php由于构造方法名为与类名无关的 __construct, 所以可以重写) 构造方法不要定义返回值类型 (这个函数是没有返回值的, 甚至连 void 都不要写), 自然也没有返回值; 在创建对象时就会自动调用构造函数 ( 所以可以利用这个特性给对象进行初始化 ) 一个类中如果没有定义过构造函数, 那么该类中会有一个 默认的空参构造器 在你编译过的.class文件中就有一个默认的空参数构造函数的 12345678class Demo&#123;&#125;就这么个简单的类, 其实在编译完之后, 生成的.class文件中其实是 :class Demo&#123; Demo()&#123;&#125;&#125; 如果一个类中指定了构造函数, 那么类中的默认构造函数就没有了, 此时.class文件中保存的是你自己写的构造函数 构造函数不能被 static, final, synchronized, abstract, native 修饰; 普通函数不能调用构造函数 (构造函数是用来给对象初始化的) 普通的函数也可以用类名作为自己的函数名 123456void Person() // 给函数加了返回类型后(尽管是void), 看似的构造函数, 其实就成了一般函数了&#123; name = &quot;baby&quot;; age = 1; System.out.println(&quot;person run&quot;);&#125; 构造函数的继承 super() 如下, 子类对象被创建时, 会访问子类的构造器, 但是下面会发现父类的构造方法其实也运行了: 1234567891011121314151617181920212223class Fu&#123; Fu() &#123; System.out.println(&quot;fu run&quot;); &#125;&#125;class Zi extends Fu&#123; Zi() &#123; System.out.println(&quot;zi run&quot;); &#125;&#125;public class ExtendsDemo&#123; public static void main(String[] args) &#123; new Zi(); &#125;&#125; 其原因是在子类的每个构造函数中, 首行都会默认有一个隐式语句: super() 来调用父类的空参构造函数 ( 这点和php不同, php只有显示指定了 parent::__construct() 才会调用父类的构造函数 ) 如果父类没有空参构造函数, 那么你需要在子类每个构造器的首行手动去使用 super(参数) 来指定具体需要父类的哪个构造器 需注意: 如果你没有在子类的构造函数中手动使用 super(参数) 指定调用父类的其他重载构造器, 而子类也没有空参构造器, 则会报错 因此, 在设计类时, 尽量提供一个空参构造器 (因为你一旦在父类中有了其他的重载的构造器, 那么空参构造器就没了, 这就导致你需要在所有子类的所有构造器首行去手动使用super()指定你使用了父类中的哪个构造器) … 父类再往上, 最终会在其父类的构造器中调用的是顶端类Object的空参构造器 其实子类中所有的构造器, 默认在首行都会执行 super() 调用父类中的 空参数的构造函数 123456789101112131415161718192021222324252627282930313233343536class Fu&#123; Fu() &#123; System.out.println(&quot;fu run&quot;); &#125; Fu(int age) &#123; System.out.println(&quot;fu age:&quot; + age + &quot;run&quot;); &#125;&#125;class Zi extends Fu&#123; Zi() &#123; // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi run&quot;); &#125; Zi(int age) &#123; // super(); // 调用的就是父类中的空参数的构造函数 System.out.println(&quot;zi age:&quot; + age + &quot; run&quot;); &#125;&#125;public class ExtendsDemo&#123; public static void main(String[] args) &#123; new Zi(); new Zi(100); &#125;&#125; 结果: 1234fu run # 父类无参构造函数zi runfu run # 父类无参构造函数zi age:100 run 如果子类构造函数中还使用了 this() 调用了子类自己的构造函数, 那么子类的这个构造函数中的 super() 就没有了, 因为 super() 和 this() 只能有一个, 并且 super() 语句也必须要定义在子类构造函数的第一行 (因为父类的初始化动作要先完成不然会提示错误) super 当子类和父类中的成员(变量或方法)重名的时候, 用 super 区分父类 (相当于 php 中的 parent::) this 和 super 的用法很相似 super 的用法跟 this 类似, this 代表对本类对象的引用, 指向本类已经创建的对象; 而 super 代表对父类对象的引用, 指向父类对象 静态方法中不可以出现 super, this 关键字 在子类的构造函数中, 第一行会隐式地执行一个: super() 如果子类没有写不带参数的构造函数, 由于默认会存在无参的构造函数, 所以还是会执行 super() this this: 在类的成员方法中使用, 代表的是当前对象; 注意: 在一般情况下, 如果在类方法中的成员属性和局部变量不重名的情况下, 其实成员属性前是不用写this.的, 因为Java默认就是用的 this. ( 这点和PHP不同 ) 虽然, java在类的成员方法中调用成员变量时, 默认可以不用写 this, 但是 成员变量 和 局部变量 重名时, 必须用 this. 来给成员变量做引用 1234public Person(name)&#123; this.name = name&#125; this() 可以在构造函数中通过 this(参数) 的方式显示地调用本类中重载的其他构造函数 this(参数) 只能用在构造函数内部, 并且使用时必须放在首行 static static(静态的): 是一个修饰符, 可以用来修饰 属性, 方法, 代码块(初始化块), 内部类 类中不被 static 修饰的成员属性, 是属于对象的, 需要实例化对象后才能调用, 所以也叫 实例变量; 而被 static 修饰的成员属性是属于类的(和成员方法类似, 是所有对象共有的), 所以也叫 类变量; static 修饰的 静态属性 和 静态方法 特点 : 类变量 存放在 静态域 随着类的加载而加载 优先于对象存在(因为它是随着类的加载而加载的, 而类加载的时候还没有对象的) 被该类的所有对象所共享 多了一种调用方式, 可以直接被类名调用(因为static成员存在的时候只有类存在, 对象尚未存在) static 修饰的数据是共享数据, 对象中存储的是对象自己特有数据 使用注意: 静态方法只能访问静态成员(成员变量和成员函数) 静态方法中不可以写 this, super关键字; 之前我们一开始写的main主函数就是静态的; 成员变量 VS 静态变量 生命周期的不同: 成员变量: 随着对象的创建而存在, 随着对象的被回收而释放; 静态变量: 随着类的加载而加载, 随着类的消失而消失; (那类什么时候消失呢 ? 一般情况下, 虚拟机结束了, 类就消失了) 调用方式不同: 成员变量: 只能被对象调用; 静态变量: 可以被对象调用, 也可以被类名调用; (一般就用类名调用) 别名不同: 成员变量: 也称为实例变量; 静态变量: 称为类变量; 数据的存储位置不一样: 成员变量: 存储在堆内存的对象中, 所以是对象的特有数据; 静态变量: 存储在于方法区(也叫共享数据区)的静态区中, 所以也叫对象的共享数据; 静态方法只能调用静态变量, 不能调用成员变量 (但是, 非静态方法是可以访问静态成员) 因为静态方法在类被加载的时候就被加载了,可能还没有对象的时候, 就已经可以用类名来访问了 (Person.show();) 如果它里面有成员变量的话, 由于此时还没有对象, 而成员变量是每个对象所特有的, 如果对象没有创建, 自然无法使用成员变量 静态方法 VS 实例方法 静态成员方法 提供给外部调用的函数应该是非静态的还是静态的呢?函数是否需要设置为静态函数, 只用参考该函数是否有访问到对象中的特有数据, 如果访问到了对象的特有数据的话, 那就不能用static来进行修饰了 (因为静态方法只能调用静态成员) 简单来说就是 : 该函数是否需要访问非静态的成员, 如果需要访问非静态成员, 那么该功能就不能是静态的(因为静态方法只能调用静态成员) 如果创建的对象仅仅是为了去调用没有访问特有数据的非静态方法, 那么这个对象的创建除了浪费空间这个坏处之外, 没有别的任何好处了; 此时这个方法完全可以设置为static, 然后使用类名来调用, 而不用创建对象, 不浪费空间 (对象的创建是为了访问特有数据的, 没有访问特有数据, 干嘛要创建对象呢) 实例方法中可以调用静态属性和成员属性, 但是如果实例方法的形参和静态属性同名, 注意不能用this.静态属性了, 需要用 类名.静态属性 静态代码块 静态代码块的特点: 静态代码块 随着类的加载而执行, 而且只执行一次 静态代码块 的加载自然是要早于 构造器, 非静态代码块的 多个静态代码块之间按照顺序结构执行 静态代码块中只能执行静态属性和方法 静态代码块中的内容不用创建对象就可以执行 (直接通过 类名.静态属性 就可以触发) 静态代码块的作用: 用于给类进行一些初始化工作 一般如果一个类里面都是静态成员, 这个类是不需要创建对象的, 直接用类名调用成员即可如果此时还要进行一些初始化工作, 那就要靠静态代码块了 （但感觉这和直接在类中初始化静态成员不一样的么） 示例: 静态代码块只被调用了一次 (即使实例化了多个对象, 静态代码块也只运行一次) 1234567891011121314151617181920class ConstructBlock1 &#123; public static String name = &quot;xingming&quot;; static &#123; System.out.println(&quot;静态代码块1被执行:&quot; + name); &#125; static &#123; System.out.println(&quot;静态代码块2被执行:&quot; + name); &#125;&#125;public class StaticConstructBlockDemo&#123; public static void main(String[] args) &#123; System.out.println(ConstructBlock1.name); &#125;&#125; 运行结果: 123静态代码块1被执行:xingming静态代码块2被执行:xingmingxingming 非静态代码块 没有名字的普通代码块 (也叫 初始化代码块、构造代码块、非静态代码块) 非静态代码块的特点: 在创建对象的时候, 构造代码块就被加载到了 可以定义多个初始化代码块, 多个代码块按顺序执行 代码块中除了赋值语句, 还可以有输出语句 每创建一个对象, 非静态代码块都会加载一遍 非静态代码块的执行要早于构造器, 但是默认的初始化赋值语句 和 非静态代码块的执行是 按照语句先后顺序来的 非静态代码块 与 构造器 都是做初始化的? 构造器 是给每个对象进行针对性的初始化, 而 构造代码块 是具备着对象初始化的通用性 ; 因为初始化的时候, 每个对象传递的值可能不同, 这样的话, 由于类中可能有构造函数的重载, 那么每个对象调用的构造函数可能就不同;但是可能对象在创建的时候都需要初始化一个相同的数据值, 这样的话, 除非给重载的每个构造函数中都初始化上该数据的值, 要不然的话, 创建对象的时候可能就会因为调用的构造函数不同而导致有些进行这个数据的初始化了, 有些对象的创建又没有进行 ; 所以可以把创建对象的时候, 通用的初始化数据写到构造代码块中 示例: 1234567891011121314151617181920212223242526272829303132class ConstructBlock &#123; private String name = &quot;xingming&quot;; public ConstructBlock(String name) &#123; this.name = name; System.out.println(&quot;构造方法被执行:&quot; + this.name); &#125; // 构造代码块 和 构造器 的执行顺序 与 代码先后位置无关 // 先执行构造代码块内容, 后执行构造器 &#123; this.name = &quot;姓名&quot;; System.out.println(&quot;构造代码块1被执行:&quot; + name); &#125; //private String name = &quot;xingming&quot;; &#123; this.name = &quot;小明&quot;; System.out.println(&quot;构造代码块2被执行:&quot; + name); &#125;&#125;public class ConstructBlockDemo&#123; public static void main(String[] args) &#123; new ConstructBlock(&quot;王进&quot;); &#125;&#125; 运行结果: 12345➜ src javac ConstructBlockDemo.java➜ src java ConstructBlockDemo 构造代码块1被执行:姓名构造代码块2被执行:小明构造方法被执行:王进 注意: 静态代码块用于给类进行初始化, 构造函数是给对象初始化, 构造代码块是给对象进行公共部分初始化 代码块示例小结 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142public class test2 &#123; public static void main(String[] args) &#123; new Student(); System.out.println(\"\\n\"); new Student(); &#125;&#125;class Person &#123; public static String name = \"姓名\"; static &#123; System.out.println(\"Person静态代码块被执行\"); &#125; &#123; System.out.println(\"Person代码块被执行\"); &#125; public Person() &#123; System.out.println(\"Person构造器被执行\"); &#125;&#125;class Student extends Person &#123; public static String name = \"学生姓名\"; static &#123; System.out.println(\"Student静态代码块被执行\"); &#125; &#123; System.out.println(\"Student代码块被执行\"); &#125; public Student() &#123; super(); System.out.println(\"Student构造器被执行\"); &#125;&#125; 结果: 123456789101112Person静态代码块被执行Student静态代码块被执行Person代码块被执行Person构造器被执行Student代码块被执行Student构造器被执行Person代码块被执行Person构造器被执行Student代码块被执行Student构造器被执行 final 在Java中声明类, 属性和方法时, 可使用关键字final来修饰, 表示 最终 final 修饰类: 不能被继承 (比如: Math, String) final 修饰的方法: 不能被子类覆盖重写 (但依然可以重载) final 修饰变量: 表示把变量作为常量 (只能被赋值一次) 某个值不需要变化, 可以用final, 这样这个变量还有名字, 具有可读性 final 修饰的常量需要显示初始化 写法规范: 常量所有字母都大写, 多个单词中间用下划线分开 通常成员被 final 了, 都用 static, 用static 和 final 修饰的常量是全局类常量 内部类 只能访问被final修饰的局部变量 内部类 在Java中, 可以将一个类定义在另一个类里面或者一个方法里面, 这样的类称为内部类。 广泛意义上的内部类一般来说包括如下四种: 成员内部类: 成员内部类是最普通的内部类, 它的定义为位于另一个类的内部 成员内部类可以无条件访问外部类的所有成员属性和成员方法(包括private成员和静态成员) 注意: 当成员内部类拥有和外部类同名的成员变量或者方法时, 会发生隐藏现象, 即默认情况下访问的是成员内部类的成员; 如果要访问外部类的同名成员, 需要以下面的形式进行访问: 12外部类.this.成员变量外部类.this.成员方法 虽然成员内部类可以无条件地访问外部类的成员, 而外部类想访问成员内部类的成员却不是这么随心所欲了。在外部类中如果要访问成员内部类的成员, 必须先创建一个成员内部类的对象, 再通过指向这个对象的引用来访问; 成员内部类是依附外部类而存在的, 也就是说, 如果要创建成员内部类的对象, 前提是必须存在一个外部类的对象。创建成员内部类对象的一般方式如下: 1234567891011121314151617181920212223242526272829public class Test &#123; public static void main(String[] args) &#123; //第一种方式： Outter outter = new Outter(); Outter.Inner inner = outter.new Inner(); //必须通过Outter对象来创建 //第二种方式： Outter.Inner inner1 = outter.getInnerInstance(); &#125;&#125; class Outter &#123; private Inner inner = null; public Outter() &#123; &#125; public Inner getInnerInstance() &#123; if(inner == null) inner = new Inner(); return inner; &#125; class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 相比较外部类只有 public 和 default 修饰符不同, 内部类作为一个成员, 可以被任意修饰符修饰(private、protected、public、default) 1234如果成员内部类用private修饰, 则只能在外部类的内部访问;如果用public修饰, 则任何地方都能访问;如果用protected修饰, 则只能在同一个包下或者继承外部类的情况下访问;如果是default访问权限, 则只能在同一个包下访问; 局部内部类: 局部内部类是定义在一个方法或者一个作用域里面的类, 它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内 注意: 局部内部类就像是方法里面的一个局部变量一样, 是不能有public、protected、private以及static修饰符 匿名内部类: 匿名内部类应该是平时我们编写代码时用得最多的, 在编写事件监听的代码时使用匿名内部类不但方便, 而且使代码更加容易维护, 下面这段代码是一段Android事件监听代码: 1 静态内部类: 静态内部类也是定义在另一个类里面的类, 只不过在类的前面多了一个关键字static。静态内部类是不需要依赖于外部类的, 这点和类的静态成员属性有点类似, 并且它不能使用外部类的非static成员变量或者方法, 这点很好理解, 因为在没有外部类的对象的情况下, 可以创建静态内部类的对象, 如果允许访问外部类的非static成员就会产生矛盾, 因为外部类的非static成员必须依附于具体的对象。 1 为什么在Java中需要内部类? 总结一下主要有以下四点: 每个内部类都能独立的继承一个接口的实现, 所以无论外部类是否已经继承了某个(接口的)实现, 对于内部类都没有影响。内部类使得多继承的解决方案变得完整, 方便将存在一定逻辑关系的类组织在一起, 又可以对外界隐藏。 方便编写事件驱动程序 方便编写线程代码个人觉得第一点是最重要的原因之一, 内部类的存在使得Java的多继承机制变得更加完善。 https://www.cnblogs.com/dolphin0520/p/3811445.html Object 类 Object类是所有Java类的根基类, 如果类的声明未使用extends关键字指明其基类, 则默认基类为Object类; 位于 package java.lang; 内 __toString() : 默认返回包名 + 类名 + @ + 哈希码 可以重写","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"06. 基础小坑 (面试必备)","slug":"JavaSE/2018-12-09-06","date":"2018-12-09T03:15:09.000Z","updated":"2019-01-26T04:13:14.000Z","comments":true,"path":"2018/12/09/JavaSE/2018-12-09-06/","link":"","permalink":"http://blog.renyimin.com/2018/12/09/JavaSE/2018-12-09-06/","excerpt":"","text":"字符做 + 运算 字符做 + 运算时, 结果不是简单地拼接字符, 而是使用字符的ascii码值进行加法运算 1234char c1 = &apos;a&apos;; // ascii码是 97char c2 = &apos;C&apos;; // ascii码是 67// \\t的ascii码是9System.out.println(c1 + &apos;\\t&apos; + c2); // 结果是 97 + 9 + 67 = 173 字符做 + 运算时, 如果运算中包含 字符串 时, 则为结果为字符串拼接 123char c1 = &apos;a&apos;; // ascii码是 97char c2 = &apos;C&apos;; // ascii码是 67System.out.println(c1 + &quot;\\t&quot; + c2); // 结果是 a C += 与 =+= 之类的运算符 与 简单赋值运算符 = 的区别 对于如下的运算, 之前已经熟悉了, 编译会报错 1234short s1 = 5;// 由于自动类型提升为int, 所以这种复制运算编译报错// 错误: 不兼容的类型: 从int转换到short可能会有损失s1 = s1 + 2; 使用 += 既可以实现运算, 又不会更改变量的数据类型 12345short s1 = 5;// 由于自动类型提升为int, 所以这种复制运算编译报错// 错误: 不兼容的类型: 从int转换到short可能会有损失s1 += 2;System.out.println(s1); // 7 字符型常量 VS 字符串常量 形式上: 字符常量是单引号引起的一个字符, 而字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整形值(ASCII 值), 可以参加表达式运算; 而字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小: 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志); (注意: char在Java中占两个字节) 注意: Java要确定每种基本类型所占存储空间的大小, 它们的大小并不像其他大多数语言那样随机器硬件架构的变化而变化, 这种所占存储空间大小的不变性是Java程序比其它大多数语言编写的程序更具可移植性的原因之一 局部变量 VS 成员变量 局部变量定义在 方法内、方法的形参部分、局部代码块内, 只在所属的区域有效; 成员变量定义在类里方法外, 整个类中都可以访问; 局部变量存在于栈内存的方法中; 成员变量存在于堆内存的对象中; 局部变量随着所属区域的执行而存在, 随着所属区域的结束而释放; 成员变量随着对象的创建而存在, 随着对象的消失而消失; 局部变量没有默认初始化值, 使用前必须先声明和初始化; 成员变量都有默认初始化值(boolean false, char \\u0000(null), byte (byte)0, short (short)0, int 0, long 0L, float 0.0f, double 0.0d, String null); 成员变量可以被 public, 缺省, protected, private, static 等修饰符修饰, 而局部变量不能被访问控制修饰符及 static 所修饰; 但是, 成员变量和局部变量都能被 final 所修饰; == , equals()== 对于基本数据类型, == 可以用来检测两个基本类型的值是否相等, 相等返回 true, 不相等返回 false; (对于两个基本类型的值, 数据类型可以不同) 12345int i = 65;short s = 65;char c = &apos;A&apos;;System.out.println(i == s); // trueSystem.out.println(i == c); // true 但是对于引用数据类型, == 比较的是引用类型数据的地址是否相等 12345678910111213141516171819202122232425262728293031public class test &#123; public static void main(String[] args) &#123; Person p1 = new Person(); Person p2 = new Person(); System.out.println(p1); // Person@1e81f4dc System.out.println(p2); // Person@4d591d15 System.out.println(p1 == p2); // false &#125;&#125;class Person&#123; String name; int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; equals()相较于 ==, 比较常用的还有一个 equals() 方法 equals() 方法只能用于引用类型的比较 对于上面的测试类, 两个对象在使用 equals() 方法比较时, 发现结果还是false, 和 == 没有不同 (难道还是用引用类型数据的地址进行比较?) 1System.out.println(p1.equals(p2)); // false 追踪 equals() : equals() 是 java.lang.Object 基类的方法 而且在 java.lang.Object 基类中, equals() 方法仍然是使用 == 来进行比较123public boolean equals(Object obj) &#123; return (this == obj);&#125; 虽然 Object 的 equals() 方法 和 == 都比较的是引用类型数据的地址, 但事实上, 很多类都对equals()进行了重写 String对equals()的重写 String类 对equals()的重写 (具体方法比较简单, 可以自行去阅读) String类 重写后的 equals(), 比较的不是两个字符串的地址, 而是对字符串中的内容进行比较 1234String s1 = new String(&quot;test&quot;);String s2 = new String(&quot;test&quot;);System.out.println(s1 == s2); // falseSystem.out.println(s1.equals(s2)); // true equals()重写的自动生成对于自定义的类, 如果希望其实例对象能使用 equals() 方法进行内容比较, 而不是地址比较, 你也可以自己重写 equals(), 不过, 不用这么麻烦, 可以使用IDE为我们生成即可 (IntellJ IDE : Code-&gt;Generate-&gt;) String类的内存解析 String s1 = &quot;test&quot; 与 Sting s2 = new String(&quot;test&quot;) 的不同 1234567891011String s1 = new String(&quot;test&quot;);String s2 = new String(&quot;test&quot;);String s3 = &quot;test&quot;;String s4 = &quot;test&quot;;System.out.println(Integer.toHexString(System.identityHashCode(s1))); // 3b22cdd0System.out.println(Integer.toHexString(System.identityHashCode(s2))); // 1e81f4dcSystem.out.println(Integer.toHexString(System.identityHashCode(s3))); // 4d591d15System.out.println(Integer.toHexString(System.identityHashCode(s4))); // 4d591d15System.out.println(s1 == s2); // new了两个不同的对象, 地址自然不同: falseSystem.out.println(s3 == s4); // true 地址为何相同? 上面例子发现 s3和s4 两个String的地址是相同的, 而new出来的两个String的地址不相同, 那么到底在内存总它们有什么不同? 这就涉及到String Pool的概念了: JVM为了提升性能和减少内存开销, 避免字符串的重复创建, 维护了一块特殊的内存空间, 即字符串池(String Pool) 内存图可以简单理解为如下: hashCode 与 equals https://github.com/Snailclimb/JavaGuide/blob/master/Java相关/Java基础知识.md#9-构造器-constructor-是否可被-override 为什么Java中只有值传递 https://github.com/Snailclimb/JavaGuide/blob/master/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87/%E6%9C%80%E6%9C%80%E6%9C%80%E5%B8%B8%E8%A7%81%E7%9A%84Java%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%882018-8-7%EF%BC%89.md","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"05. 数组, String","slug":"JavaSE/2018-12-08-05","date":"2018-12-08T08:01:13.000Z","updated":"2019-01-26T02:55:16.000Z","comments":true,"path":"2018/12/08/JavaSE/2018-12-08-05/","link":"","permalink":"http://blog.renyimin.com/2018/12/08/JavaSE/2018-12-08-05/","excerpt":"","text":"数组 数组是相同类型的多个数据元素的组合 (数据元素可以是基本类型, 也可以是引用类型), 数组属于 引用类型; 数组一旦初始化, 其长度是不可变的; Java中 创建数组 的形式比较多, 有点闹心, 感觉乱乱的, 就总结了一下: 静态创建数组: 在声明数组的时候, 同时指定数组内容, 但无论 分两步还是一步, 都不能在任何地方显示指定长度 123456789101112131415161718192021222324252627282930313233/** * 静态创建数组: 指定数组内容, 但无论 分两步还是一步, 都不能在任何地方显示指定长度 * - 虽然没有指定长度, 但系统会自动识别 * - 虽然直接指定了具体元素的内容, 但在内存中仍然有默认值被替换的过程 * - 静态创建数组有如下几种格式 */// 1. 标准格式int[] age1 = new int[]&#123;1,2,3,4,5&#125;; // 不能指定长度, 如 int[] age1 = new int[5]&#123;1,2,3,4,5&#125;; 会报错System.out.println(age1[0]); // 1// 2. 标准格式的另一种格式int age2[] = new int[]&#123;1,2,3,4,5&#125;; // 不能指定长度, 如 int age1[] = new int[5]&#123;1,2,3,4,5&#125;; 会报错System.out.println(age2[1]); // 2// 3. 标准格式 拆两步int[] age3; // 不能指定长度, 如 int[5] age3; 会报错age3 = new int[]&#123;1,2,3,4,5&#125;; // 不能指定长度, 如 age2 = new int[5]&#123;1,2,3,4,5&#125; 会报错System.out.println(age3[4]); // 5// 4. 标准格式的另一种格式也可以 拆两步int age4[]; // 不能指定长度, 如 int age4[5]; 会报错age4 = new int[]&#123;1,2,3,4,5&#125;; // 不能指定长度, 如 age2 = new int[5]&#123;1,2,3,4,5&#125; 会报错System.out.println(age4[2]); // 3// 5. 简便格式: 虽然没有new, 但内存中仍然会开辟堆空间; 简便格式不能拆两步int[] age5 = &#123;1,2,3,4,5&#125;;System.out.println(age5[1]); //2// 6. 静态创建数组的简便格式还可以如下: (但推荐上一种, 因为它将类型 int[](整型数组) 与 变量名分开了)int a6[] = &#123;1,2,3&#125;;System.out.println(a6[1]); // 2 动态创建数组, 需要显示指定数组长度 12345678910111213/** * 动态创建数组: 需要显示指定数组长度 */// 格式1:int[] age11 = new int[5];// 格式1: 拆两步int[] age12;age12 = new int[5];age11[0] = 1;age12[1] = 2;System.out.println(age11[0]); // 1System.out.println(age12[1]); // 2 不论是静态初始化, 还是动态初始化, 数组在创建后, 长度就定了, 不能改变; 数组长度使用 length 属性: 12345int[] age12;age12 = new int[5];age12[1] = 2;System.out.println(age12[1]); // 2System.out.println(age12.length); // 5 数组元素的默认初始化: byte, short, int, long 默认是 0 float, double 默认是 0.0 char: 空格 boolean: false 引用类型: null 引用类型 String Java没有内置的字符串类型, 而是在标准Java类库中提供了一个预定义类, 叫做 String; 每个用 双引号 括起来的字符串都是 String类 的一个实例; String 简单操作 截取子串: String类的 substring(起始位置, 结束位置) (不包括结束位置) 拼接串: 和绝大多数程序设计语言一样, Java 允许使用 + 连接两个字符串;当将一个字符串与一个非字符串的值进行拼接时, 后者会被转换成字符串(任何Java对象都可以转换成一个字符串) 不可变字符串: String对象一旦被创建就是固定不变的了, 对String对象的任何改变都不影响到原对象, 相关的任何change操作都会生成新的对象 String没有提供用于修改字符串的方法, 如果一个字符串s的值为 ‘Hello’, 如果希望将其内容修改为 ‘Help!’, 在Java中你是不能直接将s的最后两个位置的字符修改为 ‘p’ 和 ‘!’ 在Java中要实现上述操作, 首先提取需要的字符, 然后再批接上要替换的字符 1234567891011String s1 = &quot;Hello&quot;;String s2 = &quot;Hello&quot;;// 一开始两个String的引用是一样的, 都是Hello这个字符串对象的引用System.out.println(System.identityHashCode(s1)); // 992136656System.out.println(System.identityHashCode(s2)); // 992136656System.out.println(System.identityHashCode(&quot;Hello&quot;)); // 992136656// 字符串经过拼接整理后, 其实就是另一个新的字符串对象了s1 = s1.substring(0, 3) + &quot;p!&quot;;System.out.println(s1); // Help!System.out.println(System.identityHashCode(s1));// 511833308 Java虽然不能修改一个字符串中的字符, 但是可以修改字符串变量, 让它引用另外一个字符串; (如上述例子, 其实s指向了一个新的字符串引用) String Pool 虽然看起来修改一个代码单元要比创建一个新字符串更加简洁, 但不可变字符串有一个优点: 编译器可以让字符串共享 JVM为了提升性能和减少内存开销, 避免字符串的重复创建, 维护了一块特殊的内存空间, 即字符串池(String Pool) 检测字符串是否相等 注意: Java中, 你一定不能使用 == 来检测两个字符串是否相等, 这个运算符只能确定两个字符串是否在同一个位置上, 即只能判断两个字符串的引用是否相同; 而Java中完全有可能将两个内容相等的字符串放在不同的位置上; 如果虚拟机始终将相同的字符串共享, 就可以使用 == 运算符检测两个字符串是否相等, 但实际上只有字符串常量是共享的, 而 +, substring 等操作产生的结果并不是共享的; 因此千万不要使用 == 检测字符串的相等行, 以免在程序中出现糟糕的bug, 而且从表面上看, 这种bug很像随机产生的间歇性错误; 在Java中可以使用 s.equals(t) 方法来检测两个字符串是否相等 (相等返回true, 不相等则返回false, s与t可以是字符串常量,也可以是字符串变量) 1234567891011String s12 = &quot;你好!&quot;;String s1 = &quot;你&quot;;String s2 = &quot;好!&quot;;String s1_s2 = s1 + s2;System.out.println(s12); //你好!// System.identityHashCode可以获取对象的内存地址System.out.println(System.identityHashCode(s12)); //511833308System.out.println(s1_s2); //你好!System.out.println(System.identityHashCode(s1_s2)); //1297685781System.out.println(s12 == s1_s2); //falseSystem.out.println(s12.equals(s1_s2)); //true 如果检测两个字符串是否相等而不区分大小写, 可以使用 equalsIgnoreCase 方法: &quot;Hello&quot;.equalsIgnoreCase(&quot;Hello&quot;)","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"04. 杂项练习","slug":"JavaSE/2018-12-08-04","date":"2018-12-08T02:25:46.000Z","updated":"2019-01-28T07:59:00.000Z","comments":true,"path":"2018/12/08/JavaSE/2018-12-08-04/","link":"","permalink":"http://blog.renyimin.com/2018/12/08/JavaSE/2018-12-08-04/","excerpt":"","text":"可变个数的形参 格式, 对于方法的形参: 数据类型... 形参名 可变个数的形参方法与同名方法也构成重载 显然可变个数的形参 方法已经包含了同名方法 可变个数的形参, 在调用时, 参数个数从0开始到无穷多个 一个方法只能有一个可变个数的形参, 并且可变个数的形参要放在不可变个数形参的后面 可变个数形参的方法 和 形参为数组 (参数类型要相同) 的方法其实是一模一样的, 所以不能重载 在调用 可变个数形参 的方法时, 可以传递多个参数, 也可以传递一个数组, 因为 **可变个数形参的方法 和 形参为数组 (参数类型要相同) 的方法是一模一样的 使用可变个数形参的方法后, 部分方法可以省略 示例 12345678910111213141516171819// 下面既然有了可变个数形参的方法, 其实这个方法就可以省略了public void sayHello(String str)&#123; System.out.println(&quot;Hello&quot; + str);&#125;public void sayHello(String... args)&#123; String str = &quot;&quot;; for (int i = 0; i &lt; args.length; i++) &#123; str += args[i]; &#125; System.out.println(&quot;Hello&quot; + str);&#125;// public void sayHello(String[] str)// &#123;// System.out.println(&quot;Hello&quot;);// &#125; toString 方法 toString() 在 Object 类中定义, 其返回值是 String 类型, 返回类名和它的引用地址; 可以根据需要在自定义类中重写 toString() 基本类型转换为String类型时, 调用了对应包装类的 toString() 像String类, 包装类, File类, Date类等都实现了Object类中 toString() 的重写 自定义类的 toString() 也可以像 equals(), 构造器 那样借助IDE生成Junit单元测试 Junit单元测试方法的使用: 基本,包装,String 基本类型要转换到对应的包装类型: 调用包装类的构造器 1234567891011Integer i = new Integer(12);System.out.println(i);Float f = new Float(12.5F);System.out.println(f);// 对于Boolean来讲, 当形参类型为 &quot;true&quot; 时, 返回true, 其余均是falseBoolean b1 = new Boolean(true);Boolean b2 = new Boolean(&quot;true&quot;);Boolean b3 = new Boolean(&quot;100&quot;);System.out.println(b1); // trueSystem.out.println(b2); // trueSystem.out.println(b3); // false 包装类型要转换到对应的基本类型: 调用包装类的方法 xxxValue() 12Integer integer = new Integer(100);int i = integer.intValue(); JDK5.0以后, 基本类型和包装类型可以自动拆箱和装箱 12int i1 = 100;Integer i2 = i1; 基本类型,包装类型 -&gt; String int i1 = 10; String str1 = i1 + &quot;&quot;; // 即可 调用String类的静态的重载的 valueOf(): String str2 = String.valueOf(i1); String -&gt; 包装类型 : 调用包装类型的 parseXXX(String str) (对于基本数字类型, 字符串必须能转为数字才行) String -&gt; 基本类型 String 不能直接到基本类型 需要使用 包装类型的 parseXXX(String str) 先转换为包装类型, 然后再用包装类型到基本类型 12345String s1 = &quot;123&quot;;Integer i1 = Integer.parseInt(s1);int i2 = Integer.parseInt(s1);System.out.println(i1);System.out.println(i2);","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01. 认识 Spring Boot","slug":"Spring boot/2018-12-07-01","date":"2018-12-07T05:42:15.000Z","updated":"2018-12-07T05:49:58.000Z","comments":true,"path":"2018/12/07/Spring boot/2018-12-07-01/","link":"","permalink":"http://blog.renyimin.com/2018/12/07/Spring boot/2018-12-07-01/","excerpt":"","text":"Spring 简介 spring是一个一站式的轻量级java开发框架, 核心是控制反转(IOC)和面向切面(AOP), 针对于开发的WEB层(springMvc)、业务层(Ioc)、持久层(jdbcTemplate)等都提供了多种配置解决方案; SpringMVC springMvc是spring基础之上的一个MVC框架, 主要处理web开发的路径映射和视图渲染, 属于spring框架中WEB层开发的一部分; springMvc属于一个企业WEB开发的MVC框架, 涵盖面包括前端视图开发、文件配置、后台接口逻辑开发等, XML、config等配置相对比较繁琐复杂;Spring Boot简介 Spring 诞生时是 Java 企业版(Java Enterprise Edition, JEE, 也称 J2EE)的 轻量级代替品。无需开发重量级的 Enterprise JavaBean(EJB), Spring 为企业级 Java 开发提供了一种相对简单的方法, 通过依赖注入和面向切面编程, 用简单的Java 对象(Plain Old Java Object, POJO)实现了 EJB 的功能。 虽然 Spring 的组件代码是轻量级的, 但它的配置却是重量级的。 第一阶段：xml配置 在Spring 1.x时代, 使用Spring开发满眼都是xml配置的Bean, 随着项目的扩大, 我们需要把xml配置文件放到不同的配置文件里, 那时需要频繁的在开发的类和配置文件之间进行切换 第二阶段：注解配置 在Spring 2.x 时代, 随着JDK1.5带来的注解支持, Spring提供了声明Bean的注解(例如@Component、@Service), 大大减少了配置量。主要使用的方式是应用的基本配置(如数据库配置)用xml, 业务配置用注解 第三阶段：java配置 Spring 3.0 引入了基于 Java 的配置能力, 这是一种类型安全的可重构配置方式, 可以代替 XML。我们目前刚好处于这个时代, Spring4.x和Spring Boot都推荐使用Java配置。 所有这些配置都代表了开发时的损耗。 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换, 所以写配置挤占了写应用程序逻辑的时间。除此之外, 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了, 你还要知道这些库的哪个版本和其他库不会有冲突, 这难题实在太棘手。并且, 依赖管理也是一种损耗, 添加依赖不是写应用程序代码。一旦选错了依赖的版本, 随之而来的不兼容问题毫无疑问会是生产力杀手。作者：cnn0314来源：CSDN原文：https://blog.csdn.net/m0_37106742/article/details/64438892版权声明：本文为博主原创文章, 转载请附上博文链接！ Spring Boot是由Pivotal团队提供的全新框架, 其设计目的是用来简化新Spring应用的初始搭建以及开发过程; https://blog.csdn.net/forezp/article/details/81040925","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"03. 基本类型转换","slug":"JavaSE/2018-12-05-03","date":"2018-12-05T11:40:16.000Z","updated":"2019-01-28T02:49:51.000Z","comments":true,"path":"2018/12/05/JavaSE/2018-12-05-03/","link":"","permalink":"http://blog.renyimin.com/2018/12/05/JavaSE/2018-12-05-03/","excerpt":"","text":"8种基本数据类型 数值型 byte: Java中最小的数据类型, 在内存中占8位( bit), 即1个字节, 取值范围-128~127, 默认值 0 (对应包装类 Byte) short: 短整型在内存中占16位, 即2个字节, 取值范围-32768~32717, 默认值0 (对应包装类 Short) int: 整型, 用于存储整数, 在内在中占32位, 即4个字节, 取值范围-2147483648~2147483647, 默认值 0 (对应包装类 Integer) long: 长整型, 在内存中占64位, 即8个字节-2^63~2^63-1, 默认值 0L (对应包装类 Long) float: 浮点型, 在内存中占32位, 即4个字节, 用于存储带小数点的数字(与double的区别在于float类型有效小数点只有6~7位), 默认值 0.0 (对应包装类 Float) double: 双精度浮点型, 用于存储带有小数点的数字, 在内存中占64位, 即8个字节, 默认值 0.0 (对应包装类 Double) 注意 JAVA 各 整数, 浮点 类型都有固定的表数范围和字段长度, 不受具体OS的影响, 以保证JAVA程序的可移植性; JAVA 的整型常量默认为int型, 要声明long型常量, 必须在数字后面跟 l或者L JAVA 的浮点型常量默认为double型, 要声明float型常量, 必须在数字后面跟 f或者Ffloat f1 = 12.3; // 这种语句在编译时会报错: 可能损失精度, 要写成 float f1 = 12.3F; Java的数值类型不支持无符号数据类型, byte, short, int和long都是有符号数据类型 (不过, char型是无符号的) 非数值类型 char: 字符型, 用于存储单个字符, 占16位, 即 2个字节, 取值范围0~65535, 默认值为 空字符 (对应包装类 Character)只能表示单个字符 (中文,英文,制表符…), 用单引号括起来 boolean: 布尔类型, 占1个字节, 用于判断真或假(仅有两个值, 即 true、false), 默认值 false (对应包装类 Boolean)不能以0或者非0的整数代替false和true, 这点和C语言不同 基本类型转换 boolean 占有一个字节, 由于其本身所代表的特殊含义, boolean类型与其他基本类型不能进行类型的转换(既不能进行自动类型的提升, 也不能强制类型转换), 否则, 将编译出错; Java中甚至没有提供对基本类型的检测方法, 因此, 使用基本类型的包装类型比较多 从存储范围小的类型到存储范围大的类型: byte -&gt;short(char)-&gt;int-&gt;long-&gt;float-&gt;double 自动类型转换 将 数值范围小的类型的变量 赋给 数值范围大的类型的变量时, 会进行自动类型转换; 而将数值范围大的类型的变量 赋给 数值范围小的类型的变量时, 需要进行强制类型转换 12byte s1 = 12;short s2 = s1; 特例: byte &lt;--!--&gt; char, short &lt;--!--&gt; char char 也可以用数值来赋值 byte 和 short 虽然一个类型比 char 小, 一个和 char 相同, 但由于 byte 和 short 都是有符号类型, 而 char 是 unsigned 型(其数值范围是 0 ~ 2^16-1), 因为负数的问题, 这直接导致byte型不能自动类型提升到char, short和char之间也不会发生自动类型提升; 不过, byte当然可以直接提升到short型 如果是将 默认为int类型的数值字面量 进行赋值, 处理就比较特别: 当赋给一个比int型数值范围小的数值类型变量(byte/char/short)时, 虽然是将大类型转换为小类型, 但是编译器会进行判断, 如果此int型数值超过数值类型k, 那么会直接编译出错, 因为你将一个超过了范围的数值赋给类型为k的变量, k装不下, 而你又没有进行强制类型转换, 当然报错了; 但是如果此int型数值尚在数值类型k范围内, jvm会自定进行一次隐式自动类型转换, 将此int型数值转换成类型k 注意: 字面量的自动类型转换, 也要小心数据溢出导致的精度丢失, 如下 int 转 float 就出现了精度丢失12float b = 2147483646;System.out.println(b); // 2.14748365E9 精度丢失 JAVA运算式中, 类型小于int的变量, 在运算的时候都会被自动转换为int后进行计算 (即, 所有的byte,short,char型的值将被提升为int型) 当运算表达式中除了有数值型的字面量常量, 还有变量时, 运算结果默认会根据 字面量 来决定, 整数为int, 浮点数为 double 12345short s2 = 12.2+s1;// 12.2+s1 运算中的操作数s1会被自动提升为double类型// 12.2+s1 运算结果会成为double类型, 再赋值给byte类型的s2时, 编译器会报告需要强制转换类型的错误// 需要手动进行强制类型转换 short s2 = (short)(12.2 + s1);// 改成这样是不行的, 因为还是 short+short : short s2 = (short)(12.2) + s1; 自动转换具体如下图: 实线表示自动转换时不会造成数据丢失, 虚线则可能会出现数据丢失问题 自动类型提升 (final修饰) 自动类型提升规则: 所有的byte,short,char型的值将被提升为int型 如果两个操作数其中有一个是double类型, 另一个操作就会转换为double类型 否则, 如果其中一个操作数是float类型, 另一个将会转换为float类型 否则, 如果其中一个操作数是long类型, 另一个会转换为long类型 否则, 两个操作数都转换为int类型 被final修饰的变量不会自动改变类型, 当2个final修饰相操作时, 结果会根据左边变量的类型而转化。 测试 123456byte b1=1,b2=2,b3,b6,b8;final byte b4=4,b5=6,b7;b3=(b1+b2); // 根据上述规则, b1会转换成int, b2会转换int, 所以结果是int, 而b3是byte, 所以Type mismatch: cannot convert from int to byteb6=b4+b5; // 根据上述规则, b4,b5都是final不会转换int, 所以结果是byte, b6也是byteb8=(b1+b4); // 根据上述规则, b1会转换成int, b4不会转换int, 所以结果是int, 而b8是byte , 所以Type mismatch: cannot convert from int to byteb7=(b2+b5); // 根据上述规则, b2会转换成int, b5不会转换int, 所以结果还是int, 而b7是byte , 所以Type mismatch: cannot convert from int to byte 强制类型转换 强制类型转换, 即显示的把一个数据类型转换为另外一种数据类型; 当我们需要将数值范围较大的数值类型赋给数值范围较小的数值类型变量时, 由于此时可能会丢失精度, 因此, 需要人为进行转换, 我们称之为强制类型转换; 12int i = 300;byte b = (byte)i; // 300 已经超出了 byte 类型表示的范围, 所以会转换成一个毫无意义的数字 如下, 当运算结果超出类型范围时: 1234int count = 100000000;int price = 1999; // 由于price是int类型, 则下面 count * price 仍然是int, 并且超过了int的范围, 所以最终精度丢失后, 才将结果给了long类型, 所以结果有问题long totalPrice = count * price;System.out.println(totalPrice); 解决方案: 一般把第一个数据转换成范围大的数据类型, 然后再和其他的数据进行运算 1234int count = 100000000;int price = 1999;long totalPrice = (long)count * price; // price这个小类型也会被提升为long// 注意不能写成 long totalPrice = (long) (count * price); 因为 (count * price) 首先就已经超出了int的范围, 在强制转换已经没有意义了 包装类型 Java语言是一个面向对象的语言, 但基本数据类型却是不面向对象的, 它们只是单纯的值, 无法使用Java强大类库中的很多方法, 为与其他对象”接轨”, Java为每个基本数据类型设计了一个对应的类进行代表, 这八个和基本数据类型对应的类统称为包装类(Wrapper Class) 它相当于将基本类型”包装起来”, 使得基本类型具有了对象的性质, 并且为其添加了属性和方法, 丰富了基本类型的操作 包装类均位于 java.lang 包 使用集合类型Collection时, 一定要使用包装类型而非基本类型 java也没有检测基本类型的方法, 但是可以用 instanceof 检测包装类型 包装类型: Integer、Long、Short、Byte、Character、Double、Float、Boolean、BigInteger、BigDecmail 其中 BigInteger、BigDecimal 没有相对应的基本类型, 主要应用于高精度的运算 BigInteger 支持任意精度的整数 BigDecimal 支持任意精度带小数点的运算, 主要用于金钱的计算上 JDK5的新特性: 基本类型与其对应的包装类型之间的会默认使用 自动装箱与拆箱完成 包装类型与基本类型异同 声明方式不同, 基本类型不使用 new 关键字, 而包装类型需要使用 new 关键字来在堆中分配存储空间 存储方式及位置不同, 基本类型是直接将变量值存储在栈中, 而包装类型是将对象放在堆中, 然后通过引用来使用 初始值不同, 基本类型的初始值如int为0, boolean为false, 而包装类型的初始值为 null 使用方式不同, 基本类型直接赋值使用就好, 而包装类型在集合如 Collection、Map 时会使用到 包装类型也具有基本类型面临的类型转换的闹心问题; 基本类型转换为String类型时, 其实是调用了对应包装类的 toString()","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"02. 基础语法","slug":"JavaSE/2018-12-03-02","date":"2018-12-03T11:36:09.000Z","updated":"2019-01-28T06:27:09.000Z","comments":true,"path":"2018/12/03/JavaSE/2018-12-03-02/","link":"","permalink":"http://blog.renyimin.com/2018/12/03/JavaSE/2018-12-03-02/","excerpt":"","text":"main 主函数 格式是固定的, 被jvm所识别和调用 public : 因为权限必须是最大的; static: 不需要对象的, 直接用主函数所属类名调用即可; void: 主函数没有具体的返回值; main: 函数名, 不是关键字, 只是jvm识别的固定的名字; String[] args: 这是主函数的参数列表, 是一个数组类型的参数, 而且元素都是字符串类型; 虚拟机在运行的时候, 可以给主函数传值, 只不过一般情况下不传 main 不光是main 方法, 还可以当做普通方法 123456789101112131415161718public class mainTest &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; args.length; i++) &#123; System.out.println(args[i]); &#125; Main.main(new String[10]);&#125;&#125;class Main&#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; args.length; i++) &#123; args[i] = &quot;value:&quot; + i; System.out.println(args[i]); &#125; &#125;&#125; 编译运行: 12➜ src javac mainTest.java ➜ src java mainTest &quot;yimin&quot; &quot;nihao&quot; 基本语法 逻辑运算只有 &amp;形式与&amp;&amp;(短路)形式 (没有php中的and, or, 一般也只用短路符) switch语句: switch(变量) : switch语句的变量的数据类型只能是 byte, short, int, char, 枚举, String 常量 在Java中, 利用关键字 final 指示常量, 例如: 1234final double CM_PER_INCH = 2.54;double paperWidth = 8.5;double paperHeight = 11;System.out.println(&quot;Paper size in centimeters:&quot; + paperWidth * CM_PER_INCH + &quot; by &quot; + paperHeight * CM_PER_INCH); 用 final 修饰, 表示这个变量只能被赋值一次, 一旦被赋值后就不能再被更改了 (这可能也就是之前为什么使用final修饰的变量不会自动改变类型) 习惯上, 常量名使用全大写 类常量 在Java中, 经常希望某个常量可以在一个类中的多个方法中使用, 通常将这些常量称为 类常量; 可以使用关键字 static final 设置一个类常量, 下面是使用类常量的示例: 1234567891011public class Constants&#123; final static double CM_PER_INCH = 2.54; public static void main(String[] args) &#123; double paperWidth = 8.5; double paperHeight = 11; System.out.println(&quot;Paper size in centimeters:&quot; + paperWidth * CM_PER_INCH + &quot; by &quot; + paperHeight * CM_PER_INCH); &#125;&#125; const 是Java的保留关键字, 但是目前并未使用, 在Java中, 定义常量必须使用 final 关键字;","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"01. 入门","slug":"JavaSE/2018-11-02-01","date":"2018-11-02T00:52:28.000Z","updated":"2019-02-19T07:49:41.000Z","comments":true,"path":"2018/11/02/JavaSE/2018-11-02-01/","link":"","permalink":"http://blog.renyimin.com/2018/11/02/JavaSE/2018-11-02-01/","excerpt":"","text":"三大体系 JAVASE(J2SE): 标准版(Standard Edition), 开发个人计算机上的应用 (基础) (Java5.0版本后, 进行了更名, J2SE-&gt;JAVASE) JAVAEE(J2EE): 企业版(Enterprise Edition), Java EE是在JavaSE的基础上构建的, 提供Web服务、组建模型、管理和通信API, 主要针对企业应用的开发(例如, 电子商务网站、ERP系统), 这也是我们主要面对的版本 JAVAME(J2ME): 微缩版, (例如, 蜂窝电话和可视电话、数字机顶盒、汽车导航系统等等) 跨平台 Java语言的一个非常重要的特点就是与平台的无关性, 通过在JVM中运行Java程序实现跨平台特性, JVM屏蔽了与具体平台相关的信息 Java编译程序只需生成在Java虚拟机上运行的目标代码 .class字节码文件 Java虚拟机在执行字节码时, 把字节码解释成具体平台上的机器指令执行 注意: JVM 并是不跨平台的, 所以我们需要根据具体的平台安装不同版本的JVM虚拟机(下载不同版本的JDK即可) 运行机制 Java程序的运行必须经过 编写 -&gt; 编译 -&gt; 运行 三个步骤 编写Java源代码文件时, 要以 .java 为文件名后缀 Java源代码需要被编译成Java虚拟机能识别的 .class字节码文件 (此过程需要使用JDK里面的 javac.exe 编译器, 对Java源文件进行编译) 然后由JVM把javac.exe编译好的 .class字节码 解释成机器码并运行通过 Java.exe 命令启动JVM, 对编译后的.class类文件进行解释, 不用写.class后缀, 文件名即可 .class字节码文件 是一种和任何具体机器环境及操作系统环境无关的中间代码, 它是一种二进制文件, 但计算机无法直接读懂字节码文件, 它必须由专用的Java解释器来解释执行, 因此Java是一种在编译基础上进行解释运行的语言; Java解释器 负责将字节码文件翻译成具体硬件环境和操作系统平台下的机器代码, 以便执行 采用字节码的好处Java 语言通过字节码的方式, 在一定程度上解决了传统解释型语言执行效率低的问题, 同时又保留了解释型语言可移植的特点所以 Java 程序运行时比较高效, 而且由于字节码并不专对一种特定的机器, 因此, Java程序无须重新编译便可在多种不同的计算机上运行 JVM,JRE,JDK的区别 JVM(Java Virtual Machine Java): JVM 负责将字节码转换为特定机器代码, 提供了内存管理/垃圾回收和安全机制等, 是实现跨平台的核心 JRE(Java Runtime Envrionment): 普通用户而只需要安装 JRE(Java Runtime Environment)来运行 Java 程序 (JRE中会包含JVM), 而程序开发者必须安装JDK来编译、调试程序 JRE包含JVM, 而Java解释器就是JVM的一部分, 在运行Java程序时, 首先会启动JVM, 然后由它来负责解释执行Java的字节码(Java字节码只能运行于JVM之上) JDK (Java Development kit): 顾名思义它是给开发者提供的开发工具箱, 是给程序开发者用的, 它除了包括完整的 JRE(Java运行环境), 还包含了其他供开发者使用的工具包, 作为开发者, 我们要下的就是JDK 两个基础命令 javac.exe 命令的作用其实是在调用JDK当中的编译程序, 编译器就是对java源文件正确性进行检查; 然后把.java的源代码编译成 .class中间字节码文件 的, 编译失败的话,是不会生成.class运行文件的 javac.exe这个编译器是在JDK中才有的; JRE中的 java.exe 命令底层对应的程序是虚拟机, 它一运行就会启动虚拟机, 从而解释某个编译好的.class字节码文件并且交给操作系统运行; JRE在运行.class文件的时候是利用JRE中包含的JVM来解析.class文件的, JRE只是个运行环境, 它不需要把.java源文件编译成.class字节码文件, 所以不需要有 javac.exe; 编译,运行 第一个 .java源代码文件, 注意事项: 一个 .java源文件中, 可以写多个类, 但是最多只能有一个类是 public 类 .java源文件 使用 javac 命令进行编译后, 源文件中的每个类都会生成一个 .class字节码文件 .java源文件的名字 必须和 用 public 修饰的类 名字一致 (否则编译时会报错)如果public类的类名是 Demo, 文件名是 demo.java 也不行, 大小写也要一致 java 命令运行字节码文件时, 不用加 .class 后缀 .class字节码文件 中 是不包含注释的: 有注释的代码 和 无注释的代码通过 javac.exe 编译成的字节码文件大小是一样的 另外还有一个常用命令是 javadoc -d 文件目录名 -author -version 源文件名.java, 用来根据文档注释(/** */)生成文档 classpath 带包名文件编译 如果不使用Eclipse去创建包并运行程序, 而是创建几个java文件并指定包名, 然后手动执行命令, 此时使用 javac 源码文件 会报错 正确方法是: javac -d . 包目录/源码文件.java java 包目录/源码文件名 包名不能用数字, 要小写","categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/categories/JavaSE/"}],"tags":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://blog.renyimin.com/tags/JavaSE/"}]},{"title":"04. MySQL5.7 - YUM 安装","slug":"MySQL/2018-10-13-mysql-04","date":"2018-10-13T11:43:26.000Z","updated":"2019-04-25T09:10:01.000Z","comments":true,"path":"2018/10/13/MySQL/2018-10-13-mysql-04/","link":"","permalink":"http://blog.renyimin.com/2018/10/13/MySQL/2018-10-13-mysql-04/","excerpt":"","text":"YUM 库配置 要在Linux上使用YUM安装MySQL, 可以使用官网提供的 YUM 库 MySQL Yum库提供用于安装 MySQL server, client, MySQL Workbench, MySQL Utilities, MySQL Router, MySQL Shell, Connector/ODBC, Connector/Python 等的RPM软件包 可以通过安装MySQL提供的RPM包, 将MySQL Yum存储库添加到系统的存储库列表中 该RPM包的全称会遵循以下格式包名-MySQL版本号-Linux版本号-处理器(x64还是x86).rpm 官网提供了该RPM包的下载页但页面直接提供的 RPM Package 对应的平台版本可能不是你想要的, 可以尝试点击下载该版本, 通过下载链接找到mysql的yum库地址 Linux不同版本的标记 1234标记 对应Linux版本el6, el7 Red Hat Enterprise Linux/Oracle Linux/CentOS 6/7 |fc26, fc27, fc28 Fedora 26/27/28|sles12 SUSE Linux Enterprise Server 12| 从yum库中选择并下载适用于自己平台的发行包 (本次使用的是centos7) 找到 https://repo.mysql.com//yum/mysql-5.7-community/el/7/x86_64/ 页 (el) 最后找到 mysql57-community-release-el7-10.noarch.rpm 包wget https://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/mysql57-community-release-el7-10.noarch.rpm 安装 RPM 包: yum localinstall mysql57-community-release-el7-10.noarch.rpm (安装命令将MySQL Yum存储库添加到系统的存储库列表中, 并下载GnuPG密钥以检查软件包的完整性) 检查MySQL Yum仓库是否添加成功: yum repolist enabled | grep &quot;mysql.*-community.*&quot; 12345[root@lant ~]# yum repolist enabled | grep &quot;mysql.*-community.*&quot;mysql-connectors-community/x86_64 MySQL Connectors Community 95mysql-tools-community/x86_64 MySQL Tools Community 84mysql57-community/x86_64 MySQL 5.7 Community Server 327[root@lant ~]# 选择你要安装的版本 默认情况下启用最新GA系列(当前为MySQL 5.7)的子存储库, 默认情况下禁用所有其他系列(例如, MySQL 5.6系列)的子存储库; 使用如下命令查看MySQL Yum存储库中的所有子存储库, 并查看哪些子存储库已启用或禁用 123456789101112131415161718[root@lant ~]# yum repolist all | grep mysqlmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community disabledmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - disabledmysql-connectors-community/x86_64 MySQL Connectors Community enabled: 95mysql-connectors-community-source MySQL Connectors Community - disabledmysql-tools-community/x86_64 MySQL Tools Community enabled: 84mysql-tools-community-source MySQL Tools Community - Sourc disabledmysql-tools-preview/x86_64 MySQL Tools Preview disabledmysql-tools-preview-source MySQL Tools Preview - Source disabledmysql55-community/x86_64 MySQL 5.5 Community Server disabledmysql55-community-source MySQL 5.5 Community Server - disabledmysql56-community/x86_64 MySQL 5.6 Community Server disabledmysql56-community-source MySQL 5.6 Community Server - disabledmysql57-community/x86_64 MySQL 5.7 Community Server enabled: 327mysql57-community-source MySQL 5.7 Community Server - disabledmysql80-community/x86_64 MySQL 8.0 Community Server disabledmysql80-community-source MySQL 8.0 Community Server - disabled[root@lant ~]# 可以看到 mysql57 是开启状态, 正是我们要安装的版本 如果需要安装低版本, 如 5.5.62, 此时需要 下载 yum-config-manager: yum -y install yum-utils 关闭5.7的安装 yum-config-manager --disable mysql57-community 开启5.5的安装 yum-config-manager --enable mysql55-community yum repolist all | grep mysql12345678910111213141516mysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community disabledmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - disabledmysql-connectors-community/x86_64 MySQL Connectors Community enabled: 95mysql-connectors-community-source MySQL Connectors Community - disabledmysql-tools-community/x86_64 MySQL Tools Community enabled: 84mysql-tools-community-source MySQL Tools Community - Sourc disabledmysql-tools-preview/x86_64 MySQL Tools Preview disabledmysql-tools-preview-source MySQL Tools Preview - Source disabledmysql55-community/x86_64 MySQL 5.5 Community Server enabled: 427mysql55-community-source MySQL 5.5 Community Server - disabledmysql56-community/x86_64 MySQL 5.6 Community Server disabledmysql56-community-source MySQL 5.6 Community Server - disabledmysql57-community/x86_64 MySQL 5.7 Community Server disabledmysql57-community-source MySQL 5.7 Community Server - disabledmysql80-community/x86_64 MySQL 8.0 Community Server disabledmysql80-community-source MySQL 8.0 Community Server - disabled 安装 如下将安装MySQL服务器(mysql-community-server)的软件包以及运行服务器所需组件的软件包, 包括客户端软件包(mysql-community-client), 客户端和服务器的常见错误消息和字符集(mysql-community-common)和共享客户端库(mysql-community-libs) 1234567891011121314151617181920212223242526yum install mysql-community-server......(1/9): mysql-community-common-5.7.25-1.el7.x86_64.rpm | 274 kB 00:00:00(2/9): mysql-community-libs-5.7.25-1.el7.x86_64.rpm | 2.2 MB 00:00:03(3/9): mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm | 2.0 MB 00:00:04(4/9): net-tools-2.0-0.24.20131004git.el7.x86_64.rpm | 306 kB 00:00:02(5/9): openssl-1.0.2k-16.el7_6.1.x86_64.rpm | 493 kB 00:00:05(6/9): openssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm | 1.2 MB 00:00:06(7/9): postfix-2.10.1-7.el7.x86_64.rpm | 2.4 MB 00:00:08(9/9): mysql-community-server-5.7.25-1.el7.x86_64.rpm 37% [=================================- ] 480 kB/s | 75 MB 00:04:23 ETA......Installed: mysql-community-libs.x86_64 0:5.7.25-1.el7 mysql-community-libs-compat.x86_64 0:5.7.25-1.el7 mysql-community-server.x86_64 0:5.7.25-1.el7Dependency Installed: mysql-community-client.x86_64 0:5.7.25-1.el7 mysql-community-common.x86_64 0:5.7.25-1.el7 net-tools.x86_64 0:2.0-0.24.20131004git.el7Dependency Updated: openssl.x86_64 1:1.0.2k-16.el7_6.1 openssl-libs.x86_64 1:1.0.2k-16.el7_6.1 postfix.x86_64 2:2.10.1-7.el7Replaced: mariadb-libs.x86_64 1:5.5.44-2.el7.centosComplete! 大概熟悉一下MySQL各文件的安装位置 (比如 my.cnf 位置) 123456789[root@lant ~]# rpm -qa | grep &apos;mysql&apos;mysql-community-common-5.7.25-1.el7.x86_64mysql-community-client-5.7.25-1.el7.x86_64mysql57-community-release-el7-10.noarchmysql-community-libs-5.7.25-1.el7.x86_64mysql-community-libs-compat-5.7.25-1.el7.x86_64mysql-community-server-5.7.25-1.el7.x86_64[root@lant ~]# rpm -ql mysql-community-server-5.7.25-1.el7.x86_64.... 启动 成功启动 12[root@lant ~]# sudo service mysqld startRedirecting to /bin/systemctl start mysqld.service 检查MySQL服务状态 12345678910111213141516[root@lant ~]# service mysqld statusRedirecting to /bin/systemctl status mysqld.service● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2019-03-30 03:04:18 GMT; 1min 13s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 5731 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS) Process: 5657 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 5734 (mysqld) CGroup: /system.slice/mysqld.service └─5734 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pidMar 30 03:04:15 lant systemd[1]: Starting MySQL Server...Mar 30 03:04:18 lant systemd[1]: Started MySQL Server.[root@lant ~]# 初始化 创建超级用户帐户 “root”@”localhost”, 超级用户的密码并将其存储在错误日志文件中, 要显示它, 请使用以下命令: 12[root@lant ~]# grep &apos;temporary password&apos; /var/log/mysqld.log2019-03-30T03:04:16.141197Z 1 [Note] A temporary password is generated for root@localhost: sho&gt;kLGZf63# 连接 : mysql -uroot -p (使用上述密码), 即可成功连接 重置密码, 否则不能继续操作 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Lant123456,&#39;; (密码不能过于简单) 熟悉各文件目录 数据库文件: show global variables like &quot;%datadir%&quot;; 日志文件: show variables like &#39;%log_error%&#39;; 慢日志文件: show variables like &#39;%slow_query_log%&#39;;… vagrant 打包(可忽略) 打包环境: vagrant package --output centos7.2_mysql57_yum.box 将包导入(添加)到box列表: vagrant box add mysql57_yum centos7.2_mysql57_yum.box 参考: 官网安装步骤 已经写的比较清楚了","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"01. 认识Elastic-Job","slug":"Elastic-Job/2018-09-15-01","date":"2018-09-15T06:07:21.000Z","updated":"2018-11-22T04:36:35.000Z","comments":true,"path":"2018/09/15/Elastic-Job/2018-09-15-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/15/Elastic-Job/2018-09-15-01/","excerpt":"","text":"引出Elastic-Job 在项目中使用定时任务是避免不了的, 而在部署定时任务时, 由于部署多台机器会导致同一个任务会执行多次, 因此通常只部署一台机器 比如给用户发送邮件的定时任务, 每天定时的给用户下发邮件, 如果部署了多台, 同一个用户将发送多份邮件; 但是只部署一台机器, 可用性又无法保证 Elastic-Job 就可以帮助解决定时任务在集群部署情况下的协调调度问题, 保证任务不重复不遗漏的执行; Elastic-Job简介 Elastic-Job 是当当开源的一款非常好用的分布式调度框架, 有两个互相独立的子项目组成 Elastic-Job-Cloud: 以私有云平台的方式提供集资源、调度以及分片为一体的全量级解决方案, 依赖Mesos和Zookeeper Elastic-Job-Lite: 定位为轻量级无中心化解决方案, 使用jar包的形式提供分布式任务的协调服务 Elastic-Job-Lite 和 Elastic-Job-Cloud 提供同一套API开发作业, 开发者仅需一次开发, 然后可根据需要以Lite或Cloud的方式部署 这里主要以Elastic-Job-Lite进行调研学习 主要功能 定时任务: 基于成熟的定时任务作业框架Quartz cron表达式执行定时任务 作业注册中心: 基于Zookeeper和其客户端Curator实现的全局作业注册控制中心, 用于注册, 控制和协调分布式作业执行 作业分片: 将一个任务分片成为多个小任务项在多服务器上同时执行 弹性扩容缩容: 运行中的作业服务器崩溃或新增加n台作业服务器, 作业框架将在下次作业执行前重新分片, 不影响当前作业执行 支持多种作业执行模式: 支持OneOff, Perpetual和SequencePerpetual三种作业模式 失效转移: 运行中的作业服务器崩溃不会导致重新分片, 只会在下次作业启动时分片, 启用失效转移功能可以在本次作业执行过程中监测其他作业服务器空闲, 抓取未完成的孤儿分片项执行 运行时状态收集: 监控作业运行时状态, 统计最近一段时间处理的数据成功和失败数量, 记录作业上次运行开始时间, 结束时间和下次运行时间 作业停止, 恢复和禁用: 用于操作作业启停, 并可以禁止某作业运行(上线时常用) 被错过执行的作业重触发: 自动记录错过执行的作业, 并在上次作业完成后自动触发, 可参考Quartz的misfire 多线程快速处理数据: 使用多线程处理抓取到的数据, 提升吞吐量 幂等性: 重复作业任务项判定, 不重复执行已运行的作业任务项, 由于开启幂等性需要监听作业运行状态, 对瞬时反复运行的作业对性能有较大影响 容错处理: 作业服务器与Zookeeper服务器通信失败则立即停止作业运行, 防止作业注册中心将失效的分片分项配给其他作业服务器, 而当前作业服务器仍在执行任务, 导致重复执行 Spring支持: 支持spring容器, 自定义命名空间, 支持占位符 运维平台: 提供运维界面, 可以管理作业和注册中心 https://blog.csdn.net/adi851270440/article/details/80493367","categories":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://blog.renyimin.com/categories/ElasticJob/"}],"tags":[{"name":"ElasticJob","slug":"ElasticJob","permalink":"http://blog.renyimin.com/tags/ElasticJob/"}]},{"title":"82. 集群","slug":"rabbitmq/2018-07-03-rabbitmq-82","date":"2018-07-03T11:40:30.000Z","updated":"2018-06-19T12:16:38.000Z","comments":true,"path":"2018/07/03/rabbitmq/2018-07-03-rabbitmq-82/","link":"","permalink":"http://blog.renyimin.com/2018/07/03/rabbitmq/2018-07-03-rabbitmq-82/","excerpt":"","text":"普通模式集群镜像模式集群","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"14. 建立双索引--- text分词 + 排序","slug":"elasticsearch/2018-06-21-14","date":"2018-06-21T11:30:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/21/elasticsearch/2018-06-21-14/","link":"","permalink":"http://blog.renyimin.com/2018/06/21/elasticsearch/2018-06-21-14/","excerpt":"","text":"如果对一个 text 类型的字段进行排序, 由于该字段会进行分词处理, 这样的话, 排序的结果就可能不是我们想要的结果; 通常的解决方案是在建立 mapping 时, 同时为该字段建立两个索引: 一个进行分词用来进行全文检索 一个不进行分词, 用来进行排序 注意使用 &quot;fielddata&quot;: true 练习: 创建mapping, 构造数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253DELETE /mywebsitePUT /mywebsite&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot; : &quot;keyword&quot; # &quot;index&quot;: false &#125; &#125;, &quot;fielddata&quot;: true &#125;, &quot;contennt&quot;: &#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125;PUT /mywebsite/article/1&#123; &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57&#125;PUT /mywebsite/article/2&#123; &quot;title&quot;: &quot;JAVA Language&quot;, &quot;content&quot;: &quot;Java LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-12&quot;, &quot;author_id&quot;: 32&#125;PUT /mywebsite/article/3&#123; &quot;title&quot;: &quot;C Language&quot;, &quot;content&quot;: &quot;c LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-09&quot;, &quot;author_id&quot;: 86&#125;GET /mywebsite/_mapping/article``` - 测试, 如果使用**title字段的不分词索引进行检索**, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title.raw”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容{“took”: 9,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;PHP Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;JAVA Language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;C Language&quot; ] } ]}} 1- 测试, 如果使用title默认的分词索引进行检索, 会发现结果使用的是title的全部内容进行的排序 GET /mywebsite/article/_search{“query”: { “match_all”: {}},“sort”: { “title”: { &quot;order&quot;: &quot;desc&quot; }}} 结果, 注意看sort的内容, 是title字段分词后的内容{“took”: 8,“timed_out”: false,“_shards”: { “total”: 5, “successful”: 5, “failed”: 0},“hits”: { “total”: 3, “max_score”: null, “hits”: [ { &quot;_index&quot;: &quot;mywebsite&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;title&quot;: &quot;PHP Language&quot;, &quot;content&quot;: &quot;Php LANGUAGE is the best Language&quot;, &quot;post_date&quot;: &quot;2018-06-10&quot;, &quot;author_id&quot;: 57 }, &quot;sort&quot;: [ &quot;php&quot; ] }, { ...... &quot;sort&quot;: [ &quot;language&quot; ] }, { ......, &quot;sort&quot;: [ &quot;language&quot; ] } ]}}```","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"13. mapping","slug":"elasticsearch/2018-06-19-13","date":"2018-06-19T11:21:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/19/elasticsearch/2018-06-19-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/elasticsearch/2018-06-19-13/","excerpt":"","text":"mapping核心数据类型 es的文档中, 每个字段都有一个数据类型, 可以是: 一个简单的类型, 如 text, keyword, date, long, double, boolean 或 ip-支持JSON的分层特性的类型,如对象或嵌套 或者像 geo_point, geo_shape 或 completion 这样的特殊类型 为不同目的以不同方式索引相同字段通常很有用, 例如, 字符串字段可以被索引为用于全文搜索的文本字段, 并且可以被索引为用于排序或聚合的关键字字段, 或者, 可以使用标准分析器, 英语分析器和法语分析器索引字符串字段; 之前已经了解过: 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping;为了更准确方便地让es理解我们的意图, 一般我们会对index的type手动来创建mapping mapping操作 GET /products/_mapping/computer 只能在创建index时手动创建mapping, 或者新增field mapping, 但是不能 update filed mapping; 手动创建index并设置mapping 1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot; : &quot;date&quot; &#125;, # 如果不想进行分词, 就设置为 keyword &quot;publisher_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 尝试修改某个字段的mapping, 会报错 1234567891011121314151617181920212223242526272829303132PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; &#125;&#125;# 结果报错&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 测试mapping 测试1 123456789101112131415161718192021222324GET website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 注意 只能在不同的索引中对相同的字段设定不同的datatype, 即便是在同一个index中的不同type, 也不能对相同的field设置不同的datatype;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"12. 分词器","slug":"elasticsearch/2018-06-17-12","date":"2018-06-17T09:05:52.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/17/elasticsearch/2018-06-17-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/17/elasticsearch/2018-06-17-12/","excerpt":"","text":"之前在介绍mapping时, 已经了解到, ES会根据文档的字段类型, 来决定该字段是否需要进行分词和倒排索引, 而分词器的主要工作就是对字段内容进行分词, 通过分词器处理好的结果才会拿去建立倒排索引; ES内置的分词器: standard analyzer simple analyzer whitespace analyzer language analyzer 测试分词器: 12345678910111213141516171819202122232425262728293031GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, # 切换进行测试 &quot;text&quot; : &quot;Test to analyze&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;test&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125; ]&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"11. 诡异的搜索结果 引出 mapping","slug":"elasticsearch/2018-06-16-11","date":"2018-06-16T11:23:16.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-11/","excerpt":"","text":"诡异的搜索结果 构造数据 1234567891011121314151617181920DELETE /websiteGET /website/_mappingPUT /website/article/1&#123; &quot;post_date&quot;: &quot;2018-06-20&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;php is the best language&quot;&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2018-06-21&quot;, &quot;title&quot;: &quot;java&quot;, &quot;content&quot;: &quot;java is the second&quot;&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2018-06-22&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;C++ is third&quot;&#125; 诡异的搜索结果 1234GET /website/article/_search?q=2018 # 3条查询结果GET /website/article/_search?q=2018-06-21 # 3条查询结果GET /website/article/_search?q=post_date:2018-06-22 # 1条查询结果GET /website/article/_search?q=post_date:2018 # 0条查询结果 结果分析 前两个查询之所以能匹配到所有文档, 是因为查询时并没有指定字段进行匹配, 所以默认查询的是_all字段, 而_all是经过分词的并且有倒排索引对于第一个查询来说, 2018 这个值进行分词后还是2018, 自然是可以匹配到所有文档的而对于第二个查询来说, q=2018-06-21 进行分词后也包含2018, 所以也可以匹配到所有文档 对于第三个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 可以匹配到是比较正常的情况 而对于第四个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 但却没有结果 这就要引出 ES 的mapping机制了 Mapping映射 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping; mapping就是index的type的元数据, 每个type都有一个自己的mapping, 决定了该type下文档中每个field的数据类型, 分词及建立倒排索引的行为 以及 进行搜索的行为; ES在自动创建mapping时, 会根据字段值去自行猜测字段的类型, 不同类型的field, 有的是full-text, 有的就是exact-value 对于 full-text型的field, es会对该filed内容进行分词, 时态转换, 大小写转换, 同义词转换等一系列操作后, 建立倒排索引; 对于 exact-value型的field, es则不会进行分词等处理工作 full-text型 和 exact-value型 的不同, 也决定了当你进行搜索时, 其处理行为也是不同的 如果指明要搜索的field, ES也会根据你要搜索的字段的类型, 来决定你发送的搜索内容是否先需要进行全文分析…等一些列操作 当然, 如果你搜索时不指定你具体字段, 则搜索的是 _all, 是会先对你的发送的搜索内容进行分词等操作的 之前诡异的例子中, 其实就是因为在创建文档时, 由于 post_date 字段的值被ES自认为是date类型(exact-value精确值), 所以es不会对这种类型做分词及倒排索引, 所以 GET /website/article/_search?q=post_date:2018 在搜索时候, 其实是去精准匹配post_date字段了, 所以匹配不到; 查看你索引type的默认mapping: 123456789101112131415161718192021222324252627282930313233GET /website/_mapping&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 引出手动创建mapping 除了让es自动为我们创建mapping, 一般我们都是在创建文档前, 先手动去创建index和type, 以及type对应的mapping 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, ES 需要知道每个域中数据的类型 而很显然我们比ES更了解我们的字段类型, ES根据值去判断的话, 很容易出现误判 比如你如果字段是个日期, 可能形式为 2018-06-20 12:13:15 但ES可能不会认为这是个date类型, 如果是 2018-06-20 它又认为是date类型, 所以还是自己手动设置比较好 虽然映射是index的type的, 但事实上, 如果在相同的index中, 即使在不同的type, 你也不能对相同字段做不同的类型指定, 可参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"10. 了解 `_all` field","slug":"elasticsearch/2018-06-16-10","date":"2018-06-16T06:29:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-10/","excerpt":"","text":"_all 对于在学习 query-string 搜索时, 一般这样来用 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc 这种查指定字段进行筛选的方式; 其实ES还可以直接 GET /products/computer/_search?q=diaonao 来进行检索, 这种检索方式会对文档中的所有字段进行匹配; 之所以可以对文档中的所有字段进行匹配, 是 _all 元数据的作用 当你在ES中索引一个document时, es会自动将该文档的多个field的值全部用字符串的方式连接起来, 变成一个长的字符串, 作为 _all field值, 同时对_all分词并建立索引; 之后在搜索时, 如果没有指定对某个field进行搜索, 默认就会搜索 _all field, 而你传递的内容也会进行分词后去匹配 _all 的倒排索引 练习 1234567891011DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 下面的检索都可以搜索到上面的文档GET /products/computer/_search?q=4500GET /products/computer/_search?q=xuhang","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"09. 组合多条件搜索","slug":"elasticsearch/2018-06-16-09","date":"2018-06-16T02:07:26.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-09/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-09/","excerpt":"","text":"查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all : 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match : 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 keyword 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 12345678910111213141516171819202122232425DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot; : &#123; &quot;query&quot;: &quot;language&quot;, &quot;fields&quot;:[&quot;content&quot;, &quot;title&quot;] &#125; &#125;&#125; range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者 keyword字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678910111213141516DELETE /testGET /test/_mapping/languagePUT /test/language/1&#123; &quot;first_name&quot;: &quot;jhon&quot;, &quot;last_name&quot;: &quot;ren&quot;&#125;GET /test/language/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;Jhon&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter, 这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果 (不过, 这只是当should不在must或should下的时候) 只有 must 和 must_not 中的子句是决定了是否能查询出数据 而 should 只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分 (如果should之外的其他语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, # 增加评分 &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 上面那种 should 用法的不同之处 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344DELETE /test/article/1,2,3PUT /test/article/1&#123; &quot;title&quot;: &quot;php is the best language&quot;, &quot;content&quot;: &quot;this language is very easy to learn&quot;, &quot;author_id&quot;: 71&#125;PUT /test/article/2&#123; &quot;title&quot;: &quot;java is the second language&quot;, &quot;content&quot;: &quot;i want to learn java&quot;, &quot;author_id&quot;: 32&#125;PUT /test/article/3&#123; &quot;title&quot;: &quot;C# is very popular&quot;, &quot;content&quot;: &quot;if you want to know something of this language, you can start from C#&quot;, &quot;author_id&quot;: 56&#125;# 下面顺带演示了sort定制排序, 而不是使用默认的相关度排序GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125;GET /test/article/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;term&quot;: &#123;&quot;content&quot;: &quot;language&quot;&#125; &#125; &#125; &#125;, &quot;sort&quot;: &#123;&quot;author_id&quot;: &#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;&#125; a AND (b OR c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句此时的用法和一开始那种不同, 它不仅仅是提升结果评分, 而是直接决定了结果是否匹配 12345678910111213141516GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 123456789101112131415161718192021GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; # 过滤可以使用 constant_score &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; a OR (b AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 1234567891011121314151617181920GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 constant_score &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; 组合过滤 和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"08. 全文检索,结构化精确检索,短语检索,统计 预习","slug":"elasticsearch/2018-06-15-08","date":"2018-06-15T10:56:31.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/15/elasticsearch/2018-06-15-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/elasticsearch/2018-06-15-08/","excerpt":"","text":"查询和过滤 在es中检索文档时候, 对文档的筛选分为 查询 和 过滤, 这两种方式是不太一样的 练习, 搜索商品desc字段中包含 ‘diannao’, 并且售价大于5000的商品 1234567891011121314151617GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot; : &#123;&quot;gt&quot;: 5000&#125; &#125; &#125; &#125; &#125;&#125; 注意: 结构化检索(精确类型字段的检索) 一般会被放到filter过滤语句中, 不会进行分词和相关度排名, 但会对过滤进行缓存 全文检索(全文类型字段的检索) 一般用查询语句进行筛选, 会进行分词和相关度排名 full-text 检索 ES可以进行全文检索并可以进行相关度排名 重新准备数据 1234567891011121314151617181920212223242526272829DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer&quot;, &quot;desc&quot; : &quot;gaoqing hongji diannao&quot;, &quot;price&quot; : 4870, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;chaobao&quot;, &quot;gaoqing&quot;] &#125;PUT /products/computer/3&#123; &quot;name&quot; : &quot;dell&quot;, &quot;desc&quot; : &quot;daier chaoji diannao&quot;, &quot;price&quot; : 5499, &quot;tag&quot; : [&quot;shishang&quot;, &quot;gaoqing&quot;, &quot;gaoxingneng&quot;] &#125;POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125; 练习, 全文检索 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;&#125; 练习 全文高亮检索 12345678910111213GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;desc&quot; : &#123;&#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWbE6HmlWC0s-aachNUv&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;huawei&quot;, &quot;desc&quot;: &quot;china best diannao gaoqing&quot;, &quot;price&quot;: 6080, &quot;tag&quot;: [ &quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;desc&quot;: [ &quot;china best &lt;em&gt;diannao&lt;/em&gt; &lt;em&gt;gaoqing&lt;/em&gt;&quot; ] &#125; &#125;, ...... 结构化精确检索phrase search(短语搜索) 与全文索引不同, 全文索引会对你发送的 查询串 进行拆分(做分词处理), 然后去倒排索引中与之前在存储文档时分好的词项进行匹配, 只要你发送的查询内容拆分后, 有一个词能匹配到倒排索引中的词项, 该词项所对应的文档就可以返回; phrase search(短语搜索)则不会对你发送的 查询串 进行分词, 而是要求在指定查询的字段中必须包含和你发送的查询串一模一样的内容 才算是匹配, 否则该文档不能作为结果返回; 短语搜索 和 结构化搜索还是不一样 结构化搜索是 你的查询串 和 指定的文档字段内容 是完全一致的, 查询串和字段本身都不会做分词, 一般该字段也是精确类型的字段类型; 而 短语搜索 则是, 你的 查询串 不会做分词, 但是你查询的字段可能会做分词, 你的查询串需要包含在 指定字段中; 下一篇可以查看一下terms的用法和效果 搜索商品desc字段中包含 ‘gaoqing diannao’短语 的文档 123456789# 短语检索GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;desc&quot;: &quot;diannao gaoqing&quot; &#125; &#125;&#125; 结果发现, 虽然还是查询的全文字段desc, 但是结果却只有一个 提前了解ES统计语法 统计商品 每个tag下的商品数量, 即, 根据商品的tag进行分组 12345678GET /products/computer/_search&#123; &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 初次运行报错 1234567891011121314151617181920212223242526&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;products&quot;, &quot;node&quot;: &quot;eCgKpl8JRbqwL3QY0Vuz3A&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; &#125; ] &#125;, &quot;status&quot;: 400&#125; 解决方案: 将文本field的 filedata 属性设置为true (现在不用知道这玩意儿, 先尽快解决, 看到聚合分析的预发和效果, 后面讲在详聊该问题) 123456789PUT /products/_mapping/computer&#123; &quot;properties&quot;: &#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 重新执行统计语句, 发现返回中除了分析的结果, 还包含了查询的文档内容; 如果只想显示聚合分析的结果, 可以如下设置size为0: 123456789GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 针对名称中包含”china”的商品, 计算每个tag下的商品数 12345678910111213GET /products/computer/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;gaoqing&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot; : &#123; &quot;terms&quot; : &#123;&quot;field&quot;: &quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格 (先分组, 再计算每组的平均值) 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot; : &#123; &quot;avg&quot; : &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格, 并且按照平均价格进行排序 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;, &quot;order&quot;: &#123;&quot;avg_by_price&quot;:&quot;desc&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 按照指定的价格范围区间进行分组, 然后再每个分组内再按照tag进行分组, 最后在计算每组的平均价格 1234567891011121314151617181920212223242526GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ &#123;&quot;from&quot;:4500, &quot;to&quot;:5000&#125;, &#123;&quot;from&quot;:5000, &quot;to&quot;:5500&#125;, &#123;&quot;from&quot;:5500, &quot;to&quot;:6100&#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;:&#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;4500.0-5000.0&quot;, &quot;from&quot;: 4500, &quot;to&quot;: 5000, &quot;doc_count&quot;: 2, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4870 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5000.0-5500.0&quot;, &quot;from&quot;: 5000, &quot;to&quot;: 5500, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5500.0-6100.0&quot;, &quot;from&quot;: 5500, &quot;to&quot;: 6100, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"07. 查询小优化","slug":"elasticsearch/2018-06-14-07","date":"2018-06-14T02:50:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/14/elasticsearch/2018-06-14-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/14/elasticsearch/2018-06-14-07/","excerpt":"","text":"由于你的每个查询操作都可能会被转发到不同node的shard去执行, 现在假设你的查询, 会打到不同的10个shard上, 每个shard上都要花费1秒钟才能出结果, 这样你总共10s后才会给用户响应, 如果是个商品列表, 用户体验就会非常差 假设本来需要在10秒钟拿到100条数据(每个shard上10条), 现在你可以设置让es在1秒钟就让请求返回, 只拿到部分数据即可 此时可以在查询请求时跟上 timeout 参数(10ms, 1s， 1m): GET /_search?timeout=1ms (可灌入大量数据做测试) 深度分页问题: 假设你的列表每页展示20条数据, 总共1万页, 当我们在使用ES进行分页搜索时, 你想查询第9900页的那20条数据当你的请求到达第一个协调节点后, 它会要求ES给你返回所有该索引对应的primary-shard上的前9900页的数据, 然后es在内存中排序后, 这样会大量占用当前协调节点的计算机资源, 所以尽量避免出现这种深度分页的查询;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"12. 惰性队列","slug":"rabbitmq/2018-06-14-rabbitmq-12","date":"2018-06-13T11:14:30.000Z","updated":"2018-06-13T11:15:08.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-14-rabbitmq-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-14-rabbitmq-12/","excerpt":"","text":"https://www.rabbitmq.com/lazy-queues.html","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"10. 消息拉取和推送","slug":"rabbitmq/2018-06-11-rabbitmq-10","date":"2018-06-12T06:36:51.000Z","updated":"2018-06-13T09:24:58.000Z","comments":true,"path":"2018/06/12/rabbitmq/2018-06-11-rabbitmq-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/12/rabbitmq/2018-06-11-rabbitmq-10/","excerpt":"","text":"","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"06. mget, bulk 批量操作","slug":"elasticsearch/2018-06-10-06","date":"2018-06-10T09:06:39.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-06/","excerpt":"","text":"mget 批量查询 批量查询可以只发送一次网络请求, 返回多条查询结果, 能大大缩减网络请求的性能开销 练习 : 12345678GET /_mget&#123; &quot;docs&quot; : [ &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:1&#125;, &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:2&#125;, &#123;&quot;_index&quot;:&quot;blogs&quot;,&quot;_type&quot;:&quot;php&quot;,&quot;_id&quot;:1&#125; ]&#125; bulk 语法: 每个操作要两个json串, 语法如下: 12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; 可以执行的操作类型如: delete: 删除一个文档, 只要一个json串就可以了 create: PUT /index/type/id/_create 创建, 存在会报错 index: 即普通的 put 操作, 可以是创建也可以是全量替换文档 update: 执行部分字段更新 练习: 12345678910111213141516171819DELETE /productsPUT /products/computer/1 # 先创建一个文档&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;GET /products/computer/_searchPOST /products/_bulk&#123;&quot;delete&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 1&#125;&#125; # 删除id为1的文档 (1行json即可)&#123;&quot;create&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 2&#125;&#125; # 创建id为2的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-create-test2&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;&#125;&#125; # 创建一个文档 (es生成id, 2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3&#125;&#125; # 创建一个id为3的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;, &quot;test_field2&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;update&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3, &quot;_retry_on_conflict&quot;: 3 &#125;&#125; # 更改id为3的文档中的test_field字段&#123;&quot;doc&quot; : &#123;&quot;test_field&quot; : &quot;_bulk-index-update-test3&quot;&#125;&#125; bulk操作中, 任何一个操作失败, 不会影响其他的操作, 但是在返回结果里会有异常日志 bulk的请求会被加载到内存中, 所以如果太大的话, 性能反而会下降, 因此需要通过反复测试来获取一个比较合理的bulk size, 一般从1000~5000条数据开始尝试增加数据; 如果看大小的话, 最好在5-15M之间;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"05. ES的搜索方式 Query-string 与 query DSL, multi-index, multi-type搜索模式","slug":"elasticsearch/2018-06-10-05","date":"2018-06-10T06:33:46.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-05/","excerpt":"","text":"Query-string 搜索 之所以叫 query-string, 是因为search的参数都是以http请求的 query-string 来传递的 练习, 搜索全部商品 GET /products/computer/_search 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc query-string这种搜索比较适合在命令行使用curl快速地发一个请求来检索信息, 如果查询比较复杂, 一般不太适用, 正式开发中比较少用; query DSL DSL(Domain Specified Language): 领域特定语言 (这里即 ES的领域特定语言), 是在HTTP的请求体中通过json构建查询语法, 比较方便, 可以构建各种复杂语法; 练习, 查询所有商品 123456GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 1234567891011GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &quot;desc&quot;&#125; ]&#125; 练习, 分页查询商品 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot; : 0, &quot;size&quot; : 2&#125; 练习, 指定需要返回的字段 (使用_source元数据: 可以指定返回哪些field) 1GET /products/computer/1?_source=name,price 1234567GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot; : [&quot;name&quot;, &quot;desc&quot;, &quot;tag&quot;]&#125; query DSL 可以在HTTP请求体中构建非常复杂的查询语句, 所以比较常用; 更多复杂用法后面会聊到; multi-index, multi-type搜索模式 GET /_search : 检索所有index, 所有type下的数据 GET /index/_search : 指定一个index, 搜索其下所有type的数据 GET /index1,index2/_search : 指定多个index, 搜索他们下面所有type的数据 GET /index1,index2/type1,type2/_search : 指定多个index, 搜索他们下面指定的多个type的数据 _all/type1,type2/_search : 搜索所有index下指定的多个type的数据","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"01. 认识 ZooKeeper","slug":"zookeeper/2018-06-10-01","date":"2018-06-10T05:21:09.000Z","updated":"2019-01-25T09:23:03.000Z","comments":true,"path":"2018/06/10/zookeeper/2018-06-10-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/zookeeper/2018-06-10-01/","excerpt":"","text":"ZooKeeper (译名为 动物园管理员) 企业级应用系统中, 随着信息化水平的不断提高, 企业级系统变得越来越庞大臃肿, 性能急剧下降, 拆分系统是目前我们可选择的解决系统可伸缩性和性能问题的唯一行之有效的方法但是拆分系统同时也带来了系统的复杂性, 各子系统不是孤立存在的, 它们彼此之间需要协作和交互, 这就是我们常说的分布式系统各个子系统就好比动物园里的动物, 为了使各个子系统能正常为用户提供统一的服务, 必须需要一种机制来进行协调——这就是ZooKeeper(动物园管理员) ZooKeeper 概述: ZooKeeper 是一个开源的分布式协调服务, 由雅虎创建, 是 Google Chubby 的开源实现; 是Hadoop和Hbase的重要组件; Zookeeper特色 ZooKeeper是一个分布式小文件系统, 并且被设计为高可用性。通过选举算法和集群复制可以避免单点故障, 由于是文件系统, 所以即使所有的ZooKeeper节点全部挂掉, 数据也不会丢失, 重启服务器之后, 数据即可恢复; ZooKeeper的节点更新是原子的, 也就是说更新不是成功就是失败。通过版本号, ZooKeeper实现了更新的乐观锁, 当版本号不相符时, 则表示待更新的节点已经被其他客户端提前更新了, 而当前的整个更新操作将全部失败; 当然所有的一切ZooKeeper已经为开发者提供了保障, 我们需要做的只是调用API。与此同时, 随着分布式应用的的不断深入, 需要对集群管理逐步透明化监控集群和作业状态, 可以充分利ZK的独有特性; 应用场景 分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、配置维护, 名字服务、分布式同步、分布式锁和分布式队列等功能;","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.renyimin.com/categories/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.renyimin.com/tags/zookeeper/"}]},{"title":"04. 简单尝试 CURD","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T02:36:57.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"Cat Api ES提供的 Cat Api 可以用来查看 集群当前状态, 涉及到 shard/node/cluster 几个层次 尝试使用 GET /_cat/health?v 查看 时间戳、集群名称、集群状态、集群中节点的数量 等等 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1540815645 20:20:45 elasticsearch yellow 1 1 6 6 0 0 6 0 - 50.0% 返回信息 和 集群健康API(GET _cluster/health) 返回都一样 索引文档ES 中可以使用 POST 或 PUT 来索引一个新文档, 熟悉HTTP协议的话, 应该知道 PUT是幂等的, 而POST是非幂等的, ES也遵循了这一点 PUT PUT 创建文档的时候需要手动设定文档ID (类似已知id, 进行修改) 如果文档不存在, 则会创建新文档; 如果文档存在, 则会覆盖整个文档 (所以需要留意) 虽然使用PUT可以防止POST非幂等引起的多次创建, 但也要留意使用PUT带来的文档覆盖问题 练习: 12345678910111213141516171819202122# 此处创建一个 索引为 products , 类型为 computer, 文档ID为1的商品 PUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 返回&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, # 表示应该写入的有两个分片(1个主分片和1个副本分片, 但注意: 这里代表的可不是总分片数, 显然es的索引默认对应5个主分片, 每个主分片又对应一个副本分片, 总共会有10个分片) &quot;successful&quot;: 1, # 表示成功写入一个分片, 即写入了主分片, 但是副本分片并未写入, 因为目前只启了一个节点 &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 另外, 注意: 使用PUT创建文档时, 如果不指定ID, 则会报错 POST POST 创建文档时不需要手动传递文档ID, es会自动生成全局唯一的文档ID 练习 12345678910111213141516171819202122POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125;# 返回, 可以看到文档ID是自动生成的, 其他字段和使用`PUT`时返回的信息相同&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWa_MgAhWC0s-aachNUS&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 检索文档先尝试最简单的一种 query-string 查询方式: GET /products/computer/_search : 查询/products/computer/下的所有文档 更新文档 PUT、POST PUT 对整个文档进行覆盖更新 1234PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer-hongji&quot;&#125; partial update: 如果只是想更新文档的部分指定字段, 可以使用 POST 结合 _update : (partial update内置乐观锁并发控制) 123456POST /products/computer/2/_update?retry_on_conflict=5&#123; &quot;doc&quot;: &#123; &quot;name&quot; : &quot;acer-hongji-鸿基&quot; &#125;&#125; 这里注意一下_update的内部机制其实是: es先获取整个文档, 然后更新部分字段, 最后老文档标记为deleted, 然后创建新文档此时在标记老文档为deleted时就可能会出现并发问题, 如果线程1抢先一步将老文档标注为deleted, 那么线程2在将新文档标注为deleted时就会失败(version内部乐观锁机制)此时在es内部会做处理, 他内部完成了对乐观锁的实现, 如果失败后, 其实也是进行重试, 你可以手动传递 retry_on_conflict参数来决定其内部的重试次数 PUT如何只创建不替换: 由于创建文档与全量替换文档的语法是一样的, 都是 PUT, 而有时我们只是想新建文档, 不想替换文档 可以使用 op_type=create 来说明此命令只是用来执行创建操作的PUT /index/type/id?op_type=create 或 PUT /index/type/id/_create 可以看到, 此时, 如果文档已经存在, 会进行报错提示冲突, 而不会帮你直接替换1234567PUT /products/computer/1?op_type=create&#123; &quot;name&quot; : &quot;huawei create&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing create&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;, &quot;create&quot;] &#125; 删除文档 ES的文档替换: 上面已经了解过, 其实就是PUT创建文档, 如果传递的文档id不存在, 就是创建, 如果文档id已经存在, 则是替换操作; 注意: es在做文档的替换操作时, 会将老的document标记为deleted, 然后新增我们给定的那个document, 当后续创建越来越多的document时, es会在适当的时机在后台自动删除标记为delete的document; ES的删除: 不会直接进行物理删除, 而是在数据越来越多的时候, es在合适的时候在后台进行删除 练习: 123456789101112131415DELETE /products/computer/2# 返回&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"03. ES 一些基本概念","slug":"elasticsearch/2018-06-09-03","date":"2018-06-09T10:23:07.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-03/","excerpt":"","text":"近实时 从文档被索引到可以被检索会有轻微延时, 约1s Index(索引 n) 这里的Index是个名词, 类似于传统RDS的一个数据库, 是存储document的地方 一个Index可以包含多个 type (索引的复数词为 indices 或 indexes) index 名称必须是小写, 不能用下划线开头, 不能包含逗号 一般将不同的项目数据放到不同的index中 每个index会物理地对应多个分片, 这样, 每个项目都有自己的分片, 互相物理地独立开, 如果有项目是做复杂运算的, 也不会影响其他项目的分片 索引(v) : ES中的还会提到 索引一个文档, 这里的 索引 是动词, 存储文档并建立倒排索引的意思; Type(类型) 一个Index中可以有多个type 代表document属于index中的哪个类别(type 可以对同一个index中不同类型的document进行逻辑上的划分,可以粗略地理解成传统数据库中的数据表?) 名称可以是大小写, 不能用下划线开头, 不能包含逗号 注意: type是对index做的逻辑划分, 而shard是对index做的物理划分 Document(文档) ES中的最小数据单元, ES使用 JSON 作为文档的序列化格式 (ES中的文档可以通俗地理解成传统数据库表中的一条记录) _id: 文档id 可以手动指定, 也可以由es为我们生成; 手动指定id: 根据应用情况来判断是否符合手动指定 document id, 一般如果是从某些其他的系统中导入数据到es, 就会采用这种方式, 就是使用系统中已有的数据的唯一标识作为es中的document的id;比如从数据库中迁移数据到es中, 就比较适合采用数据在数据库中已有的primary key;put /index/type/id 自动生成id: 如果说我们目前要做的系统主要就是将数据存储到es中, 数据产生出来以后直接就会存放到es, 所以不需要手动指定document id的形式, 可以直接让es自动生成id即可;post /index/typees自动生成的id长度为20个字符, URL安全, base64编码, GUID, 分布式并行生成时, es会通过全局id来保证不会发生冲突; Cluster(集群) 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 一个ES节点属于哪个集群, 是由其配置中的 cluster.name 决定的; 节点启动后, 其默认name是elasticsearch, 因此如果在一个机器中启动一堆节点, 那它们会自动组成一个es集群(因为它们的cluster.name都是elasticsearch) 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; Shard(分片): type是对index做的逻辑划分, 而shard是对index做的物理划分 一个分片就是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 分片是数据的容器, 文档其实是保存在分片中的: 当我们将很多条document数据添加到索引中时, 索引实际上是指向一个或者多个物理分片; 因此, 你要存储到索引中的数据其实会被分发到不同的分片中, 而每个分片也仅保存了整个索引中的一部分文档; 当你的集群规模扩大或者缩小时(即增加或减少节点时), ES 会自动的在各节点中迁移分片, 而数据是存放在shard中的, 所以最终会使得数据仍然均匀分布在集群里 shard 可以分为 primary shard(主分片), replica shard(副本分片) replica shard 可以容灾, 水平扩容节点时, 还可以自动分配来提高系统负载 默认情况下, 每个index有5个parimary shard, 而每个parimary shard都有1个replica shard, 即每个index默认会对应10个shard 另外, ES规定了, 每个index的 parimary shard 和 replica shard 不能在全部都在同一个节点上, 相同内容的 replica shard 也不能在同一节点上, 不然起不到容灾作用; 集群状态 yellow 在ES中, 每个索引可能对应多个主分片, 每个主分片也都可能对应多个副本分片 对于每个索引, 要保证不会导致es集群为 yellow, 需要注意: es节点数 &gt;= number_of_replicas+1 当索引的 `number_of_replicas=1` 时, 无论 `number_of_shards` 为多少, 2个节点 (`es节点数 = number_of_replicas+1`) 就可以保证集群是 green; 当索引的 `number_of_replicas&gt;1` 时, 只有当 `es节点数 = number_of_replicas+1` 时, 集群才会变为green; 对于任何一个索引, 由于任何具有相同内容的分片(相同主分片的两个副本分片, 或者主分片和其某个副本分片)不会被放在同一个节点上, 所以如果节点数量不够的话, 有些replica-shard分片会处于未分配状态, 集群状态就不可能是green而是yellow; 比如索引 test 有 3个主分片, 每个主分片对应3个副本分片(该索引总共 3+3*3=12 个分片), 那么至少得4(number_of_replicas+1)个节点, 才能保证每个节点上都不会出现具有相同内容的分片, 即可以保证集群是green;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"02. ES 版本选择及简单安装","slug":"elasticsearch/2018-06-09-02","date":"2018-06-09T06:56:01.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-02/","excerpt":"","text":"版本选择 ES 的版本迭代比较快, 目前(06/2018)为止, 已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X; 此处之所以选用5.5.3来学习调研, 主要是因为公司选用的阿里云服务提供的是 ES 5.5.3版本 (所以你在选择版本时, 也可以根据 自建、购买云服务 来决定) 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (此处使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错(调研使用的是 centos7-minimal 版) 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加如下两行配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 12345678910111213&#123; &quot;name&quot; : &quot;rjAFeY9&quot;, # node 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, # 节点默认的集群名称 (可以在es节点的配置文件elasticsearch.yml中进行配置) &quot;cluster_uuid&quot; : &quot;zaJApkNPRryFohhEMEVH5w&quot;, &quot;version&quot; : &#123; # es 版本号 &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 上面未解释的信息暂时先不用了解 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本和集群相关的信息, 但如果能有一个可视化的环境来操作它可能会更直观一些, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装Kibana 是一个开源的分析和可视化平台, 属于 Elastic stack 技术栈中的一部分, Kibana 主要提供搜索、查看和与存储在 Elasticsearch 索引中的数据进行交互的功能, 开发者或运维人员可以轻松地执行高级数据分析, 并在各种图表、表格和地图中可视化数据;接下来主要就是使用Kibana的DevTools提供的控制台进行ES的学习 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 (因为kibana有个副本分片并没有处于正常状态, 因为当前只有一个节点, 副本分片无法被分配到其他节点, 具体细节先不用着急, 后面会进行分析) 小结到此为止, 应该对ES有了最基础的了解, 且基本环境已经安装完毕, 对于后续的练习暂时就够了","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"01. 初识 Elasticsearch","slug":"elasticsearch/2018-06-09-01","date":"2018-06-09T06:24:25.000Z","updated":"2018-11-22T03:46:48.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-01/","excerpt":"","text":"可以通过如下几个特点来认识ES: 开源 基于 Lucene, 提供比较简单的Restful APILucene 可以说是当下最先进、高性能、全功能的搜索引擎库, 由Apache软件基金会支持和提供(更多细节自行了解)但Lucene非常复杂, ES的目的是使全文检索变得简单, 通过隐藏 Lucene 的复杂性, 取而代之的提供一套简单一致的 RESTful API 高性能全文检索和分析引擎, 并可根据相关度对结果进行排序 可以快速且 近实时 地存储,检索(从文档被索引到可以被检索只有轻微延时, 约1s)以及分析 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级 结构化 或 非结构化 数据 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引 应用场景 当你的应用数据量很大, 数据结构灵活多变, 数据之间的结构比较复杂, 如果用传统数据库, 可能不仅需要面对大量的表设计及数据库的性能问题, 此时可以考虑使用ES, 它不仅可以处理非结构化数据, 而且可以帮你快速进行扩容, 承载大量数据; 具体比如多数据源聚合大列表页: 微服务架构是目前很多公司都采用的架构, 所以经常会面对 多数据源聚合的 大列表页, 一个列表中的筛选字段,展示字段可能会来自多个服务, 同时涉及到分页, 所以传统方案可能比较吃力, 而且也得不到比较好的效果; (RRC这边目前是使用 ES 做 数据视图服务, 对这种大列表页所用到的数据源字段做统一配置和聚合) 日志数据分析, RRC 使用 ElasticStack 技术栈来很方便地对各服务的日志进行查询,分析,统计; 站内搜索(电商, 招聘, 门户 等等)都可以使用 ES 来做全文检索并根据相关性进行排名, 高亮展示关键词等;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch基础","slug":"Elasticsearch基础","permalink":"http://blog.renyimin.com/tags/Elasticsearch基础/"}]},{"title":"RabbitMQ内置集群","slug":"rabbitmq/2018-06-09-rabbitmq-15","date":"2018-06-09T02:00:35.000Z","updated":"2018-06-09T02:03:49.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-15/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-15/","excerpt":"","text":"集群中: 如果队列被持久化, 而该节点掉线, 如果消费者尝试创建该队列, 并且也是持久化的, 则会报404; 但是貌似在单节点环境不会报错; 而如果消费者尝试创建该队列时, 设置的是非持久化, 则貌似能创建成功, 此时就是一个新的队列了, 由于没有去恢复节点, 所以节点中的队列等内容还是没有恢复;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"05. 一条消息的一生","slug":"rabbitmq/2018-05-28-rabbitmq-05","date":"2018-05-28T09:16:23.000Z","updated":"2018-06-12T08:06:13.000Z","comments":true,"path":"2018/05/28/rabbitmq/2018-05-28-rabbitmq-05/","link":"","permalink":"http://blog.renyimin.com/2018/05/28/rabbitmq/2018-05-28-rabbitmq-05/","excerpt":"","text":"到现在, 已经成功安装运行了Rabbit, 并且简单了解了RabbitMQ的消息通信机制及Rabbit内部的一些组件; 接下来就需要通过编码来学习如何使用RabbitMQ简单发布并消费一条消息;本系列文章代码Demo使用 Laravel5.5 + php-amqplib来做演示; 生产者应用程序 先尝试创建一个生产者, 其基本步骤可以分为如下几步 123456789创建与Rabbit Server的TCP连接创建信道通过信道--创建exchange通过信道--创建queue通过信道--对exchange与queue进行绑定创建消息通过信道--发布消息关闭信道关闭连接 代码已上传至Github中的 firstProducer() 方法 消费者应用程序 再尝试创建一个消费者, 其基本步骤可以分为如下几步 1234567创建与Rabbit Server的TCP连接创建信道通过信道--创建exchange通过信道--创建queue通过信道--对exchange与queue进行绑定创建消息回调函数消费消息 代码已上传Github 参数讲解在上面的生产者的小实例中, 对 声明交换器、声明队列、绑定交换机与队列、声明消息、消息发布 的一些基础参数属性进行了注释讲解; 声明交换器 passive 参数 默认为false: rabbit-server 会查看有没有已存在的同名exchange, 没有则直接创建, 有则不会进行创建; 结果总是返回 null; 如果你只是希望查询一下交换机是否存在, 而不是创建这个交换机, 设置为true即可; 如果存在则返回NULL, 如果交换机不存在, 则会抛出如下异常: 12PhpAmqpLib \\ Exception \\ AMQPProtocolChannelException (404)NOT_FOUND - no exchange &apos;ex1&apos; in vhost &apos;/&apos; 所以一般这个选项比较少用; 另外需要了解的是, passive设置为true时, 如果exchange已存在, 你当前创建的exchange即使是诸如type之类的参数进行了变更, 也不会报错, 因为压根不会尝试创建, 只是返回null; 但是如果passive设置为false, 则当前会尝试创建exchange, 此时, 如果exchange的参数有变更,比如已存在的exchange的type为direct, 而当前创建的同名exchange的type为fanout, 就会报错: 1PRECONDITION_FAILED - inequivalent arg &apos;type&apos; for exchange &apos;ex1&apos; in vhost &apos;/&apos;: received &apos;fanout&apos; but current is &apos;direct&apos; - 可参考[此文](https://www.kancloud.cn/xsnet/xinshangjingyan/297803) druable(消息持久化的条件之一) : true为持久化 auto_delete: 自动删除(默认是启用的, 交换器将会在所有与其绑定的队列被删除后自动删除; (所以如果做持久化的话, 需要设置为false) arguments : 声明exchange时, 可以使用AMQPTable对象来创建一些额外的说明参数, 比如: 12345678$arguments = new AMQPTable([ &apos;arguments1&apos; =&gt; &apos;想写什么信息都行&apos;, &apos;arguments2&apos; =&gt; [ &apos;想写什么信息都行, 比如声明是那条业务线的&apos;, &apos;想写什么信息都行, 比如连接信息....&apos;, ] ]); internal : 稍后实例解释 nowait : 稍后实例解释 声明队列 passive 参数 (貌似和exchange的passive稍有不同) 默认为false: rabbit-server 会查看有没有已存在的同名queue, 没有则直接创建, 有则不进行创建; 无论创建与否, 结果都返回 队列基础信息 1234array (size=3) 0 =&gt; string &apos;queue1&apos; (length=6) 1 =&gt; int 5 2 =&gt; int 0 如果你希望查询队列是否存在, 而又不想在查询时创建这个队列, 设置此为true即可; 如果存在则返回 队列基础信息, 如果队列不存在, 则会抛出一个错误的异常; 另外需要了解的是, passive设置为true时, 如果queue已存在, 你当前创建的queue即使是诸如type之类的参数进行了变更, 也不会报错, 因为压根不会尝试创建, 只是返回已存在的queue信息; 但是如果passive设置为false, 则当前会尝试创建queue, 此时, 如果queue的参数有变更,比如已存在的queue的为持久型, 而当前创建的同名queue的为非持久的, 就会报错: 1PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;queue1&apos; in vhost &apos;/&apos;: received &apos;false&apos; but current is &apos;true&apos; 和 exchange 的 passive 参数不同的是, 此处队列声明的结果会返回 队列基础信息, 但是这是依赖于 nowait 参数, 如果nowait参数为默认值false, 则会返回, 如果为true, 则就返回null; druable(消息持久化的条件之一) : true为持久化 exclusive : 如果设置为true, 则创建的为 排他队列 如果一个队列被声明为排他队列, 该队列仅对首次声明它的连接可见, 并在连接断开时自动删除。也就是说, 如果你在生产者中创建排他队列, 则连接结束, 队列就没了, 所以你可能一直看不到创建的队列; 另外需要注意三点: 1.排他队列是基于连接可见的, 同一连接的不同信道是可以同时访问同一个连接创建的排他队列的 2.如果一个连接已经声明了一个排他队列, 其他连接是不允许建立同名的排他队列的, 这个与普通队列不同 3.即使该队列是持久化的,一旦连接关闭或者客户端退出,该排他队列都会被自动删除的 所以, 貌似排他队列只能由消费者创建, 而且这种队列适用于只有一个消费者消费消息的场景 暂时没有找到特别适合的场景~~ auto_delete: 自动删除(默认是启用的, 队列将会在所有的消费者停止使用之后自动删除掉自身, 注意: 没有消费者不算, 只有在有了消费之后, 所有的消费者又断开后, 就会自动删除自己) arguments : 声明queue时, 可以使用AMQPTable对象来创建一些额外的说明参数, 同exchange的arguments参数效果一样; 创建消息AMQPMessage 类的第二个参数properties可以设置很多属性, 目前需要熟悉的是 delivery_mode : 消息持久化的条件之一, 值为2时表示持久化 content_type : 比如: ‘text/plain’消息发布消息暂时接触到的属性都比较熟悉, 稍后会接触一些特殊作用的属性 mandatory 当mandatory标志位设置为true时, 如果exchange根据自身类型和消息routeKey无法找到一个符合条件的queue, 那么rabbit server会自动去调用 basic.return 方法将消息返回给生产者(Basic.Return + Content-Header + Content-Body);生产者可以使用channel的set_return_listener()绑定一个回调函数来进行监听, 但是注意, 此时生产者貌似需要为阻塞状态在命令行启动, 所以, 我是不会这么写一个生产者的… 当mandatory设置为false时, 出现上述情形broker会直接将消息扔掉; 代码已上传github, php-amqplib文档Demo php-amqplib并未实现的immediate 当immediate标志位设置为true时, 如果exchange在将消息路由到queue(s)时发现对应的queue上没有消费者, 那么这条消息不会放入队列中, 当与消息routeKey关联的所有queue(一个或者多个)都没有消费者时, 该消息会通过 basic.return 方法返还给生产者; 但是可惜的是, 这只是AMQP的规定, 客户端不一定会严格实现, 如 php-amqplib 包就没有实现, 如果设置了immediate为true, 运行会报错: PhpAmqpLib \\ Exception \\ AMQPProtocolConnectionException (540) NOT_IMPLEMENTED - immediate=true 概括来说, mandatory标志告诉服务器至少将该消息route到一个队列中, 否则将消息返还给生产者; immediate标志告诉服务器如果该消息关联的queue上有消费者, 则马上将消息投递给它, 如果所有queue都没有消费者, 直接把消息返还给生产者, 不用将消息入队列等待消费者了; 注意AMQP协议规定了不同客户端(比如php-amqplib)需要实现的内容, 但是这些客户端不一定会完全并严格地实现;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"04. 浅析RabbitMQ消息通信模式 (内部各组件介绍)","slug":"rabbitmq/2018-05-27-rabbitmq-04","date":"2018-05-27T10:30:26.000Z","updated":"2019-04-17T14:23:18.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-04/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-04/","excerpt":"","text":"前言RabbitMQ专注于应用程序之间的消息通信, 在尝试使用Rabbit进行消息通信前, 清楚地理解通信模式的概念是非常重要的 Rabbit消息通信架构图大致如下: Rabbit Server(Broker Server)RabbitMQ是作为一个消息投递的中间代理服务器存在的 (broker有 ‘中间人’, ‘代理人’的意思), 可以认为它在应用程序之间扮演着路由器的角色; 所以在使用Rabbit进行消息通信时, 当应用程序连接到RabbitMQ时, 你就必须做个决定, 你的应用程序是准备作为生产者发送消息呢, 还是作为消费者来接收消息; ConnectionConnection其实就是一个TCP的连接, 无论你的应用程序是作为 Producer 还是 Consumer, 都是通过TCP连接到RabbitMQ Server的, 任何与Rabbit Server内部交互的应用程序都需要通过TCP连接到RabbitMQ Server, 后面会看到, 程序的起始就是建立这个TCP连接; Connection是RabbitMQ的socket链接, 它封装了socket协议相关部分逻辑; ChannelChannel 是我们与RabbitMQ打交道的最重要的一个接口, 我们大部分的业务操作是在channel这个接口中完成的 (包括 定义exchange, 定义queue, 绑定exchange与queue、发布消息 等); Channel 是虚拟连接, 它建立在上述的TCP连接 connection 中, 数据流动都是在channel中进行的, 程序起始第一步就是建立TCP连接 Connection, 第二步就是建立信道 Channel; 为什么使用Channel, 而不是直接使用TCP连接?对于OS来说, 建立和关闭TCP连接是有代价的, 频繁的建立关闭TCP连接对于系统的性能有很大的影响, 而且TCP的连接数也有限制, 这也限制了系统处理高并发的能力;但是, 在TCP连接中建立channel是没有上述代价的, 对于 producer 或者 consumer 来说, 可以并发的使用多个channel进行 ‘publish’或者’receive’; Exchange在你的应用程序连接到Rabbit Server后, 如果应用程序是作为Producer, 其发布的消息, 并不会直接被Rabbit Server投递到queue中, 这是务必要注意的; (消息永远不可能被直接发送到Rabbit Server的Queue中, 它总是会经过Rabbit Server的内部组件 — exchange交换机); 为什么Rabbit Server不是将生产者投递的消息直接发送到队列, 而必须要经过内部的exchange组件? 毕竟对于生产者来说, 只是希望消息能够到队列而已, Rabbit Server中为啥还多了一步? 如果没有exchange内部组件, 对于生产者投递来的消息, broker其实貌似也可以根据消息内容中的某个属性来判断消息该投递给哪个队列, 或者也可以指定投递给全部队列, 或者根据正则来投递给某些队列; 但这样的话, 相当于生产者和队列直接面对面做了强绑定; 而如果有exchange的话, 对于绑定和解析绑定, 直接交由exchange来进行即可, 这样在内部也做到了解耦和单一职责的划分, 可以将一些业务逻辑独立出来(比方你是要群发, 还是指定队列发送, 或者匹配队列发送, 这些绑定和解析工作都由exchange在生产者和队列中间进行处理, 生产者和队列不需要直接面对面); 在RabbitMQ中, exchange组件有四种类型 ： direct :使用这种类型, 会将生产者发送消息时设置的 routingkey 与 queue和exchange绑定时的 bindingkey 进行精确匹配, 如果 routingkey 和 bindingkey 完全一样, 那么Message就会被传递到当前exchange的对应queue中;如果使用了direct类型Exchange, 绑定队列和exchange时, 可以不必指定 routingkey 的名字, 在此类型下创建的queue有一个默认的 routingkey, 这个 routingkey 一般和 queue 同名;注意: 虽然不可以创建两个相同名字的队列, 但是多个不同名的队列与exchange进行绑定时, bindingkey 可以相同, 所以可能会有多个队列的bindingkey都 与 同一条消息发布时的routingkey可以匹配, 从而导致同一条消息被exchange路由到多个队列中! fanout:使用这种类型, 会将生产者发送消息时设置的 routingkey 与 queue和exchange绑定时的 bindingkey 进行模式匹配(正则匹配)比如 生产者发送消息时设置的routing key为 ab*, 则可以与 bindingkey 为 ab 开头的所有的queue匹配; topic ：使用这种类型, 会忽略routingkey的存在, 直接将消息广播到所有与当前exchange绑定的queue中; headers : 不太实用, 而且性能比较差, 几乎再也用不到了 BindingKey队列在声明后, 需要与exchange进行绑定, 绑定的时候可以显示地区指定一个 routingkey, 这里我们叫 bindingkey;注意: 队列在与exchange绑定时, 如果没有显示地去设置 bindingkey, 此时 bindingkey 默认为队列名字; RoutingKey生产者在消息发布时, 需要指明消息的 routingkey, exchange则会依据此routingkey去匹配 与 自身绑定的那些队列在绑定时 所显示或非显示设置的 bindingkey, 从而决定消息将会被路由到与自身绑定的哪些队列中;当然, 如果exchange设置的type是topic类型, 则会忽略 routingkey 和 bindingkey 的存在; Queue生产者投递给broker server的消息, 最终会被内部组件exchange路由到匹配的队列中, 等待consumers处理; 队列在声明后, 需要与exchange进行绑定 (绑定时会使用一个叫 routingkey 的东西, 这里我一般会叫他 bindingkey); 注意: 消息被exchange接收以后, 如果没有匹配的queue, 则消息会被丢弃, 即, 发送出去的消息如果路由到了不存在的队列的话, Rabbit 会忽略它们, 因此如果你不能承担消息丢入 黑洞 而丢失的话, 你的应用无论是生产者还是消费者, 都应该尝试去创建队列; 队列有很多参数属性可以设置, 后面在代码中经会一一展示如何使用它的这些属性; Producer图中左侧的客户端程序’A,B’, 其实就是生产者: 连接到 Rabbit Server , 并将消息投递到 Rabbit Server, 然后由exchange交换机决定该如何转发消息到队列中;Producer永远不可能直接将消息丢到queue中;Producer可以通过bindingkey与exchange进行绑定; Consumer图中右侧的客户端程序’1,2,3’, 其实就是消费者: 连接到 Rabbit Server , 并从 Rabbit Server 中接收消息; 发送消息时需要指定 routingkey, 如果没有指定routingkey 或者 指定的routingkey匹配不到对应的bindingkey, 消息会被 Rabbit 服务丢弃掉, 掉入”黑洞”; 另外, consumer在处理完消息之后, 消息的投递标识当一个消费者向RabbitMQ注册后, RabbitMQ会用 basic.deliver 方法向消费者推送消息, 这个方法携带了一个 delivery tag, 它在一个channel中唯一代表了一次投递。delivery tag的唯一标识范围限于channel;delivery-tag是单调递增的正整数, 可以在消费者中进行获取;(customer之间会不会累加? delivery-tag是生产者没法送一次就自增?) QoS 通道预取设置通过 basicQos 这个方法可以设置预取个数, 这个数值定义了一个 channel通道最多有多少个未确认的消息;值得重申的是, 投递流程和手动客户端确认是完全异步的, 因此, 如果在投递中已经有消息的情况下改变预取值, 则会出现自然竞争条件, 并且在信道上可能暂时存在多于预取未确认消息数量;https://blog.csdn.net/KuaiLeShiFu/article/details/77746431RabbitMQ提供了一种qos(服务质量保证)功能, 即在非自动确认消息的前提下, 如果一定数目的消息未被确认前, 消费者不消费新的消息;这种机制一方面可以实现限速(将消息暂存到RabbitMQ内存中)的作用, 一方面可以保证消息确认质量(比如确认了但是处理有异常的情况); 消费确认模式必须是非自动ACK机制(这个是使用baseQos的前提条件, 否则会Qos不生效), 然后设置basicQos的值;另外, 还可以基于consume和channel的粒度进行设置(global) Consumer Prefetchhttps://www.jianshu.com/p/4d043d3045ca 官方文档消费预取一节可以看到, 在RabbitMQ中, 对prefetch_count的定义与AMQP0-9-1貌似不太一样; prefetch允许为每个consumer指定最大的unacked messages数目, 简单来说就是用来指定一个consumer一次可以从Rabbit中获取多少条message并缓存在client中(RabbitMQ提供的各种语言的client library), 一旦缓冲区满了, Rabbit将会停止投递新的message到该consumer中直到它发出ack; 假设prefetch值设为10, 共有两个consumer, 意味着每个consumer每次会从queue中预抓取10条消息到本地缓存着等待消费, 同时该channel的unacked数变为20, 而Rabbit投递的顺序是，先为consumer1投递满10个message，再往consumer2投递10个message。如果这时有新message需要投递，先判断channel的unacked数是否等于20，如果是则不会将消息投递到consumer中，message继续呆在queue中。之后其中consumer对一条消息进行ack，unacked此时等于19，Rabbit就判断哪个consumer的unacked少于10，就投递到哪个consumer中。 总的来说，consumer负责不断处理消息，不断ack，然后只要unacked数少于prefetch * consumer数目，broker就不断将消息投递过去 正常情况下, 如果有多个消费者订阅了同一个Queue, Queue中的消息会被平摊给多个消费者;但是, 如果每个消息的处理时间不同, 就有可能会导致某些消费者一直在忙, 而另外一些消费者很快就处理完手头工作并处于空闲的情况;此时, 可以通过设置 prefetchCount 来限制queue每次发送给每个消费者的消息数, 比如我们设置prefetchCount=1, 则queue每次给每个消费者发送一条消息, 消费者处理完这条消息后Queue会再给该消费者发送一条消息; delivery_tagvhost最后, 还有一个在图中并未体现出来的概念 vhost 每一个RabbitMQ服务器都能创建虚拟消息服务器, 我们称之为虚拟主机(vhost), 每一个 vhost 本质上都是一个 mini 版的 RabbitMQ Server, 拥有自己的 exchagne, queue, 和 bindings rule 等等, 更重要的是, 它还拥有自己的 权限机制, 这使得你能够安全地使用一个RabbitMQ服务器来服务众多应用程序, 而不用担心A应用可能会删除B应用正在使用的队列; vhost之于Rabbit就像虚拟机之于物理服务器一样: 通过在各个实例间提供逻辑上隔离, 允许你为不同应用程序安全保密地运行数据, 这很有用, 它既能将同一Rabbit的众多客户端区分开来, 又可以避免队列和交换器的命名冲突; 否则, 你可能不得不去运行多个Rabbit, 并承担随之而来的管理问题, 相反, 你可以只运行一个Rabbit, 然后按需启动或者关闭vhost;当你有多个不同的应用要使用RabbitMQ时, 你可以为每个应用定义一个vhost来将应用从逻辑上隔离开; vhost是AMQP概念的基础, 你必须在连接时进行指定, 由于RabbitMQ包含了开箱即用的默认vhost: / , 因此时候用起来非常简单, 如果你不需要多个vhost的话, 就使用默认的即可;通过使用默认的guest用户名和密码guest, 就可以访问默认的 vhost, 为了安全起见, 你应该更改它; AMQP 并没有指定权限控制是在vhost级别还是在服务器端级别实现, 这留给了消息服务器的开发者去决定, 而在RabbitMQ中, 权限控制是以vhost为单位的, 当你在Rabbit里创建一个用户时, 用户通常会被指派给至少一个vhost, 并且只能访问被指派的vhost内的队列, 交换机和绑定;当你在设计消息通信架构时, 记住vhost之间是绝对隔离的, 你无法将 vhost ‘a’ 中的交换机绑定到 vhost ‘b’ 中的队列去; 另外需要注意: 当你在RabbitMQ集群上创建vhost时, 整个集群上都会创建该vhost; 如何创建vhost? vhost和权限控制非常独特, 它们是AMQP中唯一无法通过AMQP协议创建的基元(不同于队列, 交换机和绑定), 对于RabbitMQ来说, 你需要通过 RabbitMQ 安装路径下 ./sbin/ 目录中的 rabbitmqctl 工具来创建 如果想知道特定rabbitmq服务器上运行着哪些vhost时, 可以执行 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_vhostsListing vhosts/ 创建一个vhost 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl add_vhost testCreating vhost &quot;test&quot; 删除一个vhost 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl delete_vhost testDeleting vhost &quot;test&quot; 小结这一部分只是大概了解一下RabbitMQ服务一些概念性知识, 当然, 关于这些知识点在后面进行编码的时候会进行进一步扩展, 比如 队列, 消息, 交换机的诸多属性都会涉及到;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"03. 服务器管理","slug":"rabbitmq/2018-05-27-rabbitmq-03","date":"2018-05-27T10:20:09.000Z","updated":"2018-05-28T09:07:06.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-03/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-03/","excerpt":"","text":"前言到目前为止我们已经 下载安装并成功启动了Rabbitmq服务, 接下来, 可以简单了解一些与服务器管理相关的知识点; Erlang 和 Erlang Cookie命令行的基本使用RabbitMQ的启动可以在安装目录下的 ‘sbin/‘ 目录下, 运行 rabbitmq-server -detached 来启动后来运行;如果启动中遇到了任何错误, 可以检查RabbitMQ的日志文件; 如果不知道日志文件的位置, 可以查看: 2. 配置相关一般情况下, RabbitMQ的默认配置就足够了, 通过在Web管理平台可以看到, 默认是没有配置文件的; 如果希望特殊设置的话, 有几个位置可以进行配置;注意, 文件默认是没有的, 如果需要必须自己创建, 至于创建配置文件的位置在哪里, 可以通过日志文件来查看: 12345678910111213141516171819enyimindembp:rabbitmq renyimin$ pwd/usr/local/var/log/rabbitmqrenyimindembp:rabbitmq renyimin$ lslog rabbit@localhost-sasl.log rabbit@localhost.log rabbit@localhost_upgrade.logenyimindembp:rabbitmq renyimin$ head -20 rabbit\\@localhost.log=INFO REPORT==== 11-Dec-2017::15:37:02 ===Starting RabbitMQ 3.6.14 on Erlang 20.1.7Copyright (C) 2007-2017 Pivotal Software, Inc.Licensed under the MPL. See http://www.rabbitmq.com/=INFO REPORT==== 11-Dec-2017::15:37:02 ===node : rabbit@localhosthome dir : /Users/renyiminconfig file(s) : /usr/local/etc/rabbitmq/rabbitmq.config (not found)cookie hash : qNSOVbC4c3Bg7punCVVdaQ==log : /usr/local/var/log/rabbitmq/rabbit@localhost.logsasl log : /usr/local/var/log/rabbitmq/rabbit@localhost-sasl.logdatabase dir : /usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost rabbitmq-env.conf环境变量的配置文件 rabbitmq-env.conf : 可以用来配置rabbitmq的一些基本配置文件的路径; 这些文件的位置是特定于分发的, 默认情况下, 它们可能不会创建, 但希望位于每个平台的以下位置： 通用UNIX $ RABBITMQ_HOME/etc/rabbitmq / Debian /etc/rabbitmq/ RPM /etc/rabbitmq/ Mac OSX(Homebrew) $ {install_prefix}/etc/rabbitmq/，Homebrew前缀通常是/usr/local Windows ％APPDATA％\\RabbitMQ\\ 现在修改 rabbitmq-env.conf 中指定的 rabbitmq.conf 位置为 ‘/usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq.conf’1234567891011121314renyimindembp:rabbitmq renyimin$ pwd/usr/local/etc/rabbitmqrenyimindembp:rabbitmq renyimin$ cat rabbitmq-env.confCONFIG_FILE=/usr/local/etc/rabbitmq/rabbitmqNODE_IP_ADDRESS=127.0.0.1NODENAME=rabbit@localhostrenyimindembp:rabbitmq renyimin$ vim rabbitmq-env.conf....renyimindembp:rabbitmq renyimin$ cat rabbitmq-env.conf#CONFIG_FILE=/usr/local/etc/rabbitmq/rabbitmqCONFIG_FILE=/usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmqNODE_IP_ADDRESS=127.0.0.1NODENAME=rabbit@localhostrenyimindembp:rabbitmq renyimin$ 改完重启之后, 在Web管理界面会发现也有了配置文件的路径: rabbitmq.conf如果 rabbitmq.conf 不存在, 可以手动创建;如果更改位置, 请设置 RABBITMQ_CONFIG_FILE 环境变量, RabbitMQ会自动将.conf扩展名附加到此变量的值; 配置对于rabbitmq.config文件, rabbit官网提供了默认的rabbitmq.conf.example, 可以从此处获取;12 主要参考官方文档：http://www.rabbitmq.com/configure.html RabbitMQ权限控制解读Rabbit日志文件 在成功启动了RabbitMQ服务之后, 命令行查看虚拟机, 队列, 交换器和绑定状态; 通常情况下, 你是在服务器上直接运行 rabbitmqctl 来管理自己的rabbitmq节点, 不过你也可以通过 -n rabbit@[server_name]选项来管理远程rabbitmq节点, @符号将节点标识符(rabbit@[server_name])分成两部分: 左边的是Erlang应用程序名称, 在这里永远都是 rabbit; 右边的是服务器主机名或者IP地址; 你要确保运行Rabbit节点的服务器和运行 rabbitmqctl 的工作站点安装了相同的 Erlang Cookie;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"02. Win,MacOS,Linux下载安装RabbitMQ","slug":"rabbitmq/2018-05-27-rabbitmq-02","date":"2018-05-27T04:20:21.000Z","updated":"2018-05-27T10:19:44.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-02/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-02/","excerpt":"","text":"Windows 下载windows版RabbitMQ: 到官网下载windows installer即可; 直接傻瓜式双击安装 (可能会提示: Erlang could be detected. You must install Erlang before install RabbitMQ...., 点击是, 会跳转到 Erlang 的下载地址) 然后下载并安装对应版本的 Erlang (你的RabbitMQ对应的Erlang版本该如何选择, 可以参考Windows: With installer (recommended) | Manual) 然后傻瓜式下载安装 Erlang 即可 安装完Erlang之后, 双击安装RabbitMQ 在安装完RabbitMQ之后, 在Windows的开始菜单中会有 RabbitMQ 服务相关的一些选项可以使用, 如 RabbitMQ service - start 启动RabbitMQ服务 RabbitMQ service - stop 关闭RabbitMQ服务 RabbitMQ Command Prompt 连接RabbitMQ服务 (命令行连接工具) …… 当然, 你可以配置rabbitmq安装目录下的sbin目录到环境变量中, 这样就可以直接cmd使用相关的rabbitmq相关命令了; 注意windows下的问题: 如果启动命令行连接后, 执行 rabbitmqctl status 报错为 erlang cookie 问题, 目前找到的解决方案是: 将C:\\Windows\\System32\\config\\systemprofile\\.erlang.cookie 替换到 C:\\Users\\Administrator\\.erlang.cookie 之后, 可以开启rabbitmq-management插件, 来通过Web UI的形式来进行RabbitMQ的管理 执行如下操作打开rabbitmq-management插件 1234567891011121314151617181920E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt;rabbitmqctl list_usersListing users ...guest [administrator]E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt;rabbitmq-plugins enable rabbitmq_managementEnabling plugins on node rabbit@DHCCOEPA4DNL18R:rabbitmq_managementThe following plugins have been configured: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatchApplying plugin configuration to rabbit@DHCCOEPA4DNL18R...The following plugins have been enabled: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatchstarted 3 plugins.E:\\rabbitmq\\rabbitmq_server-3.7.5\\sbin&gt; 访问http://127.0.0.1:15672即可打开Web管理界面, 默认账号和密码为guest; Mac Mac下载安装RabbitMQ比较简单, 直接 home install rabbitmq 安装完成即可 1234567891011121314151617181920renyimin$ brew info rabbitmqrabbitmq: stable 3.7.5Messaging brokerhttps://www.rabbitmq.com/usr/local/Cellar/rabbitmq/3.7.5 (232 files, 10.1MB) * // 可以看到, hombrew安装的软件基本上会被放置到 `cd /usr/local/Cellar/` 下 Built from source on 2018-05-27 at 11:43:30From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/rabbitmq.rb==&gt; DependenciesRequired: erlang ✔ // 可以看到, 已经自动帮你安装了Erlang依赖==&gt; CaveatsManagement Plugin enabled by default at http://localhost:15672 // 并且自动开启了插件管理Bash completion has been installed to: /usr/local/etc/bash_completion.dTo have launchd start rabbitmq now and restart at login: brew services start rabbitmqOr, if you don&apos;t want/need a background service you can just run: rabbitmq-serverrenyimindembp:Cellar renyimin$ 按照指示, 启动rabbitmq后台运行 12renyimin$ brew services start rabbitmq==&gt; Successfully started `rabbitmq` (label: homebrew.mxcl.rabbitmq) 访问 http://127.0.0.1:15672, 默认账号密码为 guest Linux.. 后续完善 Docker.. 后续完善","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"01. 认识 RabbitMQ","slug":"rabbitmq/2018-05-27-rabbitmq-01","date":"2018-05-27T02:35:21.000Z","updated":"2019-04-17T14:03:36.000Z","comments":true,"path":"2018/05/27/rabbitmq/2018-05-27-rabbitmq-01/","link":"","permalink":"http://blog.renyimin.com/2018/05/27/rabbitmq/2018-05-27-rabbitmq-01/","excerpt":"","text":"RabbitMQ是一个由 Erlang 开发的 AMQP 协议的开源消息队列软件, 遵循 Mozilla Public License开源协议; (rabbit : 兔子, 行动迅速, 繁殖疯狂) AMQP 即 Advanced Message Queuing Protocol(高级消息队列协议), 一个提供统一消息服务的 应用层 标准 高级消息队列协议, 是应用层协议的一个开放标准, 为面向消息的中间件设计 基于此协议的客户端与消息中间件可传递消息, 并不受客户端/中间件不同产品, 不同的开发语言等条件的限制 在《RabbitMQ实战 高效部署分布式消息队列》1.1节 消息队列软件 的相关历史可以了解到, 很早就有很多消息队列软件, 但是由于 供应商壁垒, 导致中小型公司对高价格MQ供应商不满, 而大型公司不可避免地使用来自众多供应商的MQ产品来服务企业内部不同的应用, 这些产品使用不同的API, 不同的协议, 因而无法联合起来组成单一总线; 虽然在2001年诞生的JMS通过提供公共Java API的方式, 隐藏了单独MQ产品供应商提供的实际接口, 从而跨越了壁垒和解决了互通问题。但其实我们需要的是新的消息通信标准化方案, 所以就有了 AMQP 协议的诞生; 消息队列解决的问题 应用解耦, 比如你可能想着：如何将一个耗时的任务从触发它的应用程序中移出? 如何整合用不同语言编写的应用程序, 使得他们运行起来像单个系统?(虽然看起来是两个不同的问题, 但却有着共同的本质: 解耦 请求 和 处理, 这两个问题均需要从同步编程模型转向异步编程模型) 缓解流量高峰,提高系统吞吐量: 当系统中的同步处理方式在面对高并发涌入的大量操作时, 系统可能会无法立即做出响应, 从而严重影响系统的吞吐量时, 消息队列就可以用来缓解流量高峰, 提高系统吞吐量; 从上面的 应用解耦 可以看到, 消息队列提供了一个 异步通信协议, 消息的发送者 不用一直等待直到消息被成功处理, 而是消息被成功发送后就立即返回, 而消息则被暂存于队列当中, 暂时缓解系统压力, 系统后续再进行逐一处理; 这样自然会缓解流量高峰, 提高系统吞吐量; 系统调用中断时可以重试: 如果系统 B, C 出现中断, 系统A中的操作已经完成(无法回滚), 那么整个流程就会不完整; 此时就需要系统A的后续流程能够被保留重试; … 学习内容 安装 浅析RabbitMQ消息通信模式 (内部各组件介绍) 服务器运行及管理 日志分析 消息通信及各种细节问题 消息通信各种实际案例分析 高性能, 高可用RabbitMQ 技术点顺序: Qos 消费者预取 消费者优先级","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"01.","slug":"OpenResty/2018-05-10-01","date":"2018-05-10T11:50:36.000Z","updated":"2019-02-18T09:32:25.000Z","comments":true,"path":"2018/05/10/OpenResty/2018-05-10-01/","link":"","permalink":"http://blog.renyimin.com/2018/05/10/OpenResty/2018-05-10-01/","excerpt":"","text":"锤子科技在 T2 发布会上将门票收入捐赠给了 OpenResty 开源项目, 今天我们就来为大家介绍下 OpenResty 是个什么鬼 https://www.bilibili.com/video/av36735275?from=search&amp;seid=13575792469918679086http://i.youku.com/u/UMjczNDA2MTYwhttps://jinnianshilongnian.iteye.com/blog/2186270http://www.ttlsa.com/nginx/nginx-lua-redis-access-frequency-limit/https://github.com/orangle/study-resources/blob/master/OpenResty.mdhttp://blog.51cto.com/sofar/1837337","categories":[{"name":"OpenResty","slug":"OpenResty","permalink":"http://blog.renyimin.com/categories/OpenResty/"}],"tags":[{"name":"OpenResty","slug":"OpenResty","permalink":"http://blog.renyimin.com/tags/OpenResty/"}]},{"title":"23. Redis Replication","slug":"redis/2018-04-06-redis-23","date":"2018-04-06T08:05:39.000Z","updated":"2019-02-27T06:06:53.000Z","comments":true,"path":"2018/04/06/redis/2018-04-06-redis-23/","link":"","permalink":"http://blog.renyimin.com/2018/04/06/redis/2018-04-06-redis-23/","excerpt":"","text":"概述 Redis replication 主从复制, 是指将一台Redis服务器的数据, 复制到其他的Redis服务器, 前者称为主节点(master), 后者称为从节点(slave) 数据的复制是单向的, 只能由主节点到从节点 默认情况下, 每台Redis服务器都是master(主节点) 一个master可以有多个slave, 而一个slave只能有一个master 主从结构可以采用 一主多从 或者 级联结构 (即从服务器可以级联从服务器, M-&gt;S-&gt;S) 异步复制: M : replication时是非阻塞的(在replication期间, M依然能够处理客户端的请求) S : replication时也是非阻塞的 slave 在数据复制期间, 响应给客户端的数据是之前的旧数据 slave在replication期间, 也可以接受来自客户端的请求, 但是它用的是之前的旧数据 注意: 可以通过配置来决slave在replication时是否用旧数据响应客户端的请求, 如果配置为否, 那么slave将会返回一个错误消息给客户端 slave在新的数据接收完全后, 必须将新数据与旧数据替换, 即删除旧数据在替换数据的这个时间窗口内, slave出现阻塞, 会拒绝客户端的请求和连接那当slave拒绝请求时, 客户端会打到其他slave? 还是需要客户端自己重试进行请求? 主从复制的作用主要包括: 数据冗余: 主从复制实现了数据的热备份, 是持久化之外的一种数据冗余方式 (但不建议使用slave作为master的数据热备, 不能完全依赖slave的热备份, master仍然需要配置持久化) 故障恢复: 当主节点出现问题时, 可以由从节点提供服务, 实现快速的故障恢复, 实际上是一种服务的冗余 提高系统负载: 在主从复制的基础上, 配合读写分离, 可以由主节点提供写服务, 由从节点提供读服务, 分担服务器负载尤其是在写少读多的场景下, 通过多个从节点分担读负载, 可以大大提高Redis服务器的并发量, 提高系统QPS 高可用基石: 除了上述作用以外, 主从复制还是哨兵和集群能够实施的基础, 因此说主从复制是Redis高可用的基础 master持久化的意义 如果采用主从架构, 建议必须开启master的持久化! 不建议使用slave作为master的数据热备, 因为如果master的持久化被关闭, 一旦master意外宕机, 重启master后, 由于其数据是空的, 一经复制, salve的数据也就丢失了 master -&gt; RDB和AOF都关闭了 -&gt; master数据全部在内存中 master宕机, 重启, 是没有本地数据可以恢复的 master就会将空的数据集同步到slave上去, 所有slave的数据全部清空 最终造成100%的数据丢失 所以: master节点, 必须要使用持久化机制 redis replication 基本原理 当启动一个slave时, 它会发送一个PSYNC命令给master 如果slave是第一次连接master node, 那么会触发一次 full resynchronization 如果slave是重新连接master node, 那么master node仅仅会复制给slave部分缺少的数据; 开始full resynchronization时 master会fork一个子进程, 开始生成一份RDB快照文件 (所以此操作不会影响master继续接收客户端的请求), 因此, 同时master还会将从客户端收到的所有写命令缓存在内存中 RDB文件生成完毕之后, master会将这个RDB发送给slave, slave会先写入本地磁盘, 然后再从本地磁盘加载到内存中 然后master会将内存中缓存的写命令发送给slave, slave也会同步这些数据 slave 如果跟master 有网络故障, 断开了连接, 会自动重连; master如果发现有多个slave都来重新连接, 仅仅会启动一个rdb save操作, 用一份数据服务所有slave 主从复制的断点续传 从redis 2.8开始, 就支持主从复制的断点续传, 如果主从复制过程中, 网络连接断掉了, 那么可以接着上次复制的地方, 继续复制下去, 而不是从头开始复制一份 master会在内存中常见一个backlog, master和slave都会保存一个replica offset还有一个master id, offset就是保存在backlog中的如果master和slave网络连接断掉了, slave会让master从上次的replica offset开始继续复制, 但是如果没有找到对应的offset, 那么就会执行一次resynchronization 无磁盘化复制: 如果master配置了该项, 则master在内存中直接创建rdb, 然后发送给slave, 不会在自己本地落地磁盘了 12repl-diskless-syncrepl-diskless-sync-delay // 等待一定时长再开始复制, 因为要等更多slave重新连接过来 https://blog.csdn.net/mishifangxiangdefeng/article/details/50032357 过期key处理: slave不会过期key, 只会等待master过期key; 如果master过期了一个key, 或者通过LRU淘汰了一个key, 那么会模拟一条del命令发送给slave 延迟与不一致 主从复制完成之后, master上的写操作会异步地发送给slave, 而不会等待从节点的回复, 因此主从节点之间很难保持实时的一致性, 延迟在所难免 数据不一致的程度, 与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的 repl-disable-tcp-nodelay 配置等有关 repl-disable-tcp-nodelay no: 该配置作用于命令传播阶段当设置为yes时, TCP会对包进行合并从而减少带宽, 但是发送的频率会降低, 从节点数据延迟增加, 一致性变差 (具体发送频率与Linux内核的配置有关, 默认配置为40ms, 当设置为no时, TCP会立马将主节点的数据发送给从节点, 带宽增加但延迟变小) 一般来说, 只有当应用对Redis数据不一致的容忍度较高, 且主从节点之间网络状况不好时, 才会设置为yes, 多数情况使用默认值no 全量、部分复制 全量复制: 用于初次复制或其他无法进行部分复制的情况, 将主节点中的所有数据都发送给从节点, 是一个非常重型的操作 部分/增量复制: 用于网络中断等情况后的复制, 只将中断期间主节点执行的写命令发送给从节点, 与全量复制相比更加高效 (需要注意的是, 如果网络中断时间过长, 导致主节点没有能够完整地保存中断期间执行的写命令, 则无法进行部分复制, 仍使用全量复制) Redis通过psync命令进行全量复制的过程如下: 从节点判断无法进行部分复制, 向主节点发送全量复制的请求; 或从节点发送部分复制的请求, 但主节点判断无法进行全量复制 主节点收到全量复制的命令后, 执行 bgsave, 在后台生成RDB文件, 并使用一个缓冲区(称为复制缓冲区)记录从现在开始执行的所有写命令 主节点的bgsave执行完成后, 将RDB文件发送给从节点, 从节点首先清除自己的旧数据, 然后载入接收的RDB文件, 将数据库状态更新至主节点执行bgsave时的数据库状态 主节点将前述复制缓冲区中的所有写命令发送给从节点, 从节点执行这些写命令, 将数据库状态更新至主节点的最新状态如果从节点开启了AOF, 则会触发 bgrewriteaof 的执行, 从而保证AOF文件更新至主节点的最新状态 通过全量复制的过程可以看出, 全量复制是非常重型的操作: 主节点通过 bgsave 命令fork子进程进行RDB持久化, 该过程是非常消耗CPU、内存(页表复制)、硬盘IO的 主节点通过网络将RDB文件发送给从节点, 对主从节点的带宽都会带来很大的消耗 从节点清空老数据、载入新RDB文件的过程是阻塞的, 无法响应客户端的命令, 如果从节点执行bgrewriteaof, 也会带来额外的消耗 复制的细节: master和slave都会维护一个offsetmaster会在自身不断累加offset, slave也会在自身不断累加offset; slave每秒都会上报自己的offset给master, 同时master也会保存每个slave的offsetmaster和slave都要知道各自的数据的offset, 才能知道互相之间的数据不一致的情况 backlogmaster node有一个backlog, 默认是1MB大小master node给slave node复制数据时, 也会将数据在backlog中同步写一份backlog主要是用来做全量复制中断候的增量复制的 master run idinfo server, 可以看到master run id如果根据host+ip定位master node, 是不靠谱的, 如果master node重启或者数据出现了变化, 那么slave node应该根据不同的run id区分, run id不同就做全量复制如果需要不更改run id重启redis, 可以使用redis-cli debug reload命令 psync从节点使用psync从master node进行复制, psync runid offsetmaster node会根据自身的情况返回响应信息, 可能是FULLRESYNC runid offset触发全量复制, 可能是CONTINUE触发增量复制 全量复制小结 master执行bgsave, 在本地生成一份rdb快照文件 master node将rdb快照文件发送给salve node, 如果rdb复制时间超过60秒(repl-timeout), 那么slave node就会认为复制失败, 可以适当调节大这个参数 对于千兆网卡的机器, 一般每秒传输100MB, 6G文件, 很可能超过60s master node在生成rdb时, 会将所有新的写命令缓存在内存中, 在salve node保存了rdb之后, 再将新的写命令复制给salve node client-output-buffer-limit slave 256MB 64MB 60, 如果在复制期间, 内存缓冲区持续消耗超过64MB, 或者一次性超过256MB, 那么停止复制, 复制失败 slave node接收到rdb之后, 清空自己的旧数据, 然后重新加载rdb到自己的内存中, 同时基于旧的数据版本对外提供服务 如果slave node开启了AOF, 那么会立即执行BGREWRITEAOF, 重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite, 很耗费时间 主从复制的阻塞 slave node在做复制的时候, 也不会block对自己的查询操作, 它会用旧的数据集来提供服务; 但是复制完成的时候, 需要删除旧数据集, 加载新数据集, 这个时候就会暂停对外服务了 当出现这种问题时, slave的这次请求会失败? 需要客户端重试? 还是会自动切换到其他slave? 在 深入学习Redis（2）：持久化 一文中，讲到了fork操作对Redis单机内存大小的限制。实际上在Redis的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响： （1）切主：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果Redis单机内存达到10GB，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。 （2）从库扩容：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。 （3）缓冲区溢出：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-&gt;复制缓冲区溢出导致复制中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致复制中断……的循环。 （4）超时：如果数据量过大，全量复制阶段主节点fork+保存RDB文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-&gt;超时导致复制中断-&gt;重连-&gt;全量复制-&gt;超时导致复制中断……的循环。 此外，主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大：最好只使用50%-65%的内存，留下30%-45%的内存用于执行bgsave命令和创建复制缓冲区等。 https://www.cnblogs.com/kismetv/p/9236731.html#t1","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"21. Redis Persistence","slug":"redis/2018-04-05-redis-21","date":"2018-04-05T05:36:52.000Z","updated":"2019-03-11T02:05:28.000Z","comments":true,"path":"2018/04/05/redis/2018-04-05-redis-21/","link":"","permalink":"http://blog.renyimin.com/2018/04/05/redis/2018-04-05-redis-21/","excerpt":"","text":"Redis 在运行时, 是以数据结构的形式将数据维持在内存中的, 所以为了将数据从掉电易失的内存存放到能够永久存储的设备上(让这些数据在 Redis 重启之后仍然可用), Redis 分别提供了 RDB 和 AOF 两种持久化模式 RDB(Redis DataBase) RDB持久化默认是开启的, 关闭方式: 注释掉配置文件中默认的几行 save策略 或者 在save策略下添加一行 save &quot;&quot; 策略 RDB模式可以将redis服务器包含的所有数据库数据以 二进制 .rdb 文件的形式保存到硬盘 默认文件名为 dump.rdb (文件名配置: dbfilename dump.rdb) .rdb文件存放路径配置: dir /usr/local/redis-4.0.12/persistence-db-dir 创建 .rdb 文件常见的三种方式: 手动在客户端向redis服务器发送 SAVE 命令 手动在客户端向redis服务器发送 BGSAVE 命令 在配置文件中配置rdb持久化的 save策略, redis服务运行期间, 如果配置选项被满足, 则服务器会自动执行 BGSAVE 这三种创建RDB的方式, 前两种需要手动去执行, 而第三种是服务器自动执行的 创建RDB文件三种方式的区别SAVE命令手动在客户端向redis服务器发送 SAVE命令 速度优点: 相对于下面介绍的 BGSAVE 命令, SAVE命令执行时, redis主进程不会fork新的进程, 所以可以集中资源来创建.rdb文件, 速度相对比 BGSAVE 命令 要快 缺点: 阻塞: 在redis服务器执行SAVE命令的过程中(即创建RDB文件的过程中), redis服务器将被阻塞, 服务器端无法再去处理客户端发送的命令, 只有在SAVE命令执行完毕之后, 服务器才会重新开始处理客户端发送的命令请求 安全性: .rdb 文件覆盖创建的安全性问题 BGSAVE 命令bgsave命令的执行可以分为手动或者自动 手动在客户端向redis服务器发送 BGSAVE 命令 自动创建.rdb文件 通过在redis配置文件中配置redis服务器生成.rdb文件的条件, 就可以让服务器在满足save策略时, 自动去创建.rdb文件 redis默认情况下就是开启REDB持久化的, 并且默认设置了几个save的策略: 12345save 900 1 #900秒 (15分钟) 内至少1个key值改变 (则进行数据库保存--持久化) save 300 10 #300秒 (5分钟) 内至少10个key值改变 (则进行数据库保存--持久化) save 60 10000 #60秒 (1分钟) 内至少10000个key值改变 (则进行数据库保存--持久化)// 只要三个条件中的任意一个条件被满足时, 服务器就会自动执行 BGSAVE 命令来创建新的.rdb文件// 每次创建完.rdb文件之后, 服务器为实现自动持久化, 会将&apos;为实现持久化而设置的时间计数器和次数计数器清零并重新开始计数&apos;, 所以多个保存条件的效果是不会叠加的 非阻塞 优点: BGSAVE 命令不会造成服务器阻塞, redis服务器在执行BGSAVE命令的过程中, 仍然可以正常的处理其他客户端发送的命令 BGSAVE命令不会造成服务器阻塞的原因在于: 当redis服务器接收到BGSAVE命令后, 它不会亲自去创建rdb持久化文件, 而是通过 fork 一个子进程, 然后由子进程去生成.rdb文件, 主进程则继续处理客户端的命令请求, 当子进程创建好.rdb文件并退出后, 它会向父进程发送一个信号, 告知redis服务器.rdb文件已经创建完毕 缺点: 性能的额外消耗: fork子进程会消耗额外的内存 RDB经常需要 fork() 才能使用子进程将数据持久存储在磁盘上,如果数据集很大, fork() 可能会非常耗时,如果数据集非常大且CPU性能不佳, 可能会导致Redis停止服务客户端几毫秒甚至一秒钟AOF重写也需要fork(), 但你可以调整你想要重写日志的频率而不需要对耐久性进行任何权衡 速度的降低: 相比 save 命令, 由于 fork子进程会消耗额外性能, 所以 bgsave创建.rdb文件的速度其实会比save慢 可能产生的阻塞:父进程 fork 子进程, 这个过程中父进程是阻塞的, Redis 不能执行来自客户端的任何命令 RDB经常需要 fork() 才能使用子进程将数据持久存储在磁盘上,如果数据集很大, fork() 可能会非常耗时,如果数据集非常大且CPU性能不佳, 可能会导致Redis停止服务客户端几毫秒甚至一秒钟AOF重写也需要fork(), 但你可以调整你想要重写日志的频率而不需要对耐久性进行任何权衡 .rdb文件覆盖创建的安全性问题 save 对比 bgsavesave 和 bgsave 这两个命令没有孰好孰坏, 你要考虑哪个更适合你 如果你的数据库正在上线当中, 自然使用 bgsave (让服务器以非阻塞方式进行最好); 相反, 如果你需要在凌晨3点钟维护你的redis, 比如维护需要停机一小时, 这时系统被阻塞了也是没关系的, 这时候你使用save命令就会好一点 都会阻塞: save会阻塞redis响应客户端请求 bgsave在 fork() 时, 如果数据集很大, 也可能造成几毫秒甚至一秒钟的阻塞 .rdb文件覆盖创建的安全性问题: 无论以上哪种方式去生成.rdb文件, 由于每次都是覆盖创建 所以之前的.rdb文件你可以用定时脚本定时地拷走(可以发送到自己的云服务器进行备份), 防止外一客户端执行了 flush db 并且服务器进行了 save, 此时旧的rdb会被新的rdb文件覆盖掉, 那就完蛋了!!! 事实上在生产环境中 FLUSHDB、FLUSHALL 等命令都属于必禁命令(如果开启AOF的话, 如果.aof文件尚未被重写, 你还可以停止服务, 打开.aof文件, 删除执行的flush命令, 然后重启, 不过线上可能会涌入大量的命令, 导致你找flush命令都是个问题) BGSAVE的执行流程浅析 Redis 父进程首先判断: 当前是否存在正在执行 bgsave/bgrewriteaof 子进程, 如果有子进程正在执行, 则 bgsave 命令直接返回 bgsave/bgrewriteaof 的子进程不能同时执行, 主要是基于性能方面的考虑: 两个并发的子进程同时执行大量的磁盘写操作, 可能引起严重的性能问题 父进程 fork 子进程, 这个过程中父进程是阻塞的, Redis 不能执行来自客户端的任何命令 父进程 fork 后, bgsave 命令返回 “Background saving started” 信息并不再阻塞父进程 子进程创建 .rdb 文件, 根据父进程内存快照生成临时快照文件, 完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成, 父进程更新统计信息 RDB持久化的优点 RDB对于灾难恢复非常好，因为一个紧凑的文件可以传输到远程数据中心 (你可能想要归档最近的24小时内每个小时的RDB文件, 并且每个归档保存30天, 这允许你再灾难发生的时候开业轻松地恢复数据集到不同版本) 重建快: (重建数据库是指将数据从硬盘移到内存, 并建立起数据库的过程) 因为对于RDB模式来说, 重建就是把 dump.rdb 文件加载到内存, 并解压字符串, 就建立起了数据库 而对于AOF模式来说, 则是在启动Redis服务器的时候, 运行 appendonly.aof 日志文件, 在内存中重新建立数据库 RDB持久化的缺点 意外宕机的数据丢失问题: 当你正常关闭redis的时候, redis服务器不会参考配置中的save策略, 而是会直接先调用save命令, 将redis所有数据持久化到磁盘之后才会真正进行退出 但是当redis出现意外断电宕机时, 你会发现从上一次快照之后的数据将全部丢失, 这是因为 RDB持久化 无法频繁执行导致的 由于创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作, 所以服务器需要隔一段时间再来创建一个新的rdb文件, 也就是说rdb文件的创建操作不能执行的过于频繁, 否则将会严重影响服务器的性能由于save策略设置的是每隔一段时间再去创建.rdb文件, 所以如果在间隔的这段时间中服务器宕机, 那这段间隔中的数据就丢失了 为了解决这个问题, rdb可以结合aof来一起进行持久化, AOF持久化模式就解决了服务器不能频繁执行rdb持久化的问题 如果你非常关心你的数据，但是在发生灾难时仍然可以忍受几分钟的数据丢失，那么你可以只使用RD .rdb文件每次都是覆盖创建, 所以为了安全起见, 需要不时地去备份已经生成的rdb文件 AOF (Append Only File)由于RDB的持久化策略的执行时间间隔不能设置的特别频繁, 所以在服务器意外宕机的情况下造成的数据丢失问题可能就会比较严重, 为此redis提供了另外一种持久化方案 AOF持久化 AOF模式是将 操作日志 记在 appendonly.aof 文件里, 每次启动服务器就会运行 appendonly.aof 里的 命令 重新建立数据库 默认AOF模式是关闭的, 可以在redis配置文件(redis.conf)中, 配置 appendonly yes 来打开AOF模式 AOF仍然会丢数据 在AOF持久化模式下, 虽然redis服务器在执行修改数据的命令后, 会把执行的命令写入到aof文件中, 但这并不意味着aof文件持久化不会丢失任何数据 在常见的操作系统中, 执行系统调用write函数, 将一些内容写到某个文件中时, 为了提高效率, 系统通常不会直接将内容写入到磁盘里面, 而是先将内容放入一个内存缓冲区(buffer)里面, 等到缓冲区被填满, 或者用户执行 fsync 调用和 fdatasync 调用时, 系统才会将存储在缓冲区里面的内容真正写入到硬盘里所以对AOF持久化来说, 只有当一条命令真正的被写入到硬盘里面时, 这条命令才不会因停机而意外丢失因此, 相对于rdb模式的save策略来看, rdb模式的策略由于可能会出现的较长时间间隔, 所以在redis意外宕机时, aof丢失的命令显然可能会少很多, 但仍然可能丢失命令 aof持久化在遭遇意外停机时所丢失的命令数量, 取决于命令被写入到硬盘的时间越早将命令写入到硬盘, 发生意外停机时丢失的数据就越少, 而越迟将命令写入硬盘,发生意外停机时丢失的数据就越多为此, AOF为我们提供了几个尽快将数据写入磁盘的追加策略 由于AOF仍然面临丢失命令的风险, AOF模式提供了三种 追加 策略, 这三种追加策略主要就是用来指定什么时机将操作日志真正追加到 appendonly.aof 文件里 always : 服务器每写入一个命令, 就调用一次fdatasync, 将缓冲区里面的命令写入到磁盘文件中, 在这种模式下, 服务器即使遭遇意外停机, 也不会丢失任何自己已经成功执行的命令数据比较安全, 但比较慢; (类似mysql了) everysec: 服务器每一秒重新调用一次fdatasync, 将缓冲区里面的操作日志写入到磁盘文件中, 这是系统默认的方式, 是一种权衡折衷通常这种方式会比较好, 在这种模式下, 服务器即使遭遇意外停机时, 最多只丢失一秒钟的内执行的命令相比RDB由于不能频繁执行而设置的save策略可能会间隔较长的时间, 在出现redis意外宕机时, 这种方式只会丢失1秒的数据显然要好多了 no: 服务器不主动调用fdatasync, 由操作系统去决定什么时候将缓冲区里面的命令写入到硬盘里面在这种模式下,服务器遭遇意外停机时, 丢失的命令数量是不确定的 可以在redis.conf中配置AOF的追加策略, 可以看到默认使用的是everysec这种折中策略 123# appendfsync alwaysappendfsync everysec# appendfsync no BGREWRITEAOF 由于redis只会写一个aof文件, 随着AOF文件越来越大, 里面会有大部分是重复命令或者可以合并的命令(100次incr = set key 100) 为了让aof文件的大小控制在合理的范围, 避免它疯狂增长, redis提供了AOF重写功能, 通过这个功能, 服务器可以产生一个新的aof文件 重写的好处: 减少AOF日志尺寸, 减少内存占用, 加快数据库恢复时间 有两种方法可以触发aof文件重写 客户端手动向服务器发送 BGREWRITEAOF 命令 通过设置配置文件选项来让服务器自动执行BGWRITEAOF 命令 12345auto-aof-rewrite-min-size &lt;size&gt;触发aof重写所需的最小体积: 只要aof文件的体积大于等于size时,服务器才会考虑是否需要进行aof重写, 这个选项用于避免对体积过小的aof文件进行重写auto-aof-rewrite-percentage 100指定触发重写所需的aof文件增长体积的百分比, 当aof文件增长的体积大于auto-aof-rewrite-min-size指定的体积, 并且超过上一次重写之后的aof文件体积的percent%时, 就会触发aof重写, (如果服务器启动刚刚不就,还没有进行过aof重写,那么使用服务器启动时载入的aof文件体积来作为基准值)将这个值设置为0表示关闭自动aof重写 例子: 1234//只有当aof文件的增量大于100%的时候才进行重写auto-aof-rewrite-percentage 100// 当aof文件大于64mb之后才考虑进行aof重写, 还需要看上一条的百分比增量够不够auto-aof-rewrite-min-size 64mb AOF重写命令是redis通过fork子进程在后台执行的 AOF重写并不需要对原有AOF文件进行任何的读取, 写入, 分析等操作, 这个功能是通过读取服务器当前的数据库状态来实现的 新的aof文件会使用尽可能少的命令来记录数据库数据, 因此新的aof文件的体积通常会比原有aof文件的体积要小得多 aof重写期间, 服务器不会被阻塞, 可以正常处理客户端发送的命令请求 新的aof文件创建完成后, 旧的aof文件会被删除 aof文件的重写机制: 整个过程还是比较谨慎的, 即使redis出现意外宕机, 老的.aof文件还在 redis fork一个子进程 子进程基于当前内存中的数据, 往一个新的临时的AOF文件中写入日志 同时, redis主进程仍然继续处理新的请求, 在接收到client新的写操作之后, redis会在内存中写入日志, 同时新的日志也继续写入旧的AOF文件 子进程写完新的日志文件之后, redis主进程将内存中的新日志再次追加到新的AOF文件中 用新的日志文件替换掉旧的日志文件 BGREWRITEAOF 导致的主线程阻塞 bgrewriteaof 是主进程fork的子进程来执行的, 按照正常逻辑, 不应该影响 Redis 主进程的正常服务 但其实问题是出在硬盘上: Redis 服务设置了 appendfsync everysec, 主进程每秒钟便会调用 fsync(), 要求内核将数据写到存储硬件里 但由于此时 bgrewriteaof子进程 同时也在写硬盘, 从而导致主进程 fsync()/write() 操作被阻塞, 最终导致 Redis 主进程阻塞了 AOF文件出错 服务器可能在程序正在对AOF文件进行写入时发生意外宕机, 如果造成了AOF文件出错(corrupt), 那么 Redis 在重启时会拒绝载入这个 AOF 文件, 从而确保数据的一致性不会被破坏 当发生这种情况时, 可以用以下方法来修复出错的 AOF 文件: 首先必须先为现有的 AOF 文件创建一个备份文件 然后使用 Redis 附带的 redis-check-aof 程序, 对原来的 AOF 文件进行修复: $ redis-check-aof --fix (可选)使用 diff -u 对比修复后的 AOF 文件和原始 AOF文件的备份, 查看两个文件之间的不同之处 重启 Redis 服务器, 等待服务器载入修复后的 AOF 文件, 并进行数据恢复 模拟让aof破损 打开文件, 在末尾随意删除两行 (造成aof出错一般是redis意外宕机导致的文件末尾出错, 你不能在aof文件中间随意删除几行然后指望fix, 显然不太现实) 然后fix (会有一条数据会被fix删除) 用fix的aof文件去重启redis, 发现数据确实只剩下一条了 RDB与AOF对比 Redis的配置文件中, 两者有各自的配置方式, 默认是开启了RDB Redis的写操作在RDB模式下比AOF模式下要快: 因为RDB模式下, redis的每次写操作都是直接写redis内存, 只有当满足save策略时, 会持久化一份.rdb文件 而AOF模式下, redis的每次写操作都会触发一次os cache的写入, 然后根据追加策略的不同, 写入.aof文件 当然, .rdb文件每次都是新生成(比较耗内存), 而.aof文件只有一份,是一直追加的 RDB意外宕机丢失数据的量看上去比AOF要大 RDB模式的策略不能太频繁(创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作), 如果redis出现意外宕机, 那从上次宕机到当前时刻, redis的数据就会都丢失 而AOF模式可以做到在redis出现意外宕机时, 最多丢失1秒的数据 .rdb文件是二进制文件, 而.aof文件对我们可读 由于AOF以易于理解和解析的格式, 因此你可以轻松导出AOF文件 假设你意外执行了 FLUSHALL 命令刷新了所有数据, 如果开启了AOF, 并且在此期间未执行重写日志 你仍然可以恢复尽可能新的数据, 只需停止服务器, 删除AOF上的最新命令, 然后重新启动Redis .rdb文件恢复数据时, 重建比较快, 而 .aof文件恢复相对比较慢 aof文件和rdb的数据是不一样的: aof文件中的数据显然要比rdb文件中的数据要新一些, 所以rdb和aof都开启的情况下, redis会优先使用aof日志进行恢复 RDB + AOF在redis使用中, 可以组合AOF和RDB, 组合使用导致的阻塞问题: 子进程互相阻塞问题 如果RDB在执行snapshotting操作, 那么redis不会执行AOF bgrewrite; 如果redis在执行AOF bgrewrite, 那么就不会执行 RDB snapshotting 如果RDB在执行snapshotting, 此时用户执行 BGREWRITEAOF 命令, 那么等RDB快照生成之后, 才会去执行AOF rewrite 子进程阻塞导致主进程阻塞: 在BGREWRITEAOF写硬盘期间, 也会阻塞AOF每秒对.aof文件的写入 另外注意: 那么redis重启的时候, 会优先使用AOF进行数据恢复, 因为aof的日志相比rdb会更完整 在有rdb的dump和aof的appendonly的同时, rdb里有数据, aof里也有数据, 但是由于aof的日志是每个命令每个一秒都会写入, 所以也更新, 如果redis意外宕机, 也不可能去优先拿rdb中的旧数据 内存的飙升 AOF重写 SAVE, BGSAVE 上述两种情况都会出现在内存中持有的 .aof/.rdb文件的情况, 内存自然会飙升, 比如可能会出现如下错误: 12345625018:C 15 May 06:12:46.416 # Write error writing append only file on disk: No space left on device1548:M 15 May 06:12:48.146 # Short write while writing to the AOF file: (nwritten=309, expected=37399)1548:M 15 May 06:12:48.526 # AOF write error looks solved, Redis can write again.1548:M 15 May 06:12:48.928 # Background AOF rewrite terminated with error1548:M 15 May 06:12:49.029 * Starting automatic rewriting of AOF on 108% growth1548:M 15 May 06:12:49.368 * Background append only file rewriting started by pid 25502 持久化恢复步骤 在redis出现意外宕机后, 如果要进行数据恢复, 注意: 如果你直接将云服务器上定期备份的 .rdb 文件直接拷贝到redis指定的生成rdb文件的目录下, 然后试图重启redis重启后你可能会发现redis中并没有.rdb文件中的数据原因是线上redis一般使用 appendonly.aof + dump.rdb 的方式进行持久化, 而redis重启后会优先用 appendonly.aof 去恢复数据, 而没有用.rdb文件的数据并且redis启动的时候, 自动重新基于内存的数据, 生成了一份最新的rdb快照, 即直接用空的数据, 覆盖掉了你刚刚拷贝过去的那份dump.rdb文件 所以, 你停止redis之后, 其实应该先删除 appendonly.aof, 然后将我们的dump.rdb拷贝过去, 然后再重启redis, 但是重启后发现仍然没有 aof 文件中的数据原因很简单, 虽然你删除了 appendonly.aof, 但是因为aof持久化是打开的, redis就一定会优先基于aof去恢复, 即使文件不在, 它也会在启动后立刻创建一个新的空的aof文件 正确操作: 停止redis, 暂时在配置中关闭aof, 然后拷贝一份rdb过来, 再重启redis, 数据能不能恢复过来, 可以恢复过来 之后, 如果你再关掉redis, 手动修改配置文件, 打开aof, 再重启redis, 数据又没了, 因为之前redis内存中的数据并不会生成aof文件, 而打开aof重启redis后, 才会又创建新的空的aof文件, 然后以新的aof文件进行恢复并且会覆盖之前的rdb文件 在数据安全丢失的情况下, 基于rdb冷备, 如何完美的恢复数据, 同时还保持aof和rdb的双开 停止redis, 关闭aof, 拷贝rdb备份, 重启redis, 确认数据恢复, 直接在命令行 config set 热修改配置参数, 打开aof, 这样redis就会将内存中的数据, 写入aof文件中 此时aof和rdb两份数据文件的数据就同步了 由于配置文件中的实际的参数没有被持久化的修改, 再次停止redis, 手动修改配置文件, 打开aof的命令, 再次重启redis 参考https://www.cnblogs.com/kismetv/p/9137897.html#t53","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"03. 杂项","slug":"redis/2018-03-10-redis-03","date":"2018-03-10T02:17:23.000Z","updated":"2019-02-27T06:02:45.000Z","comments":true,"path":"2018/03/10/redis/2018-03-10-redis-03/","link":"","permalink":"http://blog.renyimin.com/2018/03/10/redis/2018-03-10-redis-03/","excerpt":"","text":"redis访问控制 redis默认会启动受保护模型, 客户端要连接redis 要么在配置文件中设置 protected-mode no 或者在配置文件中为redis设置密码 redis密码设置: redis没有实现访问控制这个功能, 但是它提供了一个轻量级的认证方式, 可以编辑 redis.conf 配置来启用认证 可以在配置文件redis.conf中, 为redis配置连接密码 requirepass 你的密码 然后重启: service redisd stop, service redisd start 设置了密码后, 客户端或slave如何连接redis服务? 客户端连接redis : redis-cli -h 127.0.0.1 -p 6379 -a 你的密码 slave连接master时, 也需要为slave配置 masterauth 主的密码 连接redis不需要账号, 只需要密码, 但是要想连接redis, 还需要配置ip白名单: #bind 127.0.0.1 默认情况会开启, 只允许本地访问redis服务, 实际情况我们生产环境下基本都是远程访问, 所以可以注释掉, 即允许本机以外的所有ip访问它 也可以配置你的客户端ip, 如: bind 192.168.1.100 10.0.0.1 危险命令的禁用 Redis的危险命令主要有 flushdb // 清空数据库 flushall // 清空所有记录, 数据库 config // 客户端连接后不能配置服务器 keys // 客户端连接后可查看所有存在的键 (keys这个命令性能真的很差, keys模糊匹配 会引发Redis锁, 并且增加Redis的CPU占用, 情况会非常恶劣)不要使用keys正则匹配操作, 包括但不限于各种形式的模糊匹配操作因为Redis是单线程处理, 在线上KEY数量较多时, 操作效率极低(时间复杂度为O(N)), 该命令一旦执行会严重阻塞线上其它命令的正常请求, 而且在高QPS情况下会直接造成Redis服务崩溃 作为服务端的redis-server, 我们常常需要禁用以上命令来使服务器更加安全 禁用的具体做法是, 修改服务器的配置文件redis.conf, 找到 Command renaming, 新增以下命令: 1234rename-command FLUSHALL &quot;&quot;rename-command FLUSHDB &quot;&quot;rename-command CONFIG &quot;&quot;rename-command KEYS &quot;&quot; ---暂时未使用 而如果想要保留命令, 但是不能轻易使用, 可以重命名命令来设定: 1234rename-command FLUSHALL joYAPNXRPmcarcR4ZDgC81TbdkSmLAzRPmcarcRrename-command FLUSHDB qf69aZbLAX3cf3ednHM3SOlbpH71yEXLAX3cf3erename-command CONFIG FRaqbC8wSA1XvpFVjCRGryWtIIZS2TRvpFVjCRGrename-command KEYS eIiGXix4A2DreBBsQwY6YHkidcDjoYA2DreBBsQ redis库名 Redis默认支持16个数据库(可以通过配置文件支持更多, 无上限), 可以通过配置 databases 来修改这一数字 1234# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1databases 16 redis的每个数据库对外都是一个从0开始的递增数字命名客户端与Redis建立连接后会自动选择0号数据库, 不过可以随时使用 SELECT 命令更换数据库 然而这些以数字命名的数据库又与我们理解的数据库有所区别 首先Redis不支持自定义数据库的名字, 每个数据库都以编号命名, 开发者必须自己记录哪些数据库存储了哪些数据 另外Redis也不支持为每个数据库设置不同的访问密码, 所以一个客户端要么可以访问全部数据库, 要么连一个数据库也没有权限访问 最重要的一点是多个数据库之间并不是完全隔离的, 比如 FLUSHALL 命令可以清空一个Redis实例中所有数据库中的数据 redis 的 key设计技巧 把表名转换为key前缀 如, user: 第2段放置用于区分key的字段(对应mysql中的主键的列名,如 userid) 第3段放置主键值, 如2,3,4…., a , b ,c 第4段, 写要存储的列名 如下, 用户表 user, 转换为key-value存储 userid | username | password | email :-: | :-: | :-: | :-: | :-: | -: 9 | Lisi | 1111111 | lisi@163.com 123set user:userid:9:username lisiset user:userid:9:password 111111set user:userid:9:email lisi@163.com redis 日志配置 redis在默认情况下, 是不会生成日志文件的, 所以需要配置 打开配置文件 1234567891011# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)# loglevel是用来设置日志等级的，具体可以看配置文件中上面的注释loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;/var/log/redis/redis.log&quot; redis必须带配置文件启动, 如果直接启动的话, 它会使用默认配置(而且并不存在这个默认配置文件, 所以不要想着你能去改它) 让消息队列更可靠 Redis List经常被用于消息队列服务, 假设消费者程序在从队列中取出消息后立刻崩溃, 但由于该消息已经被取出但却没有被正常处理, 那么该消息就丢失了 为了避免这种情况, Redis提供了 RPOPLPUSH 命令, 消费者程序 会 原子性 的 从主消息队列中取出消息并将其插入到备份队列中, 并且会返回弹出队列的数据然后等到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除同时还可以提供一个守护进程, 当发现备份队列中的消息过期时, 可以重新将其再放回到主消息队列中, 以便其它的消费者程序继续处理 未完待续~~ 未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"03. HTTP状态码详解","slug":"http/2017-11-30-HTTP-03","date":"2017-11-30T06:30:12.000Z","updated":"2019-03-12T02:22:09.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03/","excerpt":"","text":"1xx 101: 参考博文WebSocket简单示例分析 (做协议升级, 还会响应: Connection: Upgrade) 2xx Web API的设计与开发 P109 200 OK : 200码非常出名, 似乎没有对它进一步说明的必要 201 Created : 当在服务器端创建数据成功时, 会返回201状态码 (比如使用 POST 请求方法, 如 用户登录后添加了新用户, 上传了图片等新创建数据的场景) 202 Accepted : 在异步处理客户端请求时, 它用来表示服务器端已经接受了来自客户端的请求, 但处理尚未结束 在文件格式转换, 处理远程通知(Apple Push Notification等)这类很耗时的场景中, 如果等到所有处理都结束后才向客户端返回响应消息, 就会花费相当长的时间, 造成应用可用性不高; 这时采用的方法是服务器向客户端返回一次响应消息, 然后立刻开始异步处理 202状态码就被用于告知客户端服务器端已经开始处理请求, 但整个处理过程尚未结束 比如: 以LinkedIn的参与讨论的API为例如果成功参与讨论并发表意见, 服务器端通常会返回201状态码但如果需要得到群主的确认, 那么所发表的意见就无法立即在页面显示出来, 这时服务器端就需要返回202状态码; 从广义上来看, 该场景也属于异步处理, 但和程序设计里的异步执行当然不同 204 No Content : 正如其字面意思, 当响应消息为空时会返回该状态码 其实就是告诉浏览器, 服务端执行成功了, 但是没什么数据返回给你, 所以你不用刷新页面, 也不用导向新的页面 在用 DELETE 方法删除数据时, 服务器端通常会返回204状态码(阮一峰博文也提到过, 对DELETE适用) 除此之外, 也有人认为在使用 PUT或PATCH 方法更新数据时, 因为只是更新已有数据, 所以返回204状态码更加自然 书中建议 DELETE 返回204; PUT或PATCH返回200并返回该方法所操作的数据; 关于204状态码的讨论可以参考 p111; 205 Reset Content : 告诉浏览器, 页面表单需要被重置 205的意思是服务端在接收了浏览器POST请求以后, 处理成功以后, 告诉浏览器, 执行成功了, 请清空用户填写的Form表单, 方便用户再次填写; 206 Partial Content : 成功执行了一个部分或Range(范围)的请求 206响应中, 必须包含 Content-Range, Date 以及 ETag或Content-Location首部; 3xx300 Multiple Choices : 客户端驱动方式进行内容协商时, 服务器可能返回多个连接供客户端进行选择 (比如多语言网站可能会出现) 301 Moved Permanently : 在请求的URL已经被移除时使用, 响应的Location首部中应该包含资源现在所处的URL (比较适合永久重定向) 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是301 则即便稍后取消了 location.php 中的跳转(或者修改了跳转地址), 由于浏览器会认为你之前的跳转是永久性的, 再次访问www.test.com/location.php仍然会跳转到之前的跳转链接(除非清浏览器缓存) 另外注意: 假设你之前是先访问www.test.com/test.html, 然后通过 post 提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET 302 Found: 与301类似, 但是客户端应该使用Location首部给出的URL来进行临时定位资源, 将来的请求仍应该使用老的URL 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是302 如果稍后取消了location.php中的跳转, 再次访问www.test.com/location.php, 会发现不会进行跳转, 而是访问到 location.php 修改后的代码 (不用清浏览器缓存); 另外注意: 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET 303 See Other : HTTP/1.1使用303来实现和302一样的临时重定向 307 Temporary Redirect HTTP/1.1规范要求用307来取代302进行临时重定向; (302临时重定向留给HTTP/1.0) 所以他也具备302临时重定向的特点; 但是, 与 302, 303 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 308 Permanent Redirect 貌似不是rfc2616的标准 具备和301永久重定向的特点, 需要清除浏览器缓存才行; 但是, 与 301 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 304 Not Modified : 参考博文缓存相关 4xx Web API的设计与开发 P1134字头状态码主要用于描述因客户端请求的问题而引发的错误。也就是说, 服务器端不存在问题, 但服务器端无法理解客户端发送的请求, 或虽然服务器端能够理解但请求却没有被执行, 当遇到这些情况引发的错误时, 服务器端便会向客户端返回这一类别的状态码。因此, 当服务器端返回4字头的状态码时, 就表示客户端的访问方式发生了问题, 用户需要检查一下客户端的访问方式或访问的目标资源等。 400 Bad Request : 表示其他错误的意思, 即其他4字头状态码都无法描述的错误类型; 401 Unauthorized : 表示认证(Authentication)类型的错误 比如当需要先进行登录操作, 而却没有告诉服务器端所需的会话信息(比如token..), 服务器端就会返回401状态码, 告知客户端出错的大致原因; 403 Forbidden : 和401状态码比较相似, 所以也经常被混淆; 其实403表示的是 授权(Authotization) 类型的错误, 授权和认证的不同之处是: 认证表示”识别前来访问的是谁”, 而授权则表示”赋予特定用户执行特定操作的权限” 通俗地说: 401状态码表示”我不知道你是谁”, 403状态码表示”虽然知道你是谁, 但你没有执行该操作的权限” 404 Not Found : 表示访问的数据不存在, 但是 例如当客户端湿度获取不存在的用户信息时, 或者试图访问原本就不存在的端点时, 服务器就会返回404状态码; 所以, 如果客户端想要获取用户信息, 却得到服务器端返回的404状态码, 客户端仅凭”404 Not Found”将难以区分究竟是用户不存在, 还是端点URI错误导致访问了原本不存在的URI; 405 Method Not Allowed : 表示虽然访问的端点存在, 但客户端使用的HTTP方法不被服务器端允许; 比如客户端使用了POST方法来访问只支持GET方法的信息检索专用的API; 又比如客户端用了GET方法来访问更新数据专用的API等; 406 Not Acceptable : 服务器端API不支持客户端指定的数据格式时, 服务器端所返回的状态码; 比如, 服务器端只支持JSON和XML输出的API被客户端指定返回YAML的数据格式时, 服务器端就会返回406状态码; 408 Request Timeout : 当客户端发送请求至服务器端所需的时间过长时, 就会触发服务器端的超时处理, 从而使服务器端返回该状态码; 409 Conflict: 用于表示资源发生冲突时的错误 (est中就会有该错误码) 比如通过指定ID等唯一键值信息来调用注册功能的API时, 倘若已有相同ID的数据存在, 就会导致服务器端返回409状态码; 在使用邮箱地址及Facebook ID等信息进行新用户注册时, 如果该邮箱地址或者ID已经被其他用户注册, 就会引起冲突, 这时服务器端就会返回409状态码告知客户端该邮箱地址或ID已被使用; 410 Gone : 和 404状态码 相同, 都表示访问资源不存在, 只是410状态码不单表示资源不存在, 还进一步告知资源曾经存在, 只是目前已经消失了; 因此服务器端常在访问被删除的数据时返回该状态码, 但是为了返回该状态码, 服务器必须保存该数据已被删除的信息, 而且客户端也应该知晓服务器端保存了这样的信息; 但是在通过邮箱地址搜索用户信息的API中, 从保护个人信息的角度来说, 返回410状态码的做法也会受到质疑; (所以在此种资源不存在的情况下, 为了稍微安全一些, 返回410状态码需要慎重) 413 Request Entity Too Large : 413也是比较容易出现的一种状态码, 表示请求实体过大而引发的错误 请求消息体过长是指, 比如在上传文件这样的API中, 如果发送的数据超过了所允许的最大值, 就会引发这样的错误; 414 Request-URI Too Large : 414是表示请求首部过长而引发的错误 如果在进行GET请求时, 查询参数被指定了过长的数据, 就会导致服务器端返回414状态码 415 Unsupported Media Type : 和406比较相似 406我们知道是表示服务器端不支持客户端想要接收的数据格式 而415表示的是服务器端不支持客户端请求首部 Content-Type 里指定的数据格式, 也就是说, 当客户端通过POST,PUT,PATCH等方法发送的请求消息体的数据格式不被服务器支持时, 服务器端就会返回415状态码; 例如在只接收JSON格式的API里, 如果客户端请求时发送的是XML格式的数据去请求服务器端, 或者在 Content-Type 首部指定 application/xml, 都会导致该类型错误; 429 Too Many Requests : 是2012年RFC6585文档中新定义的状态码, 表示访问次数超过了所允许的范围; 例如某API存在一小时内只允许访问100次的访问限制, 这种情况下入股哦客户端视图进行第101次访问, 服务器便会返回该状态码; 表示在一定的时间内用户发送了太多的请求, 即超出了”频次限制”, 在响应中，可以提供一个 Retry-After 首部来提示用户需要等待多长时间之后再发送新的请求; 5xx 5字头状态码表示错误不发生在客户端, 而是由服务器自身问题引发的。 500 Internal Server Error : 是web应用程序开发里非常常见的错误, 当服务器代码里存在bug, 输出错误信息并停止运行等情况下, 就会返回该类型的错误; 因此, 不仅限于API, 对于5字头状态码的错误, 都要认真监视错误日志, 使系统在出错时及时告知管理员, 以便在错误发生时做好应对措施, 防止再次发生。 501 Not Implemented : ??? 502 Bad GateWay : ??? 503 Service Unavaliable : 用来表示服务器当前处于暂不可用状态 可以回送:响应首部 Retry-After 表示多久恢复; 不同的客户端与服务器端应用对于 Retry-After 首部的支持依然不太一致; 不过，一些爬虫程序，比如谷歌的爬虫程序Googlebot, 会遵循Retry-After响应首部的规则, 将其与503(Service Unavailable,当前服务不存在)响应一起发送有助于互联网引擎做出判断,在宕机结束之后继续对网站构建索引。 参考:https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After 504 Gateway Time-out: 复现这个错误码比较简单, 让你的php程序模拟耗时请求 如下代码 123&lt;?phpsleep(70);//模拟耗时，睡70秒echo &quot;睡醒了&quot;; 返回 12504 Gateway Time-outnginx/1.11.4 505 HTTP Version Not Supported: 服务器收到的请求, 使用的是它无法支持的HTTP协议版本; 参考: 《HTTP权威指南》、《Web API的设计与开发》 http://yongxiong.leanote.com/post/Nginx状态码含义及chong-xuan#title-14","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. HTTP Methods","slug":"http/2017-11-30-HTTP-02","date":"2017-11-30T02:29:12.000Z","updated":"2019-03-13T02:05:50.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-02/","excerpt":"","text":"前言 HTTP/1.1 常见的 Method 有: OPTIONS, HEAD, GET, POST, PUT, DELETE, TRACE, CONNECT (参考RFC2616) RFC2616中提到: PATCH, LINK, UNLINK 方法被定义, 但并不常见; (《图解http协议》中也提到 LINK, UNLINK 已经被http1.1废弃); 不同应用各自的实现不同, 有些应用会完整实现, 有些还会扩展, 有些可能只会实现一部分; PUT PUT: 替换资源 PUT 和 POST的区别: 在HTTP中, PUT被定义为 idempotent(幂等性) 的方法, POST则不是, 这是一个很重要的区别 应该用 PUT 还是 POST? 取决于这个REST服务的行为是否是 idempotent(幂等) 的假如发送两个请求, 希望服务器端是产生两个新数据，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用 POST 方法但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的, 那就应该使用 PUT 方法 虽然 POST 和 PUT 差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦; POST POST: 上面已经提过了, POST是非幂等的 POST 和 PUT 都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的 PATCHPATCH不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题 对已有资源的操作: 用于对资源的 部分内容 进行更新 (例如更新某一个字段, 具体比如说只更新用户信息的电话号码字段) 而 PUT 则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变 HEAD HEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回, 而仅仅返回HTTP头信息 比如: 欲判断某个资源是否存在, 我们通常使用GET, 但其实用HEAD, 意义更加明确 GET比较简单, 直接获取资源; OPTIONS这个方法使用比较少, 它用于获取当前URL所支持的方法若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST之前跨域相关博文 CORS方案 not-so-simple request 中的”预检”请求用的请求方法就是 OPTIONS CONNECT要求用隧道协议连接代理, 如使用SSL TRACE~~未完待续 DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"03. 小试牛刀","slug":"swoole/2017-11-04-03","date":"2017-11-04T09:11:37.000Z","updated":"2018-11-28T06:13:48.000Z","comments":true,"path":"2017/11/04/swoole/2017-11-04-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/04/swoole/2017-11-04-03/","excerpt":"","text":"前言本篇主要是通过简单过一下Swoole的基本功能点来对swoole有个比较基础的认知 TCP Server server 代码 (在虚拟机中通过 php server.php 启动服务器) 123456789101112131415161718192021222324252627282930313233&lt;?php// 创建tcpserver服务器对象$tcpServer = new swoole_server(&quot;192.168.1.110&quot;, 8088, SWOOLE_PROCESS, SWOOLE_SOCK_TCP);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 * $reactor_id(该形参也常被命名为 $from_id): TCP连接所在的Reactor线程ID */$tcpServer-&gt;on(&apos;connect&apos;, function ($server, $fd, $reactor_id) &#123; echo &quot;Client:Connect, fd:&#123;$fd&#125;, reactor_id:&#123;$reactor_id&#125; \\n&quot;;&#125;);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 * $reactor_id(该形参也常被命名为 $from_id): TCP连接所在的Reactor线程ID * $data: 收到的数据内容，可能是文本或者二进制内容 */$tcpServer-&gt;on(&apos;receive&apos;, function ($server, $fd, $reactor_id, $data) &#123; // 此处还根据收到的消息内容做出不同的响应(当然这属于业务部分) $server-&gt;send($fd, &quot;fd:&#123;$fd&#125;, reactor_id:&#123;$reactor_id&#125;, receive:&#123;$data&#125; \\n&quot;);&#125;);/** * $server: swoole_server对象 * $fd: TCP客户端连接的唯一标识符 */$tcpServer-&gt;on(&apos;close&apos;, function ($server, $fd, $reactor_id) &#123; echo &quot;Client: Close.\\n&quot;;&#125;);$tcpServer-&gt;start(); 物理机上使用 telnet 命令测试 telnet 192.168.1.110 8088 (telnet 退出: ctrl+], 然后输入 quit) UDP Server server 代码 (在虚拟机中通过 php server.php 启动服务器) 123456789101112131415&lt;?php$udpServer = new swoole_server(&quot;192.168.1.110&quot;, 8088, SWOOLE_PROCESS, SWOOLE_SOCK_UDP);/** * UDP服务器与TCP服务器不同, UDP没有连接的概念 * 启动Server后, 客户端无需Connect, 直接可以向Server监听的端口发送数据包, 对应的事件为 onPacket * $clientInfo: 是客户端的相关信息, 是一个数组, 有客户端的IP和端口等内容 * 调用 $server-&gt;sendto 方法向客户端发送数据 */$udpServer-&gt;on(&apos;Packet&apos;, function ($server, $data, $clientInfo) &#123; $server-&gt;sendto($clientInfo[&apos;address&apos;], $clientInfo[&apos;port&apos;], &quot;data: &quot; . $data); var_dump($clientInfo);&#125;);$udpServer-&gt;start(); telnet 用于测试 tcp 协议的端口测试,但貌似无法用于udp协议, UDP服务器可以使用 netcat -u 来连接测试 123456 brew install netcat nc -v 或者 netcat -v netcat (The GNU Netcat) 0.7.1 ``` 3. 测试向UDP服务器发送数据包 : netcat -u 192.168.1.110 8088 hello data: hello 123## HTTP Server1. server 代码 (在虚拟机中通过 php server.php 启动服务器) &lt;?php // 通过几行代码即可写出一个异步非阻塞多进程的Http服务器 /** Http\\Server继承自Server, 是一个完整的http服务器实现, Http\\Server支持同步和异步2种模式*/$http = new Swoole\\Http\\Server(“192.168.1.110”, 8080); $http-&gt;on(‘request’, function ($request, $response) { echo “client request, host: {$request-&gt;header[‘host’]}, getParam: {$request-&gt;get[‘name’]} \\n”; $response-&gt;end(“Hello Swoole. #\" . rand(1000, 9999) . \"“);});$http-&gt;start(); 123456782. 物理机访问 http://192.168.1.110:8080/?name=lant ## [Websocket Server](https://wiki.swoole.com/wiki/page/397.html)## 异步毫秒定时器1. 场景: 定时任务(毫秒级别)2. 示例代码 &lt;?php/** 毫秒精度的定时器 底层基于 epoll_wait 和 setitimer 实现, 数据结构使用最小堆, 可支持添加大量定时器*/ // 循环执行定时器swoole_timer_tick(3000, function () { echo “after 3000ms.\\n”;}); // 单次执行定时器swoole_timer_after(3000, function () { echo “after 14000ms.\\n”;});```","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"01. 基本概念扫盲","slug":"网络编程/2018-11-29-01","date":"2017-11-04T09:11:37.000Z","updated":"2018-11-30T05:07:53.000Z","comments":true,"path":"2017/11/04/网络编程/2018-11-29-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/04/网络编程/2018-11-29-01/","excerpt":"","text":"文件描述符 维基百科 文件描述符(File descriptor)是一个用于表述在操作系统中, 指向一个文件(文件可理解为信息载体, 包括socket)的引用指针的抽象化概念 对一个 socket 的读写也会有相应的描述符, 称为 socketfd(socket 描述符) 文件描述符形式上是一个非负整数, 实际上, 它是一个索引值, 指向内核中为每个进程所维护的该进程打开文件的记录表, 当程序打开一个文件或者新创建一个文件, 内核就会向进程返回一个对应的文件描述符(fd) (这个概念只适用于unix、linux操作系统) 更详细可参考: https://learn-linux.readthedocs.io/zh_CN/latest/system-programming/file-io/file-descriptor.html 上下文当一个进程在执行时, CPU 的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文 用户空间与内核空间 Linux从整体上可以分为内核态与用户态 内核态就是内核所处的空间, 内核负责调用底层硬件资源, 并为上层应用程序提供运行环境, 用户态即应用程序的活动空间 比如一个32位的操作系统, 寻址地址(虚拟内存空间)是2的32次方, 也就是4G, 操作系统将较高的1G字节作为内核空间, 而将较低的3G字节作为用户空间, 内核空间具有用户空间所不具备的操作权限。 应用程序的运行必须依托于内核提供的硬件资源(如cpu\\存储\\IO), 内核通过暴露外部接口以便应用程序调用 — 称为 “SystemCall, 系统调用” 系统调用是操作系统的最小功能单位, 应用程序通常运行在用户空间, 当某些操作需要内核权限时, 就会通过系统调用(System calls), 进入内核态执行, 这也就是一次用户态\\内核态的转换, 切换过程中涉及了各种函数的调用以及数据的复制 虽然用户态下和内核态下工作的程序有很多差别, 但最重要的差别就在于特权级的不同, 即权力的不同。运行在用户态下的程序不能直接访问操作系统内核数据结构和程序 熟悉Unix/Linux系统的人都知道, fork() 的工作实际上是以系统调用的方式完成相应功能的, 具体的工作是由系统级的 sys_fork() 负责实施, 其实无论是不是Unix或者Linux, 对于任何操作系统来说, 创建一个新的进程都是属于核心功能, 因为它要做很多底层细致地工作, 消耗系统的物理资源, 比如分配物理内存, 从父进程拷贝相关信息, 拷贝设置页目录页表等等, 这些显然不能随便让哪个程序就能去做, 于是就自然引出特权级别的概念, 显然, 最关键性的权力必须由高特权级的程序来执行, 这样才可以做到集中管理, 减少有限资源的访问和使用冲突 很多程序开始时运行于用户态, 但在执行的过程中, 一些操作需要在内核权限下才能执行, 这就涉及到一个从用户态切换到内核态的过程, 一般存在以下三种需要切换的情况: 系统调用比如调用fork()函数产生进程的时候 程序异常比如5/0, 当除数为0的时候, 就会产生异常 外围设备的中断: 当外围设备完成用户请求的操作后, 会向CPU发出相应的中断信号, 这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序, 如果先前执行的指令是用户态下的程序, 那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成, 系统会切换到硬盘读写的中断处理程序中执行后续操作等 最终通过内核态、用户态的划分与协助, 保证了操作系统的安全性、稳定性 Socket socket也叫套接字, 起源于Unix, 而Unix/Linux基本哲学之一就是一切皆文件, 都可以用 “打开open –&gt; 读写write/read –&gt; 关闭close” 模式来操作, 而Socket就是该模式的一个实现, 所以可以认为Socket是一种特殊的文件; Socket是应用层与TCP/IP协议族通信的中间软件抽象层, 它是一组接口, 在设计模式中, Socket其实就是一个门面模式, 它把复杂的TCP/IP协议族隐藏在Socket接口后面 对于网络中进程之间的通信, 应用程序几乎都使用的是socket IO模型同步和异步 其实网上有很多资料在描述 同步, 异步, 阻塞, 非阻塞 之前的关系, 往往举了一大堆例子, 最终还是说不太清除。 其实应用程序要进行网络通信, 应用程序 必然会进行 系统调用 对于 同步和异步 来说, 我们不需要关注 同步调用在等待期间 或者 异步调用在立即返回后 应用程序是 休眠等待或者活跃去处理其他任务, 而重点关注的是 消息的通信机制, 也就是结果是应用程序主动去等待获取, 还是由系统将结果通知给应用程序 同步和异步关注的是消息通信机制 同步: 发出一个调用后, 一直等待, 直到这个调用结果完成后, 才返回 (同步主要体现在主动获取结果) 异步: 发出一个调用后, 立刻返回, 不需要等待调用结果, 而是在结果处理完成后, 通过 通知机制 或者 回调函数 进行通知 (异步主要体现在结果的回调通知) 阻塞和非阻塞关注的是程序在等待调用结果（消息, 返回值）时的状态.阻塞：发出调用后, 当前线程被挂起, 直到结果返回。非阻塞：发出调用后, 当前线程继续保持运行状态。总之, 同步异步要看是否需要等待操作执行的结构。阻塞非阻塞指请求后是否能够理解返回保持Running的状态。 参考: https://blog.csdn.net/lemon89/article/details/78290389 https://blog.csdn.net/orchestra56/article/details/81005494 https://learn-linux.readthedocs.io/zh_CN/latest/system-programming/file-io/file-descriptor.html持续扫盲中~~","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://blog.renyimin.com/categories/网络编程/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://blog.renyimin.com/tags/网络编程/"}]},{"title":"02. 环境准备","slug":"swoole/2017-11-03-02","date":"2017-11-03T10:36:52.000Z","updated":"2018-11-27T02:24:26.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-02/","excerpt":"","text":"前置 此处使用的是 Vagrant+VirtualBox+CentOS7.2 进行系统环境部署 (如有对Vagrant不熟悉的兄dei~可以提前去了解一下, 也比较简单) VagrantFile 1234567891011Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :swoole do |swoole| swoole.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;swoole-study&quot;, &quot;--memory&quot;, &quot;1000&quot;] end swoole.vm.box = &quot;centos7.2&quot; swoole.vm.hostname = &quot;lant&quot; swoole.vm.network :private_network, ip: &quot;192.168.1.110&quot; swoole.vm.synced_folder &quot;/Users/renyimin/Desktop/swoole-synced_folder&quot;, &quot;/swoole-synced&quot; endend PHP7.2.12 编译安装 下载 php-7.2.12.tar.gz 到 /Users/renyimin/Desktop/swoole-synced_folder 登录虚拟机, 将 /swoole-synced 下的 php-7.2.12.tar.gz 移动到 /usr/local/src: 1234567[root@lant /]# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv swoole-synced sys tmp usr vagrant var[root@lant /]# cd swoole-synced/[root@lant swoole-synced]# lsphp-7.2.12.tar.gz[root@lant swoole-synced]# mv /swoole-synced/php-7.2.12.tar.gz /usr/local/src/[root@lant swoole-synced]# 解压, configure, make, make install 123tar -zxvf php-7.2.12.tar.gz./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-mysql-sock --with-mysqli --with-libxml-dir --with-openssl --with-mhash --with-pcre-regex --with-zlib --with-iconv --with-bz2 --with-curl --with-cdb --with-pcre-dir --with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --with-gettext --with-gmp --with-mhash --with-libmbfl --with-onig --with-pdo-mysql --with-zlib-dir --with-readline --with-libxml-dir --with-xsl --with-pear --enable-fpm --enable-soap --enable-bcmath --enable-calendar --enable-dom --enable-exif --enable-fileinfo --enable-filter --enable-ftp --enable-gd-jis-conv --enable-json --enable-mbstring --enable-mbregex --enable-mbregex-backtrack --enable-pdo --enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem --enable-sysvshm --enable-wddx --enable-zip --enable-mysqlnd-compression-supportmake &amp;&amp; make install 配置环境变量 12345vi ~/.bash_profile# 添加alias php=/usr/local/php/bin/php# 最后执行source ~/.bash_profile 1234[root@lant tmp]# php -vPHP 7.2.12 (cli) (built: Nov 22 2018 05:50:26) ( NTS )Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies 拷贝配置文件 12345cp /usr/local/src/php-7.2.12/php.ini-production /usr/local/php/etc/php.inicp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf// 把pid 改成 /run/php-fpm.pidvim /usr/local/php/etc/php-fpm.conf 检测php当前使用的配置文件 123[root@lant php-fpm.d]# php -i | grep php.iniConfiguration File (php.ini) Path =&gt; /usr/local/php/etcLoaded Configuration File =&gt; /usr/local/php/etc/php.ini 开机启动php-fpm: 将php-fpm加入到system中管理 12345678910111213141516cd /lib/systemd/systemvim php-fpm.service[Unit]Description=The PHP FastCGI Process ManagerAfter=syslog.target network.target[Service]Type=simplePIDFile=/run/php-fpm.pidExecStart=/usr/local/php/sbin/php-fpm --nodaemonize --fpm-config /usr/local/php/etc/php-fpm.confExecReload=/bin/kill -USR2 $MAINPIDExecStop=/bin/kill -SIGINT $MAINPID[Install]WantedBy=multi-user.target 启动php-fpm: systemctl start php-fpm.service 添加到开机启动: systemctl enable php-fpm.service nginx 编译安装 下载 nginx-1.14.1.tar.gz 到 /Users/renyimin/Desktop/swoole-synced_folder 登录虚拟机, 将 /swoole-synced 下的 nginx-1.14.1.tar.gz 移动到 /usr/local/src 解压, configure, make, make install 1234tar -zxvf nginx-1.14.1.tar.gzyum -y install gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel open openssl-devel./configure --prefix=/usr/local/nginxmake &amp;&amp; make install 设置环境变量 123echo &apos;PATH=/usr/local/nginx/sbin:$PATH&apos; &gt;&gt; /etc/profileecho &apos;export PATH&apos; &gt;&gt; /etc/profilesource /etc/profile 设置开机自启动 1234567891011121314151617// 如果是使用yum安装的nginx, 则会自动创建/lib/systemd/system/nginx.service文件// 由于此处是使用编译安装, 所以需要手动在系统服务目录里创建nginx.service文件vi /lib/systemd/system/nginx.service[Unit]Description=nginxAfter=network.target [Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true [Install]WantedBy=multi-user.target 设置开机启动 12[root@lant system]# systemctl enable nginx.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. 配置nginx 主配置文件, /usr/local/nginx/conf/nginx.conf 1234567891011121314151617181920212223242526272829#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; include conf.d/*.conf;&#125; 在 conf 下创建 conf.d 目录 在 conf.d 目录下创建 default.conf, swool.conf 两个配置文件 default.conf 123456789101112131415161718192021server &#123; listen 80; server_name www.myswooleenv.com; #access_log logs/host.access.log main; location / &#123; root /html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location ~ \\.php$ &#123; root /html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; swoole.conf 123456789101112131415server &#123; listen 80; server_name www.swoolestudy.com; location / &#123; root /swooleroot; index index.html index.htm; &#125; location ~ \\.php$ &#123; root /swooleroot; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 在虚拟机中 /html 下创建phpinfo.php, 最后即可在物理机通过 www.myswooleenv.com/phpinfo.php 访问 swoole编译 git clone 下载 swoole 源码 (注意需要注册码云账号并添加本地git公钥) 12cd /usr/local/src git clone git@gitee.com:swoole/swoole.git 编译安装, 使用 phpize 生成 swoole.so 外挂模块 1234567/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make install# 结果Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20170718/Installing header files: /usr/local/php/include/php/[root@lant swoole]# 也可以访问 phpinfo.php 看到上面的 extension_dir 地址, 可以查看 swoole 扩展的编译结果 123[root@lant ~]# cd /usr/local/php/lib/php/extensions/no-debug-non-zts-20170718[root@lant no-debug-non-zts-20170718]# lsopcache.a opcache.so swoole.so php 支持swoole: vi /usr/local/php/etc/php.ini 加入 extension=swoole.so, 重启php-fpm systemctl restart php-fpm.service 访问: http://192.168.1.110/phpinfo.php, 发现已经有了 swoole 扩展 测试 swoole源码中有示例: cd /usr/local/src/swoole/examples/server 运行测试文件: php echo.php 安装 nestat 命令用于查看端口运行情况 : yum -y install net-tools 查看端口运行情况, 可看到 echo.php 这个示例文件已经被成功运行 12netstat -natp | grep 9501tcp 0 0 0.0.0.0:9501 0.0.0.0:* LISTEN 27927/php 杀掉该进程, 则端口不再被占用 123[root@lant server]# php echo.php^C[root@lant server]# tool 为了方便测试, 需要安装 telnet 工具 : yum install xinetd telnet telnet-server -y 参考 https://wiki.swoole.com/wiki/page/351.html","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"01. swoole简介 及 知识扫盲","slug":"swoole/2017-11-03-01","date":"2017-11-03T02:45:17.000Z","updated":"2018-11-22T09:05:27.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-01/","excerpt":"","text":"简介引用swoole官网对swoole的介绍 Swoole:面向生产环境的 PHP 异步网络通信引擎 使 PHP 开发人员可以编写高性能的异步并发 TCP、UDP、Unix Socket、HTTP，WebSocket 服务。Swoole 可以广泛应用于互联网、移动通信、企业软件、云计算、网络游戏、物联网（IOT）、车联网、智能家居等领域。 使用 PHP + Swoole 作为网络通信框架，可以使企业 IT 研发团队的效率大大提升，更加专注于开发创新产品。 特性 Swoole 使用纯 C 语言编写，提供了 PHP 语言的异步多线程服务器，异步 TCP/UDP 网络客户端，异步 MySQL，异步 Redis，数据库连接池，AsyncTask，消息队列，毫秒定时器，异步文件读写，异步DNS查询。 Swoole内置了Http/WebSocket服务器端/客户端、Http2.0服务器端。 除了异步 IO 的支持之外，Swoole 为 PHP 多进程的模式设计了多个并发数据结构和IPC通信机制，可以大大简化多进程并发编程的工作。其中包括了并发原子计数器，并发 HashTable，Channel，Lock，进程间通信IPC等丰富的功能特性。 Swoole2.0 支持了类似 Go 语言的协程，可以使用完全同步的代码实现异步程序。PHP 代码无需额外增加任何关键词，底层自动进行协程调度，实现异步。 典型应用场景 移动互联网API服务器 物联网(IOT) 微服务(Micro Service) 高性能Web服务器 游戏服务器 在线聊天系统 很多用户案例可以参考 https://wiki.swoole.com/wiki/page/p-case.html","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"70. MySQL5.6 GTID 新特性","slug":"MySQL/2017-10-04-mysql-80","date":"2017-10-04T07:21:36.000Z","updated":"2019-03-27T06:22:37.000Z","comments":true,"path":"2017/10/04/MySQL/2017-10-04-mysql-80/","link":"","permalink":"http://blog.renyimin.com/2017/10/04/MySQL/2017-10-04-mysql-80/","excerpt":"","text":"http://cenalulu.github.io/mysql/mysql-5-6-gtid-basic/MMM, MHA","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"70. 查询性能优化","slug":"MySQL/2017-09-27-mysql-70","date":"2017-09-27T12:50:37.000Z","updated":"2018-03-08T06:03:46.000Z","comments":true,"path":"2017/09/27/MySQL/2017-09-27-mysql-70/","link":"","permalink":"http://blog.renyimin.com/2017/09/27/MySQL/2017-09-27-mysql-70/","excerpt":"","text":"前言之前已经了解了索引优化的相关内容, 它对于高性能是必不可少的, 但还不够, 还需要合理设计查询; 如果查询写的很糟糕, 即使库表结构再合理, 索引再合适, 也无法实现高性能; 查询优化, 库表结构优化, 索引优化需要齐头并进, 一个不落; 慢查询基础优化数据访问 确认应用程序是否在检索大量超过需要的数据, 你可能访问了太多的行, 也可能是太多的列;比如: 总是返回全部的列; 只展示5条数据,你却查出100条; 确认MySQL服务器层是否在分析大量超过需要的数据行; (注意: 索引是在存储引擎层, 一旦服务器层分析的数据过多, 可能你的索引不太合适, 没有在存储引擎层过滤掉数据) 未完待续~~","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"查询性能优化","slug":"查询性能优化","permalink":"http://blog.renyimin.com/tags/查询性能优化/"}]},{"title":"49. 索引和锁","slug":"MySQL/2017-09-25-mysql-49","date":"2017-09-25T13:10:40.000Z","updated":"2018-03-08T02:57:03.000Z","comments":true,"path":"2017/09/25/MySQL/2017-09-25-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/MySQL/2017-09-25-mysql-49/","excerpt":"","text":"索引可以让查询锁定更少的行 因为InnoDB只有在访问行的时候才会对其加锁, 而索引能够减少InnoDB访问的行数, 从而减少锁的数量;但这只有当InnoDB在存储引擎层就能过滤掉所有不需要的行时才行, 如果索引(处在存储引擎层)无法过滤掉无效的行, 那么在InnoDB检索到数据并发送给服务器层以后, 服务器层才能应用where子句, 这时已经无法避免锁定行了;虽然InnoDB的行锁效率很高, 内存使用也很少, 但是锁定行的时候仍然会带来额外开销;锁定超过需要的行会增加锁争用并减少并发性;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"48. 冗余和重复索引","slug":"MySQL/2017-09-25-mysql-48","date":"2017-09-25T10:27:40.000Z","updated":"2018-03-08T02:39:49.000Z","comments":true,"path":"2017/09/25/MySQL/2017-09-25-mysql-48/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/MySQL/2017-09-25-mysql-48/","excerpt":"","text":"重复索引 是指在相同的列上按照相同的顺序创建的相同类型的索引; 应该避免这样的重复索引, 发现后也应该立即删除; MySQL允许在相同的列上创建多个索引, 但是MySQL需要单独维护重复的索引, 并且优化器在优化查询的时候也需要逐个地进行考虑, 这会影响性能; 冗余索引 和 重复索引 不同, 如果创建了索引(A,B), 在创建索引(A)就是冗余索引, 因为这只是(A,B)索引的前缀索引; 大多数情况下都不需要冗余索引; 因此索引(A,B)也可以当做索引(A)来使用 但是如果再创建索引(B,A), (B) 则都不是冗余索引 有时候为了让两个查询都变快, 也会需要冗余索引 (P179) 应该尽量扩展已有的索引而不是创建新的索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"47. 使用索引扫描来做排序","slug":"MySQL/2017-09-25-mysql-47","date":"2017-09-25T10:25:11.000Z","updated":"2018-03-08T05:38:13.000Z","comments":true,"path":"2017/09/25/MySQL/2017-09-25-mysql-47/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/MySQL/2017-09-25-mysql-47/","excerpt":"","text":"简介 只有当索引的列顺序 和 ORDER BY 子句的顺序完全一致, 并且所有列的排序方向都一样时(要么都是正序, 要么都是倒序), MySQL才能够使用索引来对结果做排序; 如果查询需要关联多张表, 则只有当 ORDER BY 子句引用的字段全部为第一个表时, 才能使用索引做排序; ORDER BY 子句 和 查找型查询的限制是一样的, 需要满足索引的最左前缀的要求, 否则, MySQL都需要亲自去执行排序操作, 而无法利用索引排序; 有一种情况下, ORDER BY 子句可以不用满足最左前缀的要求, 那就是前导列为常量的时候; 比如一张表的索引是 key(a,b,c) , 而 查询语句是 ... where a=100 order by b,c, 即使 order by 不满足最左前缀的要求, 也可以使用索引做排序; P177 列出了很多不可以使用索引做排序的查询; 当查询同时有 ORDER BY 和 LIMIT 子句的时候 像select &lt;col...&gt; from profiles where sex=&#39;m&#39; order by rating limit 10;这种查询语句, 同时使用了order by和limit, 如果没有索引就会很慢; 即使有索引, 如果用户界面有翻页, 翻页比较靠后时, 也会非常慢, 因为随着偏移量的增加, MySQL需要花费大量的时间来扫描需要丢弃的数据; 但是sex的选择性又很低, 如何优化呢? 对于选择性非常低的列, 如果要做排序的话, 可以增加一些特殊的索引来做排序, 例如, 可以创建 (sex, rating)索引 然后采用 延迟关联 , 通过覆盖索引先查询返回需要的主键, 在根据这些主键关联原表获得需要的行;select &lt;col...&gt; from profiles INNER JOIN (select &lt;primart key&gt; from profiles where x.sex=&#39;m&#39; order by rating limit 100000, 10) as x using(primary key)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"45","slug":"MySQL/2017-09-24-mysql-45","date":"2017-09-24T12:10:31.000Z","updated":"2019-04-22T11:00:50.000Z","comments":true,"path":"2017/09/24/MySQL/2017-09-24-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/MySQL/2017-09-24-mysql-45/","excerpt":"","text":"简介 因为是存储引擎负责实现索引, 所以不是所有的存储引擎都支持聚簇索引, 这里主要讨论的是 InnoDB引擎的聚簇索引; InnoDB的 聚簇索引 实际上在同一个结构中保存了 B-Tree索引 和 数据行 当表有聚簇索引时, 它的数据行实际上存放在索引的叶子页中(叶子页包含了数据行的 全部列数据) 因为无法同时把数据行存放在两个不同的地方, 所以一个表只能有一个聚簇索引 不过覆盖索引, 可以模拟多个聚簇索引的情况 InnoDB默认会创建聚簇索引: InnoDB通过主键来作为聚簇索引, 如果没有定义主键, 则会选择一个唯一的非空索引代替, 如果连非空索引都没有, InnoDB会隐式定义一个主键来作为聚簇索引; 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上，若使用where id = 14这样的条件查找数据; 则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据; InnoDB只聚集 在同一个磁盘页面中的记录, 因此, 如果数据在物理上是相邻的, 那么在索引上就也是相邻的; 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的; 但是包含相邻键值的页面可能物理上会相距甚远; 聚簇索引的优点 访问速度更快: 聚簇索引将索引和数据保存在同一个B-Tree中, 因此从聚簇索引中获取数据通常比在非聚簇索引中查询要快; 使用覆盖索引的查询, 可以直接使用页节点中的主键值; 聚簇索引缺点 聚簇索引最大限度地提高了I/O密集型应用的性能, 但如果数据全部都放在内存中, 则访问顺序就没那么重要了, 聚簇索引也就没什么优势了; 插入速度严重依赖于插入顺序, 按照主键的顺序插入是速度最快的方式, 但如果不是按照主键顺序, 在完成操作后最好执行 OPTIMIZE TABLE 命令重新组织一下表; 更新聚簇索引的代价很高, 因为会强制InnoDB将每个被更新的行移动到新的位置; 基于聚簇索引的表在插入新行, 或者主键被更新导致需要移动行的时候, 可能面临 “页分裂” 问题; 当前主键值要求必须将这一行插入到某个已满的页中时, 存储引擎会将该页分裂成两个页面来容纳该行, 这就是一次页分裂操作。 页分裂操作会导致表占用更多的磁盘空间 聚簇索引会导致全表扫描变慢, 尤其是行比较稀疏, 或者由于页分裂导致数据存储不连续的时候; 二级索引(非聚簇索引)可能比想象的要更大, 因为在二级索引的叶子节点包含了引用行的主键列; 二级索引访问需要两次索引查找, 而不是一次 二级索引叶子几点保存的 “行指针” 是行的主键; 这意味着通过二级索引查找行, 存储引擎需要找到二级索引叶子节点获得对应的主键值; 然后根据这个主键值去聚簇索引中查找对应的行数据; 这里做了重复工作, 两次 B-Tree 查找, 而不是一次。 InnoDB 和 MyISAM 索引对比 InnoDB支持聚簇索引, 而MyISAM不支持; MyISAM中主键索引和其他索引在索引结构上没有区别; 而InnoDB中 (主键)聚簇索引 和 二级索引(普通索引) 是有区别的;(P167) 上图总结","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"44. 高性能索引策略 -- 多列索引","slug":"MySQL/2017-09-24-mysql-44","date":"2017-09-24T09:25:31.000Z","updated":"2018-03-07T12:23:09.000Z","comments":true,"path":"2017/09/24/MySQL/2017-09-24-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/MySQL/2017-09-24-mysql-44/","excerpt":"","text":"前言 在多个列上建立独立的单列索引, 在大部分情况下并不能提高MySQL的查询性能; 例如: film_actor 表在 film_id 和 actor_id 上各有一个单列索引, 但是对于下面这个查询WHERE条件, 这两个单列索引都不是好的选择 select film_id, actor_id from actor where actor_id=1 OR film_id=1; 对于上面的查询 老版本的MySQL会使用全表扫描; 而新版本会使用 索引合并策略(参考P158) 来进行优化, 但这更说明了表上的索引建的很糟糕 接下来除了 之前已经在博文: B-Tree索引中介绍过的多列索引的 左前缀策略; 你还需要关注的是创建索引时, 索引列的顺序 选择合适的索引列顺序 在一个多列索引中, 索引列的顺序首先决定了最左前缀策略在查询时是如何进行的; 其次还意味着索引首先按照最左列进行排序, 其次是第二列, 等等; 所以多列索引的列顺序至关重要; 在不需要考虑排序和分组的时候, 将选择性最高的列放在前面通常是很好的, 这时候索引的作用只是用于优化where条件的查找。 然而, 性能不知是依赖于所有索引列的选择性(整体基数), 也和查询条件的具体值有关, 也就是和值的分布有关; ~~未完待续","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"43. 高性能索引策略 - 独立的列, 前缀索引","slug":"MySQL/2017-09-24-mysql-43","date":"2017-09-24T09:20:31.000Z","updated":"2018-03-08T05:22:03.000Z","comments":true,"path":"2017/09/24/MySQL/2017-09-24-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/MySQL/2017-09-24-mysql-43/","excerpt":"","text":"前言前面已经对常见的索引类型及其对应的优缺点有了一定的了解, 接下来要考虑的就是如何 高效正确地选择并使用索引 独立的列独立的列是指: 在查询条件中, 索引列 不能是表达式的一部分, 也不能是函数的参数; (如: select actor_id from actor where actor_id+1=5; 就无法使用索引, 应该始终将索引列单独放在比较符号的一侧, select actor_id from actor where actor_id=4;); 前缀索引 因为B-Tree索引中存储了实际的列值, 所以如果你需要索引的列的内容很长, 就会导致 索引变得大且慢; 对于BLOB, TEXT 或者很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 正常情况下, 不论是创建 单列索引 还是 多列索引, 创建的索引都是以 某个列完整的值 来创建索引, 而 前缀索引则是以 列开始的部分字符 来创建索引, 从而大大节约索引空间, 提高索引效率, 但是这样会降低索引的选择性; 索引选择性 索引选择性: 是指不重复的索引值(基数) 和 数据表的记录总数的比值(当然, 唯一索引的所有索引值都不同, 选择性是1, 这是最好的索引选择性, 性能也是最好的); 假设一张订单表, 按照 city(城市全名) 来分 和 按照 city(第一个字)来分组 , 那肯定前一种情况分出来的组比较多, 也就是不重复的索引值多; 如果按照 city 字段的前3个字符来分组的话, 效果如下 如果按照 city 字段的前7个字符来分组的话, 可以想到, 自然可能会是 分组会更多, 每组的数据会更少 分的组越多, 也就是如果以此长度的前缀创建索引的话, 不重复的索引值也就越多, 那么选择性就越高; 选择性越高, 则查询效率越高, 因为MySQL在查找时能够通过索引就过滤掉更多的行, 否则一个索引还是对应了很多的数据行, 那效率还是很低; 而我们要做的其实就是让我们的 前缀选择性 接近 完整列的选择性 简单点说, 让 city(n) 接近 city 的选择性; 计算完整列的选择性 下面给出了同一个查询中计算不同前缀长度的选择性 前缀索引是一种能使索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 有无法使用前缀索引做覆盖扫描 创建前缀索引: alter table city add key(city(7)), 表示以 city 字段的前7个字符 来创建索引; 参考p189: 根据传统经验, 不应该在选择性低的裂伤创建索引, 但是如果很过查询都用到该列, 比如一个表中的 gender 列, 考虑到使用的频率, 还是建议在创建不同组合的索引时, 将 (sex) 作为前缀!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"42. 哈希索引","slug":"MySQL/2017-09-24-mysql-42","date":"2017-09-24T07:01:31.000Z","updated":"2018-03-07T10:04:36.000Z","comments":true,"path":"2017/09/24/MySQL/2017-09-24-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/MySQL/2017-09-24-mysql-42/","excerpt":"","text":"简介 哈希索引(hash index)基于哈希表实现, 只有精确匹配索引中所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code): 哈希索引将所有的哈希码存储在索引中 同时在哈希表中保存指向每个数据行的指针 在MySQL中, 只有Memory引擎显示支持哈希索引, 这也是Memory引擎表的默认索引类型, Memory引擎也支持B-Tree索引。 Memory引擎支持 非唯一哈希索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码, 如果出现这种情况, 索引会以链表的方式存放多个行指针到同一个哈希条目中 查找时, 会1. 先在索引中按照哈希码来找到指向数据行的指针, 2. 然后比较数据行的值是否是你查找的行 哈希索引的限制因为索引自身只要存储对应的哈希值和行指针, 所以索引的结构十分紧凑, 这也让哈希索引的查找速度非常快, 然而哈希索引也有它的限制: 哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快; 哈希索引数据并不是按照索引值顺序排序的, 所以无法用于排序; 不支持 部分索引列匹配查找 , 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 不支持范围查询 (只支持如 =, &lt;&gt;, in 等一些 等值比较) 哈希索引数据非常快, 除非有很多哈希冲突 (因为memory引擎支持非唯一索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码)当出现哈希冲突时, 存储引擎必须遍历 冲突的哈希值 所对应的 链表 中所有的行指针, 逐行到表中进行比较, 直到找到所有符合条件的行; 哈希冲突如果哈希冲突很多的话, 一些索引维护操作的代价也为很高, 如果表中删除一行数据, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的指针, 冲突越多代价越大;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"41. B-Tree 索引","slug":"MySQL/2017-09-24-mysql-41","date":"2017-09-24T06:50:19.000Z","updated":"2019-04-09T13:27:59.000Z","comments":true,"path":"2017/09/24/MySQL/2017-09-24-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/MySQL/2017-09-24-mysql-41/","excerpt":"","text":"6 B-tree索引的限制 到这里可以看到: 索引列的顺序是非常重要的! 在优化性能的时候, 可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求;","categories":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/categories/《高性能MySQL》/"}],"tags":[{"name":"《高性能MySQL》","slug":"《高性能MySQL》","permalink":"http://blog.renyimin.com/tags/《高性能MySQL》/"}]},{"title":"27. REPEATABLE READ 可重复读","slug":"MySQL/2017-09-17-mysql-27","date":"2017-09-17T14:10:52.000Z","updated":"2018-03-07T09:19:48.000Z","comments":true,"path":"2017/09/17/MySQL/2017-09-17-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/MySQL/2017-09-17-mysql-27/","excerpt":"","text":"前言 该隔离级别可以解决不可重复读问题, 脏读问题; 也就是它既可以让事务只能读其他事务已提交的的记录, 又能在同一事务中保证多次读取的数据即使被其他事务修改,读到的数据也是一致的。 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免 脏读, 不可重复读 这两个问题的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"40. 阿里云redis","slug":"redis/2017-09-07-redis-40","date":"2017-09-07T03:21:13.000Z","updated":"2019-02-22T13:33:43.000Z","comments":true,"path":"2017/09/07/redis/2017-09-07-redis-40/","link":"","permalink":"http://blog.renyimin.com/2017/09/07/redis/2017-09-07-redis-40/","excerpt":"","text":"https://help.aliyun.com/document_detail/62870.html?spm=5176.7897645.101.3.35f74e7cWoih5m","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"28. 隔离级别 与 锁","slug":"MySQL/2017-09-03-mysql-28","date":"2017-09-03T06:20:52.000Z","updated":"2018-09-03T03:54:23.000Z","comments":true,"path":"2017/09/03/MySQL/2017-09-03-mysql-28/","link":"","permalink":"http://blog.renyimin.com/2017/09/03/MySQL/2017-09-03-mysql-28/","excerpt":"","text":"前言 之前几篇博文已经介绍了Mysql事务, 高并发下事务将会面对的问题 及 MySQL的解决方案; MySQL主要采用 事务隔离性中的4种隔离级别 结合 MVCC机制 来进行解决; 而事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略; 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的; READ UNCOMMITTED 未提交读READ COMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料 《高性能MySQL》 MySQL官方文档 美团技术博客 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"27. 幻读, 快照读(snapshot read), 当前读 (current read)","slug":"MySQL/2017-09-02-mysql-27","date":"2017-09-02T11:25:07.000Z","updated":"2018-09-01T14:02:47.000Z","comments":true,"path":"2017/09/02/MySQL/2017-09-02-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/MySQL/2017-09-02-mysql-27/","excerpt":"","text":"RR + MVCC 虽然解决了 幻读 问题, 但要注意, 幻读针对的是读操作(对于其他操作就不一样了); 演示 打开 两个客户端 1,2 确保隔离级别为默认级别RR, 提供语句: 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 123456789101112mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) 回到 客户端2 的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了 select 幻读)!! 12345678910111213141516171819202122mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 但如果尝试在客户端2的事务中执行 insert/delete/update , 却会发现此类操作都可以感知到客户端1提交的新数据 123mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; 小结 虽然发现已经克服了幻读问题; 但当 在客户端2事务中 insert 插入一条id为4的新数据, 却发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 — 一致性非阻塞读中的一段介绍 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。如果一个事务去更新或删除其他事务提交的行, 则那些更改对当前事务就变得可见;但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 也就是RR隔离级别, 在同一事务中多次读取的话, 对 select 克服了 幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念: 当前读 和 快照读 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的 select 都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 快照读： 是通过MVVC(多版本控制)和 undo log 来实现的, 常见语句如下(貌似就是常见的悲观锁么): 1简单的select操作 (不包括: `select ... lock in share mode`, `select ... for update`) 而 当前读 根本不会创建任何快照, insert, update, delete都是当前读, 所以这几个操作会察觉到其他事务对数据做的更改(而普通select是察觉不到的): 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MySQL 高并发下常见的事务问题","slug":"MySQL/2017-09-02-mysql-26","date":"2017-09-02T06:56:32.000Z","updated":"2018-09-01T14:02:40.000Z","comments":true,"path":"2017/09/02/MySQL/2017-09-02-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/MySQL/2017-09-02-mysql-26/","excerpt":"","text":"前言上一篇MySQL事务简介中对MySQL事务的 基本概念 及 特性 做了简单介绍; 接下来会分析在实际生产环境中面对高并发场景时, 事务会出现的一些常见问题; 高并发事务问题在并发量比较大的时候, 很容易出现 多个事务并行 的情况; 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境 过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以 最终导致了一些问题的出现; 脏读 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读 因为 事务A 此时读取到的是 并行事务B 尚未最终持久化的数据 (该数据还不具备事务的 持久性) 事务B 最终可能会因为其事务单元内部其他后续操作的失败 或者 系统后续突然崩溃等原因, 导致事务B最终整体提交失败而回滚, 那么最终 事务A 之前拿到就是 脏的数据 了(当然, 如果 事务A 在后续操作中继续读取的话, 无论事务B是否结束, 其每次的更新操作, 事务A都会及时读到新数据, 只不过这同时涉及到了下一个讨论的 不可重复读问题, 暂时可以不了解) 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身隔离性解决了脏读问题 : READ COMMITED 或 以上隔离级别(RC+); READ COMMITED 隔离级别保证了: 在事务单元中, 某条语句执行时, 只有已经被其他事务提交的持久性落地数据, 才对该语句可见; 不可重复读 之前 脏读问题 的解决了, 仅仅只意味着事务单元中的每条语句读取到的数据都是 具备持久性的落地数据而已; 之前在讨论脏读问题时, 有个问题也同时存在着, 那就是一个事务单元中 不可重复读 的问题; 显然, RC 隔离级别只解决了 脏读的问题 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了 解决方案 : RR+ 在MySQL中, 事务已经用自身隔离性解决了 不可重复读 问题 — REPEATABLE READ 或 以上隔离级别(RR+); REPEATABLE READ 级别保证了:在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的; ( READ COMMITED )在事务中, 多次读取同一个数据(在两次读取操作之间, 无论数据被 提交 多少次(即无论落地过多少遍), 每次读取的结果都应该是和事务中第一次读取的结果一样; 幻读 可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 不可重复读 和 幻读 这两个概念容易搞混 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录); 解决方案: RR + MVCC 其实对于 幻读 问题, 在Mysql的InnoDB存储引擎中, 是通过事务的 RR + MVCC机制 进行解决的;当然, 这里的幻读不涉及 具有当前读能力的那些语句; (也就是说只是解决幻读, 所谓幻写之类的就不在范围内了) 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 之所以 不可重复读 和 幻读 容易搞混, 可能是因为: 在mysql中, 由于默认就是RR隔离级别下, 该隔离级别已经解决了幻读, 所以无法模拟出幻读的场景; 而 退回到 RC隔离级别 的话, 虽然 幻读 和 不可重复读 都会出现, 但由于现象都是两次读取结果不一样, 容易分辨不出! 想了解更多, 可以参考下一篇幻读的延伸 高并发事务问题 之 更新丢失最后聊一下高并发事务的另一个问题, 也是最常遇到的问题: 丢失更新问题; 该问题和之前几个问题需要区分开: 该问题需要我们自己来解决;更新丢失问题分为两类 第一类丢失更新(回滚覆盖)简介 事务A 回滚时, 将 事务B 已经提交的数据覆盖了 需要注意的是: 这种情况在Mysql中不会出现; RU 级别演示 对于InnoDB事务的最低隔离级别 READ UNCOMMITED, 并行事务B的未提交数据都可以读到, 更别说已提交数据了 (所以回滚也会回滚到事务B提交的最新数据) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RC 级别演示 对于 READ COMMITTED: 在事务B提交之后, 事务A在T3阶段是可以select(快照读)到事务B最终提交的数据的, 更别说update(当前读)到了, 所以事务A最终的Rollback其实也是基于事务B提交后的数据的 (关于这里提到的快照读和当前读, 下一篇会介绍) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RR 级别演示 对于 REPEATABLE READ 可重复读, 事务A在T3阶段虽然select不到事务B最终提交的数据(快照读), 但是可以update(当前读)到事务B最终提交的数据的 (注意: RR与RC虽然都会有快照读, 但是快照读的结果却不一致, 其实是因为两者的MVCC机制快找时机不同导致的, 后面会讲解) 语句如下: 1234567SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age+10 where id=2;rollback; 12345SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age-15 where id=2;commit; SERIALIZABLE 演示 SERIALIZABLE 串行化: 读写都加锁, 最容易出现死锁, 所以也不会出现第一类丢失更新的问题, 直接就死锁了 语句如下: 123456SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;SELECT @@SESSION.tx_isolation;begin;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age -15 where id=2;commit; 第二类丢失更新(提交覆盖) 直接上图 另外, 这里可以解释一下为什么 SERIALIZABLE级别 通常不会不被采用 其实 SERIALIZABLE 虽然做了串行化, 其实也就是对读写都加了锁, 但一旦事务并行, 如果将判断库存的读操作放在事务内就很容易会死锁而放在事务外, 由于更新操作仍然会依据上一个查询的结果, 所以仍然是避免不了第二类丢失更新问题的, 会造成超卖等问题; SERIALIZABLE 的串行化本身也太低效 另外, 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 解决第二类丢失更新的方案: 乐观锁 (在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制) 悲观锁 参考资料: 《高性能MySQL》 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"25. MySQL 事务简介","slug":"MySQL/2017-08-27-mysql-25","date":"2017-08-27T11:31:07.000Z","updated":"2018-09-01T14:02:34.000Z","comments":true,"path":"2017/08/27/MySQL/2017-08-27-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/08/27/MySQL/2017-08-27-mysql-25/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的工作单元, 在这个独立的工作单元中, 可以有一组操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败 随处可见的例子: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，就需要打包在一个事务中作为 独立的工作单元 来执行。并且在 这个独立工作单元中的三个操作, 只要有任何一个操作失败, 则整体就应该是失败的, 那就必须回滚所有已经执行了的步骤; 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚 (这也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元;对于一个事务来说, 不能只成功执行其中的一部分操作, 整个事务中的所有操作要么全部成功提交, 要么有操作失败导致所有操作全部回滚, 这就是事务的原子性。 Consistency 一致性此一致性非彼一致性 你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性保证的是 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态, 而不是分布式中提到的数据一致性; 比如之前转账的例子: 转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15) 转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115) 转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取 Iron Man账户 的余额, 那么这个程序读到的应该是500才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务 隔离性 的四个 隔离级别, 到时候就知道这里为什么说 通常来说 ; (确实有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读)(注意: 和RR一样都采用了MVCC机制, 但与RR级别主要区别是快照时机不同, 暂时可不必了解, 后面文章会详解) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 只有该隔离级别才会读写都加锁 Durability 持久性 一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久保存到数据库中; 所谓永久, 也只是主观上的永久, 可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方; 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"09. utf8和utf8mb4区别","slug":"MySQL/2017-08-23-mysql-09","date":"2017-08-23T10:51:28.000Z","updated":"2019-03-30T06:12:28.000Z","comments":true,"path":"2017/08/23/MySQL/2017-08-23-mysql-09/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/MySQL/2017-08-23-mysql-09/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"12. MySQL备份与恢复","slug":"MySQL/2017-08-23-mysql-12","date":"2017-08-23T10:51:28.000Z","updated":"2018-03-18T07:29:49.000Z","comments":true,"path":"2017/08/23/MySQL/2017-08-23-mysql-12/","link":"","permalink":"http://blog.renyimin.com/2017/08/23/MySQL/2017-08-23-mysql-12/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"06. MySQL 设置白名单","slug":"MySQL/2017-08-19-mysql-07","date":"2017-08-19T09:03:52.000Z","updated":"2019-03-28T02:25:48.000Z","comments":true,"path":"2017/08/19/MySQL/2017-08-19-mysql-07/","link":"","permalink":"http://blog.renyimin.com/2017/08/19/MySQL/2017-08-19-mysql-07/","excerpt":"","text":"自建设置白名单https://www.cnblogs.com/lsdb/p/6795053.html 阿里云rds白名单 ip白名单 还可以配置ecs安全组https://help.aliyun.com/document_detail/43185.html?spm=5176.11065259.1996646101.searchclickresult.433a2e6cK2So7k","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"06. MySQL DNS解析","slug":"MySQL/2017-08-19-mysql-06","date":"2017-08-19T03:10:09.000Z","updated":"2019-03-28T02:26:36.000Z","comments":true,"path":"2017/08/19/MySQL/2017-08-19-mysql-06/","link":"","permalink":"http://blog.renyimin.com/2017/08/19/MySQL/2017-08-19-mysql-06/","excerpt":"","text":"https://cloud.tencent.com/developer/article/1039828 rrc的测试库中, skip_name_resolve 是开启状态 (不能通过主机域名进行dns解析)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"05. MySQL Replication","slug":"MySQL/2017-08-18-mysql-05","date":"2017-08-18T10:51:28.000Z","updated":"2019-03-13T08:26:53.000Z","comments":true,"path":"2017/08/18/MySQL/2017-08-18-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2017/08/18/MySQL/2017-08-18-mysql-05/","excerpt":"","text":"概述 MySQL内建的复制功能是构建基于MySQL的大规模、高性能应用的基础, 这类应用使用所谓的水平扩展的架构。 可以通过为服务器配置一个或多个备库的方式来进行数据同步, 复制功能不仅有利于构建高性能的应用, 同时也是高可用、可扩展性、灾难恢复、备份以及数据仓库等工作的基础。 复制解决的基本问题是让一台服务器的数据与其他服务器的数据保持同步。一台主库的数据可以同步到多台备库上, 备库本身也可以被配置成另外一台服务器的主库。主库和备库之间可以有多种不同的组合方式。 MySQL版本对主从复制的影响: 新版本服务器可以作为老版本服务器的备库, 但是老版本服务器作为新版本服务器的备库通常是不可行的, 因为老版本可能无法解析新版本所采用的新特性或语法; 另外, 所使用的二进制文件格式也可能不相同; 小版本升级通常是兼容的; 开销 复制通常不会增加主库的开销, 主要是启用二进制带来的开销, 但出于对备份或及时从崩溃中恢复的目的, 这点开销也是必要的; 每个备库也会对主库增加一些负载(例如网络I/O开销), 尤其当备库请求从主库读取旧的二进制日志文件时, 可能会造成更高的I/O开销; 另外, 锁竞争也可能阻碍事务的提交; 最后, 如果是从一个高吞吐量的主库上复制到多个备库, 唤醒多个复制线程发送事件的开销将会累加; 复制解决的问题 数据分布: 可以在不同的地理位置来分布数据备份; 负载均衡: 通过MySQL复制可以将读操作分布到多个服务器上, 实现对读密集型应用的优化, 并且实现很方便, 通过简单的代码修改就能实现基本的负载均衡, 备份: 对于备份来说, 复制是一项很有意义的技术补充; 高可用性和故障切换: 复制能够帮助应用程序避免MySQL单点失败, 一个包含复制的设计良好的故障切换系统能够显著地缩短宕机时间; MySQL升级测试: 这种做法比较普遍, 使用一个更高版本的MySQL作为备库, 保证在升级全部实例前, 查询能够在备库按照预期执行; 复制原理概述 简单来说, MySQL的复制有如下三个步骤 在主库上把数据更改记录到二进制日志(Binary Log)中(这些记录被称为二进制日志事件) 备库将主库上的日志复制到自己的中继日志(Relay Log)中 备库读取中继日志中的事件, 将其重放到备库数据之上 高性能MySQL中用下图描述了上面三步 二进制日志记录格式 事实上, MySQL支持两种复制方式: 基于行的复制 和 基于语句的复制; 这两种复制方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制(这也就意味着, 在同一时间点, 备库上的数据可能与主库存在不一致, 并且无法保证主备之间的延迟, 一些大的语句可能导致备库产生几秒,几分钟甚至几个小时的延迟) 这两种方式主要是指在主库在记录二进制日志时所采用的日志格式(Binary Logging Formats), 其实有 STATEMENT, ROW, MIXED 三种配置; 基于语句的日志记录: 早在MySQL3.23版本中就存在; 可以通过使用 --binlog-format = STATEMENT 启动服务器来使用此格式; 基于行的日志记录: 在5.1版本中才被加进来(在5.0之前的版本中是只支持基于语句的复制); 可以通过以 --binlog-format = ROW 启动它来使服务器使用基于行的日志记录; 基于混合日志记录: 对于混合日志记录, 默认情况下使用基于语句的日志记录, 但在某些情况下, 日志记录模式会自动切换为基于行的; 当然, 您可以通过使用--binlog-format = MIXED选项启动mysqld来显式使用混合日志记录; 优缺点未完待续~~ 三个线程 MySQL使用3个线程来执行复制功能(其中1个在主服务器上, 另外两个在从服务器上); 当从服务器发出START SLAVE时, 从服务器创建一个I/O线程, 以连接主服务器并让它发送记录在其二进制日志中的语句; 主服务器创建一个binlog dump线程, 将二进制日志中的内容发送到从服务器; 从服务器I/O线程读取主服务器Binlog dump线程发送的内容, 并将该数据拷贝到从服务器的中继日志中; 第3个线程是从服务器的SQL线程, 是从服务器创建用于读取中继日志并执行日志中包含的更新; 问题?未完待续~~ 配置复制 master服务器上进行sql写操作的时候, 是会引起磁盘变化的; 所以slave服务器要想和master上的数据保持一致, 可以有两种办法: slave按照master服务器上每次的sql写语句来执行一遍; slave按照master服务器磁盘上的变化来做一次变化 ; 主服务器master上的写操作都会被记录到binlog二进制日志中, 从服务器slave去读主服务器的binlog二进制日志, 形成自己的relay中继日志, 然后执行一遍 ; 所以主从配置需要做到 主服务器要配置binlog二进制 从服务器要配置relaylog(中继日志) master要授予slave账号: 从服务器如何有权读取主服务器的 binlog (binlog非常敏感, 不可能让谁去随便读) 从服务器用账号连接master 从服务器一声令下开启同步功能 start slave 注意: 一般会在集群中的每个sql服务器中加一个server-id来做唯一标识 ; 配置启动主从 主服务器配置 1234#主从复制配置server-id=4 #服务器起一个唯一的id作为标识log-bin=mysql-bin #声明二进制日志文件名binlog-format= #二进制日志格式 mixed,row,statement 主服务器的 binlog二进制日志 有三种记录方式 mixed, row, statement ; statement: 二进制记录执行语句, 如 update….. row: 记录的是磁盘的变化 如何选择 binlog二进制日志 记录方式? update salary=salary+100; // 语句短, 但影响上万行, 磁盘变化大, 宜用statement update age=age+1 where id=3; // 语句长而磁盘变化小, 宜用row 你要是拿不准用哪个? 那就设置为mixed, 由系统根据语句来决定; 从服务器配置 首先从服务器肯定要开启relaylog日志功能 ; 从服务器一般也会开启binlog, 一方面为了备份, 一方面可能还有别的服务器作为这台从服务器的slave ; 主从之间建立关系 : 主服务器上建立一个用户: grant replication client,replication slave on *.* to repl@&#39;192.168.56.%&#39; identified by &#39;repl&#39; 告诉从服务器要连接哪个主服务器: 在从服务器上进入mysql执行如下语句 1234567reset slave #可以把之前的从服务器同步机制重置一下change master to master_host=&apos;192.168.56.4&apos;,master_user=&apos;repl&apos;,master_password=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,#当前主服务器产生的binlog走到哪儿了(需要在主服务器上`show maste status`来查看file名和position指针位置)master_log_pos=349 然后查看从服务器的slave状态 show slave status 发现已经连上主服务器了 从服务器中启动slave: start slave 注意: 可以一主多从, 但一个从有多个主就会傻逼了 ; 此时我们做的只是mysql主从复制, 但不是读写分离, 距离读写分离还差一小步; 因为读写分离还需要对sql语句进行判断(可以在php层面判断sql语句进行路由, 决定哪种sql去哪个服务器) 可以参考本人有道笔记上的记录 https://www.cnblogs.com/clsn/p/8150036.html","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"50. MySQL Optimization","slug":"MySQL/2017-08-12-mysql-50","date":"2017-08-12T06:50:21.000Z","updated":"2019-03-15T02:06:06.000Z","comments":true,"path":"2017/08/12/MySQL/2017-08-12-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2017/08/12/MySQL/2017-08-12-mysql-50/","excerpt":"","text":"查询优化 索引优化","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"31. Lua脚本","slug":"redis/2017-06-29-redis-31","date":"2017-06-29T10:40:07.000Z","updated":"2018-03-10T12:12:08.000Z","comments":true,"path":"2017/06/29/redis/2017-06-29-redis-31/","link":"","permalink":"http://blog.renyimin.com/2017/06/29/redis/2017-06-29-redis-31/","excerpt":"","text":"前言 之前已经了解到, 在redis事物中, 为了检测事务即将操作的keys, 是否有多个clients同时改变而引起冲突, 这些keys将会在事务开始前使用 watch 被监控; 如果至少有一个被监控的key在执行exec命令前被修改, 则事务会被打断, 不执行任何动作, 从而保证原子性操作。 之前的这种方案是在解决一种叫做 CAS(check-and-set) (检查后再设置) 的问题, 这种问题也可以使用 Lua 脚本来进行解决; Lua 脚本功能是Reids 2.6版本的最大亮点, 通过内嵌对Lua环境的支持, Redis解决了长久以来不能高效地处理CAS(check-and-set检查后再设置) 命令的缺点, 并且可以通过组合使用多个命令, 轻松实现以前很难实现或者不能高效实现的模式。 脚本的原子性Redis 使用单个 Lua 解释器去运行所有脚本,并且Redis 也保证脚本会以原子性(atomic)的方式执行;(当某个脚本正在运行的时候,会有其他脚本或 Redis 命令被执行, 这和使用 MULTI / EXEC 包围的事务很类似, 在其他别的客户端看来，脚本的效果要么是不可见的，要么就是已完成的;另一方面其实也意味着, 执行一个运行缓慢的脚本并不是一个好主意, 写一个跑得很快很顺溜的脚本并不难, 因为脚本的运行开销(overhead)非常少, 但是当你不得不使用一些跑得比较慢的脚本时, 请小心, 因为当这些蜗牛脚本在慢吞吞地运行的时候, 其他客户端会因为服务器正忙而无法执行命令; Redis中执行Lua脚本的好处 Lua脚本在Redis中是原子执行的，执行过程中间不会插入其他命令; (一个Lua脚本中的命令, 就相当于类似Redis中GETSET这种原生命令, 是具备原子性的) Lua脚本可以帮助开发和运维人员创造出自己定制的命令，并可以将这些命令常驻在Redis内存中，实现复用的效果; Lua脚本可以将多条命令一次性打包，有效地减少网络开销; 不应该使用Lua什么时候不应该把脚本嵌入到Redis里面?因为Redis的实现是堵塞的, 即, 一个Redis server, 在同一时候, 只能执行一个脚本;因此, 如果你写了一个逻辑非常复杂的脚本, 这个脚本的执行时间非常长, 这样, 这个时候如果有别的请求进来, 就只能排队, 等待这个脚本结束了, 这个Redis server才能处理下一个请求。在这个时候, 就连仅仅是获取数据的命令都会被堵住。这个Redis-Server的效率就会被大大的拖慢, 因此, 如果你的脚本执行非常复杂耗时, 那么这个时候你是不应该把它放在Redis里面执行的。 Lua 基础hello.lua hello.lua: 这个Lua脚本比较简单, 仅仅返回一个字符串, 没有与redis-Server进行比较有意义的操作(比如获取或设置数据) 12local msg = &quot;Hello, world!&quot; --定义了一个本地变量msg存储我们的信息return msg 保存这个文件到hello.lua, 运行: 123456 renyimindembp:test renyimin$ redis-cli eval \"$(cat /Users/renyimin/Desktop/test/test.lua)\" 0 \"Hello, world!\"-- 下面这种方法也行 renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0 \"Hello, world!\" renyimindembp:test renyimin$ 运行这段代码会打印”Hello,world!” EVAL 指令 语法: VAL script numkeys key [key ...] arg [arg ...] script 参数: 是一段lua脚本程序, 它会被运行在Redis服务器上下文中, 可以直接写在命令行, 也可以引入.lua文件; numkeys 参数: 指定即将传入lua脚本中的key的个数 (如果没有参数的话, 也需要指明参数个数为0, 否则会报错) 从第三个参数开始的numkerys个参数, 就是你要传入脚本的键名参数, 这些键名参数可以在Lua脚本中通过全局变量KEYS数组(索引从1开始)的形式访问(如:KEYS[1], KEYS[2]); 剩下的在命令最后的附加参数 arg [arg ...], 可以在Lua中通过全局变量 ARGV 数组访问, 访问的形式和KEYS变量类似; 例子: (..是lua中的字符串拼接语法) 123127.0.0.1:6379&gt; EVAL &apos;return &quot;K1: &quot;..KEYS[1]..&quot; K2: &quot;..KEYS[2]..&quot; A1: &quot;..ARGV[1]..&quot; A2: &quot;..ARGV[2]&apos; 3 k1 k2 k3 a1 a2 a3 a4&quot;K1: k1 K2: k2 A1: a1 A2: a2&quot;127.0.0.1:6379&gt; 带宽和 EVALSHA EVAL 命令要求你在每次执行脚本的时候都发送一次脚本, 但是Redis有一个内部的缓存机制, 因此它不会每次都重新编译脚本, 这样, eval发送脚本主体就是在浪费带宽了; 为了减少带宽的消耗, Redis实现了 EVALSHA 命令, 它的作用和EVAL一样, 都用于对脚本求值但它接受的第一个参数不是脚本, 而是脚本的SHA1校验和(sum)(SHA1校验和 的生成看下一节SCRIPT LOAD); EVALSHA执行后, 如果服务器还记得给定的 SHA1校验和 所代表的脚本, 那么执行这个脚本; 如果服务器不记得给定的 SHA1校验和 所代表的脚本, 那么它返回一个特殊的错误, 提醒用户使用EVAL代替EVALSHA以下是示例： 123456789101112// 现在是有a这个key的127.0.0.1:6379&gt; get a&quot;haha&quot;// 开始测试127.0.0.1:6379&gt; SCRIPT LOAD &quot;return redis.call(&apos;GET&apos;,&apos;a&apos;)&quot;&quot;8a2f221803757b26fc7d283bec7ba834d91202c9&quot;127.0.0.1:6379&gt; evalsha 8a2f221803757b26fc7d283bec7ba834d91202c9 0&quot;haha&quot;// 如下瞎写的 sha1校验和, 服务器就不认识了, 要你使用 eval 命令来直接使用脚本127.0.0.1:6379&gt; evalsha lalalala 0(error) NOSCRIPT No matching script. Please use EVAL.127.0.0.1:6379&gt; SCRIPT LOAD如果使用 EVALSHA 发送校验和给服务器, 从而调用正确脚本的话, 这个 校验和 如何生成? 这就要用到 SCRIPT LOAD 命令了;SCRIPT LOAD: 将脚本加载到脚本缓存, 而不执行它 (在将指定的命令加载到脚本缓存中之后, 之后你将需要使用EVALSHA命令和脚本的正确SHA1摘要来调用它) 该脚本被保证永远留在脚本缓存中(除非调用SCRIPT FLUSH) lua调用redis命令 Lua脚本中可以使用两个不同的Lua函数来调用Redis的命令 redis.call() redis.pcall() redis.call() 与 redis.pcall()的区别 这两个命令很类似, 他们唯一的区别是当redis命令执行结果返回错误时, redis.call()将返回给调用者一个错误; 而redis.pcall()会将捕获的错误以Lua表的形式返回; 测试 (redis.call() 和 redis.pcall() 两个函数的参数可以是任意的 Redis 命令) 12345127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;set&apos;,&apos;foo&apos;,&apos;bar&apos;)&quot; 0OK127.0.0.1:6379&gt; eval &quot;return redis.call(&apos;get&apos;,&apos;foo&apos;)&quot; 0&quot;bar&quot;127.0.0.1:6379&gt; .lua 脚本文件 其实写lua脚本还是有很多注意点的, 可以参考中纯函数脚本这一小节; 本篇学习主要是将来能写一些简单的原子命令, 并不会涉及一些复杂的逻辑操作, 所以只是简单了解了一下上面那些注意点; 演示在redis设置一个num为10的库存 set num 10; 下面通过lua脚本写一个原子操作, 将检测库存和最终减少库存放在一起 123456local good = redis.call(&apos;get&apos;, &apos;num&apos;);if tonumber(good) &gt; 0then redis.call(&apos;decr&apos;, &apos;num&apos;)endreturn &apos;ok&apos; 调用该脚本 12renyimindembp:test renyimin$ redis-cli --eval /Users/renyimin/Desktop/test/test.lua 0&quot;ok&quot; 会正常减少数字 laravel中实现如下 出现超卖的代码 123456789101112131415/** * 使用redis模拟并发超卖 * 由于check和set是分两步执行的 */public function redisOversell()&#123; $good = Redis::get(&apos;num&apos;); if ($good &gt; 0) &#123; Redis::multi(); usleep(500000); //预先已经设置好库存为10个了 Redis::decr(&apos;num&apos;); Redis::exec(); &#125;&#125; 定义脚本: 将check 和 set放到一个脚本里(但这个脚本由于没有设置sleep, 所以只能得出理论上是没有问题的) 123456protected $lua = &quot; local good = redis.call(&apos;get&apos;, &apos;num&apos;);&quot; . &quot; if tonumber(good) &gt; 0 then&quot; . &quot; redis.call(&apos;decr&apos;, &apos;num&apos;)&quot; . &quot; end&quot; . &quot; return &apos;ok&apos;&quot;; 执行 1234567/** * lua脚本解决超卖 */public function redisLua()&#123; Redis::eval($this-&gt;lua, 0);&#125; 参考https://segmentfault.com/a/1190000009811453#articleHeader0https://www.cnblogs.com/huangxincheng/p/6230129.htmlhttps://segmentfault.com/a/1190000007615411","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"24. Redis Replication 搭建","slug":"redis/2018-04-06-redis-24","date":"2017-06-27T11:43:11.000Z","updated":"2019-02-27T06:19:05.000Z","comments":true,"path":"2017/06/27/redis/2018-04-06-redis-24/","link":"","permalink":"http://blog.renyimin.com/2017/06/27/redis/2018-04-06-redis-24/","excerpt":"","text":"动态配置 动态配置有两种方式: 在启动新的redis实例的时候, 设置这个新的实例为某个redis主服务器的Slave: redis-server --port 6380 --slaveof 127.0.0.1 6379 对一个已经启动的redis服务发送命令: slaveof ip port, 则会将当前服务器从 Master 修改为别的服务器的 slave 尝试向从服务器发送写命令, 发现不会成功 在master上添加一个键值对, 发现从服务器上立马就会同步到这条数据 配置文件配置 为每个准备启动的redis从服务, 创建各自的配置文件, 然后在启动redis新实例的时候, 为新实例指定各自的配置文件即可 比如一个6380的从实例, 其配置文件为: redis-server /usr/local/redis/etc/redis-6380.conf, 配置文件内容如下: 12slaveof 127.0.0.1 6379port 6380 QPS 10万+ 比如一个电商平台的详情页, 要做到超高并发, QPS上十万, 甚至是百万, 一秒钟百万的请求量, 不可避免地就要把底层的缓存搞得很好 首先, 光是redis自然是不够的, 但是redis是整个大型的缓存架构中, 支撑高并发的架构里面, 非常重要的一个环节, 如果redis要支撑超过10万+的并发, 那应该怎么做? 单机的redis几乎不太可能说QPS超过10万+, 一般在1万+, 除非一些特殊情况, 比如你的机器性能特别好, 配置特别高, 物理机, 维护做的特别好, 而且你的整体的操作不是太复杂 要做到高并发, 提高QPS, 可以将redis做主从架构(读写分离) 一般来说, 对缓存, 都是用来支撑读高并发的, 写的请求是比较少的, 可能写请求也就一秒钟几千, 一两千, 大量的请求都是读, 一秒钟二十万次读 主从架构 -&gt; 读写分离 -&gt; 支撑10万+读QPS的架构 redis主从架构 -&gt; 读写分离架构 -&gt; 可支持水平扩展的读高并发架构 sentinel哨兵引出sentinel 之前的主从配置可以为我们解决一个redis实例读压力大的问题, 可以在一台服务器上进行主从多实例的配置, 也可以在不同机器之间进行主从配置; 如果slave服务器挂掉, 只是读性能会下降, 现在的问题是, 如果一旦主redis服务器(也就是master实例)挂了, 你目前貌似只能手动去把这台master实例启动起来; 如果确实这台机器/实例就是启动不起来的话, 那你可能就需要手动去将当前的某一个slave实例切换为master:slaveof no one //先把这个slave实例变成一个master实例slaveof ip port //在之前的从实例中执行, 将他们的master重新切换到这个新的master上 这种手动操作明显不可能被接收, 我们需要当master主挂了之后, 自动有一个slave能够勇于承担地顶上来(因为slave相对于之前的master除了分担读压力外, 还是之前master的备份), 这就引出了redis的sentinel哨兵; 简介 sentinel哨兵 是redis官方提供的高可用解决方案, 可以用它来管理多个redis服务的实例; 之前在编译安装好redis之后, 就可以在 /usr/local/redis/src/ 目录下看到 redis-sentinel 等很多命令; sentinel的监控: 它会不断地检查master和slaves是否正常; 一个sentinel可以监控任意多个master及其下的slaves; 当然, sentinel也可能挂掉, 也有单点问题 (还好Sentinel是一个分布式系统, 可以在一个架构中运行多个Sentinel进程, 他们可以组成一个sentinel网络, 之间是可以互相通信的, 可以通过”投票”来决定master是否挂了) 当一个sentinel认为被监控的服务已经下线时, 它会向网络中的其他sentinel进行确认, 判断该服务器是否真的下线; 如果下线的是一个主服务器, 那么sentinel将会对下线的主服务器进行 自动故障转移通过将下线主服务器的某个从服务器提升为新的主服务器;并将下线主服务器下的从服务器重新指向新的主服务器;来让系统从新回到正常; 之前的master下线后, 如果重新上线了, sentinel会让它作为一个salve, 去新的master中同步数据 实战 启动sentinel: 将 /usr/local/src/ 目录下的redis-sentinel程序文件复制到 /usr/local/redis/bin 目录下; 启动一个运行在sentinel模式下的redis服务实例(两种方式): redis-sentinel 或者 redis-server /usr/local/redis/sentinel.conf --sentinel sentinel配置 Sentinel之间的自动发现机制虽然sentinel集群中各个sentinel都互相连接彼此来检查对方的可用性以及互相发送消息, 但其实你是不用在任何一个sentinel中配置任何其它的sentinel的节点的, 因为sentinel利用了master的发布/订阅机制去自动发现其它也监控了同一master的sentinel节点。 同样, 你也不需要在sentinel中配置某个master的所有slave的地址, sentinel会通过询问master来得到这些slave的地址的。 https://www.cnblogs.com/leeSmall/p/8398401.htmlhttps://www.cnblogs.com/kismetv/p/9236731.html#t1","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"22","slug":"redis/2017-06-25-redis-22","date":"2017-06-25T08:35:21.000Z","updated":"2019-02-22T05:30:14.000Z","comments":true,"path":"2017/06/25/redis/2017-06-25-redis-22/","link":"","permalink":"http://blog.renyimin.com/2017/06/25/redis/2017-06-25-redis-22/","excerpt":"","text":"Redis内存不足时 LRU和AOF持久化文件","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"12. Redis -- 发布订阅","slug":"redis/2017-06-18-redis-12","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-09T09:55:58.000Z","comments":true,"path":"2017/06/17/redis/2017-06-18-redis-12/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/redis/2017-06-18-redis-12/","excerpt":"","text":"发布订阅其实 和 队列 非常相似: 这里的 频道, 就类似于任务队列中所讨论的 队列; 而订阅者就类似于 任务队列中所讨论的 消费者; 但他们还是有区别的: 对于队列来说, 即使消费者不在线, 消息也一直在队列中存放着, 等待消费者上线后进行处理; 而对于 发布订阅 中的 订阅者来说, 如果消息发布到频道中时, 订阅者不在线(没有一直阻塞等待), 那么这条消息也不会为它保存着; 另外对于队列来说, 其中的消息只要被一个消费者处理了, 其他消费者就不用处理了; 而对于发布订阅来说, 向频道中发布一条消息, 则所有在线的订阅者都可以收到这个消息; (貌似是比较是个做聊天室之类的东西) 实例可以参考有道笔记","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"11. Redis -- task queue","slug":"redis/2017-06-17-redis-11","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-16T06:08:00.000Z","comments":true,"path":"2017/06/17/redis/2017-06-17-redis-11/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/redis/2017-06-17-redis-11/","excerpt":"","text":"前言现在有很多专门的队列软件(如ActiveMQ, RabbitMQ等)，在缺少专门的任务队列可用的情况下, 也可以使用Redis的队列机制; FIFO 先进先出队列 Redis的列表结构, 允许通过 RPUSH和LPUSH以及RPOP和LPOP, 从列表的两端推入和弹出元素。 假设一个操作中, 有向用户发送电子邮件这一功能, 由于这一功能可能会有非常高的延迟,甚至可能会出现发送失败, 所以这里就不能采用平时常见的代码流方式来执行这个邮件发送操作; 为此, 可以使用 任务队列 来记录 “邮件的收信人,邮件内容,发送邮件的原因”, 以 先到先服务 的方式发送邮件(此处采用的是 从右推入队列, 从左弹出元素) 队列: redis服务中的列表类型就充当了队列服务 消费者: 阻塞等待 (在Laravel将其做成命令进行启动, 该消费者就会阻塞等待) 12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email&apos;], 0); return $res;&#125; 生产者: 直接访问如下代码推送多条消息 1234567891011121314151617181920public function rPush()&#123; $data1 = json_encode([ &apos;user_id&apos; =&gt; 11, &apos;content&apos; =&gt; &apos;最近推出新款汽车--宝马, 售价:$12W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data2 = json_encode([ &apos;user_id&apos; =&gt; 12, &apos;content&apos; =&gt; &apos;最近推出新款汽车--奔驰, 售价:$15.8W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data3 = json_encode([ &apos;user_id&apos; =&gt; 13, &apos;content&apos; =&gt; &apos;最近推出新款汽车--大众, 售价:$20W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); //一旦运行就会阻塞起来 Redis::rpush(&apos;queue:email&apos;, [$data1, $data2, $data3]);&#125; 最终消费者正常拿到消息, 并且顺序也是正确的; 多种任务的队列 一般情况下, 我们为每种任务单独使用一个队列的; 但如果有一个队列处理多种任务的场景, 实现起来也很方便; 只用在消息中指明消息所需要调用的回调函数即可; priority 优先级队列优先级队列其实在redis中可以依靠 BRPOP和BLPOP的特性; 当 BLPOP 被调用时, 如果给定key(队列)列表中, 至少有一个非空列表, 那么弹出遇到的第一个非空列表的头元素, 并弹出元素所属的列表名字一起, 组成结果返回给调用者; 也就是说, BLPOP 给定的队列列表中, 靠前的就是优先级高的;假设你有 ‘重置密码的邮件’, ‘提醒邮件’, ‘发广告的邮件’, 三种队列, 如果你期望他们按照优先级依次排列, 那么只用设置为:12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email:resetpwd&apos;, &apos;queue:email:warning&apos;, &apos;queue:email:advertisement&apos;], 0); return $res;&#125; 想优先推送的, 你就放入第一个队列’queue:email:resetpwd’中(BRPOP)也类似 也可参考: http://blog.csdn.net/woshiaotian/article/details/44757621 延迟队列未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"15. redis集群","slug":"redis/2017-06-18-redis-15","date":"2017-06-15T14:37:42.000Z","updated":"2018-03-08T07:54:10.000Z","comments":true,"path":"2017/06/15/redis/2017-06-18-redis-15/","link":"","permalink":"http://blog.renyimin.com/2017/06/15/redis/2017-06-18-redis-15/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"08. redis锁 -- 不太安全?","slug":"redis/2017-06-12-redis-08","date":"2017-06-12T08:35:08.000Z","updated":"2018-10-12T11:22:39.000Z","comments":true,"path":"2017/06/12/redis/2017-06-12-redis-08/","link":"","permalink":"http://blog.renyimin.com/2017/06/12/redis/2017-06-12-redis-08/","excerpt":"","text":"前言一般来说, 在对数据进行”加锁”时, 程序首先需要获取锁来得到对数据进行排他性访问的能力, 然后才能对数据执行一系列操作, 最后还要将锁释放给其他程序;之前已经了解过, Redis使用WATCH命令来代替对数据进行加锁, 因为WATCH只会在数据被其他客户端抢先修改了的情况下通知执行了这个命令的客户端, 所以这个命令被称为乐观锁; SETEX 实现锁介绍为了对数据进行排他性访问, 程序首先要做的就是获取锁; 而 Redis 的 SETEX 命令天生就适合用来实现锁的获取功能; 这个命令只会在键不存在的情况下为键设置值, 而其他进程一旦发现键存在, 那就只能等待之前锁的释放; 准备环境 数据表准备 123456789DROP TABLE IF EXISTS `goods`;CREATE TABLE `goods` ( `id` int(10) NOT NULL AUTO_INCREMENT, `goods_name` varchar(100) NOT NULL, `num` int(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `goods` VALUES (1, &apos;iphone 6 plus&apos;, 10); 超卖代码: 12345678public function mysqlOverSell()&#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(500000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125;&#125; Jmeter压测配置: 结果发现超卖: (后来设置压测为每秒3个线程, 也超卖了1件) 使用redis的setex加锁 redis 123127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; 代码 123456789101112131415public function setnx()&#123; $lock = Redis::setnx(&apos;tag&apos;, 1); // 如果加锁成功, 则可以进行如下操作(其他客户端只能等待锁释放) if ($lock) &#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(200000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125; // 注意: 执行完成之后, 必须释放锁 Redis::del(&apos;tag&apos;); &#125;&#125; 修改jmeter访问路由, 重新测试, 发现确实不会出现超卖现象了 SETEX锁死锁 客户端的突然下线导致锁不被释放: 由于客户端即使在使用锁的过程中也可能会因为这样或那样的原因而下线, 所以为了防止客户端在取得锁之后崩溃, 并导致锁一直处于 已获取 状态; 所以网上有不少做法是 为锁加上 超时限制, 这样 如果获得锁的进程未能在指定时限内完成操作, 那么可能会认为客户端已经crash掉线, 所以锁将需要被自动释放; 但锁的这个超时时间又会带来新的问题 设置时间过长, 会导致吞吐量的严重下降 设置时间过短, 又会导致锁自动释放导致的问题 SETEX锁设超时限制 为锁加超时限制的普通方法如下 12345678$lock = Redis::setnx(&quot;tag&quot;, 1)if ($lock) &#123; // 如果在此处突然崩溃... Redis::expire(&quot;my:lock&quot;, 10); // ... do something Redis::del(&quot;my:lock&quot;)&#125; 如果客户端是在 Redis::expire(&quot;my:lock&quot;, 10); 之前就崩溃, 锁不被释放的问题还是存在; 从 setnx 到 set 所以, 从redis2.6.12开始(set新增了可选选项), 官方建议使用set命令替代setnx来实现锁, 如下 12345if (Redis::set(&quot;tag&quot;, 1, &quot;nx&quot;, &quot;ex&quot;, 10)) &#123; ... do something Redis::del(&quot;tag&quot;)&#125; 网上有说 需要评估业务的复杂度, 来设置超时时间 - 如果设置过短会导致 释放了其他进程的锁如果持有锁的进程A因为操作时间过长, 而导致锁超时被自动释放, 这样的话, 又会导致其他进程在进程A尚未结束时获取锁, 这样还是会导致并发的出现;另外, 由于锁是自动被释放的, 进程A并不知道, 这样就会导致, 进程A在后续执行完成任务之后, 在做 释放锁 的操作时, 如果只是简单的 Redis::del(&quot;tag&quot;&quot;), 如果正好有其他进程获取了锁, 这就会导致进程A释放的并不是自己的锁, 而是释放掉其他进程持有的锁; 因此, 在获取锁的时候, 需要设置一个 token, 放入自己的锁中, 在释放锁的时候, 用来保证释放的是自己的锁; 但纵使是这样, 还是导致了并发的出现, 并未解决最根本的问题! 锁释放注意 如果客户端A是因为执行超时, 而导致锁被自动释放, 那么当客户端A最后在释放锁时, 可能此时客户端B已经加上了自己的锁, 所以在锁释放时需要做两个操作 检查token是否一致 释放锁 注意: 在上面两步之间, 比如说刚检查完token, 确认了token是当前进程的锁之后, 也还是有可能发生超时而自动释放锁, 导致锁token被换上别的客户端的, 所以 释放锁 这一步应该放在事务中, 并提前用watch监控代表锁的那个key, 伪代码如下: 123456Redis::watch(&apos;tag&apos;);if (Redis::get(&apos;tag&apos;) == $token) &#123; Redis::multi(); Redis::del(&apos;tag&apos;); Redis::exec();&#125; 多个客户端同时获取锁 假设客户端A超时后, 锁被自动释放, 此时客户端B拿到了锁, 如果客户端B也因为超时导致锁被释放(此时客户端A还没执行完, B也没执行完), 那么客户端C也能拿到锁; 可以看到甚至同时存在3个(还可能n个)客户端拿到了锁; 所以这个问题貌似就是一直没解决的问题!! 终极办法参考:基于Redis的分布式锁到底安全吗(上)https://segmentfault.com/q/1010000013626041?_ea=3427544~~未完待续","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"07. redis事务与WATCH乐观锁","slug":"redis/2017-06-11-redis-07","date":"2017-06-11T13:14:46.000Z","updated":"2018-03-16T05:41:21.000Z","comments":true,"path":"2017/06/11/redis/2017-06-11-redis-07/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/redis/2017-06-11-redis-07/","excerpt":"","text":"Redis事务与单线程 因为Redis是单线程的, 所以即使多个客户端同时来对同一数据发来很多命令, 也会被串行挨个执行; 但是需要注意的是, 这和mysql的srialize隔离级别一样, 即使是串行执行命令, redis也逃不过高并发事务时的 更新丢失 问题; mysql是使用乐观锁, 悲观锁来解决的 参考MySQL高并发事务问题 及 解决方案 而redis的事务也是通过与WATCH(乐观锁)的结合才得以解决这个问题 Redis事务与WATCH 演示Redis事务在客户端高并发时出现的 丢失更新 问题 redis 为了解决高并发事务时这种 丢失更新 的问题, 提供了 WATCH WATCH介绍 redis 并没有实现典型的加锁功能(比如MySQL中, 在访问以写入为目的数据时 SELECT FOR UPDATE), 因为加这种悲观锁可能会造成长时间的等待; 所以redis为了尽可能地减少客户端的等待时间, 采用了WATCH监控机制, 如果某个客户端A在事务开始之前 WATCH 了一个key, 那么其实就相当于对该key加了乐观锁 事务中正常执行你要执行的操作 (乐观地认为不会有其他客户端抢在你前面去改动那个key) 直到当客户端A真正exec的时候, 才会验证客户端A之前WATCH(监控)的key是否被变动过, 如果变动过, 则客户端A可以进行重试; 测试:","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"06. Redis 事务","slug":"redis/2017-06-11-redis-06","date":"2017-06-11T11:10:03.000Z","updated":"2019-03-25T07:43:32.000Z","comments":true,"path":"2017/06/11/redis/2017-06-11-redis-06/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/redis/2017-06-11-redis-06/","excerpt":"","text":"Redis事务介绍 redis事务是使用队列以先进先出(FIFO)的方法保存命令, 较先入队的命令会被放到数组的前面, 而较后入队的命令则会被放到数组的后面 redis事务从开始到结束通常会通过三个阶段: 事务开0始 命令入队 事务执行 Redis事务通常会使用 MULTI, EXEC, WATCH等命令来完成 redis事务的ACID特性在redis中, 事务具有 原子性(Atomicity) , 一致性(Consistency) 和 隔离性(Isolation);并且当redis运行在某种特定的持久化模式下,事务也具有 持久性(Durability); (弱)原子性 对于redis的事务来说, 事务队列中的命令也是要么就全部执行, 要么就一个都不执行, 因此redis的事务是具有原子性的; 不过需要注意的是: redis事务的原子性有两种情况需要区分 一种是 语法错误导致redis事务执行出错, 比如, 你事务中的某条命令语法错误, 那么你在exec的时候, 所有的命令都不会执行: 1234567891011127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age(error) ERR wrong number of arguments for &apos;set&apos; command127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; 另一种是 无语法错误,可以运行成功, 但是运行完之后会返回运行错误, 这种情况下, 错误之前的命令不会回滚; 123456789101112131415161718192021222324//比如命令或命令的参数格式错误,那么事务就会出现有可能部分命令成功,而部分命令失败的情况127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age 10QUEUED//这里就会出现运行错误127.0.0.1:6379&gt; incr nameQUEUED127.0.0.1:6379&gt; set addr yunchengQUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) (error) ERR value is not an integer or out of range4) OK127.0.0.1:6379&gt; keys *1) &quot;addr&quot;2) &quot;age&quot;3) &quot;name&quot;127.0.0.1:6379&gt; 需要说明的是: 上述的第二种情况, 从表面上看, 不符合”原子性”, 因为你所执行的命令中有一条出错后, redis并没有进行回滚; 其实是由于你的此种错误命令不属于语法错误, 对redis来说, 不会导致执行出错,所以你的这条错误命令是可以执行成功的, 不过成功后会返回错误提示, 所以redis并不会进行回滚; redis的作者在事务相关的文档中解释说:1234不支持事务回滚是因为这种复杂的功能和redis追求的简单高效的设计主旨不符合,并且他认为, redis事务的执行时错误通常都是编程错误造成的,这种错误通常只会出现在开发环境中, 而很少会在实际的生产环境中出现,所以他认为没有必要为redis开发事务回滚功能。 所以, 在事务中执行redis命令时, 最好确保所有命令都能执行成功; 一致性redis通过谨慎的错误检测和简单的设计来保证事务一致性。 隔离性 事务的隔离性指的是, 即使数据库中有多个事务并发在执行, 各个事务之间也不会互相影响, 并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同; 因为redis使用单线程的方式来执行事务(以及事务队列中的命令), 并且服务器保证, 在执行事务期间不会对事务进行中断, 因此, redis的事务总是以串行的方式运行的, 并且事务也总是具有隔离性的; Redis为单进程单线程模式, 采用队列模式将并发访问变为串行访问(Redis本身没有锁的概念, Redis对于多个客户端连接并不存在竞争) 持久性因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能, 所以redis事务的耐久性由redis使用的持久化模式(rdb/aof)来决定: 关于持久化, 可以参考博文:了解redis持久化","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"01. MySQL 知识点","slug":"MySQL/2017-06-10-mysql-01","date":"2017-06-10T03:02:51.000Z","updated":"2019-04-08T14:43:39.000Z","comments":true,"path":"2017/06/10/MySQL/2017-06-10-mysql-01/","link":"","permalink":"http://blog.renyimin.com/2017/06/10/MySQL/2017-06-10-mysql-01/","excerpt":"","text":"安装权限https://www.cnblogs.com/jackruicao/p/6068821.html?utm_source=itdadao&amp;utm_medium=referral 几个重要文件数据库层面 my.cnf (MySQL参数文件) error Log (MySQL 错误日志) slow Query Log (慢查询日志) general Query Log (全量日志) binary Log (二进制日志) relay Log (中继日志) audit Log (审计日志) 存储引擎层面 redo log undo log MySQL Storage Engines MyISAM InnoDB https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html MySQL 事务, 隔离级别, MVCC相关MySQL Optimization- [参考 手册](https://dev.mysql.com/doc/refman/5.6/en/optimization.html) - 索引优化 - 查询优化 MySQL ReplicationMySQL 高性能, 高可用阿里云 RDS 版本 https://help.aliyun.com/knowledge_detail/49059.html https://dev.mysql.com/doc/refman/8.0/en/storage-engines.htmlhttps://dev.mysql.com/doc/refman/5.6/en/optimization.html 一些资料https://www.aliyun.com/ss/bXlzcWzlronoo4XmlZnnqIs/1_h 云服务器 ECS MySQL 编译安装支持 Innodb 引擎https://help.aliyun.com/knowledge_detail/41107.html?spm=5176.11065259.1996646101.searchclickresult.35c13f8dXRN0Rx 问题 自增主键的优缺点https://blog.csdn.net/yixuandong9010/article/details/72286029自增主键这种方式是使用数据库提供的自增数值型字段作为自增主键，它的优点是：（1）数据库自动编号，速度快，而且是增量增长，按顺序存放，对于检索非常有利；（2）数字型，占用空间小，易排序，在程序中传递也方便；（3）如果通过非系统增加记录时，可以不用指定该字段，不用担心主键重复问题。其实它的缺点也就是来自其优点，缺点如下：（1）因为自动增长，在手动要插入指定ID的记录时会显得麻烦，尤其是当系统与其它系统集成时，需要数据导入时，很难保证原系统的ID不发生主键冲突（前提是老系统也是数字型的）。特别是在新系统上线时，新旧系统并行存在，并且是异库异构的数据库的情况下，需要双向同步时，自增主键将是你的噩梦；（2）在系统集成或割接时，如果新旧系统主键不同是数字型就会导致修改主键数据类型，这也会导致其它有外键关联的表的修改，后果同样很严重；（3）若系统也是数字型的，在导入时，为了区分新老数据，可能想在老数据主键前统一加一个字符标识（例如“o”，old）来表示这是老数据，那么自动增长的数字型又面临一个挑战。 select * 的问题? 废话, 纵使InnoDB有聚簇索引, 但是如果二级索引能够做到索引覆盖, 那岂不是更快! MySQL 基础知识点数据类型各种语句","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]}]}