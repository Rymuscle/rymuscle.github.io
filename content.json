{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"01. 基础 - Anaconda 环境管理","slug":"python/2018-09-29-Python-02","date":"2018-09-29T07:41:12.000Z","updated":"2018-09-29T08:44:06.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-02/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-02/","excerpt":"","text":"Anaconda 在使用Python时，我们经常需要用到很多第三方库，例如，Pillow，MySQL驱动程序，Web框架Flask，科学计算Numpy等。用pip一个一个安装费时费力，还需要考虑兼容性。我们推荐直接使用Anaconda，它已经内置了许多非常有用的第三方库，我们装上Anaconda，就相当于把数十个第三方模块自动安装好了，非常简单易用; 安装 在安装Anaconda之前, 其实不需要安装Python, 因为Anaconda中包括了Python; 可在Anaconda官网下载并双击进行安装 下载时会发现有两个不同版本的Anaconda，分别对应Python 2.7和Python 3.6, 两个版本其实除了这点区别外其他都一样 (但其实选择安装哪个版本并不重要, 因为通过Anaconda的环境管理, 可以很方便地切换运行时的Python版本) 如下就安装好了:12renyimindeMacBook-Pro:~ renyimin$ which conda/Users/renyimin/Desktop/Anaconda3/anaconda3/bin/conda 注意: Anaconda会把系统Path中的python指向自己自带的Python, 并且, Anaconda安装的第三方模块会安装在Anaconda自己的路径下, 不影响系统已安装的Python目录 可以看到, 之前的系统path中的python是Mac默认的python2.7, 而现在是Anaconda中自带的python3.6.5 12345renyimindeMacBook-Pro:~ renyimin$ pythonPython 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 另外, 之前path中的版本都还在 (只不过直接使用python3的话, 不会使用之前安装的3.7, 而是用的Anaconda中的python3.6) 12345678910renyimindeMacBook-Pro:~ renyimin$ python2 -VPython 2.7.14renyimindeMacBook-Pro:~ renyimin$ python2.7 -VPython 2.7.14renyimindeMacBook-Pro:~ renyimin$ python3 -VPython 3.6.5 :: Anaconda, Inc.// 要用自己安装的python3.7, 需要使用 python3.7renyimindeMacBook-Pro:~ renyimin$ python3.7 -VPython 3.7.0renyimindeMacBook-Pro:~ renyimin$ 另外, Anaconda会把系统Path中的pip指向自己的pip 123456789101112renyimindeMacBook-Pro:~ renyimin$ pip -Vpip 10.0.1 from /Users/renyimin/Desktop/Anaconda3/anaconda3/lib/python3.6/site-packages/pip (python 3.6)renyimindeMacBook-Pro:~ renyimin$ pip2 -Vpip 9.0.3 from /usr/local/lib/python2.7/site-packages (python 2.7)renyimindeMacBook-Pro:~ renyimin$ pip2.7 -Vpip 9.0.3 from /usr/local/lib/python2.7/site-packages (python 2.7)renyimindeMacBook-Pro:~ renyimin$ pip3 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)renyimindeMacBook-Pro:~ renyimin$ pip3.7 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7) 同时, Anaconda还内置了许多非常有用的第三方库, 由于我们现在本机既有Python2(python), 又有Python3.7(python3.7), 还有Anaconda自带的Python3.6(python3), 所以查看包列表也是有三个pip命令可以运行 1234pip (Anaconda自带的) list : 会发现有很多内置包pip2 (Mac默认带的) list : 干净的, 需要自己安装pip3 (自己装的Python3) list : 干净的, 需要自己安装 另外, 安装完后, 电脑中多了一些应用: 1234Anaconda Navigtor ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。 安装完成后，我们还需要对所有工具包进行升级，以避免可能发生的错误, 打开你电脑的终端，在命令行中输入 12renyimindeMacBook-Pro:bin renyimin$ conda upgrade --allSolving environment: | 虚拟环境管理 可以在命令中运行 conda info -e 或者 conda env list 查看 Anaconda 中已安装的环境, 当前被激活的环境会显示有一个星号或者括号: 123456renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base * /Users/renyimin/Desktop/Anaconda3/anaconda3renyimindeMacBook-Pro:~ renyimin$ 尝试 创建指定版本的独立python虚拟环境 conda create -n your_env_name python=X.X 1234// 创建一个2.7版本的python环境conda create -n my-conda-python-2 python=2.7// 创建一个3.6版本的python环境conda create -n my-conda-python-3 python=3.6 安装过程会提示你需不需要自带一些安装包 (选择 是, 这样会创建一个内置很多第三方库的虚拟环境): 创建完成之后:12345678renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base * /Users/renyimin/Desktop/Anaconda3/anaconda3my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2my-conda-python-3 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3renyimindeMacBook-Pro:~ renyimin$ 激活自己创建的python虚拟环境, 并进入环境, 安装好后, 可以使用 activate 激活某个环境 activate my-conda-python-3 # for Windows source activate my-conda-python-3 # for Linux &amp; Mac 123456789renyimindeMacBook-Pro:~ renyimin$ source activate my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base /Users/renyimin/Desktop/Anaconda3/anaconda3my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2my-conda-python-3 * /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ 激活后，会发现 终端的前缀 多了 (my-conda-python-3) 的字样 如果想从虚拟环境返回宿主环境: deactivate my-conda-python-3 # for Windows source deactivate my-conda-python-3 # for Linux &amp; Mac 123456789(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ source deactivate my-conda-python-3 renyimindeMacBook-Pro:~ renyimin$ conda info -e # conda environments: # base * /Users/renyimin/Desktop/Anaconda3/anaconda3 my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2 my-conda-python-3 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3 renyimindeMacBook-Pro:~ renyimin$ 可以看到, 该命令除了返回宿主环境, 之前的虚拟环境退出激活状态转而由Anaconda默认的base版本做虚拟环境; 如果要删除 Anaconda 中创建的一个虚拟环境, 可以 conda remove --name my-conda-python-2 --all 包管理 conda的一些常用操作如下： conda list # 查看当前环境下已安装的包 conda list -n my-conda-python-3 # 查看某个指定环境的已安装包 1234567891011121314151617renyimindeMacBook-Pro:~ renyimin$ source activate my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda list# packages in environment at /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3:## Name Version Build Channelcertifi 2016.2.28 py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeopenssl 1.0.2l 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freepip 9.0.1 py36_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freepython 3.6.2 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freereadline 6.2 2 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freesetuptools 36.4.0 py36_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freesqlite 3.13.0 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freetk 8.5.18 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freewheel 0.29.0 py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freexz 5.2.3 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freezlib 1.2.11 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda search numpy # 查找package信息 conda install -n my-conda-python-3 numpy : 安装package, 如果不用 -n 指定环境名称，则被安装在当前活跃环境 (也可以通过-c指定通过某个channel安装) conda update -n my-conda-python-3 numpy : 更新package conda remove -n my-conda-python-3 numpy : 删除package 前面已经提到, conda将conda自身、python等都视为package, 因此，完全可以使用 conda 来管理 conda和python的版本, 例如 conda update conda : 更新conda，保持conda最新 conda update anaconda : 更新 anaconda conda update python : 更新python(假设当前环境是python 3.4, conda会将python升级为3.4.x系列的当前最新版本) 项目需要选择哪个虚拟环境, 直接在 Pycharm 中指定即可(和virtualenv一样) 设置国内镜像 如果需要安装很多packages, 你会发现conda下载的速度经常很慢，因为Anaconda.org的服务器在国外, 所幸的是，清华TUNA镜像源有Anaconda仓库的镜像, 我们将其加入conda的配置即可： 设置源 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 查看当前使用的源 conda config --show-sources 1234567renyimindeMacBook-Pro:~ renyimin$ conda config --show-sources==&gt; /Users/renyimin/.condarc &lt;==ssl_verify: Truechannels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: True 导入导出环境 小结 到目前为止, 我机器上的python环境有 Mac 自带的Python2.7 自己安装的Python3.7 后来又装了Anaconda (其中包含默认的3.6, 自己创建的两个虚拟环境3.6版本和2.7版本) 参考: https://www.jianshu.com/p/eaee1fadc1e9","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"03. 基础 - virtualenv , virtualenvwrapper","slug":"python/2018-09-29-Python-03","date":"2018-09-29T07:41:12.000Z","updated":"2018-09-30T07:28:03.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-03/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-03/","excerpt":"","text":"virtualenv 在开发Python应用程序的时候, 如果系统只安装了一个Python版本, 那么所有第三方的包都会被pip安装到这个Python的 site-packages 目录下; 如果要同时开发多个应用程序, 这些应用程序就会共用这一个Python环境, 如果应用A需要 jinja 2.7, 而应用B需要jinja 2.6怎么办? 这种情况下, 每个应用可能需要各自拥有一套 各自独立 的Python运行环境, virtualenv 就是用来为一个应用创建一套 隔离 的Python运行环境; virtualenv : 可以在系统中建立多个不同并且相互不干扰的虚拟环境(另外, 值得一提的是, 在 virtualenv 的虚拟环境中使用 pip 安装依赖还可以绕过某些系统的权限设置, 因为毕竟不需要向系统目录写入数据) 总之, virtualenv是用来创建一个独立的Python虚拟环境的工具, 通过virtualenv可以创建一个拥有独立的python版本和安装库的虚拟开发环境; 这样一来我们就可以在虚拟环境中安装各种各种所需要的库, 从而不会造成本地的库过多所引起的使用混乱, 同时也可以创建不同的python版本来完成不同的需求开发 由于 virtualenv 用起来有点麻烦, virtualenvwrapper 对它进行了封装, 让它更好用, 最终我们使用 virtualenvwrapper 提供的命令, 但是实际工作都是 virtualenv 做的; virtualenv 安装 (推荐使用pip安装) 直接安装 virtualenvwrapper 即可, 你会发现 virtualenv 和 virtualenvwrapper 都被安装了 (由于这里的 pip3 是python3.7, 所以就是在 python3.7 的基础上进行 virtualenvwrapper 的安装)1234renyimindeMacBook-Pro:~ renyimin$ pip3 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)renyimindeMacBook-Pro:~ renyimin$ pip3 install virtualenvwrapperSuccessfully installed pbr-4.2.0 six-1.11.0 stevedore-1.29.0 virtualenv-16.0.0 virtualenv-clone-0.3.0 virtualenvwrapper-4.8.2 virtualenv虚拟环境管理 创建一个独立的python新环境: virtualenv myEnv_01 会在当前目录下生成 ‘myEnv_01’ 目录 12345678910renyimindeMacBook-Pro:Desktop renyimin$ mkdir virtualEnvPythonrenyimindeMacBook-Pro:Desktop renyimin$ cd virtualEnvPythonrenyimindeMacBook-Pro:virtualEnvPython renyimin$ virtualenv myEnv_01renyimindeMacBook-Pro:virtualEnvPython renyimin$ lsmyEnv_01renyimindeMacBook-Pro:virtualEnvPython renyimin$ cd myEnv_01/renyimindeMacBook-Pro:myEnv_01 renyimin$ lsbin libinclude pip-selfcheck.jsonrenyimindeMacBook-Pro:myEnv_01 renyimin$ 一个新的python虚拟环境就创建好了, 并且在这个目录下会有3个目录被创建: bin : 包含一些在这个虚拟环境中可用的命令, 以及开启虚拟环境的脚本 activate include : 包含虚拟环境中的头文件, 包括 Python 的头文件; lib : 依赖库 激活并进入虚拟环境: 进入虚拟环境目录 myEnv_01 中, 然后执行: source ./bin/activate 12345renyimindeMacBook-Pro:myEnv_01 renyimin$ lsbin libinclude pip-selfcheck.jsonrenyimindeMacBook-Pro:myEnv_01 renyimin$ source ./bin/activate(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ 此时, 我们就已经在虚拟环境中了, 可以看到, 命令提示符是 (myEnv_01) renyimindembp:myEnv_01 renyimin$ 从虚拟环境返回宿主环境: 要退出虚拟环境到达宿主环境, 无论在哪个目录下, 只要在虚拟环境中(命令提示符和宿主环境的命令提示符有区别), 直接执行 deactivate 就会退出到宿主python环境中; 如果想要删除虚拟环境, 只要把虚拟环境目录删除即可; (貌似比Anaconda简单多了) virtualenv包管理 在 ‘myEnv_01’ python虚拟环境中安装一个test依赖库: 123456789101112131415161718# myEnv_01虚拟环境中默认是没有该扩展的(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 listPackage Version---------- -------pip 18.0setuptools 40.4.3wheel 0.32.0(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 install test# 之后就有了(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 listPackage Version---------- -------pip 18.0setuptools 40.4.3test 2.3.4.5wheel 0.32.0(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ virtualenvwrapper 有了virtualenv, 为何还要 virtualenvwrapper ? virtualenv 的一个最大的缺点就是, 每次开启虚拟环境之前, 你都需要去虚拟环境所在目录下的 bin 目录下 source 一下 activate, 这就需要我们记住每个虚拟环境所在的目录; 当然, 你可以将所有的虚拟环境目录全都集中起来, 比如放到 /Users/renyimin/Desktop/virtualEnvPython/, 这个目录下专门存放所有的python虚拟环境, 对不同的虚拟环境使用不同的目录来管理;而 virtualenvwrapper 正是这样做的, 并且, 它还省去了每次开启虚拟环境时候的 source 操作, 使得虚拟环境更加好用 安装 virtualenvwrapper: 卸载之前安装的 virtualenv (因为我们要安装 virtualenvwrapper 的话, 会自动安装 virtualenv) 1pip3 uninstall virtualenv 顺便也手动删除之前的 myEnv_01 目录 然后直接安装 virtualenvwrapper 12renyimindembp:~ renyimin$ pip3 install virtualenvwrapperrenyimindembp:~ renyimin$ 现在, 我们就拥有了一个可以管理虚拟环境的神器 接下来要做的比较重要, 那就是对 virtualenvwrapper 进行配置 : 它需要指定一个环境变量, 叫做 WORKON_HOME, 并且需要运行一下它的初始化工具 virtualenvwrapper.sh, 这个脚本在 /usr/local/bin/ 目录下; WORKON_HOME 就是它将要用来存放各种虚拟环境目录的目录, 这里我们可以设置为 ‘~/Desktop/virtualEnvPython/‘ VIRTUALENVWRAPPER_PYTHON # 这句是为了防止环境变量$PATH中已有其它环境的python, 需要换成自己需要的python路径, 此处使用 python3.7 而不使用 python3, 是因为之前安装virtualenvwrapper时使用的pip3其实还是独立安装的python3.7, 而 python3 已经变成了Anaconda中的python3.6了 123export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3.7 export WORKON_HOME=&apos;~/Desktop/virtualEnvPython/&apos;source /usr/local/bin/virtualenvwrapper.sh 由于每次都需要执行这两步操作, 我们可以将其写入终端的配置文件中, 例如:如果使用 bash, 则添加到 ~/.bashrc 中;如果使用 zsh, 则添加到 ~/.zshrc 中; 这样每次启动终端的时候都会自动运行，终端其中之 virtualenvwrapper 就可以用啦;如果 ~/ 下没有 .bashrc的话, 写到.bash_profile文件中也可以 利用 virtualenvwrapper, 我们可以使用命令 mkvirtualenv myEnv_001 轻松创建一个虚拟环境, 之后我们就有了一个叫做 myEnv_001 的虚拟环境, 它被存放在 $WORKON_HOME/myEnv_001 目录下, 也就是 ~/Desktop/virtualEnvPython/myEnv_001/ 1234567891011121314renyimindeMacBook-Pro:virtualEnvPython renyimin$ mkvirtualenv myEnv_001Using base prefix &apos;/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7&apos;New python executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/python3.7Also creating executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/pythonInstalling setuptools, pip, wheel...done.virtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/predeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/postdeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/preactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/postactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/get_env_details(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$ lsmyEnv_001(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$ 新建虚拟环境之后会自动激活虚拟环境, 如果我们平时想要进入某个虚拟环境, 可以用命令 workon myEnv_001, 这样才能真正进入激活的虚拟环境中 同样, 离开虚拟环境, 可以使用 deactivate 删除虚拟环境也一样简单 rmvirtualenv myEnv_001 (不像之前只使用virtualenv那样, 需要手动删除目录来删除一个虚拟环境, 没那么low了) virtualenvwrapper 中的其他命令: 1234lsvirtualenv，虚拟环境的列表cdvirtualenv，进入当前激活的虚拟环境cdsitepackages，进入虚拟环境中的site-packages目录lssitepackages，site-packages目录的列表 同时, 你还可以使用 virtualenv 来操作!! 另外, 参考了解到**: 命令 virtualenv 就可以创建一个独立的Python运行环境，我们还加上了参数 --no-site-packages, 这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境, 好像这个参数是默认就有的, 因为测试后发现加和不加都是干净的: 1234renyimindeMacBook-Pro:virtualEnvPython renyimin$ ls myEnv_001/lib/python3.7/site-packages/__pycache__ pip pkg_resources setuptools-40.4.3.dist-info wheel-0.32.0.dist-infoeasy_install.py pip-18.0.dist-info setuptools wheelrenyimindeMacBook-Pro:virtualEnvPython renyimin$ 参考 创建虚拟环境的时候, python版本如何指定 当我的机器上有Python2.7和Python3.6两个Python版本的时候, 那么virtualenv创建的虚拟环境使用哪个Python版本呢? 可以通过 virtualenv -h 查看帮助命令 -p : 指定一个python版本, 通常当你的系统中安装了多个python版本时会用到, 默认情况下virtualenv会优先选取它的宿主python环境，也就是它的 VIRTUALENVWRAPPER_PYTHON 是哪个版本的, 默认就会选择哪个版本作为默认python隔离环境我们使用的是 Python3.7 所以virtualenv默认安装的就是Python3.6虚拟环境; 尝试创建一个 2.7 版本的python环境12345678910111213renyimindeMacBook-Pro:virtualEnvPython renyimin$ mkvirtualenv myEnv_002 -p python2.7Running virtualenv with interpreter /usr/local/bin/python2.7New python executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/python2.7Also creating executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/pythonInstalling setuptools, pip, wheel...done.virtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/predeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/postdeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/preactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/postactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/get_env_details(myEnv_002) renyimindeMacBook-Pro:virtualEnvPython renyimin$ python -VPython 2.7.14(myEnv_002) renyimindeMacBook-Pro:virtualEnvPython renyimin$ 小结 目前本机环境有 Mac 自带的Python2.7 自己安装的Python3.7 后来又装了Anaconda (其中包含默认的3.6, 自己创建的两个虚拟环境3.6版本和2.7版本) virtualenv 中创建的两个虚拟环境3.7和2.7版本 在pycharm中引入本地的虚拟环境(可以看到也可以在pycharm中直接使用 conda 或者 virtualenv 来新建虚拟环境, 不过由于之前已经了一些虚拟环境了, 所以直接add即可) 在 pycharm 的 左下角有 Python Console 和 Terminal 两个终端, 但是发现在切换 conda 或者 virtual 虚拟环境时: virtual 虚拟环境切换时候, 两个终端都会切换 而 conda 虚拟环境切换时, 貌似不会切换 Terminal 终端到虚拟环境中","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"01. 基础 - Python 环境安装","slug":"python/2018-09-29-Python-01","date":"2018-09-29T04:11:12.000Z","updated":"2018-09-29T08:46:41.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-01/","excerpt":"","text":"Python安装 目前, Python有两个版本, 一个是2.x版, 一个是3.x版, 这两个版本是不兼容的; MacOS 是10.8或者最新的10.9 Mavericks，恭喜你，系统自带了Python 2.7目前自带的是Python2.7版本12345renyimindembp:~ renyimin$ pythonPython 2.7.10 (default, Feb 7 2017, 00:08:15)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 由于3.x版越来越普及, 所以接下来将会使用的是Python3.x: 从Python官网下载Python 3.6的安装程序(傻瓜式安); Mac可以 直接 brew install python3 安装(安装后发现是python3.7, 卸载方便 brew uninstall python3.7 ); 以上两种方法都可以, 这里使用第二种, 安装完成后将会存在两个python版本:12345renyimindembp:~ renyimin$ python -VPython 2.7.10renyimindembp:~ renyimin$ python3 -VPython 3.6.4renyimindembp:~ renyimin$ 安装后, 就会得到Python解释器(就是负责运行Python程序的), 一个命令行交互环境, 还有一个简单的集成开发环境 pip 安装 在Python中, 安装第三方模块, 是通过包管理工具pip完成的 如果你正在使用Mac或Linux, 安装pip这个步骤就可以跳过了 如果你正在使用Windows，请参考安装Python一节的内容，确保安装时勾选了pip和Add python.exe to Path Mac为我们准备了Python2.7的同时, 还默认准备了pip, 另外, 自己安装Python3.6之后, 也同时安装了pip: 自己安装的Python3.7所带的pip需要运行 pip3 (注意: Mac或Linux上有可能并存Python 3.x和Python 2.x, 因此对应的pip命令是pip3) 12345678910renyimindeMacBook-Pro:bin renyimin$ which pip/usr/local/bin/piprenyimindeMacBook-Pro:bin renyimin$ which pip2/usr/local/bin/pip2renyimindeMacBook-Pro:bin renyimin$ which pip2.7/usr/local/bin/pip2.7renyimindeMacBook-Pro:bin renyimin$ which pip3/usr/local/bin/pip3renyimindeMacBook-Pro:bin renyimin$ which pip3.7/usr/local/bin/pip3.7 可以看到软链已经创建好了 123456renyimindeMacBook-Pro:bin renyimin$ ls -al pip*lrwxr-xr-x 1 renyimin admin 35 4 22 18:26 pip -&gt; ../Cellar/python@2/2.7.14_3/bin/piplrwxr-xr-x 1 renyimin admin 36 4 22 18:26 pip2 -&gt; ../Cellar/python@2/2.7.14_3/bin/pip2lrwxr-xr-x 1 renyimin admin 38 4 22 18:26 pip2.7 -&gt; ../Cellar/python@2/2.7.14_3/bin/pip2.7lrwxr-xr-x 1 renyimin admin 31 9 29 11:50 pip3 -&gt; ../Cellar/python/3.7.0/bin/pip3lrwxr-xr-x 1 renyimin admin 33 9 29 11:50 pip3.7 -&gt; ../Cellar/python/3.7.0/bin/pip3.7","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"03.","slug":"elastic-stack/2018-09-08-03","date":"2018-09-08T02:50:15.000Z","updated":"2018-09-08T02:50:38.000Z","comments":true,"path":"2018/09/08/elastic-stack/2018-09-08-03/","link":"","permalink":"http://blog.renyimin.com/2018/09/08/elastic-stack/2018-09-08-03/","excerpt":"","text":"","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"02. 安装配置","slug":"elastic-stack/2018-09-08-02","date":"2018-09-08T02:46:43.000Z","updated":"2018-09-08T02:50:44.000Z","comments":true,"path":"2018/09/08/elastic-stack/2018-09-08-02/","link":"","permalink":"http://blog.renyimin.com/2018/09/08/elastic-stack/2018-09-08-02/","excerpt":"","text":"安装Elasticsearch安装Kibana安装Logstash安装Filebeat","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"01. 从ELK Stack 到 Elastic Stack","slug":"elastic-stack/2018-09-04-01","date":"2018-09-04T09:36:39.000Z","updated":"2018-09-08T02:47:35.000Z","comments":true,"path":"2018/09/04/elastic-stack/2018-09-04-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/04/elastic-stack/2018-09-04-01/","excerpt":"","text":"ELK Stack 简介ELK Stack 是三个开源工具的统称: Elasticsearch, Logstash 和 Kibana Logstash: 开源的日志收集工具, 能按你所定义的配置信息来规范化数据, 并根据需要将其他送到指定的目的地; (监听9600端口) 拥有非常多的Input输入数据类型的插件, 这些Input插件可以用于从大量不同来源的信息中读取数据; 同时, 它也拥有非常多的Output输出数据类型插件, 可用于把数据提交到各种不同的目的地(其中的一种插件就是把数据传输到Elasticsearch中去);比如, 它可以从本地磁盘, 网络服务(自己监听端口, 接受用户日志), 消息队列……中, 收集各种各样的日志; 然后对日志进行分析整理, 输出到指定的输出(如 elasticsearch、redis、终端等); 它能帮助我们搜集原始数据, 修改/过滤数据并将其转换成某种有含义的数据, 完成数据格式化和重新组织数据等; Elasticsearch: 基于Lucene的开源分布式全文搜索引擎; Elasticsearch 服务会开启两个端口 9200和9300, 9200是对外服务的 9300是对集群内交互使用的; Logstash读取的数据可输出到Elasticsearch中, 完成数据的索引; Kibana: 是一个开源的可视化日志web展示工具, 提供友好的日志分析 Web 界面, 帮助你汇总、分析和搜索重要数据日志 (监听 5601 端口); Kibana使用Elasticsearch提供的API来读取/检索存放在Elasticsearch中的索引数据, 并以图表等形式对这些数据进行可视化分析; Elastic Stack诞生 上面在介绍 ELK Stack 时提到, 所有读取数据的工作都是由 Logstash 来完成的, 但是这是一种资源消耗, 因为 Logstash 需要运行在Java虚拟机上, 会消耗大量内存; 因此, 软件研发社区认为需要提高其性能, 并使用管道(pipeline)处理机制 — 一种友好且轻量级的方式来处理资源; 因此, 一种新的概念 Beats 诞生, 并加入到了 ELK Stack 家族成为重要组件(Beats 是由 GO 语言编写的) Beats 用于读取、解析并将数据输出到 Elasticsearch 或 Logstash 中; 不同的是, 它是一种轻量级的, 服务于某种特殊用途的代理(它可以是Metricbeat/Filebeat/Packetbeat等), 它们都是由Elastic开发团队提供; Elastic Stack 的起始版本号是5.0.0, 其虽然是原 ELK Stack 在 5.0 版本加入 Beats 套件后的新称呼, 但其实涵盖的内容还不止这些; 在产生数据管道的作用中, 所有组件都发挥了重要作用: Beats 和 Logstash 用于搜索, 解析, 传输数据; Elasticsearch 负责对数据的索引; Elasticsearch 索引的数据, 最后会被 Kibana 用于数据的可视化; 在基于 Elastic Stack 的数据处理管道中, 还有诸如 安全, 监控, 报警 等方面需要特别关注, 这些工具组件现在统称为 X-Pack 参考:《精通Elastic Stack》","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"10. 聚合","slug":"elasticsearch/2018-07-08-10","date":"2018-07-09T02:41:07.000Z","updated":"2018-09-28T09:13:55.000Z","comments":true,"path":"2018/07/09/elasticsearch/2018-07-08-10/","link":"","permalink":"http://blog.renyimin.com/2018/07/09/elasticsearch/2018-07-08-10/","excerpt":"","text":"聚合 聚合允许我们向数据提出一些复杂的问题, 虽然功能完全不同于搜索, 但它使用相同的数据结构, 这意味着聚合的执行速度很快并且就像搜索一样几乎是实时的; 要掌握聚合, 你只需要明白两个主要的概念: 桶(Buckets) : 满足特定条件的文档的集合 (桶在概念上类似于SQL的分组（GROUP BY）) 指标(Metrics) : 对桶内的文档进行统计计算 (而指标则类似于 COUNT() 、 SUM() 、 MAX() 等统计方法) 翻译成粗略的SQL语句来解释： 123SELECT COUNT(color) FROM tableGROUP BY color COUNT(color) 相当于指标GROUP BY color 相当于桶 每个聚合都是一个或者多个桶和零个或者多个指标的组合 (聚合是由桶和指标组成的) 桶 当聚合开始被执行, 每个文档里面的值通过计算来决定符合哪个桶的条件, 如果匹配到, 文档将放入相应的桶并接着进行聚合操作; 桶也可以被嵌套在其他桶里面, 提供层次化的或者有条件的划分方案; (例如, 辛辛那提会被放入俄亥俄州这个桶, 而整个俄亥俄州桶会被放入美国这个桶) Elasticsearch 有很多种类型的桶, 能让你通过很多种方式来划分文档(时间、最受欢迎的词、年龄区间、地理位置等等); 其实根本上都是通过同样的原理进行操作: 基于条件来划分文档; 指标 桶能让我们划分文档到有意义的集合, 但是最终我们需要的是对这些桶内的文档进行一些指标的计算; 分桶是一种达到目的的手段: 它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标; 大多数指标是简单的数学运算(例如最小值、平均值、最大值、还有汇总), 这些是通过文档的值来计算; 在实践中, 指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据; 桶的使用, 例子, 可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_aggregation_test_drive.html, 可能语法有些在ES5.X已经不能使用, 下面列出新的例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546DELETE /cars/PUT /cars/&#123; &quot;mappings&quot; : &#123; &quot;transactions&quot; : &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;make&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;POST /cars/transactions/_bulk&#123; &quot;index&quot;: &#123;&quot;_id&quot;:1&#125;&#125;&#123; &quot;price&quot; : 10000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:2&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:3&#125;&#125;&#123; &quot;price&quot; : 30000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:4&#125;&#125;&#123; &quot;price&quot; : 15000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:5&#125;&#125;&#123; &quot;price&quot; : 12000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:6&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:7&#125;&#125;&#123; &quot;price&quot; : 80000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;bmw&quot;, &quot;sold&quot; : &quot;2014-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:8&#125;&#125;&#123; &quot;price&quot; : 25000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-02-12&quot; &#125;GET /cars/transactions/_mappingGET /cars/transactions/_searchGET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;popular_colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; 桶和指标组合的例子 例子, 参考: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_adding_a_metric_to_the_mix.html 嵌套桶 例子, 参考: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_buckets_inside_buckets.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"09. 排序, 相关性","slug":"elasticsearch/2018-07-07-09","date":"2018-07-07T02:41:07.000Z","updated":"2018-09-27T10:18:29.000Z","comments":true,"path":"2018/07/07/elasticsearch/2018-07-07-09/","link":"","permalink":"http://blog.renyimin.com/2018/07/07/elasticsearch/2018-07-07-09/","excerpt":"","text":"排序与相关性: 默认情况下，返回的结果是按照 相关性 进行排序的——最相关的文档排在最前 按照指定字段的值进行排序. 多字段排序 相关性","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"08. 结构化搜索 , 全文搜索","slug":"elasticsearch/2018-07-02-08","date":"2018-07-02T04:46:21.000Z","updated":"2018-09-28T07:57:23.000Z","comments":true,"path":"2018/07/02/elasticsearch/2018-07-02-08/","link":"","permalink":"http://blog.renyimin.com/2018/07/02/elasticsearch/2018-07-02-08/","excerpt":"","text":"深入搜索结构化搜索 结构化搜索主要是指针对结构化数据的搜索, 要知道, 搜索不仅仅是全文搜索, 我们很大一部分数据都是结构化的; 比如日期、时间和数字都是结构化的, 它们有精确的格式，我们可以对这些格式进行逻辑操作; 比较常见的操作包括比较数字或时间的范围，或判定两个值的大小。 在结构化查询中, 我们得到的结果 要么存在，要么不存在; 结构化查询不关心文件的相关度或评分。 同样，即便是对于文本, 如果是结构化文本，一个值要么相等，要么不等。没有 更似 这种概念。 当进行结构化搜索时, 其实就是在进行 精确值查找, 我们会使用过滤器（filters）, 因为 结构化的精确查找不需要对结果进行评分 而过滤器的执行速度非常快，不会计算相关度（直接跳过了整个评分阶段），而且很容易被缓存; (请尽可能多的使用过滤式查询) 精确查找常用的子查询语句如 term、terms、rang 等, 如下 (还使用了 constant_score 来代替 bool, 因为只有精确查询) 1234567891011121314151617181920212223// 构造数据POST /my_store/products/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;price&quot; : 10, &quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;price&quot; : 20, &quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;price&quot; : 30, &quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;price&quot; : 30, &quot;productID&quot; : &quot;QQPX-R-3956-#aD8&quot; &#125;GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;price&quot; : 20 &#125; &#125; &#125; &#125;&#125; term查询文本域时, 需要注意: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_finding_exact_values.html#_term_查询文本 range 精确查询可以参考 : https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_ranges.html, 如下 (仍然使用了 constant_score 来代替 bool): 123456789101112131415GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;price&quot; : &#123; &quot;gte&quot; : 20, &quot;lt&quot; : 40 &#125; &#125; &#125; &#125; &#125;&#125; 全文搜索 匹配查询 match 是个核心查询, 无论需要查询什么字段, match 查询都应该会是首选的查询方式, 它是一个 高级全文查询, 这表示它既能处理全文字段, 又能处理精确字段; 这就是说, match 查询主要的应用场景就是进行全文搜索 多词查询 提高精度match 查询还可以接受 operator 操作符作为输入参数，默认情况下该操作符是 or , 我们可以将它修改成 and 让所有指定词项都必须匹配 控制精度在 所有 与 任意 间二选一有点过于非黑即白, 如果用户给定5个查询词项, 想查找只包含其中4个的文档, 该如何处理? 将 operator 操作符参数设置成 and 只会将此文档排除;match 查询支持 minimum_should_match 最小匹配参数, 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关, 我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量1234567891011121314151617181920212223242526DELETE /my_index PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;&#125; POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox jumps over the lazy dog&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox jumps over the quick dog&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;title&quot;: &quot;Brown fox brown dog&quot; &#125;GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;quick brown dog&quot;, &quot;minimum_should_match&quot;: &quot;99%&quot; &#125; &#125; &#125;&#125; 组合多查询中也有对 should 精度控制的介绍; 注意这里: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_how_match_uses_bool.html, 如下两个句子其实是一样的: 1234567891011121314151617&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;brown fox&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125;&#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;brown&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;fox&quot; &#125;&#125; ] &#125;&#125; boost 指定提高某些查询语句的权重 多字段查询 主要讲解了组合查询时, 不同组合情况对评分的影响 boost 值的不断尝试","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"07. 请求体查询, 查询与过滤, 查询条件的组合","slug":"elasticsearch/2018-06-30-07","date":"2018-06-30T03:07:21.000Z","updated":"2018-09-28T06:45:36.000Z","comments":true,"path":"2018/06/30/elasticsearch/2018-06-30-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/30/elasticsearch/2018-06-30-07/","excerpt":"","text":"请求体查询 简易查询(query-string search) 对于用命令行进行即席查询是非常有用的; 然而, 为了充分利用查询的强大功能, 你应该使用 请求体 search API, 之所以称之为请求体查询(Full-Body Search), 因为大部分参数是通过 Http 请求体而非查询字符串来传递的; 请求体查询不仅可以处理自身的查询请求, 还允许你对结果进行片段强调(高亮)、对所有或部分结果进行聚合分析, 同时还可以给出 你是不是想找 的建议, 这些建议可以引导使用者快速找到他想要的结果; 相对于使用晦涩难懂的查询字符串的方式, 一个带请求体的查询允许我们使用 查询领域特定语言（query domain-specific language） 或者 Query DSL 来写查询语句; 查询表达式 查询表达式(Query DSL)是一种非常灵活又富有表现力的查询语言, Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分功能。在你的应用中, 你应该用它来编写你的查询语句, 它可以使你的查询语句更灵活、更精确、易读和易调试; 要使用这种查询表达式, 只需将查询语句传递给请求体中的 query 参数: 1234GET /_search&#123; &quot;query&quot;: YOUR_QUERY_HERE&#125; 空查询(empty search) 在功能上其实就等价于使用 match_all 查询, 正如其名字一样, 匹配所有文档: 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 查询与过滤 Elasticsearch 使用的查询语言(DSL) 拥有一套查询组件, 这些组件可以以无限组合的方式进行搭配; 这套组件可以在以下两种情况下使用 过滤情况（filtering context）: 当使用于 过滤情况 时, 查询被设置成一个 不评分 的查询, 即, 查询结果只是简单的是或否, 不会计算任何评分; 查询情况（query context） : 当使用于 查询情况 时, 查询就变成了一个 评分 查询, 它不但会去判断这个文档是否匹配， 同时它还需要判断这个文档匹配的有 多好（匹配程度如何）;一个评分查询计算每一个文档与此查询的 _相关程度_，同时将这个相关程度分配给表示相关性的字段 _score，并且按照相关性对匹配到的文档进行排序。这种相关性的概念是非常适合全文搜索的情况，因为全文搜索几乎没有完全 “正确” 的答案 如何选择查询与过滤: 通常的规则是, 使用 查询（query）语句来进行 全文搜索或者其它任何需要影响相关性得分的搜索; 除此以外的情况都使用过滤（filters); 当我们想要查询一个具有精确值的 not_analyzed 未分析字段之前, 需要考虑, 是否真的采用评分查询, 或者非评分查询会更好; 单词项查询通常可以用是、非这种二元问题表示，所以更适合用过滤, 而且这样做可以有效利用缓存 查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件和 term 查询一样，terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;John&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter。这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果; (不过, 这只是should不在must或should下时) 只有must和must_not中的子句是决定了是否能查询出数据; 而should只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分; (如果非should的语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 should 子句的不同之处; 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 AND (a OR b) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配;123456789101112131415161718192021222324GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略, 语句会更简化 &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal1&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125; &#125; a OR (a AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 123456789101112131415161718192021222324GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略 &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125; &#125; 组合过滤和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"06. 搜索相关","slug":"elasticsearch/2018-06-16-06","date":"2018-06-16T09:20:09.000Z","updated":"2018-09-25T08:42:14.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-06/","excerpt":"","text":"多索引, 多类型, 多字段分页","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"05. Mapping 映射","slug":"elasticsearch/2018-06-16-05","date":"2018-06-16T02:31:39.000Z","updated":"2018-09-26T09:20:52.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-05/","excerpt":"","text":"一个奇怪的例子 先索引两个文档到不同的索引中 (为什么不在同一个映射中创建两个文档? 或者在同一个映射中的不同类型中各自创建一个文档?): 123456DELETE /test/PUT /test/mapping/1&#123; &quot;title&quot;: &quot;first time to build my mapping&quot;, &quot;text&quot;: &quot;2015/12/21 18:30:25&quot;&#125; 123456DELETE /test1/PUT /test1/mapping/1&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;build my mapping at 2015-12-28 13:20:25&quot;&#125; 尝试如下查询: GET /_search?q=2015 结果是两个文档, 因为 _all 字段:当索引一个文档的时候, Elasticsearch 会取出所有字段的值拼接成一个大的字符串, 作为 _all 字段进行索引;之后在查询时, 如果不指定字段, 则默认会去 _all 字段中进行查询 GET /_search?q=text:2015 结果只有第二个索引中的文档ES在索引文档时, 如果不手动设置索引的 mapping 映射来说明索引中文档的字段类型, ES会动态帮你生成一份 mapping (ES会根据字段值猜测字段的类型), 主要是想对文档的全文域进行分析以用来创建倒排索引;(做全文索引)当你进行搜索时, 如果指明字段, ES会根据字段类型来决定该字段是否进行全文分析, 由于第二篇文档的text是全文类型, 所以会进行全文分析, 会查询到;而第一篇文档在最初进行索引时, ES将其text字段识别为 DATE 类型, 而查询时, 当你指明查询 text 字段时, ES并不会对DATE类型字段做分析, 所以检索不到第二篇文档; 所以 date 字段和 string 字段 索引方式不同, 因此搜索结果也不一样; 他们最大的差异在于 代表精确值的字段 和 代表全文的字段 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。 当你查询一个 精确值 域时，不会分析查询字符串， 而是搜索你指定的精确值。 在实际开发中, 开发者比ES更了解自己的文档信息, 很多时候都需要自行说明自己的字段类型, 以防止ES动态创建映射可能会造成的一些小问题; Mapping映射 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, Elasticsearch 需要知道每个域中数据的类型, 这些信息就包含在映射中; 索引中每个文档都有 类型, 每种类型都有它自己的 映射; 映射定义了类型中每个域的数据类型, 以及Elasticsearch如何处理这些域。映射也用于配置与类型有关的元数据。 也就是说, 映射是类型的, 但事实上, 即使在不同的类型中, 也不能对相同字段做不同的类型指定; 参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型; 所以文章开头的实验, 要创建两个 包含不同类型text字段 的文档, 需要在不同的索引中进行创建! 查看映射, 之前已经使用过 GET /索引名/_mapping/类型名 自定义域映射 自定义映射允许你执行下面的操作: 全文字符串域 和 精确值字符串域 的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性就是 type ELasticsearch 5.X 之后的字段类型不再支持 string, 由 text 或 keyword 取代; text 取代了 string: 当一个字段是要被全文搜索的, 比如Email内容、产品描述, 应该使用text类型它们的值在索引前，会通过 一个分析器, 在生成倒排索引以前, 字段内容会被分析器分成一个一个词项, 针对于这个域的查询在搜索前也会经过一个分析器;text类型的字段不用于排序, 很少用于聚合(termsAggregation除外) 映射中的每个 text 字段都可以指定自己的分析器 analyzer;在索引时, 如果未指定分析器, 它将在索引设置中查找名为 default analyzer; 否则, 它默认使用 standard analyzer;可参考文档 keyword类型: 适用于索引结构化的字段, 比如email地址、主机名、状态码和标签, 如果字段需要进行过滤(比如查找已发布博客中status属性为published的文章)、排序、聚合; keyword类型的字段只能通过精确值搜索到; 更新映射 当你首次 创建一个索引时, 可以使用 /_mapping 为新类型(或者为存在的类型更新映射)增加映射; 尽管你可以为一个已存在的映射增加一个新域, 但不能 修改 存在的域映射如果一个域的映射已经存在, 那么该域的数据可能已经被索引, 如果你意图修改这个域的映射, 索引的数据可能会出错, 不能被正常的搜索 可以更新一个映射来添加一个新域, 但不能将一个存在的域从 analyzed 改为 not_analyzed ES5.X中, 索引属性只接收true/false来代替not_analyzed/no 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647DELETE /gb# 创建一个新索引, 指定 tweet 域使用 english 分析器PUT /gb&#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125;GET /gb/_mapping# 在 tweet 映射增加一个新的名为 `tag` 的 not_analyzed 的文本域PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125;&#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"04. 文档的概念, ES乐观并发控制, 文档基本操作","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T06:29:07.000Z","updated":"2018-09-20T07:25:26.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"前言 传统上, 我们以 行和列的形式 存储数据到关系型数据库中, 相当于使用电子表格; 正因为我们使用了这种不灵活的存储媒介导致所有我们使用对象的灵活性都丢失了; 面向对象编程语言如此流行的原因之一是对象帮我们表示和处理现实世界具有潜在的复杂的数据结构的实体 而 JSON 正是一种以人可读的文本表示对象的方法, 它已经变成 NoSQL 世界交换数据的事实标准, 当一个对象被序列化成为 JSON, 它被称为一个 JSON 文档;JSON文档可以将我们的对象按对象的方式来存储, 这样我们就能更加专注于 使用 数据, 而不是在电子表格的局限性下对我们的应用建模; 我们可以重新利用对象的灵活性; Elastcisearch 是分布式的 文档 存储, 它能存储和检索复杂的数据结构—序列化成为JSON文档 现存的 NoSQL 解决方案虽然允许我们以文档的形式存储对象, 但是他们仍旧需要我们思考如何查询我们的数据, 以及确定哪些字段需要被索引以加快数据检索; 而在 ES 中, 每个字段的所有数据都是默认被索引的, 即每个字段都有为了快速检索设置的专用倒排索引; 而且, 不像其他多数的数据库, 它能在相同的查询中使用所有这些倒排索引, 并以惊人的速度返回结果; 文档 文档及其元数据介绍 索引文档, 注意 PUT 和 POST 两个谓词的使用场景; 小心覆盖创建 当我们索引一个文档, 怎么确认我们正在创建一个完全新的文档, 而不是覆盖现有的呢? 最简单办法是, 使用索引请求的 POST 形式让 Elasticsearch 自动生成唯一 _id; 另外, 如果有自己的文档ID, 防止覆盖 (成功会返回 401码, 冲突则会返回 409码)/website/blog/123?op_type1PUT /website/blog/123/_create 返回文档中的部分字段 更新整个文档, 注意, ES中文档不能修改, 需要重建或者替换 删除比较简单… ES文档的丢失更新问题 在使用 index API 更新文档时, 一般会先读取原始文档, 然后做我们的修改, 最后重新索引整个文档; 如果两个进程同时做这系列操作的话, 最后的索引请求将获胜, 当然, 有人的更改将丢失。 假设我们使用 Elasticsearch 存储我们网上商城商品库存的数量, 每次我们卖一个商品的时候, 在 Elasticsearch 中将库存数量减少; 有一天, 管理层决定做一次促销, 突然地, 每秒要卖好几个商品, 假设有两个 web 程序并行运行, 每一个都同时处理所有商品的销售 可以看到: web_1 对 stock_count 所做的更改已经丢失; 结果就会出现超卖现象; 在数据库领域中, 有两种方法通常被用来确保并发更新时变更不会丢失: 悲观并发控制 : 这种方法被关系型数据库广泛使用, 它假定有变更冲突可能发生, 因此阻塞访问资源以防止冲突;一个典型的例子是读取一行数据之前先将其锁住, 确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制 : Elasticsearch 中使用的这种方法假定冲突是不可能发生的, 并且不会阻塞正在尝试的操作, 不过, 如果源数据在读写当中被修改, 更最后在更新时将会失败; 应用程序接下来将决定该如何解决冲突, 例如, 可以重试更新、使用新的数据、或者将相关情况报告给用户; ES乐观并发控制 ES 是分布式的, 当文档创建、更新或删除时, 新版本的文档必须复制到集群中的其他节点, 而 Elasticsearch 也是异步和并发的, 这意味着这些复制请求是被并行发送的, 所以到达目的地时也许顺序是乱的; 因此 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本: ES的每个文档都有一个 _version (版本)号, 当文档被修改时版本号递增; ES正是使用这个 _version 号来确保变更以正确顺序得到执行 (如果旧版本的文档在新版本之后到达, 它会被直接忽略) 我们可以利用 _version 号来确保 应用中相互冲突的变更不会导致数据丢失, 通过指定想要修改文档的 version 号来达到这个目的, 如果该版本不是当前版本号, 我们的请求将会失败 测试 创建一个新文档 (当然, 响应体会告诉我们, 该文档版本号目前是1) 12345PUT /website/blog/1/_create&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;&#125; 现在假设我们想编辑这个文档: 一般是先查询,将文档数据加载到 web 表单中; 然后做一些修改, 保存新的版本 1234567891011// 查询文档, 响应体中可以看到版本号仍为1GET /website/blog/1// 尝试通过重建文档的索引来保存修改, 我们指定 version 为我们的修改会被应用的版本:// 也就是我们想要索引中的文档只有 _version 为 1 时, 本次更新才能成功; // 最后此请求成功, 并且响应体告诉我们 _version 已经递增到 2PUT /website/blog/1?version=1 &#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Starting to get the hang of this...&quot;&#125; 然而, 如果我们重新运行相同的索引请求, 仍然指定 version=1, Elasticsearch 返回 409 Conflict HTTP 响应码, 和一个如下所示的响应体: 12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;llBrPVECRFuD45NCpJaDfg&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;llBrPVECRFuD45NCpJaDfg&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 409&#125; 更多请参考文档: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/optimistic-concurrency-control.html 文档的部分更新 - _update 之前在介绍 “更新整个文档” 时, 已经了解到, 文档是不可变的, 他们不能被修改, 只能被替换; 更新一个文档的方法是检索并修改它, 然后重新索引整个文档; 但其实使用 update API 我们还可以部分更新文档, 但是, update API 必须遵循同样的规则, 从外部来看, 我们在一个文档的某个位置进行部分更新, 然而在内部, update API 简单使用与之前描述相同的 检索-修改-重建索引 的处理过程 区别在于这个过程发生在分片内部, 这样就避免了多次请求的网络开销; 通过减少检索和重建索引步骤之间的时间, 我们也减少了其他进程的变更带来冲突的可能性。 update 请求最简单的一种形式是接收文档的一部分作为 doc 的参数, 它只是与现有的文档进行合并, 对象被合并到一起, 覆盖现有的字段, 增加新的字段 例如, 我们对 博客文章 增加字段 tags 和 views , 并修改 text 字段, 如下所示: 123456789// 测试成功:POST /website/blog/1/_update&#123; &quot;doc&quot; : &#123; &quot;tags&quot; : [ &quot;testing&quot; ], &quot;views&quot;: 0, &quot;text&quot;: &quot;哈哈&quot; &#125;&#125; 更多部分更新相关内容, 可参考手册: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/partial-updates.html 未完待续~~ 取回多文档","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"03. 集群相关 (节点, 分片及角色, 扩容, 故障测试)","slug":"elasticsearch/2018-06-10-03","date":"2018-06-10T06:26:39.000Z","updated":"2018-09-28T10:11:18.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-03/","excerpt":"","text":"前言 ElasticSearch 的主旨是随时可用和按需扩容, 这里主要是说 水平(横向)扩容 — 为集群添加更多的节点将负载压力分散到这些节点中; ElastiSearch 天生就是分布式的, 它知道如何通过管理多节点来提高扩容性和可用性; 接下来将讲述如何按需配置集群、节点和分片, 并在硬件故障时确保数据安全 节点 及其 角色 节点: 一个运行中的 Elasticsearch 实例就称为一个节点; 默认情况下, es 集群中每个节点都有成为主节点的资格, 也都存储数据, 还可以提供查询服务, 这些功能是由两个属性控制的 node.master: 这个属性表示节点是否具有成为主节点的资格 (注意: 此属性的值为true, 并不意味着这个节点就是主节点; 真正的主节点, 是由多个具有主节点资格的节点进行选举产生的; 所以, 这个属性只是代表这个节点是不是具有主节点选举资格) node.data: 这个属性表示节点是否存储数据; 上面两个配置属性可以有四种组合: 节点只有成为主节点的资格(有可能成为真正的主节点), 但不会存储数据; 称为master节点 12node.master: truenode.data: false 节点没有成为主节点的资格, 即, 不参与选举, 只会存储数据; 称为data(数据)节点在集群中需要单独设置几个这样的节点负责存储数据, 后期提供存储和查询服务 12node.master: falsenode.data: true 节点既不会成为主节点, 也不会存储数据 (也叫协调节点/路由节点)这个节点的意义是作为一个client(客户端)节点, 主要是针对海量请求的时候可以进行负载均衡 12node.master: falsenode.data: false 既有成为主节点的资格, 又存储数据: 默认情况下, 每个节点都有成为主节点的资格, 也会存储数据, 还会处理客户端的请求如果这个节点被选举成了真正的主节点, 那么它除了干主节点要干的活, 还要存储数据, 这样对于这个节点的压力就比较大;elasticsearch 默认每个节点都是这样的配置, 在测试环境下这样做没问题, 但是实际工作中建议不要这样设置; 因为这样相当于 主节点 和 数据节点 的角色混合到一块了; 12node.master: falsenode.data: true - 根据前面节点及其角色的介绍, 如果尝试配置唯一的节点为 `node.master: true node.data: false`, 则节点中的所有分片都会是未分配状态, 因为节点不是数据结点, 无法存放数据 - 如果配置为 `node.master: false node.data: false`, 貌似无法启动 在生产集群中我们可以对这些节点的职责进行划分 建议集群中设置3台以上的节点作为 master节点 node.master: true node.data: false, 这些节点只负责成为主节点, 维护整个集群的状态; 再根据数据量设置一批data节点 node.master: false node.data: true, 这些节点只负责存储数据, 后期提供建立索引和查询索引的服务, 这样的话如果用户请求比较频繁, 这些节点的压力也会比较大; 所以在集群中建议再设置一批client节点 node.master: false node.data: false, 这些节点只负责处理用户请求, 实现请求转发, 负载均衡等功能;master节点: 普通服务器即可(CPU 内存 消耗一般)data节点: 主要消耗磁盘, 内存client节点: 普通服务器即可(如果要进行分组聚合操作的话, 建议这个节点内存也分配多一点) 集群 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; 当一个节点被选举成为 主节点 时, 它将负责管理集群范围内的所有变更, 例如增加、删除索引, 或者增加、删除节点等; 不过, 纯粹的主节点并不需要涉及到文档级别的变更和搜索等操作, 所以, 集群所拥有的唯一一个主节点, 如果是纯粹的主节点的话, 即使流量的增加它也不会成为瓶颈; 任何节点都可以成为主节点, 到目前为止, 我们之前的示例集群就只有一个节点, 当然, 它同时也是主节点; 作为用户, 我们可以将请求发送到集群中的任何节点, 包括主节点; 每个节点都知道任意文档所处的位置, 并且能够将我们的请求直接转发到存储我们所需文档的节点, 无论我们将请求发送到哪个节点, 它都能负责从各个包含我们所需文档的节点收集回数据, 并将最终结果返回給客户端, Elasticsearch 对这一切的管理都是透明的。 集群健康, 可以通过 GET /_cluster/health 来查看, 对于返回结果, 最需要关心的是 status 字段, 它指示着当前集群在总体上是否工作正常, 它的三种颜色含义如下: green 所有的主分片和副本分片都正常运行 yellow 所有的主分片都正常运行, 但不是所有的副本分片都正常运行 red 有主分片没能正常运行 分片 我们往 Elasticsearch 添加数据时需要用到 索引(保存相关数据的地方), 而索引实际上是指向一个或者多个物理分片的逻辑命名空间; 分片: 一个分片是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 但是, 它可能仅保存了全部文档中的一部分; Elasticsearch 是利用分片将数据分发到集群内各处的, 分片是数据的容器, 文档保存在分片内, 分片又被分配到集群内的各个节点里; 当你的集群规模扩大或者缩小时(即增加或减少节点时), Elasticsearch 会自动的在各节点中迁移分片, 使得数据仍然均匀分布在集群里。 分片有两种类型: 主分片, 副本分片 索引内任意一个文档都归属于一个主分片, 所以主分片的数目决定着索引能够保存的最大数据量; 而副本分片只是一个主分片的拷贝, 副本分片作为硬件故障时保护数据不丢失的冗余备份, 并为搜索和返回文档等读操作提供服务; 注意: 在索引建立的时候就确定了主分片数,之后无法随意修改;而副本分片数可以随时修改; 每个索引在默认情况下会被分配5个主分片, 不过你也可以在创建索引前, 先指定索引的 主分片数 和 每个主分片对应的副本分片数, 如下, 给 blogs 索引分配3个主分片 和 每个主分片分配1个副本: 1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 不过, 由于当前只有一个节点, 所以集群的健康状况为 yellow , 表示全部主分片都正常运行(集群可以正常服务所有请求), 但是副本分片没有全部处在正常状态; 实际上, 所有3个副本分片都是 unassigned —— 它们都没有被分配到任何节点, 因为当前只有一个节点, 而在同一个节点上既保存原始数据又保存副本是没有意义的; 当前这种状况, 一旦失去了唯一的节点, 也就会丢失该节点上的所有副本数据; 当前我们的集群是正常运行的, 但是在唯一的结点出现硬件故障时有丢失数据的风险; 故障转移 添加故障转移: 当集群中只有一个节点在运行时, 意味着会有一个单点故障问题 —— 没有冗余; 幸运的是, 在ES中, 我们只需再启动一个节点即可防止数据丢失; 启动第二个节点非常简单, 你可以在同一个目录内, 完全依照启动第一个节点的方式来启动一个新节点(多个节点可以共享同一个目); 只要它和第一个节点有同样的 cluster.name 配置, 它就会自动发现集群并加入到其中; 但是注意: 在不同一机器上启动节点的时候, 为了加入到同一集群, 你需要配置一个可连接到的单播主机列表 单播, 组播 单播: Elasticsearch 默认被配置为使用单播发现, 以防止节点无意中加入集群, 只有在同一台机器上运行的节点才会自动组成集群;除了同一台机器上的集群会自动发现同名节点, 使用单播, 你还可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表, 当一个节点可以联系到单播列表中的成员(节点)时, 它就会得到整个集群所有节点的状态, 然后它会联系 master 节点, 并加入集群;这意味着你的单播列表不需要包含你的集群中的所有节点, 它只是需要足够的节点, 当一个新节点联系上其中一个并且说上话就可以了;如果你使用 master 候选节点作为单播列表, 你只要列出三个就可以了, 这个配置在 elasticsearch.yml 文件中: discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;] 组播: 组播貌似仍然是作为插件提供, 并且它应该永远不要在生产环境使用, 否则可能会出现一个节点意外的加入到了你的生产环境集群中;虽然组播 本身 并没有错, 但一不小心就会导致一些愚蠢的问题, 并且导致集群变的脆弱; 最好使用单播代替组播 继续上面的第6点, 由于目前是在同一台机器上启动第二个节点, 所以不同配置单播列表, 直接启动一个节点即可; 当启动第二个节点后, 由于其使用的是和第一个节点一样的配置, 集群名也就相同, 所以会自动加入到集群; 加入集群后, blogs 索引的 3个 副本分片 将会分配到这个节点上, 这意味着当集群内任何一个节点出现问题时, 我们的数据都完好无损; 所有新索引的文档都将会保存在主分片上, 然后被并行的复制到对应的副本分片上, 这就保证了我们既可以从主分片又可以从副本分片上获得文档; 并且 cluster-health 现在展示的状态为 green, 这表示所有6个分片(包括3个主分片和3个副本分片)都在正常运行; 水平扩容 如何为我们的正在增长中的应用程序按需扩容呢? 再次尝试启动第三个新的节点, 会发现集群状态如下: 可以看到 Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点 现在每个节点上都拥有2个分片, 而不是之前的3个, 这表示每个节点的硬件资源(CPU, RAM, I/O)将被更少的分片所共享, 每个分片的性能将会得到提升 分片是一个功能完整的搜索引擎, 它拥有使用一个节点上的所有资源的能力也就是说, 目前我们这个拥有6个分片(3个主分片和3个副本分片)的索引, 可以最大扩容到6个节点, 让每个节点上只有该索引的一个分片(也可能会有其他索引的分片哦); 分片数量一定, 增加节点, 则每个分片性能将会提升, 因为被赋予的更多的硬件资源; 但是当一个索引的分片数量和集群的节点数量达到一致时, 其分片性能达到最高, 继续增加更多的副本分片是不能提高性能的, 因为每个分片从节点上获得的资源会变少, 你需要增加更多的硬件资源来提升吞吐量; 更多的扩容提升搜索性能 想要继续扩容超过6个节点, 需要先知道: 由于索引的主分片数目在索引创建时就已经确定了, 这个数目定义了这个索引能够 存储 的最大数据量也就是索引能存储的最大数据量在创建索引的时候就通过设置主分片数确定了(不过实际大小还取决于你的数据、硬件和使用场景) 由于 搜索操作 和 返回数据操作 可以同时被主分片 或 副本分片所处理, 所以当你拥有越多的副本分片时, 也将拥有越高的吞吐量 在运行中的集群上是可以动态调整副本分片数目的, 比如, 可以把副本数从默认的 1 增加到 2 : 1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; blogs 索引现在拥有9个分片: 3个主分片和6个副本分片; 这就意味着可以将集群扩容到9个节点(每个节点上只放该索引的一个分片), 相比原来3个节点时(每个节点两个分片), 虽然存储量不变, 但是集群搜索性能可以提升 3 倍; 当一个索引的分片数量和集群的节点数量达到一致时, 其分片性能将达到最高, 无法再提升分片的性能! 此时可以 通过增加更多的副本分片, 同时增加节点来提升集群的搜索性能! 集群的整体性能还是需要通过增加节点来提高! 故障测试 目前的集群状态如下: 如果关闭主节点 由于而集群必须拥有一个主节点来保证正常工作, 所以发生的第一件事情就是选举一个新的主节点; 在我们关闭主节点的同时也失去了主分片 1 和 2, 并且在缺失主分片的时候索引也不能正常工作, 如果此时来检查集群的状况, 我们看到的状态将会为 red: 不是所有主分片都在正常工作 幸运的是, 在其它节点上存在着主节点上这两个主分片的完整副本, 所以新的主节点会立即将这两个主分片在 另外两个节点上 对应的副本分片提升为主分片, 此时集群的状态将会为 yellow: 这个提升主分片的过程是瞬间发生的, 如同按下一个开关一般此时, 虽然我们又拥有所有的三个主分片, 但是由于之前设置了每个主分片需要对应2份副本分片, 而此时只有两个几点, 只存在一份副本分片, 所以集群不能为 green 的状态;注意连接的端口: 如果重新启动之前关闭的节点, 集群可以将缺失的副本分片再次进行分配, 如果它依然拥有着之前的分片, 它将尝试去重用它们, 同时仅从主分片复制发生了修改的数据文件; 配置相关 注意: discovery.zen.minimum_master_nodes ??","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"02. 适应一下ES","slug":"elasticsearch/2018-06-08-02","date":"2018-06-08T13:23:07.000Z","updated":"2018-09-18T07:44:47.000Z","comments":true,"path":"2018/06/08/elasticsearch/2018-06-08-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/08/elasticsearch/2018-06-08-02/","excerpt":"","text":"文档该章节比较基础, 主要是为了对 Elasticsearch 有一个基本印象, 通过对雇员文档的基本操作, 了解 索引、搜索 及 聚合 等基础概念;可能有会遇到有些语句在5.X中无法运行, 比如 进行聚合操作时提示 Fielddata is disabled on text fields by default, 可参考此文章","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"01. Elasticsearch 入门 及 简单安装","slug":"elasticsearch/2018-06-08-01","date":"2018-06-08T06:24:25.000Z","updated":"2018-09-20T07:06:25.000Z","comments":true,"path":"2018/06/08/elasticsearch/2018-06-08-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/08/elasticsearch/2018-06-08-01/","excerpt":"","text":"主要是简单过一下 Elasticsearch 权威指南, 做一下学习记录~~ 简介 Elasticsearch 是一个基于 Lucene 的开源, 高性能, 全文检索 和 分析引擎, 可以快速且近实时地存储, 检索以及分析海量数据; 当然, ES 不仅仅是 Lucene, 也不仅仅只是一个全文搜索引擎, 它可以被下面这样准确的形容: 一个分布式近实时分析搜索引擎; 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级结构化或非结构化数据; 近实时搜索: 从文档索引到可以被检索只有轻微延时, 约1s RESTful API: ES 建立在全文搜索引擎 Lucene 之上, 通过简单的 RESTful API 来隐藏 Lucene 的复杂性, 从而让全文搜索变得简单, 各种语言的客户端甚至命令行都可以与之交互; 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引; 使用案例 在微服务架构下的多数据源聚合列表页(一个页面中的数据来自多个服务, 且筛选条件也涉及到多个服务中的数据字段), 如果用传统数据库解决该问题, 会大费周折, 并且效果并不好, 而如果使用 ES 来作数据聚合服务, 效果就比较清晰明了了; 您想要去收集日志或交易数据, 并且还想要去分析和挖掘这些数据来找出趋势, 统计, 或者异常现, 在这种情况下, 您可以使用 Logstash(Elasticsearch/Logstash/Kibana) 技术栈中的一部分, 来收集, 聚合, 以及解析数据, 然后让 Logstash 发送这些数据到 Elasticsearch; 如果这些数据存在于 Elasticsearch 中, 您就可以执行搜索和聚合以挖掘出任何您感兴趣的信息; GitHub 使用 Elasticsearch 对1300亿行代码进行查询; …… 版本选择 ES 的版本变更比较快, 目前(06/2018)为止, Elasticsearch已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X, 而且中文文档进度也比较滞后, 这也是让很多兄弟比较头疼的事情; 其实可以根据公司所选的云服务上 ES版本 来决定你的学习版本 (当前阿里云的Elasticsearch云服务为5.5.3, 因此此处也是针对5.X版本进行学习调研); 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (这里使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本集集群相关的信息, 但这不是图形化的界面, 操作起来不是很方便, 如果希望能有一个可视化的环境来操作它, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 小结到此为止, 正常学习的Es环境已经安装完毕, 不要纠结这些服务的开机启动, 调优配置, 集群, 高可用, 监控……骚年, 暂时先让它能跑起来就行!","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"28. 隔离级别 与 锁","slug":"mysql/2017-09-03-mysql-28","date":"2017-09-03T06:20:52.000Z","updated":"2018-09-03T03:54:23.000Z","comments":true,"path":"2017/09/03/mysql/2017-09-03-mysql-28/","link":"","permalink":"http://blog.renyimin.com/2017/09/03/mysql/2017-09-03-mysql-28/","excerpt":"","text":"前言 之前几篇博文已经介绍了Mysql事务, 高并发下事务将会面对的问题 及 MySQL的解决方案; MySQL主要采用 事务隔离性中的4种隔离级别 结合 MVCC机制 来进行解决; 而事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略; 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的; READ UNCOMMITTED 未提交读READ COMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料 《高性能MySQL》 MySQL官方文档 美团技术博客 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"27. 幻读, 快照读(snapshot read), 当前读 (current read)","slug":"mysql/2017-09-02-mysql-27","date":"2017-09-02T11:25:07.000Z","updated":"2018-09-01T14:02:47.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-27/","excerpt":"","text":"RR + MVCC 虽然解决了 幻读 问题, 但要注意, 幻读针对的是读操作(对于其他操作就不一样了); 演示 打开 两个客户端 1,2 确保隔离级别为默认级别RR, 提供语句: 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 123456789101112mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) 回到 客户端2 的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了 select 幻读)!! 12345678910111213141516171819202122mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 但如果尝试在客户端2的事务中执行 insert/delete/update , 却会发现此类操作都可以感知到客户端1提交的新数据 123mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; 小结 虽然发现已经克服了幻读问题; 但当 在客户端2事务中 insert 插入一条id为4的新数据, 却发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 — 一致性非阻塞读中的一段介绍 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。如果一个事务去更新或删除其他事务提交的行, 则那些更改对当前事务就变得可见;但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 也就是RR隔离级别, 在同一事务中多次读取的话, 对 select 克服了 幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念: 当前读 和 快照读 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的 select 都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 快照读： 是通过MVVC(多版本控制)和 undo log 来实现的, 常见语句如下(貌似就是常见的悲观锁么): 1简单的select操作 (不包括: `select ... lock in share mode`, `select ... for update`) 而 当前读 根本不会创建任何快照, insert, update, delete都是当前读, 所以这几个操作会察觉到其他事务对数据做的更改(而普通select是察觉不到的): 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MySQL 高并发下常见的事务问题","slug":"mysql/2017-09-02-mysql-26","date":"2017-09-02T06:56:32.000Z","updated":"2018-09-01T14:02:40.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-26/","excerpt":"","text":"前言上一篇MySQL事务简介中对MySQL事务的 基本概念 及 特性 做了简单介绍; 接下来会分析在实际生产环境中面对高并发场景时, 事务会出现的一些常见问题; 高并发事务问题在并发量比较大的时候, 很容易出现 多个事务并行 的情况; 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境 过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以 最终导致了一些问题的出现; 脏读 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读 因为 事务A 此时读取到的是 并行事务B 尚未最终持久化的数据 (该数据还不具备事务的 持久性) 事务B 最终可能会因为其事务单元内部其他后续操作的失败 或者 系统后续突然崩溃等原因, 导致事务B最终整体提交失败而回滚, 那么最终 事务A 之前拿到就是 脏的数据 了(当然, 如果 事务A 在后续操作中继续读取的话, 无论事务B是否结束, 其每次的更新操作, 事务A都会及时读到新数据, 只不过这同时涉及到了下一个讨论的 不可重复读问题, 暂时可以不了解) 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身隔离性解决了脏读问题 : READ COMMITED 或 以上隔离级别(RC+); READ COMMITED 隔离级别保证了: 在事务单元中, 某条语句执行时, 只有已经被其他事务提交的持久性落地数据, 才对该语句可见; 不可重复读 之前 脏读问题 的解决了, 仅仅只意味着事务单元中的每条语句读取到的数据都是 具备持久性的落地数据而已; 之前在讨论脏读问题时, 有个问题也同时存在着, 那就是一个事务单元中 不可重复读 的问题; 显然, RC 隔离级别只解决了 脏读的问题 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了 解决方案 : RR+ 在MySQL中, 事务已经用自身隔离性解决了 不可重复读 问题 — REPEATABLE READ 或 以上隔离级别(RR+); REPEATABLE READ 级别保证了:在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的; ( READ COMMITED )在事务中, 多次读取同一个数据(在两次读取操作之间, 无论数据被 提交 多少次(即无论落地过多少遍), 每次读取的结果都应该是和事务中第一次读取的结果一样; 幻读 可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 不可重复读 和 幻读 这两个概念容易搞混 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录); 解决方案: RR + MVCC 其实对于 幻读 问题, 在Mysql的InnoDB存储引擎中, 是通过事务的 RR + MVCC机制 进行解决的;当然, 这里的幻读不涉及 具有当前读能力的那些语句; (也就是说只是解决幻读, 所谓幻写之类的就不在范围内了) 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 之所以 不可重复读 和 幻读 容易搞混, 可能是因为: 在mysql中, 由于默认就是RR隔离级别下, 该隔离级别已经解决了幻读, 所以无法模拟出幻读的场景; 而 退回到 RC隔离级别 的话, 虽然 幻读 和 不可重复读 都会出现, 但由于现象都是两次读取结果不一样, 容易分辨不出! 想了解更多, 可以参考下一篇幻读的延伸 高并发事务问题 之 更新丢失最后聊一下高并发事务的另一个问题, 也是最常遇到的问题: 丢失更新问题; 该问题和之前几个问题需要区分开: 该问题需要我们自己来解决;更新丢失问题分为两类 第一类丢失更新(回滚覆盖)简介 事务A 回滚时, 将 事务B 已经提交的数据覆盖了 需要注意的是: 这种情况在Mysql中不会出现; RU 级别演示 对于InnoDB事务的最低隔离级别 READ UNCOMMITED, 并行事务B的未提交数据都可以读到, 更别说已提交数据了 (所以回滚也会回滚到事务B提交的最新数据) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RC 级别演示 对于 READ COMMITTED: 在事务B提交之后, 事务A在T3阶段是可以select(快照读)到事务B最终提交的数据的, 更别说update(当前读)到了, 所以事务A最终的Rollback其实也是基于事务B提交后的数据的 (关于这里提到的快照读和当前读, 下一篇会介绍) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RR 级别演示 对于 REPEATABLE READ 可重复读, 事务A在T3阶段虽然select不到事务B最终提交的数据(快照读), 但是可以update(当前读)到事务B最终提交的数据的 (注意: RR与RC虽然都会有快照读, 但是快照读的结果却不一致, 其实是因为两者的MVCC机制快找时机不同导致的, 后面会讲解) 语句如下: 1234567SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age+10 where id=2;rollback; 12345SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age-15 where id=2;commit; SERIALIZABLE 演示 SERIALIZABLE 串行化: 读写都加锁, 最容易出现死锁, 所以也不会出现第一类丢失更新的问题, 直接就死锁了 语句如下: 123456SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;SELECT @@SESSION.tx_isolation;begin;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age -15 where id=2;commit; 第二类丢失更新(提交覆盖) 直接上图 另外, 这里可以解释一下为什么 SERIALIZABLE级别 通常不会不被采用 其实 SERIALIZABLE 虽然做了串行化, 其实也就是对读写都加了锁, 但一旦事务并行, 如果将判断库存的读操作放在事务内就很容易会死锁而放在事务外, 由于更新操作仍然会依据上一个查询的结果, 所以仍然是避免不了第二类丢失更新问题的, 会造成超卖等问题; SERIALIZABLE 的串行化本身也太低效 另外, 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 解决第二类丢失更新的方案: 乐观锁 (在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制) 悲观锁 参考资料: 《高性能MySQL》 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"25. MySQL 事务简介","slug":"mysql/2017-08-27-mysql-25","date":"2017-08-27T11:31:07.000Z","updated":"2018-09-01T14:02:34.000Z","comments":true,"path":"2017/08/27/mysql/2017-08-27-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/08/27/mysql/2017-08-27-mysql-25/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的工作单元, 在这个独立的工作单元中, 可以有一组操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败 随处可见的例子: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，就需要打包在一个事务中作为 独立的工作单元 来执行。并且在 这个独立工作单元中的三个操作, 只要有任何一个操作失败, 则整体就应该是失败的, 那就必须回滚所有已经执行了的步骤; 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚 (这也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元;对于一个事务来说, 不能只成功执行其中的一部分操作, 整个事务中的所有操作要么全部成功提交, 要么有操作失败导致所有操作全部回滚, 这就是事务的原子性。 Consistency 一致性此一致性非彼一致性 你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性保证的是 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态, 而不是分布式中提到的数据一致性; 比如之前转账的例子: 转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15) 转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115) 转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取 Iron Man账户 的余额, 那么这个程序读到的应该是500才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务 隔离性 的四个 隔离级别, 到时候就知道这里为什么说 通常来说 ; (确实有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读)(注意: 和RR一样都采用了MVCC机制, 但与RR级别主要区别是快照时机不同, 暂时可不必了解, 后面文章会详解) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 只有该隔离级别才会读写都加锁 Durability 持久性 一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久保存到数据库中; 所谓永久, 也只是主观上的永久, 可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方; 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]}]}