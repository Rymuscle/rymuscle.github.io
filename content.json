{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"18. Closure","slug":"python/2018-10-15-Python-18","date":"2018-10-15T08:29:56.000Z","updated":"2018-10-15T08:44:06.000Z","comments":true,"path":"2018/10/15/python/2018-10-15-Python-18/","link":"","permalink":"http://blog.renyimin.com/2018/10/15/python/2018-10-15-Python-18/","excerpt":"","text":"装饰器 在面向对象(OOP)的设计模式中, decorator被称为装饰模式, OOP的装饰模式需要通过 继承 和 组合 来实现, 而Python除了能支持OOP的decorator外, 直接从语法层次支持decorator, Python的decorator可以用函数实现, 也可以用类实现; decorator可以增强函数的功能, 定义起来虽然有点复杂, 但使用起来非常灵活和方便 示例: 123456789101112131415161718192021222324252627282930313233import functoolsdef log(func): def wrapper(*args, **kw): print(&apos;call %s():&apos; % func.__name__) return func(*args, **kw) return wrapper@logdef now(): print(&apos;2015-3-25&apos;)# now = log(now)now()print(now.__name__)print(&apos;\\n----------------------\\n&apos;)def log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print(&apos;%s %s():&apos; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator@log(&apos;execute&apos;)def now(): print(&apos;2015-3-25&apos;)# now = log(&apos;execute&apos;)(now)now()print(now.__name__) 偏函数 简单总结 functools.partial 的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"17. Closure","slug":"python/2018-10-15-Python-17","date":"2018-10-15T06:34:28.000Z","updated":"2018-10-15T07:26:47.000Z","comments":true,"path":"2018/10/15/python/2018-10-15-Python-17/","link":"","permalink":"http://blog.renyimin.com/2018/10/15/python/2018-10-15-Python-17/","excerpt":"","text":"闭包 当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，但其内部实现起来可不容易 坑: 如下, 返回的三个函数在最终被调用后, 返回的是 9，9，9 而不是 1，4，9 1234567891011def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count()print(f1, f2, f3)print(f1(), f2(), f3()) 原因就在于返回的函数引用了变量i, 但它并非立刻执行; 等到3个函数都返回时, 它们所引用的变量i已经变成了3, 因此最终结果为9; 因此, 返回闭包时牢记一点: 返回函数不要引用任何循环变量, 或者后续会发生变化的变量; 如果一定要引用循环变量怎么办? 方法是再创建一个函数, 用该函数的参数绑定循环变量当前的值, 无论该循环变量后续如何更改, 已绑定到函数参数的值不变: 123456789101112131415 def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs ``` 缺点是代码较长，可利用lambda函数缩短代码 ## 匿名函数1. lambda - 匿名函数 `lambda x: x * x` 实际上就是: def f(x): return x * x 1234 - 关键字lambda表示匿名函数, 冒号前面的x表示函数参数 - 匿名函数有个限制, 就是只能有一个表达式, 不用写return, 返回值就是该表达式的结果2. 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数 f = lambda x: x * xf","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"15. 函数式编程","slug":"python/2018-10-14-Python-16","date":"2018-10-12T03:52:31.000Z","updated":"2018-10-15T04:06:37.000Z","comments":true,"path":"2018/10/12/python/2018-10-14-Python-16/","link":"","permalink":"http://blog.renyimin.com/2018/10/12/python/2018-10-14-Python-16/","excerpt":"","text":"高阶函数 在Python中, 函数本身也可以赋值给变量, 即 变量可以指向函数; 如果一个变量指向了一个函数, 那么你可以通过该变量来调用这个函数, 如下直接调用abs()函数和调用变量f()完全相同 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f(-10)10 函数名其实就是指向函数的变量, 对于abs()这个函数, 完全可以把函数名abs看成变量, 它指向一个可以计算绝对值的函数! 如果把abs指向其他对象，会有什么情况发生把abs指向10后( abs = 10), 就无法通过abs(-10)调用绝对值函数了, 因为abs这个变量已经不指向求绝对值函数而是指向一个整数10 当然实际代码绝对不能这么写，这里是为了说明函数名也是变量, 要恢复abs函数, 请重启Python交互环境 注: 由于abs函数实际上是定义在 import builtins 模块中的, 所以要让修改abs变量的指向在其它模块也生效, 要用import builtins; builtins.abs = 10 传入函数: 既然变量可以指向函数, 函数的参数能接收变量, 那么一个函数就可以接收另一个函数作为参数, 这种函数就称之为高阶函数 一个最简单的高阶函数：12def add(x, y, f): return f(x) + f(y) 把函数作为参数传入, 这样的函数称为高阶函数, 函数式编程就是指这种高度抽象的编程范式 内建函数map() map()函数接收两个参数 一个是函数 一个是 Iterable map将传入的函数依次作用到序列的每个元素, 并把结果作为新的Iterator返回 举例说明, 比如我们有一个函数f(x)=x2, 要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上, 就可以用map()实现如下: 1234567def f(x): return x * xr = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])print(type(r))print(list(r)) 内建函数reduce() reduce把一个函数作用在一个序列[x1, x2, x3, ...]上, 这个函数必须接收两个参数, reduce把结果继续和序列的下一个元素做累积计算, 其效果就是: 1reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) filtersorted","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"14. 迭代, 生成器","slug":"python/2018-10-12-Python-14","date":"2018-10-12T03:19:31.000Z","updated":"2018-10-12T08:44:10.000Z","comments":true,"path":"2018/10/12/python/2018-10-12-Python-14/","link":"","permalink":"http://blog.renyimin.com/2018/10/12/python/2018-10-12-Python-14/","excerpt":"","text":"迭代 如果给定一个list或tuple, 我们可以通过for循环来遍历这个list或tuple, 这种遍历我们称为迭代(Iteration); 在Python中, 迭代是通过 for ... in 来完成的 Python的for循环抽象程度要高于C的for循环, 因为Python的for循环不仅可以用在list或tuple上, 还可以作用在其他可迭代对象上, 如 dict 和 str 都是可迭代对象, 因此都可以用于for循环 所以, 当我们使用for循环时, 只要作用于一个可迭代对象, for循环就可以正常运行, 而我们不太关心该对象究竟是list还是其他数据类型; 那么, 如何判断一个对象是可迭代对象呢? 方法是通过collections模块的Iterable类型判断123456789from collections import Iterable# str是否可迭代isinstance(&apos;abc&apos;, Iterable)# dict是否可迭代dict = &#123;0: &apos;renyimin&apos;, 1:&apos;rymuscle&apos;, 2:&apos;lant&apos;, &apos;info&apos;:&#123;&apos;address&apos;:&apos;运城&apos;&#125;, 3:[3,6,9]&#125;print(dict)print(isinstance(dict, Iterable)) 生成器 通过 list(range(1, 20)) 或者 列表生成式, 我们可以直接创建一个列表, 但是, 受到内存限制, 列表容量肯定是有限的; 而且, 创建一个包含100万个元素的列表, 不仅占用很大的存储空间, 如果我们仅仅需要访问前面几个元素, 那后面绝大多数元素占用的空间都白白浪费了; 所以, 如果列表元素可以按照某种算法推算出来, 那我们是否可以在循环的过程中不断推算出后续的元素呢? 这样就不必创建完整的list, 从而节省大量的空间; 在Python中, 这种一边循环一边计算的机制, 称为生成器(generator); 要创建一个generator, 有很多种方法, 第一种方法很简单, 只要把一个 列表生成式的[]改成(), 就创建了一个generator 创建 L 和 g 的区别仅在于最外层的[]和(), L是一个list, 而g是一个generator 1234L = [x * x for x in range(10)]print(type(L))g = (x * x for x in range(10))print(type(g)) 结果: 12&lt;class &apos;list&apos;&gt;&lt;class &apos;generator&apos;&gt; 我们可以直接打印出list的每一个元素, 但我们怎么打印出generator的每一个元素呢? 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值： 123456789g = (x * x for x in range(5))print(type(g))print(g)print(next(g))print(next(g))print(next(g))print(next(g))print(next(g))print(next(g)) 结果: 1234567891011&lt;class &apos;generator&apos;&gt;&lt;generator object &lt;genexpr&gt; at 0x105faa9a8&gt;014916Traceback (most recent call last): File &quot;generator.py&quot;, line 27, in &lt;module&gt; print(next(g))StopIteration 注意: generator保存的是算法, 每次调用next(g), 就计算出g的下一个元素的值, 直到计算到最后一个元素, 没有更多的元素时, 抛出StopIteration的错误 当然, 上面这种不断调用next(g)实在是太变态了, 正确的方法是使用for循环, 因为 generator也是可迭代对象: 123456from collections import Iterableg = (x * x for x in range(5))print(type(g))# generator 是否可迭代, 返回Trueprint(isinstance(g, Iterable)) 123g = (x * x for x in range(10))for n in g: print(n) 所以, 我们创建了一个generator后, 基本上永远不会调用next(), 而是通过for循环来迭代它, 这样也不需要你关心StopIteration的错误 定义generator的另一种方法 generator非常强大, 如果推算的算法比较复杂, 用类似 列表生成式 的for循环无法实现的时候, 还可以用函数来实现 比如, 著名的斐波拉契数列(Fibonacci), 除第一个和第二个数外, 任意一个数都可由前两个数相加得到: 1, 1, 2, 3, 5, 8, 13, 21, 34, ... 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 123456789def fib(max): i, a, b = 0, 0, 1 while i &lt; max: print(b) a, b = b, a + b i = i + 1 return &apos;done&apos;fib(5) fib函数实际上是定义了斐波拉契数列的推算规则, 可以从第一个元素开始, 推算出后续任意的元素, 这种逻辑其实非常类似generator 也就是说, 上面的函数和generator仅一步之遥, 要把fib函数变成generator, 只需要把 print(b) 改为 yield b 就可以了: 1234567891011def fib(max): i, a, b = 0, 0, 1 while i &lt; max: yield b a, b = b, a + b i = i + 1 return &apos;done&apos;f = fib(5)print(type(f))print(f) 结果: 12&lt;class &apos;generator&apos;&gt;&lt;generator object fib at 0x10cc309a8&gt; 这就是定义 generator 的另一种方法, 如果一个函数定义中包含yield关键字, 那么这个函数就不再是一个普通函数, 而是一个generator: 这里, 最难理解的就是generator和函数的执行流程不一样 函数是顺序执行，遇到return语句或者最后一行函数语句就返回; 而变成generator的函数, 在每次调用next()的时候执行, 遇到yield语句返回, 再次执行时从上次返回的yield语句处继续执行; 举个简单的例子，定义一个generator，依次返回数字1，3，5: 1234567def odd(): print(&apos;step 1&apos;) yield 1 print(&apos;step 2&apos;) yield(3) print(&apos;step 3&apos;) yield(5) 调用该generator时, 首先要生成一个generator对象, 然后用next()函数不断获得下一个返回值: 12345o = odd()next(o)next(o)next(o)next(o) 结果: 1234567step 1step 2step 3Traceback (most recent call last): File &quot;generator.py&quot;, line 78, in &lt;module&gt; next(o)StopIteration 可以看到, odd不是普通函数, 而是generator, 在执行过程中, 遇到yield就中断, 下次又继续执行; 执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。 回到fib的例子，我们在循环过程中不断调用yield，就会不断中断, 当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来; 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代 12345678910def fib(max): i, a, b = 0, 0, 1 while i &lt; max: yield b a, b = b, a + b i = i + 1 return &apos;done&apos;for n in fib(6): print(n) 但是用for循环调用generator时, 发现拿不到generator的return语句的返回值, 如果想要拿到返回值, 必须捕获StopIteration错误, 返回值包含在StopIteration的value中 12345678910111213141516171819def fib(max): i, a, b = 0, 0, 1 while i &lt; max: yield b a, b = b, a + b i = i + 1 return &apos;done&apos;for n in fib(6): print(n)g = fib(6)while True: try: x = next(g) print(&apos;g:&apos;, x) except StopIteration as e: print(&apos;Generator return value:&apos;, e.value) break 杨辉三角 方法1, 使用了 列表生成式, 生成器 来实现 杨辉三角的生成和输出12345678910111213141516def yanghui(): row = [1] while True: yield(row) row = [1] + [row[k] + row[k + 1] for k in range(len(row) - 1)] + [1]g = yanghui()i = 0while i &lt; 10: try: x = next(g) print(&apos;g:&apos;, x) i = i + 1 except StopIteration as e: print(&apos;Generator return value:&apos;, e.value) break","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"15","slug":"python/2018-10-14-Python-15","date":"2018-10-12T03:19:31.000Z","updated":"2018-10-14T03:43:23.000Z","comments":true,"path":"2018/10/12/python/2018-10-14-Python-15/","link":"","permalink":"http://blog.renyimin.com/2018/10/12/python/2018-10-14-Python-15/","excerpt":"","text":"可以直接作用于for循环的数据类型有以下几种: 一类是集合数据类型: 如 list、tuple、dict、set、str 等; 一类是 generator, 包括生成器 和 带yield的 generator function; 这些可以直接作用于for循环的对象统称为可迭代对象 Iterable可以使用isinstance()判断一个对象是否是Iterable对象 而生成器不但可以作用于for循环, 还可以被next()函数不断调用并返回下一个值, 直到最后抛出StopIteration错误表示无法继续返回下一个值了 可以被next()函数调用并不断返回下一个值的对象称为 迭代器 Iterator 生成器都是Iterator对象, 但list、dict、str虽然是Iterable, 却不是Iterator也可以使用isinstance()判断一个对象是否是Iterator对象 123456&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)False 把 list、dict、str 等 Iterable 变成 Iterator 可以使用 iter() 函数 iter() 1234&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter(&apos;abc&apos;), Iterator)True 为什么list、dict、str等数据类型不是 Iterator ? 这是因为Python的Iterator对象表示的是一个数据流, Iterator对象可以被next()函数调用并不断返回下一个数据, 直到没有数据时抛出StopIteration错误;可以把这个数据流看做是一个有序序列, 但我们却不能提前知道序列的长度, 只能不断通过next()函数实现按需计算下一个数据, 所以Iterator的计算是惰性的, 只有在需要返回下一个数据时它才会计算; Iterator甚至可以表示一个无限大的数据流, 例如全体自然数, 而使用list是永远不可能存储全体自然数的 小结 凡是可作用于for循环的对象都是Iterable类型; 凡是可作用于next()函数的对象都是Iterator类型, 它们表示一个惰性计算的序列; 集合数据类型如list、dict、str等是Iterable但不是Iterator 不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的，例如 12for x in [1, 2, 3, 4, 5]: pass 实际上完全等价于： 12345678910# 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5])# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"13. 切片","slug":"python/2018-10-11-Python-13","date":"2018-10-11T13:41:39.000Z","updated":"2018-10-12T03:19:42.000Z","comments":true,"path":"2018/10/11/python/2018-10-11-Python-13/","link":"","permalink":"http://blog.renyimin.com/2018/10/11/python/2018-10-11-Python-13/","excerpt":"","text":"切片Slice 取一个 list 或 tuple 的部分元素是非常常见的操作; 但是这种经常取指定索引范围的操作, 用循环十分繁琐; 因此，Python提供了切片（Slice）操作符，能大大简化这种操作 可以取单个索引处的元素, 如 L[2], L[-2] 也可以取范围索引的多个元素 L[0:3] 表示从索引0开始取，直到索引3为止，但不包括索引3, 即索引0，1，2，正好是3个元素; L[1:2] 表示从索引1开始取，直到索引2为止，但不包括索引2, 即索引1处的元素, 正好是1个; L[-2:-1] 表示从索引-2开始取，直到索引-1为止，但不包括索引-1 (共1个元素) 如果所取的索引范围是从0开始, 还可以省略 L[:3] 如上面第2点提到, 还支持 负数 来倒着取索引或者索引范围 L[-3:] 表示从倒数第3个元素开始, 到结尾所有元素 L[-3:-1] 表示从倒数第2个元素开始, 到结尾所有元素, 但是注意: 不包含最后一个元素, 即 -1索引处的元素 每n个数, 只取一个 123456L = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]print(L[:3:2])print(L[:4:2])print(L[:5:2])# 所有数, 每隔2个取一个print(L[::2]) 结果: 1234[0, 2][0, 2][0, 2, 4][0, 2, 4, 6, 8] 什么都不写，只写[:]就可以原样复制一个list (tuple也是一种list, 唯一区别是tuple不可变, 因此tuple也可以用切片操作, 只是操作的结果仍是tuple)","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"12. 函数的参数","slug":"python/2018-10-10-Python-12","date":"2018-10-10T10:00:31.000Z","updated":"2018-10-16T07:30:30.000Z","comments":true,"path":"2018/10/10/python/2018-10-10-Python-12/","link":"","permalink":"http://blog.renyimin.com/2018/10/10/python/2018-10-10-Python-12/","excerpt":"","text":"Python的函数定义非常简单, 但灵活度却非常大, 除了正常定义的 必选参数 外, 还可以使用 默认参数, 可变参数 和 关键字参数, 使得函数定义出来的接口, 不但能处理复杂的参数, 还可以简化调用者的代码; 必选参数即, 函数调用时必须要传的参数 默认参数 设置默认参数时, 有几点要注意： 必选参数在前, 默认参数在后, 否则Python的解释器会报错; 当函数有多个参数时, 把变化大的参数放前面, 变化小的参数放后面作为默认参数; 使用默认参数最大的好处是能降低调用函数的难度; 而当需要更复杂的调用时, 也可以传递更多的参数来实现: 有多个默认参数的函数在调用时 既可以按顺序提供参数 也可以不按顺序提供默认参数(甚至不按顺序且只提供部分默认参数), 此时, 需要把参数名写上, 如下: 12345678def info(name, gender, age=6, city=&apos;Beijing&apos;): print(&apos;name:&apos;, name) print(&apos;gender:&apos;, gender) print(&apos;age:&apos;, age) print(&apos;city:&apos;, city)info(&apos;rrc&apos;, &apos;male&apos;, city=&apos;北京&apos;)# 注意: 如果在传递city参数时, 既没有传前面的默认参数age, 又不指定city的参数名, 则city的值将会传递给age 大坑: 默认参数有个大坑, 那就是 默认参数必须指向不变对象 如下正常传参, 结果没什么问题 123456def add_end(L=[]): L.append(&apos;END&apos;) return Lprint(add_end([1, 2, 3]))print(add_end([1, 2, 3])) 但是如果 使用默认参数调用时, 问题就出来了 123456def add_end(L=[]): L.append(&apos;END&apos;) return Lprint(add_end())print(add_end()) 结果如下: 默认参数是[], 但是函数似乎每次都“记住了”上次添加了’END’后的list 123(myEnv_001) renyimindeMacBook-Pro:PythonStudy renyimin$ python keng.py [&apos;END&apos;][&apos;END&apos;, &apos;END&apos;] 上述原因是因为, Python函数在定义的时候, 默认参数 L 的值就被计算出来了, 即 [], 因为默认参数 L 也是一个变量, 它指向对象 [] 每次调用该函数, 如果改变了L的内容, 则下次调用时, 默认参数的内容就变了, 不再是函数定义时的 [] 了, 除非下次调用时显示地通过传参来重新设置L的内容; 因此: 定义默认参数要牢记一点, 默认参数必须指向不变对象; 要修改上面的例子, 我们可以用None这个不变对象来实现: 12345def add_end(L=None): if L is None: L = [] L.append(&apos;END&apos;) return L 现在, 无论调用多少次都不会有问题 为什么要设计 str、None 这样的不变对象呢?? 因为不变对象一旦创建, 对象内部的数据就不能修改, 这样就减少了由于修改数据导致的错误; 此外, 由于对象不变, 多任务环境下同时读取对象不需要加锁, 同时读一点问题都没有, 我们在编写程序时, 如果可以设计一个不变对象，那就尽量设计成不变对象; 可变参数 * 可变参数是指: 传入参数的个数是可变的, 可以是1个、2个到任意个, 还可以是0个 例: 要定义一个 计算一组数中每个数平方的和的函数, 就必须确定输入的参数, 如果参数个数不确定 首先想到的是, 可以把 这组数字 作为一个 list 或 tuple 型的参数传进来, 为此你需要先组装出一个 list 或 tuple; 如果利用可变参数, 则上面那一步将会被省略, 代码就被简化了 要定义可变参数, 仅仅需要在参数前面加一个 * 号: 可变参数在函数内部 是作为 tuple 存在的; 因此, 相比于使用tuple参数, 使用可变参数的函数代码完全不变; 方便的是, 在调用该函数时, 你可以传入任意个参数, 包括0个参数, 而不用在调用函数前再组装一个 list 或 tuple 123456789101112131415161718# tuple参数def func1(param): sum = 0; for v in param: sum = sum + v * v return sumt = (1, 2, 3, 6, 8)print(func1(t))# 可变参数def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sumprint(calc(1, 2, 3, 6, 8)) list或tuple转变为可变参数, 如果已经有一个list或者tuple, 要调用一个参数是 可变参数 的函数 可以这样做: 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(nums[0], nums[1], nums[2])14 上述写法的问题是太繁琐, 所以Python允许你 在list或tuple前面加一个 * 号, 把list或tuple的元素变成可变参数传进去: 123456789t1 = (1, 2, 3, 6, 8)def func2(*param): sum = 0 for v in param: sum = sum + v * v return sum# 看这里print(func2(*t1)) *nums 就表示把nums这个list的所有元素作为可变参数传进去, 这种写法相当有用, 而且很常见; 注意下例: 12345def func2(name, age, *address): print(&apos;name: %s, age: %s, address:%s&apos; % (name, age, address))func2(&apos;renyimin&apos;, 102, *(1,2,3,4,&apos;运城&apos;), &apos;ddd&apos;)func2(&apos;renyimin&apos;, 102, (1,2,3,4,&apos;运城&apos;), &apos;ddd&apos;) 结果: 12name: renyimin, age: 102, address:(1, 2, 3, 4, &apos;运城&apos;, &apos;ddd&apos;)name: renyimin, age: 102, address:((1, 2, 3, 4, &apos;运城&apos;), &apos;ddd&apos;) 关键字参数 ** 可变参数允许你传入0个或任意个参数, 这些可变参数在函数调用时自动组装为一个 tuple; 而关键字参数允许你传入0个或任意个含参数名的参数, 这些关键字参数在函数内部自动组装为一个 dict; 示例: 函数person除了必选参数name和age外, 还接受关键字参数kw12def person(name, age, **kw): print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw) 函数的调用 可以只传入必选参数: 12&gt;&gt;&gt; person(&apos;Michael&apos;, 30)name: Michael age: 30 other: &#123;&#125; 也 可以传入任意个数的关键字参数: 1234&gt;&gt;&gt; person(&apos;Bob&apos;, 35, city=&apos;Beijing&apos;)name: Bob age: 35 other: &#123;&apos;city&apos;: &apos;Beijing&apos;&#125;&gt;&gt;&gt; person(&apos;Adam&apos;, 45, gender=&apos;M&apos;, job=&apos;Engineer&apos;)name: Adam age: 45 other: &#123;&apos;gender&apos;: &apos;M&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; 和 可变参数 类似, 也可以先组装出一个 dict, 然后把该 dict 转换为 关键字参数 传进去(只不过这里用的是 **): 123&gt;&gt;&gt; extra = &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125;&gt;&gt;&gt; person(&apos;Jack&apos;, 24, city=extra[&apos;city&apos;], job=extra[&apos;job&apos;])name: Jack age: 24 other: &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; 上面复杂的调用可以简化为: 123&gt;&gt;&gt; extra = &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125;&gt;&gt;&gt; person(&apos;Jack&apos;, 24, **extra)name: Jack age: 24 other: &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; **extra 表示把 extra这个dict的所有key-value用关键字参数传入到函数的 **kw 参数, kw将获得一个dict, 注意kw获得的dict是extra的一份拷贝, 对kw的改动不会影响到函数外的extra; 命名关键字参数 *, 对于关键字参数, 函数的调用者可以传入任意不受限制的关键字参数, 至于到底传入了哪些, 就需要在函数内部进行检查 (可以用 in 检查参数是否在 关键字参数 中) 仍以person()函数为例, 我们希望检查是否有city和job参数: 12345678def person(name, age, **kw): if &apos;city&apos; in kw: # 有city参数 pass if &apos;job&apos; in kw: # 有job参数 pass print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw) 但上面的函数, 调用者仍可以传入不受限制的关键字参数, 如果要限制关键字参数的名字, 就可以用 命名关键字参数, 例如, 只接收 city 和 job 作为关键字参数, 这种方式定义的函数如下: 123456 def person(name, age, *, city, job): print(name, age, city, job) ``` 和关键字参数 `**kw` 不同, 命名关键字参数需要一个特殊分隔符 `*`, *后面的参数被视为 命名关键字参数 4. 调用方式如下: person(‘Jack’, 24, city=’Beijing’, job=’Engineer’) Jack 24 Beijing Engineer 125. 如果函数定义中已经有了一个可变参数, 后面跟着的命名关键字参数就不再需要特殊分隔符 `*,` 了: def person(name, age, *args, city, job): print(name, age, args, city, job) 1- 命名关键字参数必须传入参数名, 如果没有传入参数名, 否则调用将报错: person(‘Jack’, 24, ‘Beijing’, ‘Engineer’) Traceback (most recent call last): File ““, line 1, in TypeError: person() takes 2 positional arguments but 4 were given 123 - 由于调用时缺少参数名city和job, Python解释器把这4个参数均视为位置参数, 但person()函数仅接受2个位置参数6. 命名关键字参数 也可以有默认值, 从而简化调用: def person(name, age, *, city=’Beijing’, job): print(name, age, city, job) 1- 由于命名关键字参数city具有默认值, 调用时可不传入city参数: person(‘Jack’, 24, job=’Engineer’) Jack 24 Beijing Engineer 12- 使用命名关键字参数时, 要特别注意, 如果没有可变参数, 就必须加一个 `*` 作为特殊分隔符如果缺少`*`, **Python解释器将无法识别位置参数和命名关键字参数** def person(name, age, city, job): # 缺少 *，city和job被视为位置参数 pass 123456## 参数组合1. 在Python中定义函数, 必选参数、默认参数、可变参数、关键字参数 和 命名关键字参数, 这5种参数都可以组合使用; 但是需要注意: **参数定义的顺序必须是 必选参数、默认参数、可变参数、命名关键字参数 和 关键字参数**2. 比如定义一个函数，包含上述若干种参数： def f1(a, b, c=0, args, *kw): print(&#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;args =&#39;, args, &#39;kw =&#39;, kw) 1 def f2(a, b, c=0, , d, *kw): print(&#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;d =&#39;, d, &#39;kw =&#39;, kw) 1在函数调用的时候, Python解释器自动按照参数位置和参数名把对应的参数传进去 f1(1, 2) a = 1 b = 2 c = 0 args = () kw = {}f1(1, 2, c=3) a = 1 b = 2 c = 3 args = () kw = {}f1(1, 2, 3, ‘a’, ‘b’) a = 1 b = 2 c = 3 args = (‘a’, ‘b’) kw = {}f1(1, 2, 3, ‘a’, ‘b’, x=99) a = 1 b = 2 c = 3 args = (‘a’, ‘b’) kw = {‘x’: 99}f2(1, 2, d=99, ext=None) a = 1 b = 2 c = 0 d = 99 kw = {‘ext’: None} 1最神奇的是通过一个 tuple 和 dict, 你也可以调用上述函数： args = (1, 2, 3, 4)kw = {‘d’: 99, ‘x’: ‘#’}f1(args, **kw) a = 1 b = 2 c = 3 args = (4,) kw = {‘d’: 99, ‘x’: ‘#’}args = (1, 2, 3)kw = {‘d’: 88, ‘x’: ‘#’}f2(args, **kw) a = 1 b = 2 c = 3 d = 88 kw = {‘x’: ‘#’} 123456789101112131415 所以, 对于任意函数, 都可以通过类似 func(*args, **kw) 的形式调用它, 无论它的参数是如何定义的3. 虽然可以组合多达5种参数, 但不要同时使用太多的组合, 否则函数接口的可理解性很差## 小结1. 默认参数一定要用不可变对象, 如果是可变对象, 程序运行时会有逻辑错误! 2. 要注意定义可变参数和关键字参数的语法: - `*args` 是可变参数, args接收的是一个 `tuple` - `**kw` 是关键字参数, kw接收的是一个 `dict`## 递归1. 使用递归函数的优点是逻辑简单清晰, 缺点是 **过深的调用会导致栈溢出**; 使用递归函数需要注意防止栈溢出: 在计算机中, 函数调用是通过栈(stack)这种数据结构实现的, 每当进入一个函数调用, 栈就会加一层栈帧, 每当函数返回, 栈就会减一层栈帧; 由于栈的大小不是无限的, 所以, 递归调用的次数过多, 会导致栈溢出2. 比如求阶乘的过程, 递归过程可以展示如下 ===&gt; fact(5) ===&gt; 5 fact(4) ===&gt; 5 (4 fact(3)) ===&gt; 5 (4 (3 fact(2))) ===&gt; 5 (4 (3 (2 fact(1)))) ===&gt; 栈层的递减 ===&gt; 5 (4 (3 (2 1))) ===&gt; 5 (4 (3 2)) ===&gt; 5 (4 6) ===&gt; 5 24 ===&gt; 120 12 3. **递归 vs 循环** 求阶乘: 貌似循环看起来更省资源, 但是递归确实逻辑清晰 def jiecheng(n): if n == 1 : return 1 return n * jiecheng(n-1) print(jiecheng(5)) t = range(1, 6) def forJieCheng(t): res = 1 for v in t: res = res * v return res print(forJieCheng(t)) ```","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"11. 函数","slug":"python/2018-10-10-Python-11","date":"2018-10-10T09:52:45.000Z","updated":"2018-10-10T10:00:14.000Z","comments":true,"path":"2018/10/10/python/2018-10-10-Python-11/","link":"","permalink":"http://blog.renyimin.com/2018/10/10/python/2018-10-10-Python-11/","excerpt":"","text":"函数 抽象是数学中非常常见的概念, 如计算数列的和, 比如：1 + 2 + 3 + … + 100, 写起来十分不方便, 于是数学家发明了求和符号 ∑, 这种抽象记法非常强大，因为我们看到 ∑ 就可以理解成求和，而不是还原成低级的加法运算; 而函数就是最基本的一种代码抽象的方式; 在Python中，定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用return语句返回; 测试 1234567def my_abs(param): if param &gt; 0: return param else: return -paramprint(my_abs(-5.6)) # 结果为5.6 如果没有return语句，函数执行完毕后也会返回结果，只是结果为None, return None可以简写为return 1234567def my_abs(param): if param &gt; 0: return param else: returnprint(my_abs(-5.6))# 结果为 None 如果你已经把 my_abs() 的函数定义保存为abstest.py文件了, 那么, 可以在该文件的当前目录下启动Python解释器, 用 from abstest import my_abs 来导入my_abs()函数, 注意abstest是文件名（不含.py扩展名） 1234# 可以理解为从abstest.py文件中导入abs函数from abstest import my_absmy_abs(-9)# 运行后结果也是正确的9 空函数 如果想定义一个什么事也不做的空函数，可以用pass语句： 12def nop(): pass pass语句什么都不做, 那有什么用? 实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass, 让代码能运行起来; (因为如果不写pass, 空函数运行就会有语法错误) pass还可以用在其他语句里，比如：12if age &gt;= 18: pass 参数检查 调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出TypeError; 但是如果参数类型不对，Python解释器就无法帮我们检查; 数据类型检查可以用内置函数 isinstance() 实现 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&apos;bad operand type&apos;) if x &gt;= 0: return x else: return -x 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个错误 函数返回多个值 函数返回多个值其实只是一种假象, Python函数返回的仍然是单一值, 由于这个值是个 tuple, 而在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值 12345678910import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, nyx, y = move(100, 100, 60, math.pi / 6)print(x, y)# 返回 &lt;class &apos;tuple&apos;&gt;print(type(move(100, 100, 60, math.pi / 6))) 小结 定义函数时，需要确定函数名和参数个数； 如果有必要，可以先对参数的数据类型做检查； 函数体内部可以用return随时返回函数结果； 函数执行完毕也没有return语句时，自动return None； 函数可以同时返回多个值，但其实就是一个tuple；","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"10. 条件判断, 循环, 列表生成式","slug":"python/2018-10-10-Python-10","date":"2018-10-10T02:54:35.000Z","updated":"2018-10-12T06:28:44.000Z","comments":true,"path":"2018/10/10/python/2018-10-10-Python-10/","link":"","permalink":"http://blog.renyimin.com/2018/10/10/python/2018-10-10-Python-10/","excerpt":"","text":"https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431675624710bb20e9734ef343bbb4bd64bcd37d4b52000https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431676242561226b32a9ec624505bb8f723d0027b3e7000 for in 可用于 list, tuple, dict 1234567classmates = (&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;, [&apos;你好&apos;, &apos;大家好&apos;])for v in classmates: print(v)list = [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;, [1,2,3,4]]for v in list: print(v) 123456789(myEnv_001) renyimindeMacBook-Pro:PythonStudy renyimin$ python for-tuple.py MichaelBobTracy[&apos;你好&apos;, &apos;大家好&apos;]MichaelBobTracy[1, 2, 3, 4] 列表生成式 列表生成式(List Comprehensions), 是Python内置的非常简单却强大的可以用来创建list的生成式 运用列表生成式, 可以写出非常简洁的代码, 如 要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11)); 但是如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做? 方法一是循环 12345L = list(range(1,11))res = []for v in L: res.append(v*v)print(res) 可以看到循环仍然比较繁琐, 而列表生成式则可以用一行语句代替循环生成上面的list写列表生成式时, 把要生成的元素x * x放到前面, 后面跟for循环, 就可以把list创建出来 1print([x * x for x in range(1, 11)]) 列表生成式的for循环后面还可以加上if判断, 如下仅筛选出偶数的平方 1[x * x for x in range(1, 11) if x % 2 == 0] 列表生成式的for循环还可以是多层的(不过一般三层和三层以上的就很少用到了) 1print([m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;]) 运用列表生成式，可以写出非常简洁的代码, 再如, 列出当前目录下的所有文件和目录名, 可以通过一行代码实现 123# 导入os模块，模块的概念后面讲到import osprint([d for d in os.listdir(&apos;.&apos;)])","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"09. dict 和 set","slug":"python/2018-10-10-Python-09","date":"2018-10-10T02:53:33.000Z","updated":"2018-10-12T06:32:20.000Z","comments":true,"path":"2018/10/10/python/2018-10-10-Python-09/","link":"","permalink":"http://blog.renyimin.com/2018/10/10/python/2018-10-10-Python-09/","excerpt":"","text":"dict {} Python内置了字典 dict 的支持, dict全称dictionary, 在其他语言中也称为map, 使用键-值(key-value)存储, 具有极快的查找速度; 举个例子, 假设要根据同学的名字查找对应的成绩, 如果用list实现, 需要两个list: 12names = [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;]scores = [95, 75, 85] 给定一个名字，要查找对应的成绩，就先要在names中找到对应的位置，再从scores取出对应的成绩, list越长耗时越长 而如果用dict实现, 只需要一个 “名字”-“成绩” 的对照表, 直接根据名字查找成绩, 无论这个表有多大, 查找速度都不会变慢, 用Python写一个dict如下： 123&gt;&gt;&gt; d = &#123;&apos;Michael&apos;: 95, &apos;Bob&apos;: 75, &apos;Tracy&apos;: 85&#125;&gt;&gt;&gt; d[&apos;Michael&apos;]95 为什么dict查找速度这么快? 因为dict的实现原理和查字典是一样的, 假设字典包含了1万个汉字, 我们要查某一个字, 一个办法是把字典从第一页往后翻, 直到找到我们想要的字为止, 这种方法就是在list中查找元素的方法, list越大, 查找越慢; 第二种方法是先在字典的索引表里(比如部首表)查这个字对应的页码, 然后直接翻到该页, 找到这个字;无论找哪个字, 这种查找速度都非常快, 不会随着字典大小的增加而变慢dict就是第二种实现方式，给定一个名字，比如’Michael’，dict在内部就可以直接计算出Michael对应的存放成绩的“页码”，也就是95这个数字存放的内存地址，直接取出来，所以速度非常快。这种key-value存储方式，在放进去的时候，必须根据key算出value的存放位置，这样，取的时候才能根据key直接拿到value。 把数据放入dict的方法，除了初始化时指定外，还可以通过key放入： 123&gt;&gt;&gt; d[&apos;Adam&apos;] = 67&gt;&gt;&gt; d[&apos;Adam&apos;]67 如果获取的key不存在，dict就会报错： 要避免key不存在的错误，有两种办法，一是通过in判断key是否存在： 12&gt;&gt;&gt; &apos;Thomas&apos; in dFalse 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get(&apos;Thomas&apos;)&gt;&gt;&gt; d.get(&apos;Thomas&apos;, -1)-1 注意：返回None的时候Python的交互环境不显示结果。 要删除一个key，用pop(key)方法，对应的value也会从dict中删除； 和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反：查找和插入的时间随着元素的增加而增加；占用空间小，浪费内存很少。所以，dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。 要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key 12345&gt;&gt;&gt; key = [1, 2, 3]&gt;&gt;&gt; d[key] = &apos;a list&apos;Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: unhashable type: &apos;list&apos; dict 也可以使用 for … in 来进行迭代 因为dict的存储不是按照list的方式顺序排列, 所以, 迭代出的结果顺序很可能不一样 默认情况下, dict迭代的是key; 如果要迭代value, 可以用 for value in d.values(); 如果要同时迭代key和value, 可以用 for k, v in d.items()1234dict = &#123;0:&quot;renyimin&quot;, &quot;age&quot;:500&#125;# 注意: 下面不能写成 for k, v in dict:for k, v in dict.items(): print(&apos;k=&gt;v : %s =&gt; %s&apos; % (k, v)) 问题 顺序存储, 链式存储, hash : https://yq.aliyun.com/ziliao/430573 set set([]) set和dict类似, 不过它只是一组key的集合, 不存储value; 另外, 它的 key不重复; 要创建一个set, 需要提供一个list作为输入集合: 123&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; 注意, 传入的参数 [1, 2, 3] 是一个list, 而显示的 {1, 2, 3} 只是告诉你这个set内部有1，2，3这3个元素，显示的顺序也不表示set是有序的; 重复元素在set中自动被过滤 通过 add(key) 方法可以添加元素到set中，可以重复添加，但不会有效果; remove(key) 方法可以删除元素 set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作 123456&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2&#123;2, 3&#125;&gt;&gt;&gt; s1 | s2&#123;1, 2, 3, 4&#125; set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”; set和dict的唯一区别仅在于没有存储对应的value; 如果把list放入set, 会报错12345&gt;&gt;&gt; s = set([1,2,3,4,2,3,4,[57,8]])Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: unhashable type: &apos;list&apos;&gt;&gt;&gt; 对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。 123456789101112# str是不变对象&gt;&gt;&gt; a = &apos;abc&apos;&gt;&gt;&gt; a.replace(&apos;a&apos;, &apos;A&apos;)&apos;Abc&apos;&gt;&gt;&gt; a&apos;abc&apos;# 而list是可变对象&gt;&gt;&gt; b = [&apos;c&apos;, &apos;b&apos;, &apos;a&apos;]&gt;&gt;&gt; b.sort()&gt;&gt;&gt; b[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"08. list 和 tuple","slug":"python/2018-10-09-Python-08","date":"2018-10-09T14:06:23.000Z","updated":"2018-10-10T08:32:48.000Z","comments":true,"path":"2018/10/09/python/2018-10-09-Python-08/","link":"","permalink":"http://blog.renyimin.com/2018/10/09/python/2018-10-09-Python-08/","excerpt":"","text":"list [] Python内置的有序列表list, 可以随时添加和删除其中的元素 1234&gt;&gt;&gt; classmates = [&apos;小明&apos;, &apos;小强&apos;, &apos;小张&apos;, &apos;小明&apos;]&gt;&gt;&gt; classmates[&apos;小明&apos;, &apos;小强&apos;, &apos;小张&apos;, &apos;小明&apos;]&gt;&gt;&gt; 用 len() 函数可以获得list元素的个数 123&gt;&gt;&gt; len(classmates)4&gt;&gt;&gt; 可以用索引来访问list中每一个位置的元素(索引是从0开始的), 当索引超出了范围时, Python会报一个IndexError错误, 所以, 要确保索引不要越界, 最后一个元素的索引是len(classmates) - 1; 还可以用负数做索引, 比如-1做索引, 会直接获取最后一个元素; 当然, -5 就越界了 list是一个可变的有序表 可以往list中追加元素到末尾 classmates.append(&#39;小王&#39;) 也可以把元素插入到指定的位置, 比如在索引号为1的位置插入: classmates.insert(1, &#39;Jack&#39;) 要删除list末尾的元素, 用 pop() 方法 要删除指定位置的元素, 用 pop(i) 方法, 其中i是索引位置 list里面的元素的数据类型可以不同, 还可以是另一个list 123&gt;&gt;&gt; s = [&apos;python&apos;, &apos;java&apos;, [&apos;asp&apos;, &apos;php&apos;], &apos;scheme&apos;]&gt;&gt;&gt; len(s)4 要注意s只有4个元素, 其中 s[2] 又是一个list，如果拆开写就更容易理解了：12&gt;&gt;&gt; p = [&apos;asp&apos;, &apos;php&apos;]&gt;&gt;&gt; s = [&apos;python&apos;, &apos;java&apos;, p, &apos;scheme&apos;] 要拿到’php’可以写 p[1] 或者 s[2][1]，因此s可以看成是一个二维数组，类似的还有三维、四维……数组，不过很少用到 tuple () 另一种有序列表叫元组: tuple, tuple和list非常类似, 但是tuple一旦初始化就不能修改, 比如同样是列出同学的名字 1&gt;&gt;&gt; classmates = (&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;) 现在, classmates这个tuple不能变了, 它也没有append(), insert()这样的方法, 其他获取元素的方法和list是一样的, 你可以正常地使用classmates[0], classmates[-1]; 但不能赋值成另外的元素 因为tuple不可变, 所以代码更安全, 如果可能, 能用 tuple 代替 list 就尽量用tuple; tuple 的坑: 当你定义一个tuple时，在定义的时候，tuple的元素就必须被确定下来，比如： 123&gt;&gt;&gt; t = (1, 2)&gt;&gt;&gt; t(1, 2) 如果要定义一个空的tuple，可以写成(): 123&gt;&gt;&gt; t=tuple()&gt;&gt;&gt; t() 但是，如果要定义一个只有1个元素的tuple，如果你这么定义 123&gt;&gt;&gt; t = (1)&gt;&gt;&gt; t1 定义的不是tuple, 而是1这个数!! 因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。 所以, 如果只有1个元素的tuple, 定义时必须加一个逗号,，来消除歧义 123&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; t(1,) Python在显示只有1个元素的tuple时，也会加一个逗号,，以免你误解成数学计算意义上的括号 最后来看一个 可变的 tuple: tuple所谓的“不变”是说，tuple的每个元素，指向永远不变, 即指向’a’, 就不能改成指向’b’，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的; 另外, 即便是tuple中有变量, tuple也是拿到变量的具体的值(这个值需要是不变的) 例1 123456789101112# tuple的每个元素，指向永远不变&gt;&gt;&gt; a=1&gt;&gt;&gt; b=2&gt;&gt;&gt; t=(a, b, 3)&gt;&gt;&gt; t(1, 2, 3)# 改变a,b后, 发现tuple并没有发生变化, 可以理解为 tuple在创建时, 在拿到了 a, b 变量的值之后就 和 a, b 变量没关系了; 再次改变a,b的指向, 只是a,b变量本身的问题, 和tuple中的前两个元素就没关系了&gt;&gt;&gt; a=10&gt;&gt;&gt; b=20&gt;&gt;&gt; t(1, 2, 3)&gt;&gt;&gt; 再如下例子 tuple中 t[0]的地址和 值100在内存中的地址一样; 当a 重新指向 值1000在内存中的地址后, a 的地址变了, 但是 t[0] 的地址并没有变化 12345678910111213141516&gt;&gt;&gt; a = 100&gt;&gt;&gt; id(a)4437875824&gt;&gt;&gt; t = (a, 200, 300)&gt;&gt;&gt; t(100, 200, 300)# t[0]的地址和 值100在内存中的地址一样&gt;&gt;&gt; id(t[0])4437875824# 当a 重新指向 值1000在内存中的地址后, a 的地址变了, 但是 t[0] 的地址并没有变化&gt;&gt;&gt; a = 1000&gt;&gt;&gt; id(a)4439732016&gt;&gt;&gt; id(t[0])4437875824 扩展: 两个变量 a, b都是2.0时, 你会发现它们的地址不一样, 说明系统为a,b分配了不同的内存空间; 但是, 当a,b都是2时, 0它们的地址是一样的，怎么回事呢? 这是因为，为了提高内存空间的利用效率，对于一些比较小的整型变量(int)使用了相同的内存空间。如果数值比较大，地址就不一样了。 12345678910111213&gt;&gt;&gt; a=2.0&gt;&gt;&gt; b=2.0&gt;&gt;&gt; id(a)4438892784&gt;&gt;&gt; id(b)4438893024&gt;&gt;&gt; a=2&gt;&gt;&gt; b=2&gt;&gt;&gt; id(a)4437872688&gt;&gt;&gt; id(b)4437872688&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"07. 字符串相关","slug":"python/2018-10-09-Python-07","date":"2018-10-09T09:36:27.000Z","updated":"2018-10-17T11:37:59.000Z","comments":true,"path":"2018/10/09/python/2018-10-09-Python-07/","link":"","permalink":"http://blog.renyimin.com/2018/10/09/python/2018-10-09-Python-07/","excerpt":"","text":"概述 Python 不支持单字符类型, 单字符在 Python 中也是作为一个字符串使用, 也就是字符和字符串都一样, 都是 单引号或双引号包起来即可; Python 访问子字符串 可以使用 [索引] 来获取字符串 也可以使用 切片Slice (str[0:3], str[-3:-2]) 来截取字符串 (切片遵循左闭右开原则, 即左边包含,右边不包含) 字符串运算符 拼接用 + (注意: 字符串不能 + 其他类型如 &#39;python&#39;+10 会报错) 重复输出字符串用 * : 12&gt;&gt;&gt; &quot;Python&quot;*3&apos;PythonPythonPython&apos; in 成员运算符 - 如果字符串中包含给定的字符返回 True not in 成员运算符 - 如果字符串中不包含给定的字符返回 True % 格式化字符串 三引号(&#39;&#39;&#39;...&#39;&#39;&#39;)让程序员从引号和特殊字符串的泥潭里面解脱出来, 在三引号中的内容会所见即所得地输出, 允许字符串跨多行, 里面的单引号, 双引号, 换行符都不用转义; 字符串内建函数 title() : 返回”标题化”的字符串, 即 所有单词都是以大写开始, 其余字母均为小写 12&gt;&gt;&gt; &apos;hELlo world&apos;.title()&apos;Hello World&apos; istitle() : 如果字符串是标题化的则返回 True, 否则返回 False 1234&gt;&gt;&gt; &apos;Hello World&apos;.istitle()True&gt;&gt;&gt; &apos;Hello world&apos;.istitle()False swapcase() : 将字符串中大写转小写, 小写转大写 (汉字自然就不变) upper() : 将字符串中所有小写字母转换成大写 lower() : 将字符串中所有大写字母转换成小写 capitalize() :将字符串的第一个字符转换为大写 count(str, beg= 0,end=len(string)) : 返回 str 在 string 里面出现的次数, 如果 beg 或者 end 指定, 则返回指定范围内 str 出现的次数 1234&gt;&gt;&gt; &quot;renyimin&quot;.count(&apos;n&apos;, 2)2&gt;&gt;&gt; &quot;renyimin&quot;.count(&apos;n&apos;, 2, -1)1 startswith(str, beg=0,end=len(string)) : 检查字符串是否是以 obj 开头, 是则返回 True, 否则返回 False, 如果beg 和 end 指定值, 则在指定范围内检查 split(str=&quot;&quot;, num) : 以 str 为分隔符截取字符串, 如果 num 有指定值, 则仅截取 num 个子字符串 …… % 格式化str 在Python中, 采用的格式化方式和C语言是一致的, 用 % 实现, 举例如下: 12&gt;&gt;&gt; &apos;Hello, %s&apos; % &apos;world&apos;&apos;Hello, %s&apos; % &apos;world&apos; % 运算符就是用来格式化字符串的 在字符串内部, %s 表示用字符串替换, %d 表示用整数替换, 有几个占位符, 后面就跟几个变量或者值, 顺序要对应好, 多个变量或者值用括号括起来; 如果只有一个占位符, 则后面变量或值的括号可以省略; 占位符有多种(%d, %f, %s, %x), 如果你不太确定应该用什么占位符, %s 永远起作用, 它会把任何数据类型转换为字符串; format()格式化字符串 它会用传入的参数依次替换字符串内的占位符{0}、{1} ……, 不过这种方式写起来比%要麻烦得多 12&gt;&gt;&gt; &apos;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&apos;.format(&apos;小明&apos;, 17.125)&apos;Hello, 小明, 成绩提升了 17.1%&apos; str 与 bytes转换 对于单个字符的编码, Python提供了 ord() 函数获取字符的整数表示, chr() 函数把编码转换为对应的字符; 如果知道字符的十六进制编码, 还可以如下: 12&gt;&gt;&gt; &apos;\\u4e2d\\u6587&apos;&apos;中文&apos; 由于Python的字符串类型是str, 在内存中以Unicode表示, 一个字符对应若干个字节; 如果要在网络上传输或者保存到磁盘上, 就需要把str变为以字节为单位的bytes: Python对 bytes类型 的数据用带 b前缀 的单引号或双引号表示 以Unicode表示的str通过encode()方法可以编码为指定的bytes纯英文的str可以用ASCII编码为bytes, 内容是一样的;含有中文的str可以用UTF-8编码为bytes;含有中文的str无法用ASCII编码, 因为中文编码的范围超过了ASCII编码的范围, Python会报错; 反过来, 如果我们从网络或磁盘上读取了字节流, 那么读到的数据就是bytes, 要把bytes变为str, 就需要用decode()方法 1234&gt;&gt;&gt; b&apos;ABC&apos;.decode(&apos;ascii&apos;)&apos;ABC&apos;&gt;&gt;&gt; b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos;.decode(&apos;utf-8&apos;)&apos;中文&apos; 要计算str包含多少个字符, 可以用 len() 函数; len()函数计算的是str的字符数, 如果换成bytes, len()函数就计算字节数 123456789101112&gt;&gt;&gt; len(&apos;ABC&apos;)3&gt;&gt;&gt; len(&apos;中文&apos;)2// 可见, 1个中文字符经过UTF-8编码后通常会占用3个字节, 而1个英文字符只占用1个字节&gt;&gt;&gt; len(b&apos;ABC&apos;)3&gt;&gt;&gt; len(b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos;)6&gt;&gt;&gt; len(&apos;中文&apos;.encode(&apos;utf-8&apos;))6 在操作字符串时, 经常遇到str和bytes的互相转换, 为了避免乱码问题, 应当始终坚持使用UTF-8编码对str和bytes进行转换; 当str和bytes使用 encode(), decode() 互相转换时, 需要指定编码; 最常用的编码是UTF-8; Python当然也支持其他编码方式, 比如把Unicode编码成GB2312; 但这纯属自找麻烦, 如果没有特殊业务要求, 请牢记仅使用UTF-8编码;","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"05. 基本数据类型简介","slug":"python/2018-10-08-Python-05","date":"2018-10-08T12:36:23.000Z","updated":"2018-10-16T09:47:43.000Z","comments":true,"path":"2018/10/08/python/2018-10-08-Python-05/","link":"","permalink":"http://blog.renyimin.com/2018/10/08/python/2018-10-08-Python-05/","excerpt":"","text":"概述 Python3 六个标准数据类型 Number(数字) String(字符串) List(列表) Tuple(元组) Set(集合) Dictionary(字典) 注意: 不可变数据(3 个): Number、String、Tuple 可变数据(3 个): List、Dictionary、Set 内置的 type() 函数可以用来查询变量所指的对象类型 Number(数字)Python3 支持 int、float、bool、complex(复数) 这几种数字类型; int 注意: 在Python3里, 整型只有 int, 并且是长整型(没有 python2 中的 Long) 适合大数据运算, 不会溢出, 也不会有其他语言那样还分 短整型, 整型, 长整型; python中的整数理论上支持的大数是无限位的, 对于超大整数运算是直接支持的, 没有大小限制(只要内存足够大); 而某些语言的整数根据其存储长度是有大小限制的, 例如Java对int型的范围限制在 $-2^{31}$(-2147483648) 到 $2^{31}-1$(2147483647) 获取 python 支持的最大整数 (但实际上可以支持更大整数的运算, 可参考 https://bbs.csdn.net/wap/topics/310241283, https://my.oschina.net/748/blog/2221779)123import sysmax = sys.maxsizeprint (max) 注意: python 对较小的整型变量(int)使用了相同的内存空间, 但是如果数值比较大, 地址就不一样了 参考: https://blog.csdn.net/WSBruce/article/details/79234389 当 a,b 的值都是2.0时, 它们的地址不一样, 说明系统为a,b分配了不同的内存空间; 但是当 a,b 的值都是2时, 它们的地址是一样的, 这是因为, 为了提高内存空间的利用效率, 对于一些比较小的整型变量(int)使用了相同的内存空间, 如果数值比较大, 地址就不一样了 float Python的浮点数也没有大小限制, 但是超出一定范围就直接表示为inf（无限大） round问题 如果只有一个参数: round(num), 返回最靠近num的整数, 如果num出现.5, 此时两边的距离都一样, round()取偶数 123456&gt;&gt;&gt; round(3.5)4&gt;&gt;&gt; round(4.5)4&gt;&gt;&gt; round(4.51)5 第二个参数是保留的小数位数 在操作 浮点数 时, 除非对精确度没什么要求, 否则尽量避开用round()函数 ceil: math模块的 math.ceil(x), 取大于等于x的最小整数 floor: math模块的 math.floor(x), 取小于等于x的最大整数 浮点数精度要求如果很高的话，请用 decimal 模块 bool python 中布尔值使用常量 True 和 False来表示, 注意大小写 注意: python中, bool是int的子类(继承int), 故 True==1 False==0 是会返回Ture的; 如要切实判断用 xxx is True 1234print(1 == True)print(0 == False)print(True is True)print(False is False) 由于bool是int, 可进行数字计算 print(True+True) True False 判定 以下会被判定为 False:NoneFalsezero of any numeric type, for example, 0, 0.0, 0jany empty sequence, for example, &#39;&#39;, (), []any empty mapping, for example, {}instances of user-defined classes, if the class defines a __bool__() or __len__() method, when that method returns the integer zero or bool value False 除了以上的, 其他的表达式均会被判定为 True (这个需要注意, 与其他的语言有比较大的不同)(注意, 字符串的’0’,”0.0” 可不包含在False的范畴中) inf Python中可以用如下方式表示正负无穷 12float(&quot;inf&quot;)float(&quot;-inf&quot;) inf 运算 inf 做简单加、乘算术运算仍会得到 inf inf 乘以0会得到 NaN (not-a-number) inf 外的其他数 除以inf, 会得到0 inf不等式 所有数都比-inf大 所有数都比+inf小 inf等式: +inf 和 +inf相等, -inf 和 -inf相等 1234&gt;&gt;&gt; print(infinity&lt;-infinity)False&gt;&gt;&gt; print(infinity&gt;-infinity)True 待补充知识 不同语言的 int, long 整数类型范围为何不同?? 有的还和操作系统有关 ?? 为什么Python中整型不会溢出 ?? 大整数加减乘除算法专题 ??","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"04. 基础语法","slug":"python/2018-09-30-Python-04","date":"2018-09-30T07:38:57.000Z","updated":"2018-10-17T11:44:36.000Z","comments":true,"path":"2018/09/30/python/2018-09-30-Python-04/","link":"","permalink":"http://blog.renyimin.com/2018/09/30/python/2018-09-30-Python-04/","excerpt":"","text":"两种执行模式 命令行模式 可以执行 python XXX.py 运行一个.py文件 可以执行 python 进入Python交互模式 Python交互模式 进入到Python交互模式, 它的提示符是 &gt;&gt;&gt; 在Python交互模式下输入 exit() 并回车, 就退出了Python交互模式, 并回到命令行模式 (也可以 Ctrl+d) 两种模式小区别: 进入python交互模式, 相当于启动了Python解释器, 等待你一行一行地输入源代码, 每输入一行就执行一行; 直接运行 .py文件 相当于启动了Python解释器, 然后一次性把 .py文件的源代码 给执行了; python源码文件头: 当Python解释器读取源代码时, 为了让它按UTF-8编码读取, 我们通常在文件开头写上如下两行: 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统, 这是一个Python可执行程序, Windows系统会忽略这个注释;第二行注释是为了告诉Python解释器, 按照UTF-8编码读取源代码, 否则, 你在源代码中写的中文输出可能会有乱码; 可以为你的pycharm编辑器设置文件头部模板 1234567#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : $&#123;DATE&#125; $&#123;TIME&#125;# @Author : Ryu# @Site : $&#123;SITE&#125;# @File : $&#123;NAME&#125;.py# @Software: $&#123;PRODUCT_NAME&#125; IO 输出 print() 用 print() 在括号中加上字符串, 就可以向屏幕上输出指定的文字, 比如输出’hello, world’: &gt;&gt;&gt; print(&#39;hello, world&#39;) print() 函数也可以接受多个字符串, 用逗号 , 隔开, 就可以连成一串输出 (用逗号隔开时, 遇到逗号 , 会输出一个 空格) 输入 input() 基本语法 对编码格式要求比较严格, 采用缩进方式(推荐使用4个空格缩进); 如果随意缩进, 是会报错的; 大小写敏感 单行注释: 以 # 开头, 并且需要确保 注释与其下方的代码处于同一缩进级别; 多行注释: 3个单引号 &#39;&#39;&#39; ... &#39;&#39;&#39;, 或者 3个双引号 &quot;&quot;&quot; ... &quot;&quot;&quot; 当语句以冒号 : 结尾时, 缩进的语句视为代码块 PyCharm忽略警告信息 使用 PyCharm 编写Python代码非常高效, 但有一点体验不太好, 就是代码编写时要按照 PEP8 代码风格编写, 不然会有波浪线的警告信息, 如 PEP 8: expected 2 blank lines, found 0 : 解决方法: 方法一: 将鼠标移到提示的地方, 按 alt+Enter, 选择忽略(Ignore)这个错误即可 (会自动像方法二中那样对pycharm进行配置) 方法二: 配置pycharm忽略该提示, 参考: &quot;File –&gt;Settings–&gt;Editor–&gt;Inspections–&gt;Python–&gt;PEP8 coding style violation&quot;https://blog.csdn.net/zgljl2012/article/details/51907663http://www.zgljl2012.com/pycharmxuan-ze-xing-hu-lue-pep8dai-ma-feng-ge-jing-gao-xin-xi/https://pep8.readthedocs.io/en/latest/intro.html#configuration Function name should be lowercase, 决方法同上 : &quot;File –&gt;Settings–&gt;Editor–&gt;Inspections–&gt;Python–&gt;PEP 8 naming convention violation&quot; 参考 PEP8 中文翻译","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"01. 基础 - Anaconda 环境管理","slug":"python/2018-09-29-Python-02","date":"2018-09-29T07:41:12.000Z","updated":"2018-09-29T08:44:06.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-02/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-02/","excerpt":"","text":"Anaconda 在使用Python时，我们经常需要用到很多第三方库，例如，Pillow，MySQL驱动程序，Web框架Flask，科学计算Numpy等。用pip一个一个安装费时费力，还需要考虑兼容性。我们推荐直接使用Anaconda，它已经内置了许多非常有用的第三方库，我们装上Anaconda，就相当于把数十个第三方模块自动安装好了，非常简单易用; 安装 在安装Anaconda之前, 其实不需要安装Python, 因为Anaconda中包括了Python; 可在Anaconda官网下载并双击进行安装 下载时会发现有两个不同版本的Anaconda，分别对应Python 2.7和Python 3.6, 两个版本其实除了这点区别外其他都一样 (但其实选择安装哪个版本并不重要, 因为通过Anaconda的环境管理, 可以很方便地切换运行时的Python版本) 如下就安装好了:12renyimindeMacBook-Pro:~ renyimin$ which conda/Users/renyimin/Desktop/Anaconda3/anaconda3/bin/conda 注意: Anaconda会把系统Path中的python指向自己自带的Python, 并且, Anaconda安装的第三方模块会安装在Anaconda自己的路径下, 不影响系统已安装的Python目录 可以看到, 之前的系统path中的python是Mac默认的python2.7, 而现在是Anaconda中自带的python3.6.5 12345renyimindeMacBook-Pro:~ renyimin$ pythonPython 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 另外, 之前path中的版本都还在 (只不过直接使用python3的话, 不会使用之前安装的3.7, 而是用的Anaconda中的python3.6) 12345678910renyimindeMacBook-Pro:~ renyimin$ python2 -VPython 2.7.14renyimindeMacBook-Pro:~ renyimin$ python2.7 -VPython 2.7.14renyimindeMacBook-Pro:~ renyimin$ python3 -VPython 3.6.5 :: Anaconda, Inc.// 要用自己安装的python3.7, 需要使用 python3.7renyimindeMacBook-Pro:~ renyimin$ python3.7 -VPython 3.7.0renyimindeMacBook-Pro:~ renyimin$ 另外, Anaconda会把系统Path中的pip指向自己的pip 123456789101112renyimindeMacBook-Pro:~ renyimin$ pip -Vpip 10.0.1 from /Users/renyimin/Desktop/Anaconda3/anaconda3/lib/python3.6/site-packages/pip (python 3.6)renyimindeMacBook-Pro:~ renyimin$ pip2 -Vpip 9.0.3 from /usr/local/lib/python2.7/site-packages (python 2.7)renyimindeMacBook-Pro:~ renyimin$ pip2.7 -Vpip 9.0.3 from /usr/local/lib/python2.7/site-packages (python 2.7)renyimindeMacBook-Pro:~ renyimin$ pip3 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)renyimindeMacBook-Pro:~ renyimin$ pip3.7 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7) 同时, Anaconda还内置了许多非常有用的第三方库, 由于我们现在本机既有Python2(python), 又有Python3.7(python3.7), 还有Anaconda自带的Python3.6(python3), 所以查看包列表也是有三个pip命令可以运行 1234pip (Anaconda自带的) list : 会发现有很多内置包pip2 (Mac默认带的) list : 干净的, 需要自己安装pip3 (自己装的Python3) list : 干净的, 需要自己安装 另外, 安装完后, 电脑中多了一些应用: 1234Anaconda Navigtor ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。 安装完成后，我们还需要对所有工具包进行升级，以避免可能发生的错误, 打开你电脑的终端，在命令行中输入 12renyimindeMacBook-Pro:bin renyimin$ conda upgrade --allSolving environment: | 虚拟环境管理 可以在命令中运行 conda info -e 或者 conda env list 查看 Anaconda 中已安装的环境, 当前被激活的环境会显示有一个星号或者括号: 123456renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base * /Users/renyimin/Desktop/Anaconda3/anaconda3renyimindeMacBook-Pro:~ renyimin$ 尝试 创建指定版本的独立python虚拟环境 conda create -n your_env_name python=X.X 1234// 创建一个2.7版本的python环境conda create -n my-conda-python-2 python=2.7// 创建一个3.6版本的python环境conda create -n my-conda-python-3 python=3.6 安装过程会提示你需不需要自带一些安装包 (选择 是, 这样会创建一个内置很多第三方库的虚拟环境): 创建完成之后:12345678renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base * /Users/renyimin/Desktop/Anaconda3/anaconda3my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2my-conda-python-3 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3renyimindeMacBook-Pro:~ renyimin$ 激活自己创建的python虚拟环境, 并进入环境, 安装好后, 可以使用 activate 激活某个环境 activate my-conda-python-3 # for Windows source activate my-conda-python-3 # for Linux &amp; Mac 123456789renyimindeMacBook-Pro:~ renyimin$ source activate my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda info -e# conda environments:#base /Users/renyimin/Desktop/Anaconda3/anaconda3my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2my-conda-python-3 * /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ 激活后，会发现 终端的前缀 多了 (my-conda-python-3) 的字样 如果想从虚拟环境返回宿主环境: deactivate my-conda-python-3 # for Windows source deactivate my-conda-python-3 # for Linux &amp; Mac 123456789(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ source deactivate my-conda-python-3 renyimindeMacBook-Pro:~ renyimin$ conda info -e # conda environments: # base * /Users/renyimin/Desktop/Anaconda3/anaconda3 my-conda-python-2 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-2 my-conda-python-3 /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3 renyimindeMacBook-Pro:~ renyimin$ 可以看到, 该命令除了返回宿主环境, 之前的虚拟环境退出激活状态转而由Anaconda默认的base版本做虚拟环境; 如果要删除 Anaconda 中创建的一个虚拟环境, 可以 conda remove --name my-conda-python-2 --all 包管理 conda的一些常用操作如下： conda list # 查看当前环境下已安装的包 conda list -n my-conda-python-3 # 查看某个指定环境的已安装包 1234567891011121314151617renyimindeMacBook-Pro:~ renyimin$ source activate my-conda-python-3(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda list# packages in environment at /Users/renyimin/Desktop/Anaconda3/anaconda3/envs/my-conda-python-3:## Name Version Build Channelcertifi 2016.2.28 py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeopenssl 1.0.2l 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freepip 9.0.1 py36_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freepython 3.6.2 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freereadline 6.2 2 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freesetuptools 36.4.0 py36_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freesqlite 3.13.0 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freetk 8.5.18 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freewheel 0.29.0 py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freexz 5.2.3 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freezlib 1.2.11 0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free(my-conda-python-3) renyimindeMacBook-Pro:~ renyimin$ conda search numpy # 查找package信息 conda install -n my-conda-python-3 numpy : 安装package, 如果不用 -n 指定环境名称，则被安装在当前活跃环境 (也可以通过-c指定通过某个channel安装) conda update -n my-conda-python-3 numpy : 更新package conda remove -n my-conda-python-3 numpy : 删除package 前面已经提到, conda将conda自身、python等都视为package, 因此，完全可以使用 conda 来管理 conda和python的版本, 例如 conda update conda : 更新conda，保持conda最新 conda update anaconda : 更新 anaconda conda update python : 更新python(假设当前环境是python 3.4, conda会将python升级为3.4.x系列的当前最新版本) 项目需要选择哪个虚拟环境, 直接在 Pycharm 中指定即可(和virtualenv一样) 设置国内镜像 如果需要安装很多packages, 你会发现conda下载的速度经常很慢，因为Anaconda.org的服务器在国外, 所幸的是，清华TUNA镜像源有Anaconda仓库的镜像, 我们将其加入conda的配置即可： 设置源 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 查看当前使用的源 conda config --show-sources 1234567renyimindeMacBook-Pro:~ renyimin$ conda config --show-sources==&gt; /Users/renyimin/.condarc &lt;==ssl_verify: Truechannels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: True 导入导出环境 小结 到目前为止, 我机器上的python环境有 Mac 自带的Python2.7 自己安装的Python3.7 后来又装了Anaconda (其中包含默认的3.6, 自己创建的两个虚拟环境3.6版本和2.7版本) 参考: https://www.jianshu.com/p/eaee1fadc1e9","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"03. 基础 - virtualenv , virtualenvwrapper","slug":"python/2018-09-29-Python-03","date":"2018-09-29T07:41:12.000Z","updated":"2018-09-30T07:28:03.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-03/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-03/","excerpt":"","text":"virtualenv 在开发Python应用程序的时候, 如果系统只安装了一个Python版本, 那么所有第三方的包都会被pip安装到这个Python的 site-packages 目录下; 如果要同时开发多个应用程序, 这些应用程序就会共用这一个Python环境, 如果应用A需要 jinja 2.7, 而应用B需要jinja 2.6怎么办? 这种情况下, 每个应用可能需要各自拥有一套 各自独立 的Python运行环境, virtualenv 就是用来为一个应用创建一套 隔离 的Python运行环境; virtualenv : 可以在系统中建立多个不同并且相互不干扰的虚拟环境(另外, 值得一提的是, 在 virtualenv 的虚拟环境中使用 pip 安装依赖还可以绕过某些系统的权限设置, 因为毕竟不需要向系统目录写入数据) 总之, virtualenv是用来创建一个独立的Python虚拟环境的工具, 通过virtualenv可以创建一个拥有独立的python版本和安装库的虚拟开发环境; 这样一来我们就可以在虚拟环境中安装各种各种所需要的库, 从而不会造成本地的库过多所引起的使用混乱, 同时也可以创建不同的python版本来完成不同的需求开发 由于 virtualenv 用起来有点麻烦, virtualenvwrapper 对它进行了封装, 让它更好用, 最终我们使用 virtualenvwrapper 提供的命令, 但是实际工作都是 virtualenv 做的; virtualenv 安装 (推荐使用pip安装) 直接安装 virtualenvwrapper 即可, 你会发现 virtualenv 和 virtualenvwrapper 都被安装了 (由于这里的 pip3 是python3.7, 所以就是在 python3.7 的基础上进行 virtualenvwrapper 的安装)1234renyimindeMacBook-Pro:~ renyimin$ pip3 -Vpip 18.0 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)renyimindeMacBook-Pro:~ renyimin$ pip3 install virtualenvwrapperSuccessfully installed pbr-4.2.0 six-1.11.0 stevedore-1.29.0 virtualenv-16.0.0 virtualenv-clone-0.3.0 virtualenvwrapper-4.8.2 virtualenv虚拟环境管理 创建一个独立的python新环境: virtualenv myEnv_01 会在当前目录下生成 ‘myEnv_01’ 目录 12345678910renyimindeMacBook-Pro:Desktop renyimin$ mkdir virtualEnvPythonrenyimindeMacBook-Pro:Desktop renyimin$ cd virtualEnvPythonrenyimindeMacBook-Pro:virtualEnvPython renyimin$ virtualenv myEnv_01renyimindeMacBook-Pro:virtualEnvPython renyimin$ lsmyEnv_01renyimindeMacBook-Pro:virtualEnvPython renyimin$ cd myEnv_01/renyimindeMacBook-Pro:myEnv_01 renyimin$ lsbin libinclude pip-selfcheck.jsonrenyimindeMacBook-Pro:myEnv_01 renyimin$ 一个新的python虚拟环境就创建好了, 并且在这个目录下会有3个目录被创建: bin : 包含一些在这个虚拟环境中可用的命令, 以及开启虚拟环境的脚本 activate include : 包含虚拟环境中的头文件, 包括 Python 的头文件; lib : 依赖库 激活并进入虚拟环境: 进入虚拟环境目录 myEnv_01 中, 然后执行: source ./bin/activate 12345renyimindeMacBook-Pro:myEnv_01 renyimin$ lsbin libinclude pip-selfcheck.jsonrenyimindeMacBook-Pro:myEnv_01 renyimin$ source ./bin/activate(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ 此时, 我们就已经在虚拟环境中了, 可以看到, 命令提示符是 (myEnv_01) renyimindembp:myEnv_01 renyimin$ 从虚拟环境返回宿主环境: 要退出虚拟环境到达宿主环境, 无论在哪个目录下, 只要在虚拟环境中(命令提示符和宿主环境的命令提示符有区别), 直接执行 deactivate 就会退出到宿主python环境中; 如果想要删除虚拟环境, 只要把虚拟环境目录删除即可; (貌似比Anaconda简单多了) virtualenv包管理 在 ‘myEnv_01’ python虚拟环境中安装一个test依赖库: 123456789101112131415161718# myEnv_01虚拟环境中默认是没有该扩展的(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 listPackage Version---------- -------pip 18.0setuptools 40.4.3wheel 0.32.0(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 install test# 之后就有了(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ pip3 listPackage Version---------- -------pip 18.0setuptools 40.4.3test 2.3.4.5wheel 0.32.0(myEnv_01) renyimindeMacBook-Pro:myEnv_01 renyimin$ virtualenvwrapper 有了virtualenv, 为何还要 virtualenvwrapper ? virtualenv 的一个最大的缺点就是, 每次开启虚拟环境之前, 你都需要去虚拟环境所在目录下的 bin 目录下 source 一下 activate, 这就需要我们记住每个虚拟环境所在的目录; 当然, 你可以将所有的虚拟环境目录全都集中起来, 比如放到 /Users/renyimin/Desktop/virtualEnvPython/, 这个目录下专门存放所有的python虚拟环境, 对不同的虚拟环境使用不同的目录来管理;而 virtualenvwrapper 正是这样做的, 并且, 它还省去了每次开启虚拟环境时候的 source 操作, 使得虚拟环境更加好用 安装 virtualenvwrapper: 卸载之前安装的 virtualenv (因为我们要安装 virtualenvwrapper 的话, 会自动安装 virtualenv) 1pip3 uninstall virtualenv 顺便也手动删除之前的 myEnv_01 目录 然后直接安装 virtualenvwrapper 12renyimindembp:~ renyimin$ pip3 install virtualenvwrapperrenyimindembp:~ renyimin$ 现在, 我们就拥有了一个可以管理虚拟环境的神器 接下来要做的比较重要, 那就是对 virtualenvwrapper 进行配置 : 它需要指定一个环境变量, 叫做 WORKON_HOME, 并且需要运行一下它的初始化工具 virtualenvwrapper.sh, 这个脚本在 /usr/local/bin/ 目录下; WORKON_HOME 就是它将要用来存放各种虚拟环境目录的目录, 这里我们可以设置为 ‘~/Desktop/virtualEnvPython/‘ VIRTUALENVWRAPPER_PYTHON # 这句是为了防止环境变量$PATH中已有其它环境的python, 需要换成自己需要的python路径, 此处使用 python3.7 而不使用 python3, 是因为之前安装virtualenvwrapper时使用的pip3其实还是独立安装的python3.7, 而 python3 已经变成了Anaconda中的python3.6了 123export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3.7 export WORKON_HOME=&apos;~/Desktop/virtualEnvPython/&apos;source /usr/local/bin/virtualenvwrapper.sh 由于每次都需要执行这两步操作, 我们可以将其写入终端的配置文件中, 例如:如果使用 bash, 则添加到 ~/.bashrc 中;如果使用 zsh, 则添加到 ~/.zshrc 中; 这样每次启动终端的时候都会自动运行，终端其中之 virtualenvwrapper 就可以用啦;如果 ~/ 下没有 .bashrc的话, 写到.bash_profile文件中也可以 利用 virtualenvwrapper, 我们可以使用命令 mkvirtualenv myEnv_001 轻松创建一个虚拟环境, 之后我们就有了一个叫做 myEnv_001 的虚拟环境, 它被存放在 $WORKON_HOME/myEnv_001 目录下, 也就是 ~/Desktop/virtualEnvPython/myEnv_001/ 1234567891011121314renyimindeMacBook-Pro:virtualEnvPython renyimin$ mkvirtualenv myEnv_001Using base prefix &apos;/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7&apos;New python executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/python3.7Also creating executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/pythonInstalling setuptools, pip, wheel...done.virtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/predeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/postdeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/preactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/postactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_001/bin/get_env_details(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$ lsmyEnv_001(myEnv_001) renyimindeMacBook-Pro:virtualEnvPython renyimin$ 新建虚拟环境之后会自动激活虚拟环境, 如果我们平时想要进入某个虚拟环境, 可以用命令 workon myEnv_001, 这样才能真正进入激活的虚拟环境中 同样, 离开虚拟环境, 可以使用 deactivate 删除虚拟环境也一样简单 rmvirtualenv myEnv_001 (不像之前只使用virtualenv那样, 需要手动删除目录来删除一个虚拟环境, 没那么low了) virtualenvwrapper 中的其他命令: 1234lsvirtualenv，虚拟环境的列表cdvirtualenv，进入当前激活的虚拟环境cdsitepackages，进入虚拟环境中的site-packages目录lssitepackages，site-packages目录的列表 同时, 你还可以使用 virtualenv 来操作!! 另外, 参考了解到**: 命令 virtualenv 就可以创建一个独立的Python运行环境，我们还加上了参数 --no-site-packages, 这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境, 好像这个参数是默认就有的, 因为测试后发现加和不加都是干净的: 1234renyimindeMacBook-Pro:virtualEnvPython renyimin$ ls myEnv_001/lib/python3.7/site-packages/__pycache__ pip pkg_resources setuptools-40.4.3.dist-info wheel-0.32.0.dist-infoeasy_install.py pip-18.0.dist-info setuptools wheelrenyimindeMacBook-Pro:virtualEnvPython renyimin$ 参考 创建虚拟环境的时候, python版本如何指定 当我的机器上有Python2.7和Python3.6两个Python版本的时候, 那么virtualenv创建的虚拟环境使用哪个Python版本呢? 可以通过 virtualenv -h 查看帮助命令 -p : 指定一个python版本, 通常当你的系统中安装了多个python版本时会用到, 默认情况下virtualenv会优先选取它的宿主python环境，也就是它的 VIRTUALENVWRAPPER_PYTHON 是哪个版本的, 默认就会选择哪个版本作为默认python隔离环境我们使用的是 Python3.7 所以virtualenv默认安装的就是Python3.6虚拟环境; 尝试创建一个 2.7 版本的python环境12345678910111213renyimindeMacBook-Pro:virtualEnvPython renyimin$ mkvirtualenv myEnv_002 -p python2.7Running virtualenv with interpreter /usr/local/bin/python2.7New python executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/python2.7Also creating executable in /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/pythonInstalling setuptools, pip, wheel...done.virtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/predeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/postdeactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/preactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/postactivatevirtualenvwrapper.user_scripts creating /Users/renyimin/Desktop/virtualEnvPython/myEnv_002/bin/get_env_details(myEnv_002) renyimindeMacBook-Pro:virtualEnvPython renyimin$ python -VPython 2.7.14(myEnv_002) renyimindeMacBook-Pro:virtualEnvPython renyimin$ 小结 目前本机环境有 Mac 自带的Python2.7 自己安装的Python3.7 后来又装了Anaconda (其中包含默认的3.6, 自己创建的两个虚拟环境3.6版本和2.7版本) virtualenv 中创建的两个虚拟环境3.7和2.7版本 在pycharm中引入本地的虚拟环境(可以看到也可以在pycharm中直接使用 conda 或者 virtualenv 来新建虚拟环境, 不过由于之前已经了一些虚拟环境了, 所以直接add即可) 在 pycharm 的 左下角有 Python Console 和 Terminal 两个终端, 但是发现在切换 conda 或者 virtual 虚拟环境时: virtual 虚拟环境切换时候, 两个终端都会切换 而 conda 虚拟环境切换时, 貌似不会切换 Terminal 终端到虚拟环境中","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"01. 基础 - Python 环境安装","slug":"python/2018-09-29-Python-01","date":"2018-09-29T04:11:12.000Z","updated":"2018-09-29T08:46:41.000Z","comments":true,"path":"2018/09/29/python/2018-09-29-Python-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/29/python/2018-09-29-Python-01/","excerpt":"","text":"Python安装 目前, Python有两个版本, 一个是2.x版, 一个是3.x版, 这两个版本是不兼容的; MacOS 是10.8或者最新的10.9 Mavericks，恭喜你，系统自带了Python 2.7目前自带的是Python2.7版本12345renyimindembp:~ renyimin$ pythonPython 2.7.10 (default, Feb 7 2017, 00:08:15)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 由于3.x版越来越普及, 所以接下来将会使用的是Python3.x: 从Python官网下载Python 3.6的安装程序(傻瓜式安); Mac可以 直接 brew install python3 安装(安装后发现是python3.7, 卸载方便 brew uninstall python3.7 ); 以上两种方法都可以, 这里使用第二种, 安装完成后将会存在两个python版本:12345renyimindembp:~ renyimin$ python -VPython 2.7.10renyimindembp:~ renyimin$ python3 -VPython 3.6.4renyimindembp:~ renyimin$ 安装后, 就会得到Python解释器(就是负责运行Python程序的), 一个命令行交互环境, 还有一个简单的集成开发环境 pip 安装 在Python中, 安装第三方模块, 是通过包管理工具pip完成的 如果你正在使用Mac或Linux, 安装pip这个步骤就可以跳过了 如果你正在使用Windows，请参考安装Python一节的内容，确保安装时勾选了pip和Add python.exe to Path Mac为我们准备了Python2.7的同时, 还默认准备了pip, 另外, 自己安装Python3.6之后, 也同时安装了pip: 自己安装的Python3.7所带的pip需要运行 pip3 (注意: Mac或Linux上有可能并存Python 3.x和Python 2.x, 因此对应的pip命令是pip3) 12345678910renyimindeMacBook-Pro:bin renyimin$ which pip/usr/local/bin/piprenyimindeMacBook-Pro:bin renyimin$ which pip2/usr/local/bin/pip2renyimindeMacBook-Pro:bin renyimin$ which pip2.7/usr/local/bin/pip2.7renyimindeMacBook-Pro:bin renyimin$ which pip3/usr/local/bin/pip3renyimindeMacBook-Pro:bin renyimin$ which pip3.7/usr/local/bin/pip3.7 可以看到软链已经创建好了 123456renyimindeMacBook-Pro:bin renyimin$ ls -al pip*lrwxr-xr-x 1 renyimin admin 35 4 22 18:26 pip -&gt; ../Cellar/python@2/2.7.14_3/bin/piplrwxr-xr-x 1 renyimin admin 36 4 22 18:26 pip2 -&gt; ../Cellar/python@2/2.7.14_3/bin/pip2lrwxr-xr-x 1 renyimin admin 38 4 22 18:26 pip2.7 -&gt; ../Cellar/python@2/2.7.14_3/bin/pip2.7lrwxr-xr-x 1 renyimin admin 31 9 29 11:50 pip3 -&gt; ../Cellar/python/3.7.0/bin/pip3lrwxr-xr-x 1 renyimin admin 33 9 29 11:50 pip3.7 -&gt; ../Cellar/python/3.7.0/bin/pip3.7","categories":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.renyimin.com/tags/Python/"}]},{"title":"03.","slug":"elastic-stack/2018-09-08-03","date":"2018-09-08T02:50:15.000Z","updated":"2018-09-08T02:50:38.000Z","comments":true,"path":"2018/09/08/elastic-stack/2018-09-08-03/","link":"","permalink":"http://blog.renyimin.com/2018/09/08/elastic-stack/2018-09-08-03/","excerpt":"","text":"","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"02. 安装配置","slug":"elastic-stack/2018-09-08-02","date":"2018-09-08T02:46:43.000Z","updated":"2018-09-08T02:50:44.000Z","comments":true,"path":"2018/09/08/elastic-stack/2018-09-08-02/","link":"","permalink":"http://blog.renyimin.com/2018/09/08/elastic-stack/2018-09-08-02/","excerpt":"","text":"安装Elasticsearch安装Kibana安装Logstash安装Filebeat","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"01. 从ELK Stack 到 Elastic Stack","slug":"elastic-stack/2018-09-04-01","date":"2018-09-04T09:36:39.000Z","updated":"2018-09-08T02:47:35.000Z","comments":true,"path":"2018/09/04/elastic-stack/2018-09-04-01/","link":"","permalink":"http://blog.renyimin.com/2018/09/04/elastic-stack/2018-09-04-01/","excerpt":"","text":"ELK Stack 简介ELK Stack 是三个开源工具的统称: Elasticsearch, Logstash 和 Kibana Logstash: 开源的日志收集工具, 能按你所定义的配置信息来规范化数据, 并根据需要将其他送到指定的目的地; (监听9600端口) 拥有非常多的Input输入数据类型的插件, 这些Input插件可以用于从大量不同来源的信息中读取数据; 同时, 它也拥有非常多的Output输出数据类型插件, 可用于把数据提交到各种不同的目的地(其中的一种插件就是把数据传输到Elasticsearch中去);比如, 它可以从本地磁盘, 网络服务(自己监听端口, 接受用户日志), 消息队列……中, 收集各种各样的日志; 然后对日志进行分析整理, 输出到指定的输出(如 elasticsearch、redis、终端等); 它能帮助我们搜集原始数据, 修改/过滤数据并将其转换成某种有含义的数据, 完成数据格式化和重新组织数据等; Elasticsearch: 基于Lucene的开源分布式全文搜索引擎; Elasticsearch 服务会开启两个端口 9200和9300, 9200是对外服务的 9300是对集群内交互使用的; Logstash读取的数据可输出到Elasticsearch中, 完成数据的索引; Kibana: 是一个开源的可视化日志web展示工具, 提供友好的日志分析 Web 界面, 帮助你汇总、分析和搜索重要数据日志 (监听 5601 端口); Kibana使用Elasticsearch提供的API来读取/检索存放在Elasticsearch中的索引数据, 并以图表等形式对这些数据进行可视化分析; Elastic Stack诞生 上面在介绍 ELK Stack 时提到, 所有读取数据的工作都是由 Logstash 来完成的, 但是这是一种资源消耗, 因为 Logstash 需要运行在Java虚拟机上, 会消耗大量内存; 因此, 软件研发社区认为需要提高其性能, 并使用管道(pipeline)处理机制 — 一种友好且轻量级的方式来处理资源; 因此, 一种新的概念 Beats 诞生, 并加入到了 ELK Stack 家族成为重要组件(Beats 是由 GO 语言编写的) Beats 用于读取、解析并将数据输出到 Elasticsearch 或 Logstash 中; 不同的是, 它是一种轻量级的, 服务于某种特殊用途的代理(它可以是Metricbeat/Filebeat/Packetbeat等), 它们都是由Elastic开发团队提供; Elastic Stack 的起始版本号是5.0.0, 其虽然是原 ELK Stack 在 5.0 版本加入 Beats 套件后的新称呼, 但其实涵盖的内容还不止这些; 在产生数据管道的作用中, 所有组件都发挥了重要作用: Beats 和 Logstash 用于搜索, 解析, 传输数据; Elasticsearch 负责对数据的索引; Elasticsearch 索引的数据, 最后会被 Kibana 用于数据的可视化; 在基于 Elastic Stack 的数据处理管道中, 还有诸如 安全, 监控, 报警 等方面需要特别关注, 这些工具组件现在统称为 X-Pack 参考:《精通Elastic Stack》","categories":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/categories/Elastic-Stack/"}],"tags":[{"name":"Elastic-Stack","slug":"Elastic-Stack","permalink":"http://blog.renyimin.com/tags/Elastic-Stack/"}]},{"title":"10. 聚合","slug":"elasticsearch/2018-07-08-10","date":"2018-07-09T02:41:07.000Z","updated":"2018-09-28T09:13:55.000Z","comments":true,"path":"2018/07/09/elasticsearch/2018-07-08-10/","link":"","permalink":"http://blog.renyimin.com/2018/07/09/elasticsearch/2018-07-08-10/","excerpt":"","text":"聚合 聚合允许我们向数据提出一些复杂的问题, 虽然功能完全不同于搜索, 但它使用相同的数据结构, 这意味着聚合的执行速度很快并且就像搜索一样几乎是实时的; 要掌握聚合, 你只需要明白两个主要的概念: 桶(Buckets) : 满足特定条件的文档的集合 (桶在概念上类似于SQL的分组（GROUP BY）) 指标(Metrics) : 对桶内的文档进行统计计算 (而指标则类似于 COUNT() 、 SUM() 、 MAX() 等统计方法) 翻译成粗略的SQL语句来解释： 123SELECT COUNT(color) FROM tableGROUP BY color COUNT(color) 相当于指标GROUP BY color 相当于桶 每个聚合都是一个或者多个桶和零个或者多个指标的组合 (聚合是由桶和指标组成的) 桶 当聚合开始被执行, 每个文档里面的值通过计算来决定符合哪个桶的条件, 如果匹配到, 文档将放入相应的桶并接着进行聚合操作; 桶也可以被嵌套在其他桶里面, 提供层次化的或者有条件的划分方案; (例如, 辛辛那提会被放入俄亥俄州这个桶, 而整个俄亥俄州桶会被放入美国这个桶) Elasticsearch 有很多种类型的桶, 能让你通过很多种方式来划分文档(时间、最受欢迎的词、年龄区间、地理位置等等); 其实根本上都是通过同样的原理进行操作: 基于条件来划分文档; 指标 桶能让我们划分文档到有意义的集合, 但是最终我们需要的是对这些桶内的文档进行一些指标的计算; 分桶是一种达到目的的手段: 它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标; 大多数指标是简单的数学运算(例如最小值、平均值、最大值、还有汇总), 这些是通过文档的值来计算; 在实践中, 指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据; 桶的使用, 例子, 可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_aggregation_test_drive.html, 可能语法有些在ES5.X已经不能使用, 下面列出新的例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546DELETE /cars/PUT /cars/&#123; &quot;mappings&quot; : &#123; &quot;transactions&quot; : &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;make&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;POST /cars/transactions/_bulk&#123; &quot;index&quot;: &#123;&quot;_id&quot;:1&#125;&#125;&#123; &quot;price&quot; : 10000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:2&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:3&#125;&#125;&#123; &quot;price&quot; : 30000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:4&#125;&#125;&#123; &quot;price&quot; : 15000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:5&#125;&#125;&#123; &quot;price&quot; : 12000, &quot;color&quot; : &quot;green&quot;, &quot;make&quot; : &quot;toyota&quot;, &quot;sold&quot; : &quot;2014-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:6&#125;&#125;&#123; &quot;price&quot; : 20000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;honda&quot;, &quot;sold&quot; : &quot;2014-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:7&#125;&#125;&#123; &quot;price&quot; : 80000, &quot;color&quot; : &quot;red&quot;, &quot;make&quot; : &quot;bmw&quot;, &quot;sold&quot; : &quot;2014-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&quot;_id&quot;:8&#125;&#125;&#123; &quot;price&quot; : 25000, &quot;color&quot; : &quot;blue&quot;, &quot;make&quot; : &quot;ford&quot;, &quot;sold&quot; : &quot;2014-02-12&quot; &#125;GET /cars/transactions/_mappingGET /cars/transactions/_searchGET /cars/transactions/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;popular_colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; 桶和指标组合的例子 例子, 参考: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_adding_a_metric_to_the_mix.html 嵌套桶 例子, 参考: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_buckets_inside_buckets.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"09. 排序, 相关性","slug":"elasticsearch/2018-07-07-09","date":"2018-07-07T02:41:07.000Z","updated":"2018-09-27T10:18:29.000Z","comments":true,"path":"2018/07/07/elasticsearch/2018-07-07-09/","link":"","permalink":"http://blog.renyimin.com/2018/07/07/elasticsearch/2018-07-07-09/","excerpt":"","text":"排序与相关性: 默认情况下，返回的结果是按照 相关性 进行排序的——最相关的文档排在最前 按照指定字段的值进行排序. 多字段排序 相关性","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"08. 结构化搜索 , 全文搜索","slug":"elasticsearch/2018-07-02-08","date":"2018-07-02T04:46:21.000Z","updated":"2018-09-28T07:57:23.000Z","comments":true,"path":"2018/07/02/elasticsearch/2018-07-02-08/","link":"","permalink":"http://blog.renyimin.com/2018/07/02/elasticsearch/2018-07-02-08/","excerpt":"","text":"深入搜索结构化搜索 结构化搜索主要是指针对结构化数据的搜索, 要知道, 搜索不仅仅是全文搜索, 我们很大一部分数据都是结构化的; 比如日期、时间和数字都是结构化的, 它们有精确的格式，我们可以对这些格式进行逻辑操作; 比较常见的操作包括比较数字或时间的范围，或判定两个值的大小。 在结构化查询中, 我们得到的结果 要么存在，要么不存在; 结构化查询不关心文件的相关度或评分。 同样，即便是对于文本, 如果是结构化文本，一个值要么相等，要么不等。没有 更似 这种概念。 当进行结构化搜索时, 其实就是在进行 精确值查找, 我们会使用过滤器（filters）, 因为 结构化的精确查找不需要对结果进行评分 而过滤器的执行速度非常快，不会计算相关度（直接跳过了整个评分阶段），而且很容易被缓存; (请尽可能多的使用过滤式查询) 精确查找常用的子查询语句如 term、terms、rang 等, 如下 (还使用了 constant_score 来代替 bool, 因为只有精确查询) 1234567891011121314151617181920212223// 构造数据POST /my_store/products/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;price&quot; : 10, &quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;price&quot; : 20, &quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;price&quot; : 30, &quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;price&quot; : 30, &quot;productID&quot; : &quot;QQPX-R-3956-#aD8&quot; &#125;GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;price&quot; : 20 &#125; &#125; &#125; &#125;&#125; term查询文本域时, 需要注意: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_finding_exact_values.html#_term_查询文本 range 精确查询可以参考 : https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_ranges.html, 如下 (仍然使用了 constant_score 来代替 bool): 123456789101112131415GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;price&quot; : &#123; &quot;gte&quot; : 20, &quot;lt&quot; : 40 &#125; &#125; &#125; &#125; &#125;&#125; 全文搜索 匹配查询 match 是个核心查询, 无论需要查询什么字段, match 查询都应该会是首选的查询方式, 它是一个 高级全文查询, 这表示它既能处理全文字段, 又能处理精确字段; 这就是说, match 查询主要的应用场景就是进行全文搜索 多词查询 提高精度match 查询还可以接受 operator 操作符作为输入参数，默认情况下该操作符是 or , 我们可以将它修改成 and 让所有指定词项都必须匹配 控制精度在 所有 与 任意 间二选一有点过于非黑即白, 如果用户给定5个查询词项, 想查找只包含其中4个的文档, 该如何处理? 将 operator 操作符参数设置成 and 只会将此文档排除;match 查询支持 minimum_should_match 最小匹配参数, 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关, 我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量1234567891011121314151617181920212223242526DELETE /my_index PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;&#125; POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox jumps over the lazy dog&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;title&quot;: &quot;The quick brown fox jumps over the quick dog&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;title&quot;: &quot;Brown fox brown dog&quot; &#125;GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;quick brown dog&quot;, &quot;minimum_should_match&quot;: &quot;99%&quot; &#125; &#125; &#125;&#125; 组合多查询中也有对 should 精度控制的介绍; 注意这里: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_how_match_uses_bool.html, 如下两个句子其实是一样的: 1234567891011121314151617&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;brown fox&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125;&#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;brown&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;fox&quot; &#125;&#125; ] &#125;&#125; boost 指定提高某些查询语句的权重 多字段查询 主要讲解了组合查询时, 不同组合情况对评分的影响 boost 值的不断尝试","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"07. 请求体查询, 查询与过滤, 查询条件的组合","slug":"elasticsearch/2018-06-30-07","date":"2018-06-30T03:07:21.000Z","updated":"2018-09-28T06:45:36.000Z","comments":true,"path":"2018/06/30/elasticsearch/2018-06-30-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/30/elasticsearch/2018-06-30-07/","excerpt":"","text":"请求体查询 简易查询(query-string search) 对于用命令行进行即席查询是非常有用的; 然而, 为了充分利用查询的强大功能, 你应该使用 请求体 search API, 之所以称之为请求体查询(Full-Body Search), 因为大部分参数是通过 Http 请求体而非查询字符串来传递的; 请求体查询不仅可以处理自身的查询请求, 还允许你对结果进行片段强调(高亮)、对所有或部分结果进行聚合分析, 同时还可以给出 你是不是想找 的建议, 这些建议可以引导使用者快速找到他想要的结果; 相对于使用晦涩难懂的查询字符串的方式, 一个带请求体的查询允许我们使用 查询领域特定语言（query domain-specific language） 或者 Query DSL 来写查询语句; 查询表达式 查询表达式(Query DSL)是一种非常灵活又富有表现力的查询语言, Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分功能。在你的应用中, 你应该用它来编写你的查询语句, 它可以使你的查询语句更灵活、更精确、易读和易调试; 要使用这种查询表达式, 只需将查询语句传递给请求体中的 query 参数: 1234GET /_search&#123; &quot;query&quot;: YOUR_QUERY_HERE&#125; 空查询(empty search) 在功能上其实就等价于使用 match_all 查询, 正如其名字一样, 匹配所有文档: 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 查询与过滤 Elasticsearch 使用的查询语言(DSL) 拥有一套查询组件, 这些组件可以以无限组合的方式进行搭配; 这套组件可以在以下两种情况下使用 过滤情况（filtering context）: 当使用于 过滤情况 时, 查询被设置成一个 不评分 的查询, 即, 查询结果只是简单的是或否, 不会计算任何评分; 查询情况（query context） : 当使用于 查询情况 时, 查询就变成了一个 评分 查询, 它不但会去判断这个文档是否匹配， 同时它还需要判断这个文档匹配的有 多好（匹配程度如何）;一个评分查询计算每一个文档与此查询的 _相关程度_，同时将这个相关程度分配给表示相关性的字段 _score，并且按照相关性对匹配到的文档进行排序。这种相关性的概念是非常适合全文搜索的情况，因为全文搜索几乎没有完全 “正确” 的答案 如何选择查询与过滤: 通常的规则是, 使用 查询（query）语句来进行 全文搜索或者其它任何需要影响相关性得分的搜索; 除此以外的情况都使用过滤（filters); 当我们想要查询一个具有精确值的 not_analyzed 未分析字段之前, 需要考虑, 是否真的采用评分查询, 或者非评分查询会更好; 单词项查询通常可以用是、非这种二元问题表示，所以更适合用过滤, 而且这样做可以有效利用缓存 查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件和 term 查询一样，terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;John&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter。这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果; (不过, 这只是should不在must或should下时) 只有must和must_not中的子句是决定了是否能查询出数据; 而should只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分; (如果非should的语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 should 子句的不同之处; 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 AND (a OR b) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配;123456789101112131415161718192021222324GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略, 语句会更简化 &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal1&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125; &#125; a OR (a AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 123456789101112131415161718192021222324GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略 &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125; &#125; 组合过滤和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"06. 搜索相关","slug":"elasticsearch/2018-06-16-06","date":"2018-06-16T09:20:09.000Z","updated":"2018-09-25T08:42:14.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-06/","excerpt":"","text":"多索引, 多类型, 多字段分页","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"05. Mapping 映射","slug":"elasticsearch/2018-06-16-05","date":"2018-06-16T02:31:39.000Z","updated":"2018-09-26T09:20:52.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-05/","excerpt":"","text":"一个奇怪的例子 先索引两个文档到不同的索引中 (为什么不在同一个映射中创建两个文档? 或者在同一个映射中的不同类型中各自创建一个文档?): 123456DELETE /test/PUT /test/mapping/1&#123; &quot;title&quot;: &quot;first time to build my mapping&quot;, &quot;text&quot;: &quot;2015/12/21 18:30:25&quot;&#125; 123456DELETE /test1/PUT /test1/mapping/1&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;build my mapping at 2015-12-28 13:20:25&quot;&#125; 尝试如下查询: GET /_search?q=2015 结果是两个文档, 因为 _all 字段:当索引一个文档的时候, Elasticsearch 会取出所有字段的值拼接成一个大的字符串, 作为 _all 字段进行索引;之后在查询时, 如果不指定字段, 则默认会去 _all 字段中进行查询 GET /_search?q=text:2015 结果只有第二个索引中的文档ES在索引文档时, 如果不手动设置索引的 mapping 映射来说明索引中文档的字段类型, ES会动态帮你生成一份 mapping (ES会根据字段值猜测字段的类型), 主要是想对文档的全文域进行分析以用来创建倒排索引;(做全文索引)当你进行搜索时, 如果指明字段, ES会根据字段类型来决定该字段是否进行全文分析, 由于第二篇文档的text是全文类型, 所以会进行全文分析, 会查询到;而第一篇文档在最初进行索引时, ES将其text字段识别为 DATE 类型, 而查询时, 当你指明查询 text 字段时, ES并不会对DATE类型字段做分析, 所以检索不到第二篇文档; 所以 date 字段和 string 字段 索引方式不同, 因此搜索结果也不一样; 他们最大的差异在于 代表精确值的字段 和 代表全文的字段 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。 当你查询一个 精确值 域时，不会分析查询字符串， 而是搜索你指定的精确值。 在实际开发中, 开发者比ES更了解自己的文档信息, 很多时候都需要自行说明自己的字段类型, 以防止ES动态创建映射可能会造成的一些小问题; Mapping映射 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, Elasticsearch 需要知道每个域中数据的类型, 这些信息就包含在映射中; 索引中每个文档都有 类型, 每种类型都有它自己的 映射; 映射定义了类型中每个域的数据类型, 以及Elasticsearch如何处理这些域。映射也用于配置与类型有关的元数据。 也就是说, 映射是类型的, 但事实上, 即使在不同的类型中, 也不能对相同字段做不同的类型指定; 参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型; 所以文章开头的实验, 要创建两个 包含不同类型text字段 的文档, 需要在不同的索引中进行创建! 查看映射, 之前已经使用过 GET /索引名/_mapping/类型名 自定义域映射 自定义映射允许你执行下面的操作: 全文字符串域 和 精确值字符串域 的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性就是 type ELasticsearch 5.X 之后的字段类型不再支持 string, 由 text 或 keyword 取代; text 取代了 string: 当一个字段是要被全文搜索的, 比如Email内容、产品描述, 应该使用text类型它们的值在索引前，会通过 一个分析器, 在生成倒排索引以前, 字段内容会被分析器分成一个一个词项, 针对于这个域的查询在搜索前也会经过一个分析器;text类型的字段不用于排序, 很少用于聚合(termsAggregation除外) 映射中的每个 text 字段都可以指定自己的分析器 analyzer;在索引时, 如果未指定分析器, 它将在索引设置中查找名为 default analyzer; 否则, 它默认使用 standard analyzer;可参考文档 keyword类型: 适用于索引结构化的字段, 比如email地址、主机名、状态码和标签, 如果字段需要进行过滤(比如查找已发布博客中status属性为published的文章)、排序、聚合; keyword类型的字段只能通过精确值搜索到; 更新映射 当你首次 创建一个索引时, 可以使用 /_mapping 为新类型(或者为存在的类型更新映射)增加映射; 尽管你可以为一个已存在的映射增加一个新域, 但不能 修改 存在的域映射如果一个域的映射已经存在, 那么该域的数据可能已经被索引, 如果你意图修改这个域的映射, 索引的数据可能会出错, 不能被正常的搜索 可以更新一个映射来添加一个新域, 但不能将一个存在的域从 analyzed 改为 not_analyzed ES5.X中, 索引属性只接收true/false来代替not_analyzed/no 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647DELETE /gb# 创建一个新索引, 指定 tweet 域使用 english 分析器PUT /gb&#123; &quot;mappings&quot;: &#123; &quot;tweet&quot; : &#123; &quot;properties&quot; : &#123; &quot;tweet&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;user_id&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; &#125;&#125;GET /gb/_mapping# 在 tweet 映射增加一个新的名为 `tag` 的 not_analyzed 的文本域PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125;&#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tweet&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;GET /gb/_analyze&#123; &quot;field&quot;: &quot;tag&quot;, &quot;text&quot;: &quot;Black-cats&quot; &#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"04. 文档的概念, ES乐观并发控制, 文档基本操作","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T06:29:07.000Z","updated":"2018-09-20T07:25:26.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"前言 传统上, 我们以 行和列的形式 存储数据到关系型数据库中, 相当于使用电子表格; 正因为我们使用了这种不灵活的存储媒介导致所有我们使用对象的灵活性都丢失了; 面向对象编程语言如此流行的原因之一是对象帮我们表示和处理现实世界具有潜在的复杂的数据结构的实体 而 JSON 正是一种以人可读的文本表示对象的方法, 它已经变成 NoSQL 世界交换数据的事实标准, 当一个对象被序列化成为 JSON, 它被称为一个 JSON 文档;JSON文档可以将我们的对象按对象的方式来存储, 这样我们就能更加专注于 使用 数据, 而不是在电子表格的局限性下对我们的应用建模; 我们可以重新利用对象的灵活性; Elastcisearch 是分布式的 文档 存储, 它能存储和检索复杂的数据结构—序列化成为JSON文档 现存的 NoSQL 解决方案虽然允许我们以文档的形式存储对象, 但是他们仍旧需要我们思考如何查询我们的数据, 以及确定哪些字段需要被索引以加快数据检索; 而在 ES 中, 每个字段的所有数据都是默认被索引的, 即每个字段都有为了快速检索设置的专用倒排索引; 而且, 不像其他多数的数据库, 它能在相同的查询中使用所有这些倒排索引, 并以惊人的速度返回结果; 文档 文档及其元数据介绍 索引文档, 注意 PUT 和 POST 两个谓词的使用场景; 小心覆盖创建 当我们索引一个文档, 怎么确认我们正在创建一个完全新的文档, 而不是覆盖现有的呢? 最简单办法是, 使用索引请求的 POST 形式让 Elasticsearch 自动生成唯一 _id; 另外, 如果有自己的文档ID, 防止覆盖 (成功会返回 401码, 冲突则会返回 409码)/website/blog/123?op_type1PUT /website/blog/123/_create 返回文档中的部分字段 更新整个文档, 注意, ES中文档不能修改, 需要重建或者替换 删除比较简单… ES文档的丢失更新问题 在使用 index API 更新文档时, 一般会先读取原始文档, 然后做我们的修改, 最后重新索引整个文档; 如果两个进程同时做这系列操作的话, 最后的索引请求将获胜, 当然, 有人的更改将丢失。 假设我们使用 Elasticsearch 存储我们网上商城商品库存的数量, 每次我们卖一个商品的时候, 在 Elasticsearch 中将库存数量减少; 有一天, 管理层决定做一次促销, 突然地, 每秒要卖好几个商品, 假设有两个 web 程序并行运行, 每一个都同时处理所有商品的销售 可以看到: web_1 对 stock_count 所做的更改已经丢失; 结果就会出现超卖现象; 在数据库领域中, 有两种方法通常被用来确保并发更新时变更不会丢失: 悲观并发控制 : 这种方法被关系型数据库广泛使用, 它假定有变更冲突可能发生, 因此阻塞访问资源以防止冲突;一个典型的例子是读取一行数据之前先将其锁住, 确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制 : Elasticsearch 中使用的这种方法假定冲突是不可能发生的, 并且不会阻塞正在尝试的操作, 不过, 如果源数据在读写当中被修改, 更最后在更新时将会失败; 应用程序接下来将决定该如何解决冲突, 例如, 可以重试更新、使用新的数据、或者将相关情况报告给用户; ES乐观并发控制 ES 是分布式的, 当文档创建、更新或删除时, 新版本的文档必须复制到集群中的其他节点, 而 Elasticsearch 也是异步和并发的, 这意味着这些复制请求是被并行发送的, 所以到达目的地时也许顺序是乱的; 因此 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本: ES的每个文档都有一个 _version (版本)号, 当文档被修改时版本号递增; ES正是使用这个 _version 号来确保变更以正确顺序得到执行 (如果旧版本的文档在新版本之后到达, 它会被直接忽略) 我们可以利用 _version 号来确保 应用中相互冲突的变更不会导致数据丢失, 通过指定想要修改文档的 version 号来达到这个目的, 如果该版本不是当前版本号, 我们的请求将会失败 测试 创建一个新文档 (当然, 响应体会告诉我们, 该文档版本号目前是1) 12345PUT /website/blog/1/_create&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;&#125; 现在假设我们想编辑这个文档: 一般是先查询,将文档数据加载到 web 表单中; 然后做一些修改, 保存新的版本 1234567891011// 查询文档, 响应体中可以看到版本号仍为1GET /website/blog/1// 尝试通过重建文档的索引来保存修改, 我们指定 version 为我们的修改会被应用的版本:// 也就是我们想要索引中的文档只有 _version 为 1 时, 本次更新才能成功; // 最后此请求成功, 并且响应体告诉我们 _version 已经递增到 2PUT /website/blog/1?version=1 &#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Starting to get the hang of this...&quot;&#125; 然而, 如果我们重新运行相同的索引请求, 仍然指定 version=1, Elasticsearch 返回 409 Conflict HTTP 响应码, 和一个如下所示的响应体: 12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;llBrPVECRFuD45NCpJaDfg&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[blog][1]: version conflict, current version [2] is different than the one provided [1]&quot;, &quot;index_uuid&quot;: &quot;llBrPVECRFuD45NCpJaDfg&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 409&#125; 更多请参考文档: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/optimistic-concurrency-control.html 文档的部分更新 - _update 之前在介绍 “更新整个文档” 时, 已经了解到, 文档是不可变的, 他们不能被修改, 只能被替换; 更新一个文档的方法是检索并修改它, 然后重新索引整个文档; 但其实使用 update API 我们还可以部分更新文档, 但是, update API 必须遵循同样的规则, 从外部来看, 我们在一个文档的某个位置进行部分更新, 然而在内部, update API 简单使用与之前描述相同的 检索-修改-重建索引 的处理过程 区别在于这个过程发生在分片内部, 这样就避免了多次请求的网络开销; 通过减少检索和重建索引步骤之间的时间, 我们也减少了其他进程的变更带来冲突的可能性。 update 请求最简单的一种形式是接收文档的一部分作为 doc 的参数, 它只是与现有的文档进行合并, 对象被合并到一起, 覆盖现有的字段, 增加新的字段 例如, 我们对 博客文章 增加字段 tags 和 views , 并修改 text 字段, 如下所示: 123456789// 测试成功:POST /website/blog/1/_update&#123; &quot;doc&quot; : &#123; &quot;tags&quot; : [ &quot;testing&quot; ], &quot;views&quot;: 0, &quot;text&quot;: &quot;哈哈&quot; &#125;&#125; 更多部分更新相关内容, 可参考手册: https://www.elastic.co/guide/cn/elasticsearch/guide/cn/partial-updates.html 未完待续~~ 取回多文档","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"03. 集群相关 (节点, 分片及角色, 扩容, 故障测试)","slug":"elasticsearch/2018-06-10-03","date":"2018-06-10T06:26:39.000Z","updated":"2018-09-28T10:11:18.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-03/","excerpt":"","text":"前言 ElasticSearch 的主旨是随时可用和按需扩容, 这里主要是说 水平(横向)扩容 — 为集群添加更多的节点将负载压力分散到这些节点中; ElastiSearch 天生就是分布式的, 它知道如何通过管理多节点来提高扩容性和可用性; 接下来将讲述如何按需配置集群、节点和分片, 并在硬件故障时确保数据安全 节点 及其 角色 节点: 一个运行中的 Elasticsearch 实例就称为一个节点; 默认情况下, es 集群中每个节点都有成为主节点的资格, 也都存储数据, 还可以提供查询服务, 这些功能是由两个属性控制的 node.master: 这个属性表示节点是否具有成为主节点的资格 (注意: 此属性的值为true, 并不意味着这个节点就是主节点; 真正的主节点, 是由多个具有主节点资格的节点进行选举产生的; 所以, 这个属性只是代表这个节点是不是具有主节点选举资格) node.data: 这个属性表示节点是否存储数据; 上面两个配置属性可以有四种组合: 节点只有成为主节点的资格(有可能成为真正的主节点), 但不会存储数据; 称为master节点 12node.master: truenode.data: false 节点没有成为主节点的资格, 即, 不参与选举, 只会存储数据; 称为data(数据)节点在集群中需要单独设置几个这样的节点负责存储数据, 后期提供存储和查询服务 12node.master: falsenode.data: true 节点既不会成为主节点, 也不会存储数据 (也叫协调节点/路由节点)这个节点的意义是作为一个client(客户端)节点, 主要是针对海量请求的时候可以进行负载均衡 12node.master: falsenode.data: false 既有成为主节点的资格, 又存储数据: 默认情况下, 每个节点都有成为主节点的资格, 也会存储数据, 还会处理客户端的请求如果这个节点被选举成了真正的主节点, 那么它除了干主节点要干的活, 还要存储数据, 这样对于这个节点的压力就比较大;elasticsearch 默认每个节点都是这样的配置, 在测试环境下这样做没问题, 但是实际工作中建议不要这样设置; 因为这样相当于 主节点 和 数据节点 的角色混合到一块了; 12node.master: falsenode.data: true - 根据前面节点及其角色的介绍, 如果尝试配置唯一的节点为 `node.master: true node.data: false`, 则节点中的所有分片都会是未分配状态, 因为节点不是数据结点, 无法存放数据 - 如果配置为 `node.master: false node.data: false`, 貌似无法启动 在生产集群中我们可以对这些节点的职责进行划分 建议集群中设置3台以上的节点作为 master节点 node.master: true node.data: false, 这些节点只负责成为主节点, 维护整个集群的状态; 再根据数据量设置一批data节点 node.master: false node.data: true, 这些节点只负责存储数据, 后期提供建立索引和查询索引的服务, 这样的话如果用户请求比较频繁, 这些节点的压力也会比较大; 所以在集群中建议再设置一批client节点 node.master: false node.data: false, 这些节点只负责处理用户请求, 实现请求转发, 负载均衡等功能;master节点: 普通服务器即可(CPU 内存 消耗一般)data节点: 主要消耗磁盘, 内存client节点: 普通服务器即可(如果要进行分组聚合操作的话, 建议这个节点内存也分配多一点) 集群 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; 当一个节点被选举成为 主节点 时, 它将负责管理集群范围内的所有变更, 例如增加、删除索引, 或者增加、删除节点等; 不过, 纯粹的主节点并不需要涉及到文档级别的变更和搜索等操作, 所以, 集群所拥有的唯一一个主节点, 如果是纯粹的主节点的话, 即使流量的增加它也不会成为瓶颈; 任何节点都可以成为主节点, 到目前为止, 我们之前的示例集群就只有一个节点, 当然, 它同时也是主节点; 作为用户, 我们可以将请求发送到集群中的任何节点, 包括主节点; 每个节点都知道任意文档所处的位置, 并且能够将我们的请求直接转发到存储我们所需文档的节点, 无论我们将请求发送到哪个节点, 它都能负责从各个包含我们所需文档的节点收集回数据, 并将最终结果返回給客户端, Elasticsearch 对这一切的管理都是透明的。 集群健康, 可以通过 GET /_cluster/health 来查看, 对于返回结果, 最需要关心的是 status 字段, 它指示着当前集群在总体上是否工作正常, 它的三种颜色含义如下: green 所有的主分片和副本分片都正常运行 yellow 所有的主分片都正常运行, 但不是所有的副本分片都正常运行 red 有主分片没能正常运行 分片 我们往 Elasticsearch 添加数据时需要用到 索引(保存相关数据的地方), 而索引实际上是指向一个或者多个物理分片的逻辑命名空间; 分片: 一个分片是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 但是, 它可能仅保存了全部文档中的一部分; Elasticsearch 是利用分片将数据分发到集群内各处的, 分片是数据的容器, 文档保存在分片内, 分片又被分配到集群内的各个节点里; 当你的集群规模扩大或者缩小时(即增加或减少节点时), Elasticsearch 会自动的在各节点中迁移分片, 使得数据仍然均匀分布在集群里。 分片有两种类型: 主分片, 副本分片 索引内任意一个文档都归属于一个主分片, 所以主分片的数目决定着索引能够保存的最大数据量; 而副本分片只是一个主分片的拷贝, 副本分片作为硬件故障时保护数据不丢失的冗余备份, 并为搜索和返回文档等读操作提供服务; 注意: 在索引建立的时候就确定了主分片数,之后无法随意修改;而副本分片数可以随时修改; 每个索引在默认情况下会被分配5个主分片, 不过你也可以在创建索引前, 先指定索引的 主分片数 和 每个主分片对应的副本分片数, 如下, 给 blogs 索引分配3个主分片 和 每个主分片分配1个副本: 1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 不过, 由于当前只有一个节点, 所以集群的健康状况为 yellow , 表示全部主分片都正常运行(集群可以正常服务所有请求), 但是副本分片没有全部处在正常状态; 实际上, 所有3个副本分片都是 unassigned —— 它们都没有被分配到任何节点, 因为当前只有一个节点, 而在同一个节点上既保存原始数据又保存副本是没有意义的; 当前这种状况, 一旦失去了唯一的节点, 也就会丢失该节点上的所有副本数据; 当前我们的集群是正常运行的, 但是在唯一的结点出现硬件故障时有丢失数据的风险; 故障转移 添加故障转移: 当集群中只有一个节点在运行时, 意味着会有一个单点故障问题 —— 没有冗余; 幸运的是, 在ES中, 我们只需再启动一个节点即可防止数据丢失; 启动第二个节点非常简单, 你可以在同一个目录内, 完全依照启动第一个节点的方式来启动一个新节点(多个节点可以共享同一个目); 只要它和第一个节点有同样的 cluster.name 配置, 它就会自动发现集群并加入到其中; 但是注意: 在不同一机器上启动节点的时候, 为了加入到同一集群, 你需要配置一个可连接到的单播主机列表 单播, 组播 单播: Elasticsearch 默认被配置为使用单播发现, 以防止节点无意中加入集群, 只有在同一台机器上运行的节点才会自动组成集群;除了同一台机器上的集群会自动发现同名节点, 使用单播, 你还可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表, 当一个节点可以联系到单播列表中的成员(节点)时, 它就会得到整个集群所有节点的状态, 然后它会联系 master 节点, 并加入集群;这意味着你的单播列表不需要包含你的集群中的所有节点, 它只是需要足够的节点, 当一个新节点联系上其中一个并且说上话就可以了;如果你使用 master 候选节点作为单播列表, 你只要列出三个就可以了, 这个配置在 elasticsearch.yml 文件中: discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;] 组播: 组播貌似仍然是作为插件提供, 并且它应该永远不要在生产环境使用, 否则可能会出现一个节点意外的加入到了你的生产环境集群中;虽然组播 本身 并没有错, 但一不小心就会导致一些愚蠢的问题, 并且导致集群变的脆弱; 最好使用单播代替组播 继续上面的第6点, 由于目前是在同一台机器上启动第二个节点, 所以不同配置单播列表, 直接启动一个节点即可; 当启动第二个节点后, 由于其使用的是和第一个节点一样的配置, 集群名也就相同, 所以会自动加入到集群; 加入集群后, blogs 索引的 3个 副本分片 将会分配到这个节点上, 这意味着当集群内任何一个节点出现问题时, 我们的数据都完好无损; 所有新索引的文档都将会保存在主分片上, 然后被并行的复制到对应的副本分片上, 这就保证了我们既可以从主分片又可以从副本分片上获得文档; 并且 cluster-health 现在展示的状态为 green, 这表示所有6个分片(包括3个主分片和3个副本分片)都在正常运行; 水平扩容 如何为我们的正在增长中的应用程序按需扩容呢? 再次尝试启动第三个新的节点, 会发现集群状态如下: 可以看到 Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点 现在每个节点上都拥有2个分片, 而不是之前的3个, 这表示每个节点的硬件资源(CPU, RAM, I/O)将被更少的分片所共享, 每个分片的性能将会得到提升 分片是一个功能完整的搜索引擎, 它拥有使用一个节点上的所有资源的能力也就是说, 目前我们这个拥有6个分片(3个主分片和3个副本分片)的索引, 可以最大扩容到6个节点, 让每个节点上只有该索引的一个分片(也可能会有其他索引的分片哦); 分片数量一定, 增加节点, 则每个分片性能将会提升, 因为被赋予的更多的硬件资源; 但是当一个索引的分片数量和集群的节点数量达到一致时, 其分片性能达到最高, 继续增加更多的副本分片是不能提高性能的, 因为每个分片从节点上获得的资源会变少, 你需要增加更多的硬件资源来提升吞吐量; 更多的扩容提升搜索性能 想要继续扩容超过6个节点, 需要先知道: 由于索引的主分片数目在索引创建时就已经确定了, 这个数目定义了这个索引能够 存储 的最大数据量也就是索引能存储的最大数据量在创建索引的时候就通过设置主分片数确定了(不过实际大小还取决于你的数据、硬件和使用场景) 由于 搜索操作 和 返回数据操作 可以同时被主分片 或 副本分片所处理, 所以当你拥有越多的副本分片时, 也将拥有越高的吞吐量 在运行中的集群上是可以动态调整副本分片数目的, 比如, 可以把副本数从默认的 1 增加到 2 : 1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; blogs 索引现在拥有9个分片: 3个主分片和6个副本分片; 这就意味着可以将集群扩容到9个节点(每个节点上只放该索引的一个分片), 相比原来3个节点时(每个节点两个分片), 虽然存储量不变, 但是集群搜索性能可以提升 3 倍; 当一个索引的分片数量和集群的节点数量达到一致时, 其分片性能将达到最高, 无法再提升分片的性能! 此时可以 通过增加更多的副本分片, 同时增加节点来提升集群的搜索性能! 集群的整体性能还是需要通过增加节点来提高! 故障测试 目前的集群状态如下: 如果关闭主节点 由于而集群必须拥有一个主节点来保证正常工作, 所以发生的第一件事情就是选举一个新的主节点; 在我们关闭主节点的同时也失去了主分片 1 和 2, 并且在缺失主分片的时候索引也不能正常工作, 如果此时来检查集群的状况, 我们看到的状态将会为 red: 不是所有主分片都在正常工作 幸运的是, 在其它节点上存在着主节点上这两个主分片的完整副本, 所以新的主节点会立即将这两个主分片在 另外两个节点上 对应的副本分片提升为主分片, 此时集群的状态将会为 yellow: 这个提升主分片的过程是瞬间发生的, 如同按下一个开关一般此时, 虽然我们又拥有所有的三个主分片, 但是由于之前设置了每个主分片需要对应2份副本分片, 而此时只有两个几点, 只存在一份副本分片, 所以集群不能为 green 的状态;注意连接的端口: 如果重新启动之前关闭的节点, 集群可以将缺失的副本分片再次进行分配, 如果它依然拥有着之前的分片, 它将尝试去重用它们, 同时仅从主分片复制发生了修改的数据文件; 配置相关 注意: discovery.zen.minimum_master_nodes ??","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"02. 适应一下ES","slug":"elasticsearch/2018-06-08-02","date":"2018-06-08T13:23:07.000Z","updated":"2018-09-18T07:44:47.000Z","comments":true,"path":"2018/06/08/elasticsearch/2018-06-08-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/08/elasticsearch/2018-06-08-02/","excerpt":"","text":"文档该章节比较基础, 主要是为了对 Elasticsearch 有一个基本印象, 通过对雇员文档的基本操作, 了解 索引、搜索 及 聚合 等基础概念;可能有会遇到有些语句在5.X中无法运行, 比如 进行聚合操作时提示 Fielddata is disabled on text fields by default, 可参考此文章","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"01. Elasticsearch 入门 及 简单安装","slug":"elasticsearch/2018-06-08-01","date":"2018-06-08T06:24:25.000Z","updated":"2018-09-20T07:06:25.000Z","comments":true,"path":"2018/06/08/elasticsearch/2018-06-08-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/08/elasticsearch/2018-06-08-01/","excerpt":"","text":"主要是简单过一下 Elasticsearch 权威指南, 做一下学习记录~~ 简介 Elasticsearch 是一个基于 Lucene 的开源, 高性能, 全文检索 和 分析引擎, 可以快速且近实时地存储, 检索以及分析海量数据; 当然, ES 不仅仅是 Lucene, 也不仅仅只是一个全文搜索引擎, 它可以被下面这样准确的形容: 一个分布式近实时分析搜索引擎; 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级结构化或非结构化数据; 近实时搜索: 从文档索引到可以被检索只有轻微延时, 约1s RESTful API: ES 建立在全文搜索引擎 Lucene 之上, 通过简单的 RESTful API 来隐藏 Lucene 的复杂性, 从而让全文搜索变得简单, 各种语言的客户端甚至命令行都可以与之交互; 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引; 使用案例 在微服务架构下的多数据源聚合列表页(一个页面中的数据来自多个服务, 且筛选条件也涉及到多个服务中的数据字段), 如果用传统数据库解决该问题, 会大费周折, 并且效果并不好, 而如果使用 ES 来作数据聚合服务, 效果就比较清晰明了了; 您想要去收集日志或交易数据, 并且还想要去分析和挖掘这些数据来找出趋势, 统计, 或者异常现, 在这种情况下, 您可以使用 Logstash(Elasticsearch/Logstash/Kibana) 技术栈中的一部分, 来收集, 聚合, 以及解析数据, 然后让 Logstash 发送这些数据到 Elasticsearch; 如果这些数据存在于 Elasticsearch 中, 您就可以执行搜索和聚合以挖掘出任何您感兴趣的信息; GitHub 使用 Elasticsearch 对1300亿行代码进行查询; …… 版本选择 ES 的版本变更比较快, 目前(06/2018)为止, Elasticsearch已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X, 而且中文文档进度也比较滞后, 这也是让很多兄弟比较头疼的事情; 其实可以根据公司所选的云服务上 ES版本 来决定你的学习版本 (当前阿里云的Elasticsearch云服务为5.5.3, 因此此处也是针对5.X版本进行学习调研); 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (这里使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本集集群相关的信息, 但这不是图形化的界面, 操作起来不是很方便, 如果希望能有一个可视化的环境来操作它, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 小结到此为止, 正常学习的Es环境已经安装完毕, 不要纠结这些服务的开机启动, 调优配置, 集群, 高可用, 监控……骚年, 暂时先让它能跑起来就行!","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"28. 隔离级别 与 锁","slug":"mysql/2017-09-03-mysql-28","date":"2017-09-03T06:20:52.000Z","updated":"2018-09-03T03:54:23.000Z","comments":true,"path":"2017/09/03/mysql/2017-09-03-mysql-28/","link":"","permalink":"http://blog.renyimin.com/2017/09/03/mysql/2017-09-03-mysql-28/","excerpt":"","text":"前言 之前几篇博文已经介绍了Mysql事务, 高并发下事务将会面对的问题 及 MySQL的解决方案; MySQL主要采用 事务隔离性中的4种隔离级别 结合 MVCC机制 来进行解决; 而事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略; 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的; READ UNCOMMITTED 未提交读READ COMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料 《高性能MySQL》 MySQL官方文档 美团技术博客 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"27. 幻读, 快照读(snapshot read), 当前读 (current read)","slug":"mysql/2017-09-02-mysql-27","date":"2017-09-02T11:25:07.000Z","updated":"2018-09-01T14:02:47.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-27/","excerpt":"","text":"RR + MVCC 虽然解决了 幻读 问题, 但要注意, 幻读针对的是读操作(对于其他操作就不一样了); 演示 打开 两个客户端 1,2 确保隔离级别为默认级别RR, 提供语句: 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 123456789101112mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) 回到 客户端2 的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了 select 幻读)!! 12345678910111213141516171819202122mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 但如果尝试在客户端2的事务中执行 insert/delete/update , 却会发现此类操作都可以感知到客户端1提交的新数据 123mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; 小结 虽然发现已经克服了幻读问题; 但当 在客户端2事务中 insert 插入一条id为4的新数据, 却发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 — 一致性非阻塞读中的一段介绍 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。如果一个事务去更新或删除其他事务提交的行, 则那些更改对当前事务就变得可见;但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 也就是RR隔离级别, 在同一事务中多次读取的话, 对 select 克服了 幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念: 当前读 和 快照读 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的 select 都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 快照读： 是通过MVVC(多版本控制)和 undo log 来实现的, 常见语句如下(貌似就是常见的悲观锁么): 1简单的select操作 (不包括: `select ... lock in share mode`, `select ... for update`) 而 当前读 根本不会创建任何快照, insert, update, delete都是当前读, 所以这几个操作会察觉到其他事务对数据做的更改(而普通select是察觉不到的): 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MySQL 高并发下常见的事务问题","slug":"mysql/2017-09-02-mysql-26","date":"2017-09-02T06:56:32.000Z","updated":"2018-09-01T14:02:40.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-26/","excerpt":"","text":"前言上一篇MySQL事务简介中对MySQL事务的 基本概念 及 特性 做了简单介绍; 接下来会分析在实际生产环境中面对高并发场景时, 事务会出现的一些常见问题; 高并发事务问题在并发量比较大的时候, 很容易出现 多个事务并行 的情况; 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境 过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以 最终导致了一些问题的出现; 脏读 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读 因为 事务A 此时读取到的是 并行事务B 尚未最终持久化的数据 (该数据还不具备事务的 持久性) 事务B 最终可能会因为其事务单元内部其他后续操作的失败 或者 系统后续突然崩溃等原因, 导致事务B最终整体提交失败而回滚, 那么最终 事务A 之前拿到就是 脏的数据 了(当然, 如果 事务A 在后续操作中继续读取的话, 无论事务B是否结束, 其每次的更新操作, 事务A都会及时读到新数据, 只不过这同时涉及到了下一个讨论的 不可重复读问题, 暂时可以不了解) 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身隔离性解决了脏读问题 : READ COMMITED 或 以上隔离级别(RC+); READ COMMITED 隔离级别保证了: 在事务单元中, 某条语句执行时, 只有已经被其他事务提交的持久性落地数据, 才对该语句可见; 不可重复读 之前 脏读问题 的解决了, 仅仅只意味着事务单元中的每条语句读取到的数据都是 具备持久性的落地数据而已; 之前在讨论脏读问题时, 有个问题也同时存在着, 那就是一个事务单元中 不可重复读 的问题; 显然, RC 隔离级别只解决了 脏读的问题 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了 解决方案 : RR+ 在MySQL中, 事务已经用自身隔离性解决了 不可重复读 问题 — REPEATABLE READ 或 以上隔离级别(RR+); REPEATABLE READ 级别保证了:在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的; ( READ COMMITED )在事务中, 多次读取同一个数据(在两次读取操作之间, 无论数据被 提交 多少次(即无论落地过多少遍), 每次读取的结果都应该是和事务中第一次读取的结果一样; 幻读 可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 不可重复读 和 幻读 这两个概念容易搞混 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录); 解决方案: RR + MVCC 其实对于 幻读 问题, 在Mysql的InnoDB存储引擎中, 是通过事务的 RR + MVCC机制 进行解决的;当然, 这里的幻读不涉及 具有当前读能力的那些语句; (也就是说只是解决幻读, 所谓幻写之类的就不在范围内了) 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 之所以 不可重复读 和 幻读 容易搞混, 可能是因为: 在mysql中, 由于默认就是RR隔离级别下, 该隔离级别已经解决了幻读, 所以无法模拟出幻读的场景; 而 退回到 RC隔离级别 的话, 虽然 幻读 和 不可重复读 都会出现, 但由于现象都是两次读取结果不一样, 容易分辨不出! 想了解更多, 可以参考下一篇幻读的延伸 高并发事务问题 之 更新丢失最后聊一下高并发事务的另一个问题, 也是最常遇到的问题: 丢失更新问题; 该问题和之前几个问题需要区分开: 该问题需要我们自己来解决;更新丢失问题分为两类 第一类丢失更新(回滚覆盖)简介 事务A 回滚时, 将 事务B 已经提交的数据覆盖了 需要注意的是: 这种情况在Mysql中不会出现; RU 级别演示 对于InnoDB事务的最低隔离级别 READ UNCOMMITED, 并行事务B的未提交数据都可以读到, 更别说已提交数据了 (所以回滚也会回滚到事务B提交的最新数据) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RC 级别演示 对于 READ COMMITTED: 在事务B提交之后, 事务A在T3阶段是可以select(快照读)到事务B最终提交的数据的, 更别说update(当前读)到了, 所以事务A最终的Rollback其实也是基于事务B提交后的数据的 (关于这里提到的快照读和当前读, 下一篇会介绍) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RR 级别演示 对于 REPEATABLE READ 可重复读, 事务A在T3阶段虽然select不到事务B最终提交的数据(快照读), 但是可以update(当前读)到事务B最终提交的数据的 (注意: RR与RC虽然都会有快照读, 但是快照读的结果却不一致, 其实是因为两者的MVCC机制快找时机不同导致的, 后面会讲解) 语句如下: 1234567SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age+10 where id=2;rollback; 12345SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age-15 where id=2;commit; SERIALIZABLE 演示 SERIALIZABLE 串行化: 读写都加锁, 最容易出现死锁, 所以也不会出现第一类丢失更新的问题, 直接就死锁了 语句如下: 123456SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;SELECT @@SESSION.tx_isolation;begin;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age -15 where id=2;commit; 第二类丢失更新(提交覆盖) 直接上图 另外, 这里可以解释一下为什么 SERIALIZABLE级别 通常不会不被采用 其实 SERIALIZABLE 虽然做了串行化, 其实也就是对读写都加了锁, 但一旦事务并行, 如果将判断库存的读操作放在事务内就很容易会死锁而放在事务外, 由于更新操作仍然会依据上一个查询的结果, 所以仍然是避免不了第二类丢失更新问题的, 会造成超卖等问题; SERIALIZABLE 的串行化本身也太低效 另外, 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 解决第二类丢失更新的方案: 乐观锁 (在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制) 悲观锁 参考资料: 《高性能MySQL》 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"25. MySQL 事务简介","slug":"mysql/2017-08-27-mysql-25","date":"2017-08-27T11:31:07.000Z","updated":"2018-09-01T14:02:34.000Z","comments":true,"path":"2017/08/27/mysql/2017-08-27-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/08/27/mysql/2017-08-27-mysql-25/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的工作单元, 在这个独立的工作单元中, 可以有一组操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败 随处可见的例子: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，就需要打包在一个事务中作为 独立的工作单元 来执行。并且在 这个独立工作单元中的三个操作, 只要有任何一个操作失败, 则整体就应该是失败的, 那就必须回滚所有已经执行了的步骤; 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚 (这也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元;对于一个事务来说, 不能只成功执行其中的一部分操作, 整个事务中的所有操作要么全部成功提交, 要么有操作失败导致所有操作全部回滚, 这就是事务的原子性。 Consistency 一致性此一致性非彼一致性 你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性保证的是 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态, 而不是分布式中提到的数据一致性; 比如之前转账的例子: 转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15) 转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115) 转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取 Iron Man账户 的余额, 那么这个程序读到的应该是500才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务 隔离性 的四个 隔离级别, 到时候就知道这里为什么说 通常来说 ; (确实有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读)(注意: 和RR一样都采用了MVCC机制, 但与RR级别主要区别是快照时机不同, 暂时可不必了解, 后面文章会详解) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 只有该隔离级别才会读写都加锁 Durability 持久性 一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久保存到数据库中; 所谓永久, 也只是主观上的永久, 可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方; 最后更新时间 2018/09/01","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]}]}