{"meta":{"title":"Rymuscle's","subtitle":null,"description":null,"author":"Rymuscle","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"单进程单线程高并发?","slug":"redis/2018-08-24-redis-04","date":"2018-08-24T08:22:51.000Z","updated":"2018-08-24T08:23:14.000Z","comments":true,"path":"2018/08/24/redis/2018-08-24-redis-04/","link":"","permalink":"http://blog.renyimin.com/2018/08/24/redis/2018-08-24-redis-04/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"事务","slug":"redis/2018-08-20-redis-03","date":"2018-08-20T09:32:27.000Z","updated":"2018-08-20T10:42:56.000Z","comments":true,"path":"2018/08/20/redis/2018-08-20-redis-03/","link":"","permalink":"http://blog.renyimin.com/2018/08/20/redis/2018-08-20-redis-03/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"持久化分析","slug":"redis/2018-08-20-redis-02","date":"2018-08-20T09:32:27.000Z","updated":"2018-08-20T10:42:56.000Z","comments":true,"path":"2018/08/20/redis/2018-08-20-redis-02/","link":"","permalink":"http://blog.renyimin.com/2018/08/20/redis/2018-08-20-redis-02/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"日志","slug":"redis/2018-08-20-redis-01","date":"2018-08-20T09:32:11.000Z","updated":"2018-08-20T12:24:13.000Z","comments":true,"path":"2018/08/20/redis/2018-08-20-redis-01/","link":"","permalink":"http://blog.renyimin.com/2018/08/20/redis/2018-08-20-redis-01/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"postman 中 form-data、x-www-form-urlencoded、raw、binary的区别","slug":"http/2018-08-20-postman-01","date":"2018-08-20T03:21:09.000Z","updated":"2018-08-21T06:09:05.000Z","comments":true,"path":"2018/08/20/http/2018-08-20-postman-01/","link":"","permalink":"http://blog.renyimin.com/2018/08/20/http/2018-08-20-postman-01/","excerpt":"","text":"form-data 就是http请求中的 multipart/form-data, 它会将表单的数据处理为一条消息, 以标签为单元, 用分隔符分开; 既可以上传键值对, 也可以上传文件, 当上传的字段是文件时, 会有Content-Type来说明文件类型; content-disposition, 用来说明字段的一些信息; 由于有 boundary 隔离, 所以 multipart/form-data 既可以上传文件, 也可以上传键值对, 它采用了键值对的方式, 所以可以上传多个文件; 测试: 发送请求 查看请求头 x-www-form-urlencoded 就是http请求中的 application/x-www-from-urlencoded, 会将表单内的数据转换为键值对, 比如, name=java&amp;age=23 测试: 发送请求 查看请求头 注意, url中的gender参数, 也只能用$_GET方法来获取参数(不能用$_post) raw 可以上传任意格式的文本, 可以上传text、json、xml、html等 binary 相当于 Content-Type:application/octet-stream , 从字面意思得知, 只可以上传二进制数据, 通常用来上传文件, 由于没有键值, 所以, 一次只能上传一个文件;","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"","slug":"nginx/2017-04-22-03","date":"2018-08-01T03:02:02.000Z","updated":"2018-08-01T03:02:02.000Z","comments":true,"path":"2018/08/01/nginx/2017-04-22-03/","link":"","permalink":"http://blog.renyimin.com/2018/08/01/nginx/2017-04-22-03/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"40.  Linux 负载相关基本命令","slug":"linux/2018-07-02-40","date":"2018-07-02T11:26:17.000Z","updated":"2018-08-07T03:28:57.000Z","comments":true,"path":"2018/07/02/linux/2018-07-02-40/","link":"","permalink":"http://blog.renyimin.com/2018/07/02/linux/2018-07-02-40/","excerpt":"","text":"uptime 命令 测试 12[work@trade-sandbox ~]$ uptime 16:47:10 up 91 days, 23:01, 10 users, load average: 1.55, 1.27, 1.45 说明: 123416:47:10 // 系统当前时间up 91 days, 23:01, // 主机已运行时间,时间越大,说明你的机器越稳定10 users //用户连接数, 是总连接数而不是用户数load average: 1.55, 1.27, 1.45 // 系统平均负载,统计最近1,5,15分钟的系统平均负载 w命令 w 命令用于显示已经登陆系统的用户列表, 并显示用户正在执行的指令; 执行这个命令可得知目前登入系统的用户有哪些人, 以及他们正在执行的程序; 单独执行w命令会显示所有的用户, 您也可指定用户名称, 仅显示某位用户的相关信息; 选项 12345-h：不打印头信息；-u：当显示当前进程和cpu时间时忽略用户名；-s：使用短输出格式；-f：显示用户从哪登录；-V：显示版本信息。 测试: (第一行其实就是 uptime 命令的执行效果) 12345678910111213[work@trade-sandbox ~]$ w 16:41:35 up 91 days, 22:55, 10 users, load average: 0.36, 0.87, 1.40USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATwork pts/0 10.51.64.132 10:50 5:49m 0.03s 0.00s tail -f push_remind-2018-07-19.logwork pts/2 10.51.64.132 Wed15 0.00s 0.01s 0.00s wwork pts/3 10.51.64.132 Wed17 23:08m 1:39 1:39 -bashwork pts/4 10.51.64.132 Wed17 23:10m 3:39 3:39 topwork pts/5 10.51.64.132 Wed17 2:16m 0.20s 0.00s tail -f solr.request-2018-07-19.logwork pts/6 10.51.64.132 14:22 1:46m 0.00s 0.00s -bashwork pts/7 10.162.220.93 14:34 36:17 0.05s 0.05s -bashwork pts/8 10.162.220.93 14:42 1:51m 0.02s 0.00s tail -f AgainPushToSolr-2018-07-19.logwork pts/9 10.162.220.93 14:43 30:45 0.07s 0.07s -bashwork pts/10 10.51.64.132 15:21 34:16 0.07s 0.07s -bash top top 命令能够清晰的展现出系统的状态，而且它是实时的监控，按q退出; 测试 12345678910111213141516[work@trade-sandbox ~]$ toptop - 19:16:30 up 92 days, 1:30, 6 users, load average: 0.21, 0.10, 0.09Tasks: 265 total, 1 running, 256 sleeping, 0 stopped, 8 zombieCpu(s): 3.0%us, 1.8%sy, 0.0%ni, 94.9%id, 0.3%wa, 0.0%hi, 0.0%si, 0.0%stMem: 8057768k total, 5748564k used, 2309204k free, 812724k buffersSwap: 0k total, 0k used, 0k free, 548248k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4497 work 20 0 253m 22m 10m S 1.0 0.3 0:06.57 php 3911 work 20 0 253m 22m 10m S 0.7 0.3 0:06.99 php 3916 work 20 0 253m 22m 10m S 0.7 0.3 0:06.98 php 6738 root 20 0 263m 16m 3624 S 0.7 0.2 163:41.71 ilogtail13226 work 20 0 253m 22m 10m S 0.7 0.3 0:00.77 php13714 work 20 0 253m 22m 10m S 0.7 0.3 0:00.30 php20559 work 20 0 255m 25m 10m S 0.7 0.3 1:04.16 php20562 work 20 0 255m 25m 10m S 0.7 0.3 1:04.10 php 说明 第一行仍然是系统运行时间 Tasks行: 展示了目前的进程总数, 处于运行状态的进程数, 处于睡眠状态的进程数, 处于停止状态的进程数, 要注意zombie,表示僵尸进程,不为0则表示有进程出现问题 Cpu(s)行:3.0%us 用户空间占用CPU百分比,1.0%sy 内核空间占用CPU百分比,0.0%ni 用户进程空间内改变过优先级的进程占用CPU百分比,94.9%id 空闲CPU百分比,0.3%wa 等待输入输出的CPU时间百分比,0.0%hi ,0.0%si ,0.1%st Mem 行:4147888k total 物理内存总量,2493092k used 使用的物理内存总量,1654796k free 空闲内存总量,158188k buffers 用作内核缓存的内存量 Swap 行:5144568k total 交换区总量 ,56k used 使用的交换区总量 ,5144512k free 空闲交换区总量 ,2013180k cached 缓冲的交换区总量 负载值分析 load average: 0.21, 0.10, 0.09 表示的是 1分钟, 5分钟, 15分钟的CPU负载情况 (一般5和15分钟才具有参考意义) 对于单核处理器来说(值的大小和cpu的核数有关系), 可以把值分为3个级别 小于1.0 如果值小于1, 那么说明系统cpu处理很流畅, 不会出现等待, 堵塞 等于1.0 说明cpu能力刚刚满负荷 大于1.0 说明cpu已经超负荷, 进程处理需要等待了, 效率低下 对于多核处理器说(假设双核), 等于说处理能力增加了一倍, 比较的值就是2了, 小于2.0才不用担心 如下是四核服务器, 所以比较的值就是4123456[work@trade-sandbox ~]$ cat /proc/cpuinfo | grep &apos;model name&apos;model name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz[work@trade-sandbox ~]$ 也可以在运行top命令之后, 直接按 1, 查看 各cpu的负载 (再按1, 又回到现实CPU总负载) 123456789101112131415[work@trade-sandbox ~]$ toptop - 20:38:47 up 92 days, 2:53, 7 users, load average: 1.31, 0.84, 0.53Tasks: 245 total, 1 running, 237 sleeping, 0 stopped, 7 zombieCpu0 : 8.4%us, 2.0%sy, 0.0%ni, 89.0%id, 0.3%wa, 0.0%hi, 0.3%si, 0.0%stCpu1 : 3.6%us, 2.3%sy, 0.0%ni, 93.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.3%stCpu2 : 0.3%us, 0.7%sy, 0.0%ni, 99.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu3 : 1.3%us, 0.0%sy, 0.0%ni, 98.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 8057768k total, 5505852k used, 2551916k free, 816568k buffersSwap: 0k total, 0k used, 0k free, 470568k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1091 root 20 0 2446m 65m 3200 S 4.7 0.8 1653:45 java 1484 root 20 0 212m 14m 1100 S 0.7 0.2 550:15.91 supervisord 3911 work 20 0 253m 22m 10m S 0.7 0.3 0:41.20 php 4497 work 20 0 253m 22m 10m S 0.7 0.3 0:40.92 php","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"33. 单机部署集群 -- 镜像模式 (x-ha-policy)","slug":"rabbitmq/2018-06-29-rabbitmq-33","date":"2018-06-29T13:28:16.000Z","updated":"2018-08-04T13:04:36.000Z","comments":true,"path":"2018/06/29/rabbitmq/2018-06-29-rabbitmq-33/","link":"","permalink":"http://blog.renyimin.com/2018/06/29/rabbitmq/2018-06-29-rabbitmq-33/","excerpt":"","text":"前言 上一篇在学习普通模式的集群时, 已经知道在该模式下, 队列只存活于集群中的一个节点上(在RabbitMQ2.6.0之前, 这也是唯一的选择); 在RabbitMQ2.6.0时, RabbitMQ团队带来了内建的双活冗余选项: 镜像队列; 像普通队列那样, 镜像队列的主拷贝仅存在于一个节点(主队列, master)上, 但与普通队列的不同点是, 镜像节点在集群中的其他节点上拥有从队列(slave拷贝); 一旦队列主节点不可用, 最老的从队列将会被选举为新的主队列; (这貌似就是探索集群时一直寻找的高可用) 声明并使用镜像队列 你的应用程序并不使用 rabbitmqctl 来定义镜像(mirrored)队列; 声明镜像队列就像声明普通队列一样, 不过, 你需要传入一个额外的参数 x-ha-policy 参数到 queue.declare 调用中; 测试:","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"32. 单机部署集群 -- 普通模式","slug":"rabbitmq/2018-06-26-rabbitmq-32","date":"2018-06-26T07:28:16.000Z","updated":"2018-07-27T07:49:14.000Z","comments":true,"path":"2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","link":"","permalink":"http://blog.renyimin.com/2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","excerpt":"","text":"前言当你需要在生产环境中部署RabbitMQ时, 需要注意的是, 单实例在生产环境虽然部署起来很容易, 但是当你的rabbitmq服务器遇到内存崩溃或者断电的情况时, 这款高性能的产品就要成为你的耻辱了, 将会为你造成极大的问题!因此你需要将你的RabbitMQ变成高可用的才行; 内建集群简介 RabbitMQ最优秀的功能之一就是其内建集群, 这款消息队列中间件产品本身是基于Erlang编写, Erlang语言天生具备分布式特性(通过同步Erlang集群各节点的magic cookie来实现), 因此, RabbitMQ天然支持Clustering, 这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。 RabbitMQ内建集群用来完成两个目标: 允许生产者和消费者在RabbitMQ节点崩溃的情况下继续运行;你可以失去一个RabbitMQ节点, 同时客户端可以重新连接到集群中的任何其他节点并继续生产或者消费消息, 就像什么都没有发生一样; 通过增加更多的节点来线性扩展消息吞吐量;如果RabbitMQ正疲于应对庞大的消息通信量的话, 那么线性地增加更多的节点则会增加更多性能; 集群的类型Rabbit集群模式大概分为两种: 普通模式、镜像模式; 本篇主要介绍普通模式 普通模式 普通模式(也就是默认的集群模式), 对于该集群模式, 当你将多个节点组合成集群后, 需要注意的是: 不是每一个节点都有所有队列的完全拷贝 在非集群的单一节点中, 所有关于队列的信息(元数据、状态、内容)都完全存储在该节点上; 但是如果在普通集群模式下创建队列的话, 集群只会在当前节点而不是所有节点上创建完整的队列信息(元数据、状态、内容); 而其他非所有者的节点, 只知道队列的元数据和指向该队列存在的哪个节点的指针; 因此当集群中队列所有者的节点崩溃时, 该节点的队列和关联的绑定就都消失了, 并且附加在这些队列上的消费者就会无法获取其订阅的信息, 并且生产者也无法将匹配该队列绑定信息的消息发送到队列中; 接下来需要了解的一个问题是: 为什么在默认的集群模式下, RabbitMQ不将队列内容和状态复制到所有的节点上? 其实有两个原因 存储空间: 如果每个集群节点都拥有所有Queue的完全数据拷贝, 那么每个节点的存储空间会非常大, 集群的消息积压能力会非常弱(无法通过集群节点的扩容提高消息积压能力); 性能: 消息的发布者需要将消息复制到每一个集群节点, 对于持久化消息来说, 网络和磁盘的负载都会明显增加, 最终只能保持集群性能平稳(甚至更糟); 所以, 通过设置集群中的唯一节点来负责特定队列, 只有该负责节点才会因队列消息而遭受磁盘活动的影响所有其他节点需要将接受到的该队列的消息传递给该队列的所有者节点, 因此, 往RabbitMQ集群添加更多的节点意味着你将拥有更多的节点来传播队列, 这些新增节点为你带来了性能的提升; 但是有人可能会想: 是否可以让消费者重新连接到集群上, 这样不就可以重新创建队列了? 但需要注意的是: 因为一般如果我们的队列设置的是持久化的, 而在该队列的主节点挂掉之后, 重新连接到队列时, 一般也不会修改队列的持久化属性; 这就需要注意一个问题, 仅当你之前创建的队列为非持久化时, 你才可以重新创建该队列为持久化, 因为这是为了保证你之前的持久化队列节点在重新被恢复启动后, 其中的消息还会被恢复, 而如果你创建一个新的持久化队列, 如果覆盖之前的持久化队列, 那消息不就丢了!!所以如果之前是持久化队列, 而且还是以持久化的方式创建该队列, 集群就会报错误, 后面会进行测试! 了解内部元数据RabbitMQ内部会始终同步四种类型的内部元数据: 队列元数据: 队列名称和它的属性 (是否可持久化, 是否自动删除); 交换器元数据: 交换器名称、类型和属性 (可持久化等); 绑定元数据: 一张简单的表格展示了如何将消息路由到队列; vhost元数据: 为vhost内的队列、交换器和绑定提供命名空间和安全属性; 内存or磁盘节点 每个Rabbitmq节点, 不管是单一节点系统或者是庞大集群的一部分, 要么是内存节点(RAM node), 要么是磁盘节点(disk node): 内存节点将所有的队列、交换器、绑定、用户、权限和vhost的元数据定义都仅存储在内存中; 而磁盘节点则将元数据存储在磁盘中; 非集群单一节点: 在单一节点的非集群环境中, RabbitMQ默认会将元数据都存放在内存中; 但是, 会将标记为可持久化的队列和交换器(以及它们的绑定)存储到硬盘上, 存储到硬盘上可以确保队列和交换器在重启Rabbitmq节点后重新被创建; 集群节点类型 当你引入Rabbitmq集群后, RabbitMQ需要追踪的元数据类型包括: 集群节点位置, 以及节点与已记录的其他类型的元数据的关系; 集群对元数据的存储提供了选择:将元数据存储到磁盘上 (集群中创建节点时的默认设置) 或者 存储到RAM内存中 注意, RabbitMQ要求在集群中至少要有一个磁盘节点, 所有其他节点可以是内存节点。当节点加入或者离开集群时, 它们必须要将变更至少通知到一个磁盘节点; 如果只有一个磁盘节点, 而不凑巧的是它有刚好崩溃, 那么集群虽然可以继续路由消息, 但是不能做一下操作: 创建队列 创建交换器 创建绑定 添加用户 更改权限 添加或删除集群节点 集群配置钱准备 在开始配置集群前, 首先要确保现存的Rabbitmq没有运行, 因此需要关闭节点 (本机为mac, 关闭操作如下) 123renyimindeMacBook-Pro:~ renyimin$ brew services stop rabbitmqStopping `rabbitmq`... (might take a while)==&gt; Successfully stopped `rabbitmq` (label: homebrew.mxcl.rabbitmq) 可以发现一个问题, 就是停止Rabbitmq服务之后, 貌似 RabbitMQ Management 的Web UI界面还是可以正常打开运行; 所以正确的关闭节点貌似是 rabbitmqctl stop 开始配置集群前需要注意: 通常来讲, 使用 rabbitmq-server 命令启动节点之后就大功告成了, 但是如果不用额外参数的话, 该命令会使用默认的节点名称 rabbit 和监听端口 5672;所以如果你想用该命令在一台机器上同时启动3个节点的话, 那么第2，3个节点都会因为节点名称和端口号冲突而导致启动失败; 因此, 为了在本机正常启动5个节点, 可以在每次调用 rabbitmq-server前, 通过设置环境变量 RABBITMQ_NODENAME, RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口号!在此处做实验时, 将会采用 rabbit, rabbit_1,rabbit_2 命名节点名; 端口号为5612，5613, 5614 注意, 到目前为止, 虽然尚未谈论RabbitMQ的插件, 不过你有可能已经启用了一部分插件了; 如果确实如此的话, 你需要在启动集群节点前将插件禁用!这是因为像 RabbitMQ Management 这样的插件会监听专门的端口来提供服务(例如 Management 插件的 Web UI), 目前还没讲到如何设置插件监听不同的端口, 所以当第二个节点和之后的节点启动了它们的插件后, 就会和第一个启动节点的c插件相冲突, 然后节点就都崩溃了;可以先不禁用插件, 这样在启动多个节点时, 可以根据报错一个个关闭插件也可以; (rabbitmq-plugins disable 插件名) RabbitMQ集群的搭建 启动节点 注意: 启动的时候, 直接加上 -detached 参数的话, 可能会有些报错信息比如 error : cannot_delete_plugins_expand_dir, 这就是因为需要使用root权限才可以, 你可以使用 pa aux | grep rabbitmq 查看是否三个进程都成功启动了 注意: 启动时, 貌似不能像书上那样, RABBITMQ_NODENAME 只设置节点名, 最好设置上节点host 如下: 1234567renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit_1@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5674 RABBITMQ_NODENAME=rabbit_2@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ 然后可以查看个节点状态 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost status 现在启动了三个节点 rabbit, rabbit_1, rabbit_2, 并且每个节点都会有系统的主机名在@后; 但是每个节点仍然是独立节点, 拥有自己的元数据, 并且不知道其他节点的存在; 集群中的第一个节点rabbit,将初始元数据带入集群, 并且无需被告知加入; 而第二个和之后的节点, 将加入第一个节点rabbit, 并获取rabbit节点的元数据; 要将rabbit_1和rabbit_2节点加入rabbit, 要停止该Erlang节点上运行的rabbitmq应用程序, 并重设它们的元数据, 这样它们才可以被加入rabbit节点并且获取rabbit节点的元数据; 可以使用 rabbitmqctl 来完成这些工作 停止rabbit_1节点上的应用程序 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost stop_appStopping rabbit application on node rabbit_1@renyimindeMacBook-Pro ... 重设rabbit_1节点的元数据和状态为清空状态 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost resetResetting node rabbit_1@renyimindeMacBook-Pro ... 这样你就准备好了一个 停止运行的并且清空了的 rabbit 应用, 现在可以准备好将其加入到集群中的第一个节点rabbit中:注意书上的 cluster 命令好像已经不用了, 换成了 join_cluster 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost join_cluster rabbit@localhostClustering node rabbit_1@localhost with rabbit@localhostrenyimindeMacBook-Pro:~ renyimin$ 最后, 可以重启第二个节点的应用程序 1234renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_appStarting node rabbit_1@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ 节点rabbit_2加入集群的步骤同上, 具体操作如下: 12345678910111213renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_appStarting node rabbit_1@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_appStopping rabbit application on node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost resetResetting node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost join_cluster rabbit@localhostClustering node rabbit_2@localhost with rabbit@localhostrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_appStarting node rabbit_2@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ 查看集群状态, 可以在任意一个节点通过 rabbitmqctl cluster_status 进行查看 123456789101112131415161718192021222324252627282930renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_statusCluster status of node rabbit@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_statusCluster status of node rabbit_1@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost cluster_statusCluster status of node rabbit_2@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit_2@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ 注意: 上面使用比较多的 rabbitmqctl 命令的关键参数是 -n, 这会告诉rabbitmqctl命令, 你想在指定节点而非默认节点rabbit@上执行命令; 记住, Erlang节点间通过Erlang cookie的方式来允许互相通信。因为rabbitmqctl使用Erlang OPT通信机制来和Rabbit节点通信, 运行rabbitmqctl的机器和所要连接的Rabbit节点必须使用相同的Erlang cookie, 否则你会得到一个错误;当然, 上面的集群是在本机做伪集群, Erlang cookie 自然也都是一致的! 将节点从集群中删除 forget_cluster_node 1234567renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_1@localhostRemoving node rabbit_1@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_2@localhostRemoving node rabbit_2@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_3@localhostRemoving node rabbit_3@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ 集群节点类型设置与修改 可以在将节点加入集群时, 设定节点的类型 (参考) 比如 rabbitmqctl -n rabbit_3@localhost join_cluster --ram rabbit@localhost 之前已经通过 rabbitmqctl cluster_status 查看了集群的状态, 里面比较重要的是 nodes 部分 下面告诉你有三个节点加入了集群, 并且三个节点都是 disc 磁盘节点! 1234567[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit_2@localhost,[]&#125;]&#125;] running_nodes 部分告诉你集群中的哪些节点正在运行; 现在你可以连接到这三个running_nodes中的任何一个, 并且开始创建队列, 发布消息或者执行任何其他AMQP任务; 你也可以对节点类型进行修改, 如下将rabbit_2节点类型修改为内存节点 (注意: 修改节点类型, 需要先停止节点应用) 1234567891011121314151617181920renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_appStopping rabbit application on node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost change_cluster_node_type ramTurning rabbit_2@localhost into a ram noderenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_appStarting node rabbit_2@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_statusCluster status of node rabbit_1@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost]&#125;, &#123;ram,[rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ 测试 运行生产者代码, 在集群中的rabbit节点中创建持久化队列 初始集群状态123456789renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_statusCluster status of node rabbit@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindemacbook-pro.rrcoa.com&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;]&#125;] - 运行生产者, 查看创建的队列(已经有一条msg放入队列中) 123456789101112131415161718192021renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ kill掉该持久化队列localClusterQueue所在的主节点rabbit 查看节点进程 12345renyimindeMacBook-Pro:~ renyimin$ ps aux | grep rabbitmqroot 2656 0.4 0.3 4150148 58156 ?? S 三01下午 5:09.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5672&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost&quot; -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672 -noshell -noinputrenyimin 28537 0.0 0.0 2423384 232 s007 R+ 3:12下午 0:00.00 grep rabbitmqroot 72516 0.0 0.5 4143168 79400 ?? S 1:03下午 0:16.71 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_2@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5674&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost&quot; -kernel inet_dist_listen_min 25674 -kernel inet_dist_listen_max 25674 -noshell -noinputroot 71841 0.0 0.5 4138448 77104 ?? S 1:01下午 0:15.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_1@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5673&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost&quot; -kernel inet_dist_listen_min 25673 -kernel inet_dist_listen_max 25673 -noshell -noinput sudo kill 2656 将生产者改连 rabbit_1 节点, 重新运行生产者 报错: 挂掉的主节点中已存在该持久化队列, 如果在主节点挂掉后, 你能直接连接其他节点创建该队列的话, 此时创建的是个新队列, 要知道, 宕机的主节点中的持久化队列还在等待恢复呢, 它内部可能让然有很多msg需要恢复并被处理;所以Rabbit集群的这个问题是有原因的!! 可以重新启动该节点 sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detached 会发现之前的持久化队列会被恢复123456789101112131415161718renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ 此时即使生产者连接着 rabbit_1 也可以创建该同名持久化队列了 重新运行刚才连接到 rabbit_1 的生产者, 不会报错了, 而是正确往队列发布了一条消息123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 2prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"20. 消费者预取 Consumer Prefetch","slug":"rabbitmq/2018-06-13-rabbitmq-20","date":"2018-06-13T11:23:36.000Z","updated":"2018-07-19T02:06:14.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","excerpt":"","text":"Consumer Prefetch 作为限制 unacked 消息数量的更自然有效的方法; AMQP 0-9-1 指定了 basic.qos 方法, 以便你在消费者进行消费时, 可以限制channel(或connection)上未确认消息的数量; 但是值得注意的是: channel 并不是理想的设定范围, 因为单个channel可能从多个队列进行消费, channel和queue需要为每个发送的消息相互协调, 以确保它们不会超出限制, 这在单台机器上会慢, 而在整个集群中使用时会非常慢; 此外, 对于许多用途, 指定适用于每个消费者的预取计数更会简单一些; 因此, RabbitMQ在 basic.qos 方法中重新定义了全局标志的含义 (在php-amqplib中basic_qos()的第三个参数a_global): 请注意, 在大多数API中, 全局标志的默认值为false; (php-amqplib的basic_qos()方法的第三个参数a_global默认也为false) 简要分析 在使用RabbitMQ时, 如果完全不配置QoS, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是采用默认方式, 将队列中的所有消息按照网络和客户端允许的速度尽快轮发到与队列绑定的consumers端; 而consumers会在本地缓存所有投递过来的messages, 这样的话, 就可能会导致 如果某个消费者的业务逻辑处理比较复杂(将会在较长时间之后才会操作完成并进行ack), 这也就导致消费慢的Consumer将会在本地堆积很多消息, 从而导致内存不足或者对其他进程造成影响 (消费者可能被撑到假死); 而其他消费能力强的Consumers, 可能已经很快地消费完成处于闲置状态, 从而造成资源浪费; 同时, 新启的消费者也无法分担已经被之前消费者缓存到其本地的消息, 所以此时即便启动更多消费者, 也无力缓解大量的 unacked 消息积压, 让你产生疑惑; 而当你设置了Qos之后, RabbitMQ虽然也是将队列中的消息尽快轮发到Consumers中, 但是因为消费者具有的 prefetch_count 消息预取值上限, 所以RabbitMQ在轮发消息的时候, 如果发现消费者的 unacked 消息达到了 prefetch_count 的值, 即使rabbitmq中有很多ready的就绪消息, 也不会给该Consumer继续投递消息了(只有消费者的 unacked 消息小于prefetch_count的值时, 才会继续通过轮发方式给该consumer投递ready消息), 如果此时有新的消费者加入, 它也将会拿到未投递出去的ready消息! 可以通过启动 prefetchCountConsumer1，prefetchCountConsumer2 两个消费者(prefetch_count 均为10), 然后使用下面测试中的生产者发送100条消息, 前期观察会发现队列中消息的最大 unacked 为20, 并且你会发现队列中处于ready状态的消息会每次2个的递减, 这就预示着, 每次这两个消费者只要 unacked 的消息书小于prefetch_count(10), Rabbitmq才会给这两个consumer各自发送一条msg; 之后如果启动了 prefetchCountConsumer3(prefetch_count为20), 此时会发现队列中消息的最大 unacked 会为40, prefetchCountConsumer3的加入会使得队列中处于ready状态的消息直接骤减20个, 最后rabbitmq中的ready消息已经为0, 每个消费者还在继续消费各自未 unacked 的消息, 最终消费完成后, 整个队列中的 unacked 消息为0; Qos的设置只有在开启手动ack后才会生效 (即, prefetch_count 在 no_ask=false 的情况下生效) 测试 一般情况下, 同一队列绑定的多个消费者都是处理同一个业务, 而且如果在同一台机器启动, 消费能力应该都差不多, 但也难免出现如: 消费者资源分配不均 或者 两个消费者在处理业务时所请求的服务端机器配置有差异(假设SLB后又2台配置不均的机器), 这种情况还是应该考虑进来的! 本测试比较简单, 主要测试在默认不设置Qos的情况下, 两个消费能力不同的消费者在处理消息时存在的问题之一: 由于这种情况下, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是只顾自己轮发消息, 这样就会导致消息被轮发完成后, 消费能力高的消费者可能很快消费完消息并处于闲置状态, 而消费能力低的消费者却在很慢地进行消费, 这样就造成了资源的浪费; 准备 创建消费者1 ‘qosCustomer1’ (简单打印消息内容) , 代码参考, 启动消费者 php artisan qosConsumer1 创建消费者2 ‘qosCustomer2’ (sleep 5秒, 模拟处理能力比较差) , 代码参考, 启动消费者 php artisan qosConsumer2 创建生产者一次向队列 ‘qosQueue’ 中推送10条消息 , 代码参考, 请求一次生产者 http://www.rabbit.com/testQos 注意需要先启动消费者, 再请求生产者; (如果先请求了生产者, 可能在启动第一个消费者之后, 其会迅速消费完10条消息, 这样就无法模拟效果了) 测试发现 qosCustomer1 : 迅速打印出结果(1,3,5,7,9), 然后就处于闲置状态了 qosCustomer2 : 还在缓慢打印(2,4,6,8,10) 可以看到, 如果不设置Qos, Rabbitmq会尽快将消息从队列中轮发投递出去, 不会对消费者的消费能力进行任何评估! 所以: 为了避免这种浪费资源的情况, 你可能就需要根据上一篇讲解的 prefetch_count 来针对不同消费者进行设置; 问题答疑测试 根据上面的描述, 有个疑问: 在默认不设置Qos的情况下, 既然生产者发布的消息会尽可能全部推送给消费者进程, 队列中会尽可能将消息全部推出, 缓存在消费者本地, 那当消费者断开时, 消息是如何恢复到队列中的? 或者不会恢复到队列中? 为了答疑, 下面进行测试 准备测试代码 创建消费者1 ‘prefetchCountConsumer1’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 创建消费者2 ‘prefetchCountConsumer2’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 生产者一次向队列 ‘prefetchCountQueue’ 中推送100条消息 , 代码参考 测试: 在生产者请求一次之后(http://www.rabbit.com/prefetchCount), ready : 100, unacked: 0, total : 100, 表示队列中已经有100条消息已经就绪, 等待发出 运行第一个php artisan prefetchCountConsumer1之后, ready : 0, unacked : 100, total : 100 (也就是说, queue中已经没有 ready状态, 即准备好待发送的消息了, 消息都传递给消费者1了) 随着消费者的缓慢消费, ready : 0, unacked : 94, total : 94 () 如果模拟 挂掉第一个消费者之后, 会发现, ready : 83， unacked : 0, total : 83 (也就是说消费者意外宕掉之后, 队列中的消息会重新处于就绪状态, 等待着新的消费者来消费) 再次启动消费者2 php artisan testQosConsumerPrefetchCount2之后, ready : 0, unacked : 80, total : 80 (消息又会被全量发送给消费者2) 注意: 如果此时启动消费者1, 你会发现, 它是无法帮助消费者2进行消费的, 因为消息都在消费者2的本地, 所以队列中并没有 ready状态的就绪消息; 测试注意: 上述测试过程如果先启动两个消费者, 然后再发布消息进行测试, 你会发现, 由于两个消费者都设置了预取值, 而且相等, 所以消息仍然会快速轮发给这两个消费者; 如果将两个消费者的 prefetch_count 都设置为10, 那么你会发现, unacked 最多也就是两个消费者的prefetch_count和, 即20个 小结 消费者的 unacked 消息数量如果未达到Qos设置的 prefetch_count 量, Rabbit不会顾及消费者的消费能力, 会尽可能将queue中的消息全部推送出去给消费者; 因此, 当你发现消费者消费缓慢, 产生大量 unacked 消息时, 即便增加新的消费者, 也无法帮助之前的消费者分担消息(除非消费者1的 unacked 达到了 prefetch_count 限制), 只能分担队列中处于 ready 状态的消息; 除非你断开之前的消费者, 然后启动一个新的消费者, 消费者中积压的消息才会重新放入队列中 (因为之前的消费者挂掉之后, 其处理后的剩余消息在 queue中会恢复为 ready 状态) 但是注意: 新启动的这个消费者如果设置额prefetch_count不合理的话, 假设与之前消费者的 预取值 设置一样大, 它很快也会产生大量 unacked 消息 所以, 在新启消费者的时候, 需要设计好 prefetch_count 的大小, 然后可以启动多个消费者来共同进行消费; 扩展 rabbitmq对 basic.qos 信令的处理 首先, basic.qos 是针对 channel 进行设置的, 也就是说只有在channel建立之后才能发送basic.qos信令; RabbitMQ只支持通道级的预取计数, 而不是connection级的 或者 基于大小的预取;预取 在rabbitmq的实现中, 每个channel都对应会有一个rabbit_limiter进程, 当收到basic.qos信令后, 在rabbit_limiter进程中记录信令中prefetch_count的值, 同时记录的还有该channel未ack的消息个数; 在php-amqplib中, 可以使用 channel 的 basic_qos() 方法来进行控制, basic_qos() 有三个参数: prefetch_size : 限制预取的消息大小的参数, rabbitmq暂时没有实现 (如果prefetch_size字段不是默认值0, 则会通知客户端出错, 通知客户端RabbitMQ系统没有实现该参数的功能, 还可以参考此文)当你设置prefetch_size大于0的时候, 会出现如下报错 prefetch_count : 预取消息数量 global: 在3.3.0版本中对global这个参数的含义进行了重新定义, 即glotal=true时表示在当前channel上所有的consumer都生效(包括已有的), 否则只对设置了之后新建的consumer生效;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"19. 消费者预取 Consumer Prefetch (避免队列大量unacked积压及Consumer假死)","slug":"rabbitmq/2018-06-12-rabbitmq-19","date":"2018-06-12T03:26:55.000Z","updated":"2018-07-21T03:21:13.000Z","comments":true,"path":"2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","link":"","permalink":"http://blog.renyimin.com/2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","excerpt":"","text":"RabbitMQ关于吞吐量,延迟和带宽的一些理论 假设你在Rabbit中有一个队列, 并有一些客户端从这个队列中进行消费, 如果你根本没有设置QoS, 那么Rabbit将尽可能快地按照网络和客户端允许的速度将所有队列的消息推送到客户端; 因此, 消费者所占用的内存将会激增, 因为它们将所有消息都缓存在自己的RAM中; 同时, 值得注意的是: 此时如果你询问Rabbit, 队列可能会显示为空, 但是会有大量的 unacked 消息; 并且此时如果你添加新的消费者, 由于消息已经在现有的客户端中缓存, 队列中并没有 ready状态的 消息, 所以即使增加更多新的消费者, 也无法缓解队列中 unacked 消息数量, 这是相当次优的! 所以，默认的QoS预取给客户端(consumer)设置了无限的缓冲区, 这可能导致不良的行为和性能; 那么, 应该将QoS预取缓冲区大小设置为多少呢? 目标是让消费者保持工作饱和状态, 但要尽量减少客户端的缓冲区大小, 以便让更多的消息保留在Rabbit的队列中, 这样就可以供新消费者来消费; 比方说, Rabbit从这个队列中拿出一条消息, 把它投递给消费者, 需要50ms, 而Consumer处理消息需要4ms; 一旦消费者处理了消息, 它就会发送一个ack给Rabbit, 这将再次花费50ms发送给Rabbit并被Rabbit进行处理; 所以 消费完成并进行一次ack的时间 + 一次消息从队列到Consumer的投递时间 总共会花费104ms的往返时间。 如果我们消息设置了QoS预取值为1, 那么直到这个往返行程完成之前, Rabbit是不会发送下一个消息给客户端的;因此, 每次往返的104ms中, Consumer只有4ms, 或者说只有3.8％的时间忙碌, 而我们希望Consumer百分之百的时间都在忙碌中; 如果我们在每个消息的客户端上执行 总的往返时间/处理时间, 会得到 104/4 = 26如果我们设置消息的QoS预取值为26, 那就解决了我们的问题: 如果每条消息需要4ms的处理来处理, 那么总共需要 26×4 = 104ms 来处理整个缓冲区(中的消息);第一个4ms是第一个消息的处理时间, 处理完成后, 客户端然后发出一个确认(这需要50ms才能到达代理), 然后继续处理缓冲区中的下一条消息, 一次ack时间 + 新一轮消息的投递时间 = 100s, Consumer正好完成缓冲区剩下的25条消息, 然后新的26条消息也已经到达, 并准备好等待客户端来处理它;因此, 客户端始终处于忙碌状态: 具有较大的QoS预取值也不会使其更快了, 但是我们最大限度地减少了缓冲区的大小, 并且减少了客户端消息的延迟;客户端能够在下一条消息到达之前完全排空缓冲区, 因此缓冲区实际上保持为空; 如果处理时间和网络行为保持不变, 此解决方案绝对没问题 但考虑一下如果网络突然间速度减半会发生什么情况? 显然, 网络传输时间就加长了, 此时你的预取缓冲区(也就是你设置的prefetch预取值)就不够大了, 现在Consumer会就会稍有闲置, 等待新消息到达, 因为客户端能够处理消息的速度比Rabbit能够提供新消息的速度要快; 为了解决这个问题, 我们可能会决定将QoS预取大小加倍(或接近两倍), 如果我们从26开始将它推到51, 那么如果客户端处理保持在每个消息4ms, 我们现在在缓冲区中会有51 * 4 = 204ms的消息处理时间, 其中4ms将用于处理消息, 而200ms用于发送消息回复rabbit并收到下一条消息, 因此, 我们现在可以应对网络速度的减半; 再次分析: 如果网络又恢复正常运行, 现在将QoS预取加倍, 意味着每个消息都会驻留在客户端缓冲区中一段时间​​, 而不是在到达客户端时立即处理; 从现在51条消息的完整缓冲区开始, 我们知道新消息将在客户端完成处理第一条消息之后的100ms处开始出现在客户端, 但在这100毫秒内, 客户只能处理100/4 = 25个消息, 这意味着当新消息到达客户端时, 它会在客户端从缓冲区头部移除时被添加到缓冲区的末尾; 而缓冲区将始终保持(50 - 25 = 25)个消息长度, 因此每个消息将在缓冲区中保持 25 * 4 = 100ms; 因此, 增加预取缓冲区大小, 可以使consumer应对恶化的网络性能, 同时保持客户端繁忙; 同样, 如果不是网络性能的恶化, 而是客户端开始花费40ms来处理每条消息而不是之前的4ms, 会发生什么情况? 假设原始的预取缓冲区大小设置的是26条消息, 客户端现在需要花40ms处理第一条消息, 然后将确认消息发送回Rabbit并移至下一条消息;ack仍然需要50ms才能到达Rabbit, 而Rabbit发出一条新的消息需要50ms, 但在100ms内, 客户端只处理了 100/40 = 2.5 条消息, 而不是剩余的25条消息;因此当新消息到来时, 缓冲区在这一点上仍然是有 25 - 3 = 22 个消息, 这样的话, 来自Rabbit的新消息就不会被立即处理, 而是位于第23位, 落后于其他22条仍在等待处理的消息;客户端(Consumer)将会有 22 * 40 = 880ms 的时间都不会触及到那个新到的消息, 鉴于从Rabbit到客户端的网络延迟仅为50ms, 这个额外的880ms延迟现在为延迟的95％ (880 / (880 + 50) = 0.946); 当你决定尝试通过添加更多消费者来处理这种增长的积压时, 需要注意, 现在有消息正在被现有客户端缓冲, 并不是说你增加消费者就能缓解这部分的压力! 更糟糕的是, 如果我们将缓冲区大小设置为可以预取51条消息以应对网络性能下降,会发生什么?处理第一条消息后, 将在客户端缓冲另外50条消息, 100ms后(假设网络运行正常), 一条新消息将从Rabbit到达客户端, consumer在100ms中只能处理这50条消息中的两条消息(缓冲区现在为47条消息长),因此新消息将会在缓冲区中是第48位, 这样的话, 知道 47 40 = 1880ms 之后, 消费者才会开始处理新来的消息, 同样, 考虑到向客户端发送消息的网络延迟仅为50ms, 现在这个1880ms的延迟意味着客户端缓冲占延迟的97％(1880/(1880 + 50)= 0.974);这可能是不可接受的: 数据只能在客户端收到后2秒内立即处理, 才能有效且有用！*如果其他消费客户端空闲, 他们无能为力: 一旦Rabbit向客户端发送消息, 消息就是客户端的责任, 直到他们拒绝或拒绝消息; 消息发送到客户端后，客户端不能窃取彼此的消息;您希望客户端保持繁忙状态, 但客户端尽可能少地缓存消息, 以便客户端缓冲区不会延迟消息, 因此新消费客户端可以快速接收来自Rabbit队列的消息; 因此, 如果网络变慢, 缓冲区太小会导致客户端空闲; 但如果网络正常运行, 缓冲区太大会导致大量额外的延迟;如果客户端突然开始花费更长时间来处理每个缓冲区, 则会导致大量额外的延迟;很明显, 你真正想要的是可以变化的缓冲区大小, 这些问题在网络设备中很常见, 并且一直是很多研究的主题;主动队列管理算法试图尝试放弃或拒绝消息，以避免消息长时间处于缓冲区。当缓冲区保持空闲时（每条消息只遭受网络延迟，并且根本不在缓冲区中），缓冲区在那里吸收峰值，从而实现最低延迟。从网络路由器的角度来看，Jim Gettys一直在研究这个问题：局域网和广域网性能之间的差异会遇到完全相同的问题。实际上，无论何时，在生产者（在我们的例子中为Rabbit）和消费者（客户端应用程序逻辑）之间都有一个缓冲区，双方的性能可以动态变化，您将会遇到这些问题。最近发布了一种名为Controlled Delay的新算法，该算法似乎在解决这些问题方面效果很好。 小结 针对Qos的提前预习 信道预取设置(QoS)由于消息是异步发送(推送)给客户端的, 因此在任何给定时刻通常都有不止一条消息在信道上运行; 此外, 客户的手动确认本质上也是异步的, 所以有一个 未确认的交付标签的滑动窗口, 开发人员通常会倾向于限制此窗口的大小, 以避免消费者端无限制的缓冲区问题。这是通过使用 basic.qos 方法设置 预取计数 值完成的, 该值定义了channel上允许的最大未确认递送数量, 一旦数字达到配置的计数, RabbitMQ将停止在通道上传送更多消息, 除非至少有一个未确认的消息被确认;例如, 假设在通道 “Ch” 上有未确认的交付标签5,6,7和8, 并且通道 “Ch” 的预取计数(后面会学到是prefetch_count)设置为4, 则RabbitMQ将不会在 “Ch” 上推送更多交付, 除非至少有一个未完成的交付被确认(当确认帧在 delivery_tag=8 的频道上到达时, RabbitMQ将会注意到并再发送一条消息) QoS预取设置对使用 basic.get(pull API) 获取的消息没有影响, 即使在手动确认模式下也是如此; 消费者确认模式, 预取和吞吐量 确认模式 和 QoS预取值 对消费者吞吐量有显着影响, 一般来说, 增加预取值将提高向消费者传递消息的速度, 当然, 自动确认模式可以产生最佳的传送速率 但是, 在上面两种情况下, 尚未完成交付处理的消息(unacked)数量也会增加, 从而增加消费者RAM消耗; 自动确认模式或带无限预取的手动确认模式应谨慎使用, 消费者在没有确认的情况下消耗大量消息将导致其所连接的节点上的内存消耗增长; 预取值1是最保守的, 但这将显着降低吞吐量, 特别是在消费者连接延迟较高的环境中, 对于许多应用来说, 更高的价值是合适和最佳的; 100到300范围内的Qos(prefetch_count)预取值通常提供最佳的吞吐量, 并且不会面临压垮consumer的重大风险, 而更高的值往往会遇到效率递减的规律;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"08. 事务 VS Publisher Confirms(发布者确认机制)","slug":"rabbitmq/2018-06-05-rabbitmq-08","date":"2018-06-05T11:20:56.000Z","updated":"2018-07-20T11:29:01.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","excerpt":"","text":"问题的出现 和消息持久化相关的一个概念是 AMQP 的事务(transaction)机制; 到目前为止, 我们讨论的是将 消息, 队列 和 交换器 设置为持久化; 这一切都工作的很好, 并且RabbitMQ也负责保证消息的安全, 但是由于 发布消息的操作并不会反回任何信息给生产者, 所以你也无法得知是否消息已经到达了服务器并且服务器是否已经将消息持久化到了硬盘; 服务器可能会在把消息写入到硬盘前就宕机了, 或者消息压根就还没有发送到服务器, 服务器就宕机了, 消息会因此而丢失, 而你却不知道; 另外, 你可能是发送多条消息, 如果部分发送成功, 部分失败呢? 这你也无法得知; 事务机制 为了确保消息能够被安全发布到Broker, 如果使用标准的AMQP 0-9-1, 保证消息不会丢失的唯一方法是使用 事务机制 (将channel事务化) php-amqplib 中与事务机制有关的方法有三个, 分别是Channel里面的 txSelect(), txCommit() 以及 txRollback(); txSelect(): 用于将当前Channel设置成是transaction模式 txCommit(): 用于提交事务 txRollback(): 用于回滚事务 但是值得注意的是事务存在的问题: AMQP 0-9-1 中的事务几乎吸干了RabbitMQ的性能, 会导致事务吞吐量严重下降; 事务会使得生产者应用程序变成同步的, 而你使用消息通信就是为了避免同步; 鉴于上面的问题, 你可能不会在生产中使用事务机制, 此处只做了个简单的事务测试, 测试代码 Publisher Confirms 既然事务存在的问题让你拒绝使用它, 但是确保消息被成功投递到服务器这个问题仍需要解决; 为了避免事务机制在解决问题时导致的新问题, RabbitMQ团队拿出了更好的方案来保证消息的投递: 发送方确认模式 它模仿协议中已经存在的 消费者确认机制 要启用这个确认机制，客户端可以通过使用 channel 的 confirm.select 方法 如果设置了 confirm.select 方法的 no-wait, 代理会用 confirm.select-ok 进行响应, 不过这点你貌似也只能通过抓包来观察: 这里说的 confirm.select-ok 是代理对发布者的响应信息 (和 php-amqplib包中的 confirm_select_ok() 方法可不是一个意思, 而且php-amqplib也没对confirm_select_ok做实现) 上面也提到了, 该确认机制是模仿已经存在的 消费者确认机制, 所以, Broker也会使用类似 ack, nack 来响应Publisher: 可以通过为 set_ack_handler , set_nack_handler 设置回调, 来监测消息是否成功到达服务器, 成功则会触发 set_ack_handler, 失败则会触发 set_nack_handler 只有在负责队列的Erlang进程中发生内部错误时才会回应nack, 所以这个在测试中也一直没有使用 set_nack_handler 捕获到错误 (是对于nack的消息, 可以设置进行重发); 注意: 这两监听函数是监听服务器对 publisher 的应答的, 可不是监听 consumer 对服务器的应答的; 一旦在channel上使用 confirm.select 方法, 就说 channel 处于 确认模式, 一旦通道处于确认模式, 就不能进行事务处理; 也就是说 事务 和 Publisher Confirm 不能同时使用; 一旦通道处于确认模式, 代理和客户端都会对消息进行计数(在第一次confirm.select时从1开始计数), 然后, broker通过在相同channel上发送 basic.ack 来处理它们, 从而确认消息; delivery-tag 字段包含确认消息的序列号;最大 Delivery Tag, 递送标签是一个64位长的值，因此其最大值为9223372036854775807.由于递送标签的范围是按每个通道划分的，因此发布商或消费者在实践中不太可能运行该值 Publisher Confirms 的顺序考虑 在大多数情况下, RabbitMQ将按发布顺序向publisher确认消息(这适用于在单个频道上发布的消息); 但是, 发布者确认是异步发出的, 并且可以确认一条消息或一组消息;由于消息确认可以以不同的顺序到达, 所以, 应用程序应尽可能不取决于确认的顺序; 发布者确认存在的问题 mandatory 属性问题 测试代码publisher confirm 不需要消费者参与, 代码参考","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"06. 持久化策略","slug":"rabbitmq/2018-05-28-rabbitmq-06","date":"2018-05-28T09:32:11.000Z","updated":"2018-07-20T09:43:29.000Z","comments":true,"path":"2018/05/28/rabbitmq/2018-05-28-rabbitmq-06/","link":"","permalink":"http://blog.renyimin.com/2018/05/28/rabbitmq/2018-05-28-rabbitmq-06/","excerpt":"","text":"持久化原理 RabbitMQ 默认情况下, Exchange, 队列, 消息 都是非持久的, 这意味着一旦消息服务器重启, 所有已声明的 Exchange, 队列, 以及 队列中的消息 都会丢失; RabbitMQ确保持久化的消息能在服务器重启之后恢复的方式是, 将它们写入磁盘上的一个持久化日志文件。当发布一条持久性消息到一个持久交换机上时, Rabbit会在消息提交到日志文件中之后才发送响应; 还需要注意的是, 如果之后这条消息被路由到一个非持久化队列, 则消息又会从上面的日志文件中删除, 并且无法从服务器重启中恢复; 一旦你从持久化队列中消费了一条持久性消息(并且进行了确认), RabbitMQ会在持久化日志中把这条消息标记为等待垃圾收集; 持久化方案 要做到消息持久化, 必须保证如下三点设置正确: exchange交换器: durable属性为true; queue队列: durable属性为true; 除了上述两点之外, 还需要在投递消息时候, 设置message的 delivery_mode 模式为2来标识消息为持久化消息; 另外: 一个包含持久化消息的非持久化队列, 在Rabbit Server重启之后, 该队列将会不复存在, 消息就会变成孤儿; 具体代码 持久化的问题 持久化由于会写磁盘, 所以会极大降低RabbitMQ每秒处理的消息总数, 降低吞吐量; 持久化在Rabbit内建集群环境下工作的并不好, 虽然RabbitMQ集群允许你和集群中的任何节点的任一队列进行通信, 但是如果队列所在的节点崩溃后, 如果队列是持久化的, 那么直到这个节点恢复之前, 这个队列都不会在整个集群中被创建出来; 后面在学习集群时, 会给出相应的解决方案;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"56. awk","slug":"linux/2018-05-18-56","date":"2018-05-18T02:03:51.000Z","updated":"2018-08-14T06:20:44.000Z","comments":true,"path":"2018/05/18/linux/2018-05-18-56/","link":"","permalink":"http://blog.renyimin.com/2018/05/18/linux/2018-05-18-56/","excerpt":"","text":"简介 Awk是一种便于使用且表达能力强的程序设计语言, 它拥有数组, 支持if-else控制语句, for/while循环语句, 内置函数等功能; 同时, 它是一个强大的文本分析工具; 之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的Family Name的首字符; awk 命令的执行过程 awk是逐行处理的, 当处理一个文本时, 会一行一行进行处理, 处理完当前行, 再处理下一行, awk默认以 换行符 为标记来识别每一行, 每次遇到”回车换行”, 就认为是当前行的结束, 新的一行的开始; awk在逐行处理文本时, 会搜索能够匹配程序中任意模式(也就是在awk命令中的’program’中可以有多个模式)的行, 每个模式依次测试每个输入行, 对于匹配到模式的行, 会执行awk命令中模式后面的动作(也许包含多步), 然后读取下一行并继续匹配, 直到所有的输入读取完毕; 同时, awk会按照用户指定的分割符去分割当前行为多个字段(每个字段分别被标识为 $1 一直到 $NF, $NF为awk中的变量, 表示当前行被分隔出的最大字段数), 如果没有指定分割符, 默认使用空格作为分隔符, 另外, 默认会将空格符和制表符都视为空格, 而且多个连续的空格都视为一个; 作为一个PHP开发人员, 在工作中, 可能经常需要对开发, 沙盒, 预发, 线上等各环境的日志做分析工作; 语法结构基本语法结构为: awk [options] &#39;program&#39; file1 , file2 ..., 其中, &#39;program&#39; 又可以细分为 pattern模式 和 action动作; 也就是说, awk的完整语法其实为 awk [options] &#39;pattern {action}&#39; file1 , file2 ... option选项 -F fs or --field-separator fs : 指定分隔符, fs可以是一个字符串或是一个正则表达式 也可以使用参数 -v 设置内置变量FS的值来指定分隔符, 如 -v FS=&#39;分隔符&#39;其实还有输出分隔符, 可以使用 -v OFS=&#39;分隔符&#39;, 来定义输出分隔符 (一般你会使用,来分隔各字段, 会发现输出分隔符会是 空格, 没有逗号则各字段会紧连着) -F 指定多个分隔符: 可以使用 [] 来放多个分隔字符, 只要遇到[]中的一个分隔符, 就会进行分隔 例子: nginx的access日志使用 [ 或者 ] 来分隔, 不过, 对于包裹字符串集合的[], 分隔字符[就显得比较特殊, 所以[字符的顺序需要注意; 或者可以使用 tail -10 c2b_oms.2018-03-30-access.log | awk -F&#39;[\\\\[\\\\]]&#39; &#39;{print $2}&#39;, 就不用在意 [ 字符和包裹字符的 [] 之间会出现的问题 123456$ tail -2 c2b_oms.2018-03-30-access.logremote_addr=[172.16.254.2] http_x_forward=[-] time=[30/Mar/2018:16:43:17 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[200] byte=[884] elapsed=[2.189] refer=[-] body=[-] ua=[PostmanRuntime/6.1.6] cookie=[-] gzip=[7.55]remote_addr=[172.16.254.2] http_x_forward=[-] time=[30/Mar/2018:17:11:45 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[500] byte=[22005] elapsed=[0.136] refer=[-] body=[-] ua=[PostmanRuntime/6.1.6] cookie=[-] gzip=[-]$ tail -2 c2b_oms.2018-03-30-access.log | awk -F &apos;[][]&apos; &apos;&#123;print $8, $7&#125;&apos;GET /api/v1/car_dealer/list HTTP/1.1 request=GET /api/v1/car_dealer/list HTTP/1.1 request= - 可以按照任意一段字符串进行分隔 1234[work@VM_0_206_centos nginx_log]$ tail -1 c2b_oms.2018-03-30-access.logremote_addr=[172.16.254.2] http_x_forward=[-] time=[30/Mar/2018:11:26:09 +0800] request=[GET /api/v1/car_dealer/list?car_dealer_name_or_phone=18625036504 HTTP/1.1] status=[200] byte=[276] elapsed=[0.005] refer=[-] body=[-] ua=[PostmanRuntime/6.1.6] cookie=[-] gzip=[0.61][work@VM_0_206_centos nginx_log]$ tail -1 c2b_oms.2018-03-30-access.log | awk -F&apos;+0800&apos; &apos;&#123;print $2&#125;&apos;] request=[GET /api/v1/car_dealer/list?car_dealer_name_or_phone=18625036504 HTTP/1.1] status=[200] byte=[276] elapsed=[0.005] refer=[-] body=[-] ua=[PostmanRuntime/6.1.6] cookie=[-] gzip=[0.61] -v 变量赋值选项: 该选项将一个值赋予一个变量(变量可以是awk内置的, 也可以是自定义的), 它会在程序开始之前进行赋值 12$ awk -v name=Rymuscle &apos;BEGIN&#123;printf &quot;Name = %s\\n&quot;, name&#125;&apos;Name = Jerry -f scriptfile 或者 --file scriptfile : 从脚本文件中读取awk命令 pattern模式 用来匹配awk命令所处理的文本中的每一行内容; 注意模式匹配字符串时, 需要使用双引号 12345678910111213141516renyimindeMacBook-Pro:Desktop renyimin$ cat awk.logrenyimin 200 1200W 男renfumin 500 200W 男renjuanfang 1000 900W 女renyimindeMacBook-Pro:Desktop renyimin$ awk &apos;$1==&quot;renyimin&quot; &#123;print $1,$2,$3&#125;&apos; awk.logrenyimin 200 1200W// 下面单引号就不行renyimindeMacBook-Pro:Desktop renyimin$ awk &apos;$1==&apos;renyimin&apos; &#123;print $1,$2,$3&#125;&apos; awk.logrenyimindeMacBook-Pro:Desktop renyimin$ awk &apos;$1==renyimin &#123;print $1,$2,$3&#125;&apos; awk.logrenyimindeMacBook-Pro:Desktop renyimin$ 模式可以组合(可以使用括号和逻辑操作符与 &amp;&amp;、||、! 对模式进行组合) 123renyimindeMacBook-Pro:Desktop renyimin$ awk &apos;$1==&quot;renyimin&quot; &amp;&amp; $2&gt;20 &#123;print $1,$2,$3&#125;&apos; awk.logrenyimin 200 1200WrenyimindeMacBook-Pro:Desktop renyimin$ awk &apos;$1==&quot;renyimin&quot; &amp;&amp; $2&gt;200 &#123;print $1,$2,$3&#125;&apos; awk.log 模式可以使用运算符 123456789101112renyimindeMacBook-Pro:linux renyimin$ cat test1.logBeth 4.00 0Dan 3.75 0kathy 4.00 10Mark 5.00 20Mary 5.50 22Susie 4.25 18renyimindeMacBook-Pro:linux renyimin$ awk &apos;$2*$3&gt;50 &#123;print $0&#125;&apos; test1.logMark 5.00 20Mary 5.50 22Susie 4.25 18renyimindeMacBook-Pro:linux renyimin$ 模式也可以使用正则表达式 123renyimindeMacBook-Pro:linux renyimin$ awk &apos;/Susie/ &#123;print $0&#125;&apos; test1.logSusie 4.25 18renyimindeMacBook-Pro:linux renyimin$ action动作 awk 擅长文本格式化并将格式化后的文本输出, 所以它比较常用的 action 是 print 和 printf; ‘模式+动作’ 语句中的 模式 或 动作都可以省略, 但不是同时省略 有的awk命令有 一个模式 + 动作 有的awk命令只有一个模式, 没有动作(如果没有动作, 那么模式匹配到的每一行都会被完整打印出来, 相当于动作是{print} 或者 {print $0}) 有的awk命令只有多个模式, 没有动作(同上) 有的awk命令可以有多个模式 + 一个动作 多文件处理, 用到了再补充~~ 内置的一些变量变量可以在 BEGIN 语句块中声明, 也可以在 program 中声明, 也可以使用参数 -v 声明 NF: 行的字段数量, 你可以在awk命令的program中使用action, Awk会对当前输入的行有多少个字段进行计数, 并且将当前行的字段数量存储在一个内建的称作 NF 的变量中 因此, 程序 { print NF, $1, $NF } 会依次打印出每一行的字段数量, 第一个字段的值, 最后一个字段的值; NR: 它会存储当前已经读取了多少行的计数, 可以使用 $NR 给结果的每一行加上行号 1234567renyimindeMacBook-Pro:linux renyimin$ awk &apos;&#123;print NR,$0&#125;&apos; awk.log1 renyimin &quot;200&quot; 1200W 男23 renfumin 500 &quot;200W&quot; 男4 renjuanfang 1000 900W &quot;女&quot;5renyimindeMacBook-Pro:linux renyimin$ FS: 除了使用 -F参数指定字段分隔符, 也可以通过 -v 设定 FS 内置变量来指定字段分隔符 (或者在BEGIN中指定FS变量) OFS : 上面介绍了如何设定输入分隔符, 其实还有输出分隔符, 可以使用 -v OFS=&#39;分隔符&#39;(或者在BEGIN中指定OFS变量), 来定义输出分隔符 123456789[work@VM_0_206_centos nginx_log]$ tail -3 c2b_oms.2018-03-30-access.log | awk -F[][] -v OFS=&apos;---&apos; &apos;&#123;print $2,$3&#125;&apos;172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=[work@VM_0_206_centos nginx_log]$ tail -3 c2b_oms.2018-03-30-access.log | awk -F[][] &apos;BEGIN&#123;OFS=&quot;---&quot;&#125; &#123;print $2,$3&#125;&apos;172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=[work@VM_0_206_centos nginx_log]$ 另外声明变量也可以直接在 program 中进行 123456[work@VM_0_206_centos nginx_log]$ tail -3 c2b_oms.2018-03-30-access.log | awk &apos;FS=&quot;[][]&quot;,OFS=&quot;---&quot; &#123;print $2,$3&#125;&apos;http_x_forward=[-]---time=[30/Mar/2018:11:26:09172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=172.16.254.2--- http_x_forward=[work@VM_0_206_centos nginx_log]$ 其余的, 用到了再补充~~ BEGIN 语句块 BEGIN 语句块的语法: 在awk的program中 BEGIN {awk-commands}: BEGIN语句块在程序开始的使用执行, 它只执行一次, 在这里可以初始化变量或者打印一个开头什么的; BEGIN 是AWK的关键字, 因此它必须为大写, 注意, 这个语句块是可选的; 测试: 可以使用 print &quot;&quot; 在输出之前先打印一个空行1234567renyimindeMacBook-Pro:linux renyimin$ awk &apos;BEGIN &#123;print &quot;&quot;&#125; &#123;print NR,$0&#125;&apos; awk.log1 renyimin &quot;200&quot; 1200W 男23 renfumin 500 &quot;200W&quot; 男4 renjuanfang 1000 900W &quot;女&quot;5 END 语句块END 则用于处理匹配过的最后一个文件的最后一行之后的位置; 练习下面根据nginx的access.log日志来做一些练习 (这个日志还是挺大量的)12wc c2b_bid.2018-04-23-access.log 65608 1501442 29580860 c2b_bid.2018-04-23-access.log 统计独立ip的访问量 - 共有多少独立IP访问过本站点 12-bash-4.2$ awk &apos;&#123;print $1&#125;&apos; c2b_bid.2018-04-23-access.log | sort -n | uniq | wc -l240 表示共有240个不同ip访问过站点 统计独立ip的访问情况 - 每个独立ip的访问次数 1234567bash-4.2$ awk &apos;&#123;print $1&#125;&apos; c2b_bid.2018-04-23-access.log | sort -n | uniq -c 190 remote_addr=[100.116.226.0] 195 remote_addr=[100.116.226.1] 176 remote_addr=[100.116.226.10] 176 remote_addr=[100.116.226.100] ..... 就不全部列出了 统计访问最频繁的前10个ip 123456789101112-bash-4.2$ awk &apos;&#123;print $1&#125;&apos; c2b_bid.2018-04-23-access.log | sort -n | uniq -c | sort -rn | head -10 407 remote_addr=[100.116.251.123] 399 remote_addr=[100.116.251.65] 394 remote_addr=[100.116.251.56] 390 remote_addr=[100.116.251.97] 388 remote_addr=[100.116.251.11] 387 remote_addr=[100.116.251.110] 384 remote_addr=[100.116.251.98] 383 remote_addr=[100.116.251.83] 382 remote_addr=[100.116.251.8] 381 remote_addr=[100.116.251.40]-bash-4.2$ 查看访问380次以上的ip 1234567891011121314-bash-4.2$ awk &apos;&#123;print $1&#125;&apos; c2b_bid.2018-04-23-access.log | sort -n | uniq -c | awk &apos;$1 &gt; 380 &#123;print&#125;&apos; | sort -rn 407 remote_addr=[100.116.251.123] 399 remote_addr=[100.116.251.65] 394 remote_addr=[100.116.251.56] 390 remote_addr=[100.116.251.97] 388 remote_addr=[100.116.251.11] 387 remote_addr=[100.116.251.110] 384 remote_addr=[100.116.251.98] 383 remote_addr=[100.116.251.83] 382 remote_addr=[100.116.251.8] 381 remote_addr=[100.116.251.40] 381 remote_addr=[100.116.251.38] 381 remote_addr=[100.116.251.118]-bash-4.2$ 这是平时日常的日志分析命令 日志为: 123456tail -5 c2b_oms.2018-03-29-access.logremote_addr=[172.16.254.2] http_x_forward=[-] time=[29/Mar/2018:18:37:48 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[200] byte=[19305] elapsed=[0.002] refer=[-] body=[-] ua=[Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36] cookie=[-] gzip=[3.19]remote_addr=[172.16.254.2] http_x_forward=[-] time=[29/Mar/2018:18:41:24 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[200] byte=[24131] elapsed=[0.004] refer=[-] body=[-] ua=[Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36] cookie=[-] gzip=[3.96]remote_addr=[172.16.254.2] http_x_forward=[-] time=[29/Mar/2018:18:42:06 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[200] byte=[24130] elapsed=[0.004] refer=[-] body=[-] ua=[Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36] cookie=[-] gzip=[3.96]remote_addr=[172.16.254.2] http_x_forward=[-] time=[29/Mar/2018:18:42:40 +0800] request=[GET /api/v1/car_dealer/list HTTP/1.1] status=[200] byte=[786] elapsed=[1.078] refer=[-] body=[-] ua=[Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36] cookie=[-] gzip=[8.27]remote_addr=[172.16.254.2] http_x_forward=[-] time=[29/Mar/2018:18:42:57 +0800] request=[GET /favicon.ico HTTP/1.1] status=[404] byte=[9879] elapsed=[0.010] refer=[http://172.16.0.206:8888/] body=[-] ua=[Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36] cookie=[-] gzip=[2.27] 分析为: 全都是action, 没有匹配 123-bash-4.2$ awk -F&apos;] &apos; &apos;&#123;status=substr($5,9);elapsed=substr($7,10);if(elapsed &gt;= 2 || status &gt;=500)print NR,&quot;:&quot;,$4,status,elapsed; &#125;&apos; /home/work/nginx_log/c2b_bid.2018-08-14-access.log72700 : request=[PUT /api/v1/bid?car_id=5696964&amp;bid_id=2613419&amp;car_dealer_id=26690&amp;bid_status=0&amp;operation_type=0 HTTP/1.1 200 3.25385585 : request=[PUT /api/v1/bid?car_id=5673673&amp;bid_id=2614287&amp;car_dealer_id=9264&amp;bid_status=0&amp;operation_type=0 HTTP/1.1 200 5.162","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"30. 开机启动相关","slug":"linux/2018-03-18-23","date":"2018-05-16T07:28:21.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/05/16/linux/2018-03-18-23/","link":"","permalink":"http://blog.renyimin.com/2018/05/16/linux/2018-03-18-23/","excerpt":"","text":"initsystemctl加入systemctl, 设置开机启动设置环境变量, 设置开机启动 init 开机启动","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"50. sort, uniq, wc","slug":"linux/2018-05-16-50","date":"2018-05-16T07:28:21.000Z","updated":"2018-08-07T03:29:11.000Z","comments":true,"path":"2018/05/16/linux/2018-05-16-50/","link":"","permalink":"http://blog.renyimin.com/2018/05/16/linux/2018-05-16-50/","excerpt":"","text":"sort Linux sort命令用于将文本文件内容以 行 为单位进行排序; 语法: sort [-bcdfimMnr][-o&lt;输出文件&gt;][-t&lt;分隔字符&gt;][+&lt;起始栏位&gt;-&lt;结束栏位&gt;][--help][--verison][文件] sort 命令既可以从特定的文件, 也可以从 stdin 中获取输入; 选项 123456789101112-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 目前常用的选项有 -nr uniq uniq 命令用于报告或忽略文件中的重复行，一般与sort命令结合使用; 之所以要和sort命令结合使用, 是因为uniq命令值只可以删除相邻的重复行; 如果一文本中有重复却不相邻的行, 单独使用uniq命令则无法删除, 需要结合sort命令; 语法: uniq [-cdu][-f&lt;栏位&gt;][-s&lt;字符位置&gt;][-w&lt;字符位置&gt;][--help][--version][输入文件][输出文件] 选项: 123456-c或——count：在每列旁边(貌似是左边)显示该行重复出现的次数；-d或--repeated：仅显示重复出现的行列；-f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt;：忽略比较指定的栏位；-s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;：忽略比较指定的字符；-u或——unique：仅显示出一次的行列；-w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;：指定要比较的字符。 目前常用的选项有 -c 参数介绍 输入文件: 指定要去除的重复行文件, 如果不指定此项, 则从标准读取数据; 输出文件: 指定要去除重复行后的内容要写入的输出文件, 如果不指定此选项, 则将内容显示到标准输出设备(显示到终端); 实例 uniq 无法去除不相邻的重复行 1234567891011121314151617181920renyimindeMacBook-Pro:linux renyimin$ cat uniq.logrenyimin is a it manhe has many articlerenyimin comes from shanxi provincehe has many article// 可以看到uniq并没有对不相邻的重复行进行去重操作renyimindeMacBook-Pro:linux renyimin$ uniq uniq.logrenyimin is a it manhe has many articlerenyimin comes from shanxi provincehe has many articlerenyimindeMacBook-Pro:linux renyimin$ // 如下进行排序后即可renyimindeMacBook-Pro:linux renyimin$ sort uniq.log | uniqhe has many articlerenyimin comes from shanxi provincerenyimin is a it manrenyimindeMacBook-Pro:linux renyimin$ -c 参数, 对文件相同行进行去重, 并统计每行重复出现的次数 (类似于mysql的groupby分组统计) 1234renyimindeMacBook-Pro:linux renyimin$ sort uniq.log | uniq -c2 he has many article1 renyimin comes from shanxi province1 renyimin is a it man 当然, 还可以继续结合sort来按照重复条数最多的来排序 1234renyimindeMacBook-Pro:linux renyimin$ sort uniq.log | uniq -c | sort1 renyimin comes from shanxi province1 renyimin is a it man2 he has many article wc 利用wc指令我们可以计算文件的 行数、Byte数、字数、或是列数, 若不指定文件名称、或是所给予的文件名为”-“, 则wc指令会从标准输入设备读取数据; 语法: wc [-clw][--help][--version][文件...] 选项 123456789-c或--bytes或--chars 只显示Bytes数-l或--lines 只显示行数-w或--words 只显示字数--help 在线帮助--version 显示版本信息 目前常用的选项有 -lwc (貌似也是默认的选项, 而且无论选项的顺序是怎样的, 结果总是: “行数, 字数, 字节数, 文件名”) 实例 基本用法 12345678renyimindeMacBook-Pro:linux renyimin$ cat uniq.logrenyimin is a it manhe has many articlerenyimin comes from shanxi provincehe has many articlerenyimindeMacBook-Pro:linux renyimin$ wc uniq.log 4 18 97 uniq.logrenyimindeMacBook-Pro:linux renyimin$ 统计多文件 12345renyimindeMacBook-Pro:linux renyimin$ wc -l uniq.log uniq.log 4 uniq.log // 第一个文件行数为4 4 uniq.log // 第二个文件行数为4 8 total // 两个文件总的行数为8renyimindeMacBook-Pro:linux renyimin$ 结合之前的 uniq, sort 命令, 统计去重后的行数 123456789101112renyimindeMacBook-Pro:linux renyimin$ cat uniq.logrenyimin is a it manhe has many articlerenyimin comes from shanxi provincehe has many article// 总共有4行renyimindeMacBook-Pro:linux renyimin$ sort uniq.log | wc -l 4// 去重后有3行renyimindeMacBook-Pro:linux renyimin$ sort uniq.log | uniq -c | wc -l 3 nlnl命令在linux系统中用来计算文件中行号, nl 可以将输出的文件内容自动的加上行号, 其默认的结果与 cat -n 有点不太一样, nl 可以将行号做比较多的显示设计, 包括位数与是否自动补齐 0 等等的功能1.命令格式 : nl [选项]... [文件]... 2.命令参数:-b ：指定行号指定的方式，主要有两种：-b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)；-b t ：如果有空行，空的那一行不要列出行号(默认值)；-n ：列出行号表示的方法，主要有三种:-n ln ：行号在萤幕的最左方显示-n rn ：行号在自己栏位的最右方显示，且不加 0-n rz ：行号在自己栏位的最右方显示，且加 0-w ：行号栏位的占用的位数-p 在逻辑定界符处不重新开始计算 3.试用 12345678910111213141516171819202122$ nl -ba Test.class.php 1 &lt;?php 2 interface Huma 3 &#123; 4 public static function say(); 5 &#125; 6 7 class Male implements Huma 8 &#123; 9 private static function say()10 &#123;11 echo &quot;I am a Male&quot;;12 &#125;13 &#125;1415 class Female implements Huma16 &#123;17 public static function say()18 &#123;19 echo &quot;I am a Female&quot;;20 &#125;21 &#125; 其他案例在多个文件中匹配某个字符串(比如 ‘&lt;?php’), 并列出文件名: grep &#39;&lt;?php&#39; *.php","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"PHP 性能追踪及分析工具(XHPROF)","slug":"php/2018-05-02-Xhprof","date":"2018-05-02T14:26:39.000Z","updated":"2018-08-17T12:16:13.000Z","comments":true,"path":"2018/05/02/php/2018-05-02-Xhprof/","link":"","permalink":"http://blog.renyimin.com/2018/05/02/php/2018-05-02-Xhprof/","excerpt":"","text":"简介 Xhprof 是facebook开源出来的一个php轻量级的性能分析工具, 跟Xdebug类似, 但性能开销更低, 还可以用在生产环境中, 也可以由程序开关来控制是否进行profile; XHProf的一些特性: Flat Profile: 提供函数级的汇总信息, 比如调用次数、执行时间、内存使用、CPU占用等; Hierarchical Profile: 对每个程序, 进行了父级调用和子级调用的分解; Diff Reports(差异报告)有很多种情况, 我们希望能够对比, 比如新版本比旧版本提升了多少速度, 两个版本的差距究竟在哪里;Diff Report 就是这样的工具, 接收两个输入, 并且分别给出各自的 Flat Profile 和 Hierarchical Profile 报告; Callgraph View(调用视图): 性能监测的数据可以绘制成调用视图, 方便我们查看; Memory Profile(内存监控): 这个特性帮助我们了解PHP如何分配和释放内存, 值得注意的是, XHProf并不是严格的监测内存的分配和释放动作, 而是计算每个函数进入和退出时的内存状况, 这是一个相对简单的实现方式, 但是基本上也能够满足我们日常的监控需求; 如何处理外部文件?XHProf将 include,require,include_once,require_once进来的文件视作是一个 function;XHProf目前只支持一个级别的函数追踪, 但是貌似也没有特别大的影响; 对比Xdebug: Xdebug 是一个开放源代码的PHP程序调试器; 对于本地开发环境来说, 进行性能分析 Xdebug 是够用了; 但如果是线上环境的话, Xdebug 消耗较大, 配置也不够灵活; XHProx的安装配置 注意pecl和github上star比较多的xhprof有如下问题 php5.4及以上版本, pecl不支持安装xhprof, 需要通过phpize编译; http://pecl.php.net/package/xhprof上的代码包, xhprof最后一次更新是在2013年; 但是两种都不支持php7; 如果需要支持php7, 需要到此处下载 https://laravel-china.org/articles/6474/custom-sampling-method-for-xhprof https://github.com/tideways/php-xhprof-extension 动态编译安装扩展过程: 12345678910111213141516cd /usr/local/srcgit clone https://github.com/longxinH/xhprof.gitcd xhprof/extension/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-config make &amp;&amp; make install// 结果....----------------------------------------------------------------------Build complete.Don&apos;t forget to run &apos;make test&apos;.Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20151012/[root@lant extension]# 配置php.ini 12345vi /usr/local/php/etc/php.ini// 添加如下配置块[xhprof]extension=xhprof.so;xhprof.output_dir=/tmp/xhprof // 注意提前创建好该目录 成功后, 访问phpinfo, 会发现xhprof模块 或者可以如下检测: 123[root@lant extension]# /usr/local/php/sbin/php-fpm -m | grep xhprofxhprof[root@lant extension]# 测试xhprof提供的示例 复制clone下的xhprof目录下的 examples,xhprof_html,xhprof_lib 到网站目录/html下 123[root@lant html]# cp -r /usr/local/src/xhprof/examples/ /html[root@lant html]# cp -r /usr/local/src/xhprof/xhprof_html/ /html[root@lant html]# cp -r /usr/local/src/xhprof/xhprof_lib/ /html 访问虚拟站点: https://www.vhostnginx.com/examples/sample.php, 得到 1Array ( [foo==&gt;bar] =&gt; Array ( [ct] =&gt; 5 [wt] =&gt; 19 ) [bar==&gt;bar@1] =&gt; Array ( [ct] =&gt; 4 [wt] =&gt; 8 ) [bar@1==&gt;bar@2] =&gt; Array ( [ct] =&gt; 3 [wt] =&gt; 5 ) [bar@2==&gt;bar@3] =&gt; Array ( [ct] =&gt; 2 [wt] =&gt; 2 ) [bar@3==&gt;bar@4] =&gt; Array ( [ct] =&gt; 1 [wt] =&gt; 0 ) [main()==&gt;foo] =&gt; Array ( [ct] =&gt; 1 [wt] =&gt; 34 ) [main()==&gt;xhprof_disable] =&gt; Array ( [ct] =&gt; 1 [wt] =&gt; 0 ) [main()] =&gt; Array ( [ct] =&gt; 1 [wt] =&gt; 43 ) ) --------------- Assuming you have set up the http based UI for XHProf at some address, you can view run at http:///index.php?run=5b751a19786eb&amp;source=xhprof_foo --------------- UI界面可以访问: https://www.vhostnginx.com/xhprof_html/index.php?**run=5b751a19786eb&amp;source=xhprof_foo** (为上一步访问结果中给出的参数) 点击 [View Full Callgraph] 如果报错, 那是因为系统需要安装graphviz, graphviz是一个绘制图形的工具, 可以更为直观的让你查看性能的瓶颈 12yum -y install libpngyum -y install graphviz 编写代码测试 代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?phpxhprof_enable();function test1()&#123; foreach ($i=0; $i &lt; 100; $i++) &#123; foreach ($j=0; $j &lt; 10; $j++)&#123; if ($i==99 &amp;&amp; $j==9) &#123; echo &apos;test1&apos;; &#125; &#125; &#125;&#125;function test2()&#123; foreach ($i=0; $i &lt; 10; $i++) &#123; foreach ($j=0; $j &lt; 10; $j++)&#123; if ($i==9 &amp;&amp; $j==9) &#123; test1();&lt;?phpxhprof_enable();// 测试代码function test1()&#123; for ($i=0; $i &lt; 100; $i++) &#123; for ($j=0; $j &lt; 10; $j++)&#123; if ($i==99 &amp;&amp; $j==9) &#123; echo &apos;test1&apos;; &#125; &#125; &#125;&#125;function test2()&#123; for ($i=0; $i &lt; 10; $i++) &#123; for ($j=0; $j &lt; 10; $j++)&#123; if ($i==9 &amp;&amp; $j==9) &#123; test1();test2();// 测试代码结束$xhprof_data = xhprof_disable();print_r($xhprof_data);$XHPROF_ROOT = realpath(dirname(__FILE__));include_once $XHPROF_ROOT . &quot;/xhprof_lib/utils/xhprof_lib.php&quot;;include_once $XHPROF_ROOT . &quot;/xhprof_lib/utils/xhprof_runs.php&quot;;$xhprof_runs = new XHProfRuns_Default();$run_id = $xhprof_runs-&gt;save_run($xhprof_data, &quot;xhprof_foo&quot;);echo &quot;http://&lt;xhprof-ui-address&gt;/index.php?run=$run_id&amp;source=xhprof_foo&quot;; 访问: https://www.vhostnginx.com/test.php 访问: http://www.vhostnginx.com/xhprof_html/index.php?run=5b751ee6d1da8&amp;source=xhprof_foo XHProf报告字段含义 Function Name: 方法名称; Calls: 方法被调用的次数; Calls%: 方法调用次数在同级方法总数调用次数中所占的百分比; Incl.Wall Time(microsec): 方法执行花费的时间, 包括子方法的执行时间(单位:微秒) IWall%: 方法执行花费的时间百分比; Excl. Wall Time(microsec): 方法本身执行花费的时间, 不包括子方法的执行时间(单位:微秒) EWall%: 方法本身执行花费的时间百分比; Incl. CPU(microsecs): 方法执行花费的CPU时间, 包括子方法的执行时间(单位:微秒) ICpu%: 方法执行花费的CPU时间百分比; Excl. CPU(microsec): 方法本身执行花费的CPU时间, 不包括子方法的执行时间(单位:微秒) ECPU%: 方法本身执行花费的CPU时间百分比; Incl.MemUse(bytes): 方法执行占用的内存, 包括子方法执行占用的内存(单位:字节) IMemUse%: 方法执行占用的内存百分比。 Excl.MemUse(bytes): 方法本身执行占用的内存, 不包括子方法执行占用的内存(单位:字节) EMemUse%: 方法本身执行占用的内存百分比。 Incl.PeakMemUse(bytes): Incl.MemUse峰值(单位:字节) IPeakMemUse%: Incl.MemUse峰值百分比; Excl.PeakMemUse(bytes): Excl.MemUse峰值(单位:字节) EPeakMemUse%: Excl.MemUse峰值百分比; 实际项目测试 以公司目前的lumen项目为例, 可以将xhprof_html和xhprof_lib目录拷贝到/public下, 然后在入口文件加上 1xhprof_enable( XHPROF_FLAGS_NO_BUILTINS | XHPROF_FLAGS_CPU | XHPROF_FLAGS_MEMORY); 然后在程序最后输出之前加上 12345678910//在程序结束后收集数据$xhprof_data = xhprof_disable();print_r($xhprof_data);$XHPROF_ROOT = &quot;/wwwroot/oms/public&quot;;//realpath(dirname(__FILE__));echo $XHPROF_ROOT . &quot;/xhprof_lib/utils/xhprof_lib.php&quot;;include_once $XHPROF_ROOT . &quot;/xhprof_lib/utils/xhprof_lib.php&quot;;include_once $XHPROF_ROOT . &quot;/xhprof_lib/utils/xhprof_runs.php&quot;;$xhprof_runs = new XHProfRuns_Default();$run_id = $xhprof_runs-&gt;save_run($xhprof_data, &quot;xhprof_foo&quot;);echo &quot;http://&lt;xhprof-ui-address&gt;/index.php?run=$run_id&amp;source=xhprof_foo&quot;; 另外, 其实网上还有资料介绍了php.ini中的两个配置项 auto_prepend_file 和 auto_append_file 可以用来在所有php程序运行前和运行后设置对应的运行文件; 不过在公司lumen项目测试后发现auto_append_file可能并不生效; 不过自己写一些简单的代码例子做测试, 倒是可以尝试配置使用; 问题 可以看到xhprof作为年久失修的性能分析工具, 也只能下载一些不知名的包才能在php7上成功使用; 而且目前很多项目都是使用了PHP框架的, 要做性能分析, 其实对于xhprof输出的结构图来说, 有很多都是框架本身的性能分析结果, 也无法直观看到开发人员编写部分的代码性能问题; 参考 https://haofly.net/xhprof/ https://www.cnblogs.com/cocowool/archive/2010/06/02/1750198.html https://mp.weixin.qq.com/s/VQ5F_-09EAaJePe5o1Bedg https://www.jianshu.com/p/8fb9ad0719c2 https://www.jianshu.com/p/c69e368de756 https://blog.csdn.net/qq_28602957/article/details/72697901","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"","slug":"php/2018-05-06-opcache","date":"2018-05-02T14:26:39.000Z","updated":"2018-08-15T10:23:35.000Z","comments":true,"path":"2018/05/02/php/2018-05-06-opcache/","link":"","permalink":"http://blog.renyimin.com/2018/05/02/php/2018-05-06-opcache/","excerpt":"","text":"https://www.abcdocker.com/abcdocker/2151https://www.cnblogs.com/wajika/p/6249003.html http://gywbd.github.io/posts/2016/1/best-config-for-zend-opcache.html","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"05. 聚合查询","slug":"elasticsearch/2018-04-14-05","date":"2018-04-14T03:21:46.000Z","updated":"2018-08-31T09:57:17.000Z","comments":true,"path":"2018/04/14/elasticsearch/2018-04-14-05/","link":"","permalink":"http://blog.renyimin.com/2018/04/14/elasticsearch/2018-04-14-05/","excerpt":"","text":"桶 与 指标 其实简单来说, 桶 和 指标 其实就是以前关系型数据中常用的 分组 和 统计; 桶 能让我们划分文档到有意义的集合, 但是最终我们需要的是对这些桶内的文档进行一些指标的计算;分桶是一种达到目的的手段:它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。 指标 大多数指标是简单的数学运算(例如最小值、平均值、最大值, 还有汇总), 这些是通过文档的值来计算在实践中, 指标能让你计算像平均薪资、最高出售价格、95%的查询延迟这样的数据 聚合其实就是由桶和指标组成的 (可能只有一个桶, 可能只有一个指标, 或者可能两个都有; 也有可能有一些桶嵌套在其他桶里面) 由于桶可以被嵌套, 所以我们可以实现非常多并且非常复杂的聚合, 如: 通过国家划分文档(桶), 然后通过性别划分每个国家(桶), 然后通过年龄区间划分每种性别（桶）, 最后，为每个年龄区间计算平均薪酬（指标） 结果将告诉你每个 &lt;国家, 性别, 年龄&gt; 组合的平均薪酬, 所有的这些都在一个请求内完成并且只遍历一次数据！ 测试 构建数据 _bulk 1234POST /_bulk&#123; &quot;create&quot;: &#123; &quot;_index&quot;: &quot;aggregations&quot;, &quot;_type&quot;: &quot;test&quot;, &quot;_id&quot;:1&#125;&#125;&#123; &quot;country&quot;: &quot;Uruguay&quot;, &quot;gender&quot;: &quot;female&quot;, &quot;age&quot;: &quot;29&quot;&#125;.... 求 /aggregations/test/ 所有文档中的平均年龄 123456789GET /aggregations/test/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;people_avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot;&#125; &#125; &#125;&#125; 求 /aggregations/test/ 所有文档中按某字段去重后的数量 去重是一个很常见的操作,可以回答很多基本的业务问题:网站独立访客是多少?卖了多少种汽车?每月有多少独立用户购买了商品? 例子: 不同年龄的总共有多少人123456789GET /aggregations/test/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;people_avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot;&#125; &#125; &#125;&#125; 安时间统计 (比如: 今年每月销售多少台汽车) 1 全文搜索 截止目前的搜索相对都很简单:单个姓名, 通过年龄过滤; 现在尝试下稍微高级点儿的全文搜索 — 一项传统数据库确实很难搞定的任务 搜索下所有喜欢攀岩（rock climbing）的雇员： 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; 结果: 显然我们依旧使用之前的match查询在about属性上搜索 “rock climbing”, 得到两个匹配的文档: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, // 相关性得分 &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.2876821, // 相关性得分 &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125; ] &#125;&#125; Elasticsearch 默认按照相关性得分排序, 即每个文档跟查询的匹配程度 第一个最高得分的结果很明显: John Smith 的 about 属性清楚地写着 “rock climbing”; 但为什么 Jane Smith 也作为结果返回了呢？原因是她的 about 属性里提到了 “rock”, 因为只有 “rock” 而没有 “climbing”, 所以她的相关性得分低于 John 的; 这是一个很好的案例, 阐明了 Elasticsearch 如何在全文属性上搜索并返回相关性最强的结果;Elasticsearch中的 相关性 概念非常重要, 也是完全区别于传统关系型数据库的一个概念, 数据库中的一条记录要么匹配要么不匹配; 短语搜索 找出一个属性中的独立单词是没有问题的, 但有时候想要精确匹配一系列单词或者短语, 比如, 我们想执行这样一个查询, 仅匹配同时包含 “rock” 和 “climbing”, 并且二者以短语 “rock climbing” 的形式紧挨着的雇员记录; 为此对 match 查询稍作调整, 使用一个叫做 match_phrase 的查询: 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; 毫无悬念，返回结果仅有 John Smith 的文档; 1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 12, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125; ] &#125;&#125; 高亮搜索 许多应用都倾向于在每个搜索结果中高亮部分文本片段, 以便让用户知道为何该文档符合查询条件, 在 Elasticsearch 中检索出高亮片段也很容易; 再次执行前面的查询, 并增加一个新的 highlight 参数: 12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;about&quot; : &#123;&#125; &#125; &#125;&#125; 结果: 当执行该查询时, 返回结果与之前一样, 与此同时结果中还多了一个叫做 highlight 的部分, 这个部分包含了 about 属性匹配的文本片段, 并以 HTML 标签 封装: 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 75, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;about&quot;: [ // 原始文本中的高亮片段 &quot;I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;&quot; ] &#125; &#125; ] &#125;&#125; 分析 终于到了最后一个业务需求: 支持管理者对雇员目录做分析; Elasticsearch 有一个功能叫聚合(aggregations), 允许我们基于数据生成一些精细的分析结果, 聚合与 SQL 中的 GROUP BY 类似但更强大 例子, 挖掘出雇员中最受欢迎的兴趣爱好 注意: 5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启，官方解释在此fielddata 因此需要先开启fielddata 123456789PUT megacorp/_mapping/employee/&#123; &quot;properties&quot;: &#123; &quot;interests&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 然后运行如下: 12345678910GET /megacorp/employee/_search&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 暂时忽略掉语法，直接看看结果 可以看到, 两位员工对音乐感兴趣, 一位对林地感兴趣, 一位对运动感兴趣; 这些聚合并非预先统计, 而是从匹配当前查询的文档中即时生成12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&#123; &quot;took&quot;: 56, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Douglas&quot;, &quot;last_name&quot;: &quot;Fir&quot;, &quot;age&quot;: 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ] &#125; &#125; ] &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; 如果想知道叫 Smith 的雇员中最受欢迎的兴趣爱好, 可以直接添加适当的查询来组合查询 123456789101112131415GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;last_name&quot;: &quot;smith&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 聚合还支持分级汇总, 比如, 查询特定兴趣爱好员工的平均年龄 12345678910111213GET /megacorp/employee/_search&#123; &quot;aggs&quot; : &#123; &quot;all_interests&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;interests&quot; &#125;, &quot;aggs&quot; : &#123; &quot;avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 结果输出基本是第一次聚合的加强版, 依然有一个兴趣及数量的列表, 只不过每个兴趣都有了一个附加的 avg_age 属性, 代表有这个兴趣爱好的所有员工的平均年龄12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&#123; &quot;took&quot;: 19, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Douglas&quot;, &quot;last_name&quot;: &quot;Fir&quot;, &quot;age&quot;: 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ] &#125; &#125; ] &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2, &quot;avg_age&quot;: &#123; &quot;value&quot;: 28.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 35 &#125; &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 25 &#125; &#125; ] &#125; &#125;&#125; 即使现在不太理解这些语法也没有关系, 依然很容易了解到复杂聚合及分组通过 Elasticsearch 特性实现得很完美, 可提取的数据类型毫无限制; 交互 计算集群中文档的数量 12345678910111213141516$ curl -XGET &apos;localhost:9200/_count?pretty&apos; -H &apos;Content-Type:application/json&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;&apos;// 结果&#123; &quot;count&quot; : 0, &quot;_shards&quot; : &#123; &quot;total&quot; : 0, &quot;successful&quot; : 0, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;&#125; 在返回结果中没有看到 HTTP 头信息是因为我们没有要求curl 显示它们, 想要看到头信息, 需要结合 -i 参数来使用 curl 命令;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"","slug":"guzzle/2018-04-11","date":"2018-04-11T05:16:21.000Z","updated":"2018-08-21T02:11:37.000Z","comments":true,"path":"2018/04/11/guzzle/2018-04-11/","link":"","permalink":"http://blog.renyimin.com/2018/04/11/guzzle/2018-04-11/","excerpt":"","text":"","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"04. Query DSL","slug":"elasticsearch/2018-04-09-04","date":"2018-04-09T14:06:32.000Z","updated":"2018-08-31T09:57:12.000Z","comments":true,"path":"2018/04/09/elasticsearch/2018-04-09-04/","link":"","permalink":"http://blog.renyimin.com/2018/04/09/elasticsearch/2018-04-09-04/","excerpt":"","text":"Match All Query 最简单的查询, 匹配所有文档, 给它们一个 _score 1.0 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 可以通过 boost 参数修改权重 (boost为1时, 和上面一样) 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &quot;boost&quot; : 1.2 &#125; &#125;&#125; 与match_all查询相反, 它不匹配任何文档: 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_none&quot;: &#123;&#125; &#125;&#125; 全文查询全文查询通常用于在全文字段(如电子邮件正文)上运行全文查询, 它了解如何分析被查询的字段, 并在执行之前将每个字段的分析器(或search_analyzer)应用于查询字符串;在该组中的查询主要有如下 match query 用于执行全文查询的标准查询, 包括 模糊匹配 和 短语 或 邻近 查询 match 查询接受 text/numerics/dates，分析它们并构造查询, 例如 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;title&quot;: &quot;BROWN DOG!&quot; // 如果需要对字段设置一些其他选项, 这里就不是简单字符串, 而是一个&#123;&#125;, 里面用 **query** 指定这里的匹配内容, 后面有例子 &#125; &#125;&#125; 注意, title 是字段的名称, 你可以替换任何字段的名称(包括_all) 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123;&quot;_all&quot;:&quot;25&quot;&#125; &#125;&#125; operator 之前的查询, 任何文档只要 title 字段里包含 “brown” 或者 “dog”, 就能匹配; 如果你想搜索的是包含 所有 词项的文档, 也就是说匹配 “brown” 和 “dog”1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;BROWN DOG!&quot;, &quot;operator&quot;: &quot;and&quot; // 默认为 or &#125; &#125; &#125;&#125; minimum_should_match: 控制匹配精度 在 所有 与 任意 间二选一有点过于非黑即白, 如果用户给定 5 个查询词项, 想查找只要包含其中任意 4 个的文档, 将 operator 操作符参数设置成 and 只会将此文档排除; 比较常用的是可以通过将minimum_should_match设置为百分比 1234567891011GET _search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;quick brown dog&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; &#125; &#125; &#125;&#125; analyzer match_phrase 短语匹配: 类似match查询, match_phrase 查询首先将查询字符串解析成一个词项列表, 然后对这些词项进行搜索, 但只保留那些包含全部搜索词项且位置与搜索词项相同的文档, 如下: 1234567891011GET /test_dsl/test_dsl/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;woman is&quot;, &quot;type&quot;: &quot;phrase&quot; &#125; &#125; &#125;&#125; 是对 “woman is” 的短语搜索, 如果没有文档包含 woman 之后紧跟着 is, 则不会匹配到任何文档; 也可以如下: 12345678GET /test_dsl/test_dsl/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &quot;woman is&quot; &#125; &#125;&#125; [match_phrase_prefix] query 与match_phrase查询一样, 但是对最后的单词进行通配符搜索; 如下: 查询包含 woman 后直接跟 is, 然后跟t开头的词, 并且只去前两个文档 1234567891011GET /test_dsl/test_dsl/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;woman is t&quot;, &quot;max_expansions&quot;:2 &#125; &#125; &#125;&#125; max_expansions: max_expansions参数(默认值为50), 该参数可以控制最后一个术语前缀匹配的文档数量, 强烈建议将其设置为可接受的值以控制查询的执行时间; multi_match multi_match 查询基于 match查询构建, 以允许多字段查询; 如下: 123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot; : &#123; // 不是针对一个字段了 &quot;query&quot;: &quot;this is a test&quot;, &quot;fields&quot;: [ &quot;subject&quot;, &quot;message&quot; ] // 只要这两个字段中可以查询到上面的词项, 该文档就会被选中 &#125; &#125;&#125; 可以使用通配符指定字段, 例如: &quot;fields&quot;: [ &quot;title&quot;, &quot;*_name&quot; ] common_terms[query_string] [simple_query_string]1. Term级别查询这些查询通常用于结构化数据, 如数字, 日期和枚举, 而不是全文字段 Term 精确匹配 123456POST /test_dsl/test_dsl/_search // GET也可以&#123; &quot;query&quot;: &#123; &quot;term&quot; : &#123; &quot;age&quot; : &quot;32&quot; &#125; &#125;&#125; 不能拿来查询全文域, 因为全文域会分词, term是精确查找,不会分词; Terms terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值, 那么这个文档满足条件: { &quot;terms&quot;: { &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] }} 和 term 查询一样, terms 查询对于输入的文本不分析; Range 范围匹配 对于日期的范围匹配也可以 ExistsType过滤与提供的文档类型/映射类型匹配的文档 Ids复合查询Bool 现实的查询需求从来都没有那么简单, 它们需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤。为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 你可以用 bool 查询来实现你的需求, 这种查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must: 文档 必须 匹配这些条件才能被包含进来; must_not: 文档 必须不 匹配这些条件才能被包含进来; should: 如果满足这些语句中的任意语句, 将增加 _score, 否则, 无任何影响。它们主要用于修正每个文档的相关性得分; filter: 必须 匹配, 但它以不评分、过滤模式来进行。这些语句对评分没有贡献, 只是根据过滤标准来排除或包含文档; 如下, 先全文匹配, 然后过滤(结果可以发现, 即使去掉filter, 结果_score也不会变, 所以filter并不会贡献分数) 123456789101112131415161718GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;author&quot;: &quot;Micheal&quot; &#125;&#125; ], &quot;filter&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123;&quot;match&quot;:&#123;&quot;age&quot;:32&#125;&#125; ] &#125; &#125; &#125; &#125;&#125; 实战 mysql中 where A or (B and C) 123456789101112131415161718GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;should&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: 78 &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;author&quot;: &quot;Smith&quot; &#125;&#125; ] &#125; &#125; ] &#125; &#125;&#125; mysql中 A and (B or C) 123456789101112131415161718GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: 78 &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;author&quot;: &quot;Micheal&quot; &#125;&#125; ] &#125; &#125; ] &#125; &#125;&#125; must和should同级? 如下, 其实是 where (title=&#39;woman&#39;) and (age=32 or 没有了) 即 where (title=&#39;woman&#39;) and (age=32) 12345678910111213GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125; ], &quot;should&quot; : [ &#123;&quot;term&quot;: &#123; &quot;age&quot;: &quot;32&quot; &#125;&#125; ] &#125; &#125;&#125; 在filter中也一样","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"03. Query DSL, 查询上下文, 过滤上下文 了解","slug":"elasticsearch/2018-04-09-03","date":"2018-04-09T08:45:57.000Z","updated":"2018-08-31T09:57:05.000Z","comments":true,"path":"2018/04/09/elasticsearch/2018-04-09-03/","link":"","permalink":"http://blog.renyimin.com/2018/04/09/elasticsearch/2018-04-09-03/","excerpt":"","text":"Query DSLElasticsearch 提供基于JSON的完整 Query DSL(领域特定语言) 来定义查询, 主要由两种类型的子句组成: 叶查询: 叶查询语句查找特定字段中的特定值, 例如 match, term or range 查询, 这些查询可以单独使用; 复合查询: 复合查询子句包装其他叶子或复合查询, 用于以逻辑方式组合多个查询(例如bool或dis_max查询), 或更改其行为(如constant_score查询) query子句的行为取决于它是在查询上下文中还是在过滤器上下文中使用; Elasticsearch 默认按照相关性得分排序, 即每个文档跟查询的匹配程度 (Elasticsearch中的 相关性 概念非常重要, 也是完全区别于传统关系型数据库的一个概念, 数据库中的一条记录要么匹配要么不匹配;) 查询上下文 查询上下文中使用的查询子句回答了 “此文档与此查询子句的匹配程度如何?”, 即 除了确定文档是否匹配之外, 查询子句还计算一个_score, 表示文档相对于其他文档的匹配程度; 只要将查询子句传递给查询参数, 查询上下文就会生效; 过滤上下文 在过滤器上下文中, 查询子句回答了 “此文档是否与此查询子句匹配?” 答案是简单的是或否, 不计算任何分数, 如下, 同样的结果, 前者的score比后者低很多 1234567891011121314151617181920212223242526272829GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125; ], &quot;filter&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123;&quot;term&quot;:&#123;&quot;age&quot;:54&#125;&#125; ] &#125; &#125; &#125; &#125;&#125;GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;woman&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;age&quot;: &quot;54&quot; &#125;&#125; ] &#125; &#125;&#125; 过滤器上下文主要用于过滤结构化数据, 例如: 此时间戳是否属于2015年至2016年的范围？ 状态字段是否设置为“已发布”？ Elasticsearch会自动缓存经常使用的过滤器, 以加快性能; 只要将查询子句传递给过滤器参数, 过滤器上下文就会生效, 例如 bool 查询中的 filter 或 must_not 参数, constant_score 查询中的filter参数或filter聚合; 一般是 全文匹配 + 过滤(精确匹配) 如果要想精确匹配来加分, 则可以全文+精确 ？？ 测试 下面是在搜索API中的 查询上下文 和 过滤器上下文 中使用的查询子句的示例, 查询将匹配满足以下所有条件的文档: 1234567891011121314151617181920标题字段包含单词搜索;内容字段包含单词elasticsearch;状态字段包含已发布的确切单词;publish_date字段包含从2015年1月1日起的日期;GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Elasticsearch&quot; &#125;&#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ] &#125; &#125;&#125; 如何选择查询与过滤通常的规则是, 使用 查询(query) 来进行全文搜索或者其它任何需要影响相关性得分的搜索; 除此以外的情况都使用过滤(filters)","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"02. 集群, 分片","slug":"elasticsearch/2018-04-08-02","date":"2018-04-08T14:02:30.000Z","updated":"2018-09-01T11:11:25.000Z","comments":true,"path":"2018/04/08/elasticsearch/2018-04-08-02/","link":"","permalink":"http://blog.renyimin.com/2018/04/08/elasticsearch/2018-04-08-02/","excerpt":"","text":"","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"02. 搜索","slug":"elasticsearch/2018-04-08-03","date":"2018-04-08T13:03:25.000Z","updated":"2018-08-31T09:57:00.000Z","comments":true,"path":"2018/04/08/elasticsearch/2018-04-08-03/","link":"","permalink":"http://blog.renyimin.com/2018/04/08/elasticsearch/2018-04-08-03/","excerpt":"","text":"有两种形式的搜索API: 一种是 轻量的 查询字符串版本, 要求在查询字符串中传递所有的参数; 另一种是更完整的 请求体 版本, 要求使用 JSON 格式和更丰富的查询表达式作为搜索语言 空搜索 简单地返回集群中所有索引下的所有文档 轻量: GET /_search 请求体:1234GET /_search&#123; &#125; 多索引,多类型 要在一个或多个索引 或者 在一个或多个类型中进行搜索: 轻量: 可以通过在URL中指定特殊的索引和类型达到这种效果 123456/gb/_search : 在 gb 索引中搜索所有的类型/gb,us/_search : 在 gb 和 us 索引中搜索所有的文档/g*,u*/_search : 在任何以 g 或者 u 开头的索引中搜索所有的类型 (正则只能用于索引, 不可以用于类型GET /megacorp/e*/_search, GET /megacorp/*/_search do都不可以)/gb/user/_search : 在 gb 索引中搜索 user 类型的所有文档/gb,us/user,tweet/_search : 在 gb 和 us 索引中搜索 user 和 tweet 类型的所有文档/_all/user,tweet/_search : 在所有的索引中搜索 user 和 tweet 类型的所有文档 请求体: URL中 索引和类型的指定同上 12GET /index_2014*/type1,type2/_search&#123;&#125; 过滤 轻量搜索: ?q 单个字段过滤 GET /megacorp/employee/_search?q=last_name:Smith 多字段 OR 过滤, default_operator参数的使用, 可参考http://cwiki.apachecn.org/display/Elasticsearch/URI+Search 1GET /megacorp/employee/_search?default_operator=OR&amp;q=last_name:Smith+first_name:Jane 多字段 AND 过滤, default_operator参数的使用, 可参考http://cwiki.apachecn.org/display/Elasticsearch/URI+Search 1GET /megacorp/employee/_search?default_operator=AND&amp;q=last_name:Smith+first_name:Jane 返回指定字段,_source_include,_source_excludes 可参考https://www.elastic.co/guide/en/elasticsearch/reference/5.5/search-request-source-filtering.html 12GET /megacorp/employee/_search?default_operator=AND&amp;_source_include=first_name,age&amp;q=last_name:Smith+first_name:JaneGET /megacorp/employee/_search?default_operator=AND&amp;_source_include=*e&amp;q=last_name:Smith+first_name:Jane 分页: size 和 from(从0开始) 1GET /megacorp/employee/_search?q=first_name:Jane&amp;size=1&amp;from=0","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"01. 简介, 安装","slug":"elasticsearch/2018-04-08-01","date":"2018-04-08T06:24:25.000Z","updated":"2018-09-01T11:08:04.000Z","comments":true,"path":"2018/04/08/elasticsearch/2018-04-08-01/","link":"","permalink":"http://blog.renyimin.com/2018/04/08/elasticsearch/2018-04-08-01/","excerpt":"","text":"认识Elasticsearch Elasticsearch 是一个基于Lucene的(隐藏了内部引擎Lucene的复杂运行原理), 高度可扩展, 且开源的全文检索和分析引擎, 可以快速且近实时地存储(从文档索引到可以被检索只有轻微延时, 约1s), 检索以及分析海量数据, 通常用作那些具有复杂搜索功能和需求的应用的底层引擎或者技术; 它是面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引; 提供了一套基于Restful风格的全文检索服务组件, 基本上所有操作(索引、查询、甚至是配置)都可以通过 HTTP 接口进行; 使用案例 最简单的案例就是, 在微服务架构下的多数据源聚合列表页, 即, 一个页面中的数据来自多个服务, 且筛选条件也涉及到多个服务中的数据字段; 您想要去收集日志或交易数据, 并且还想要去分析和挖掘这些数据来找出趋势, 统计, 或者异常现, 在这种情况下, 您可以使用 Logstash(Elasticsearch/Logstash/Kibana) 技术栈中的一部分, 来收集, 聚合, 以及解析数据, 然后让 Logstash 发送这些数据到 Elasticsearch; 如果这些数据存在于 Elasticsearch 中, 您就可以执行搜索和聚合以挖掘出任何您感兴趣的信息; … GitHub 使用 Elasticsearch 对1300亿行代码进行查询; 版本选择 目前(04/2018)为止, Elasticsearch已经到6.X了, 可参考官网文档; 不过中文文档翻译进度比较滞后, 当前阿里云的Elasticsearch云服务为5.5.3, 这里也是针对5.X版本进行学习 安装 elasticsearch 下载安装: https://www.elastic.co/downloads/past-releases (可选择需要的版本), 此处选择了5.5.3 (Mac版) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 这里是使用tar包进行安装, 当然你也可以选择去用brew安装$ cd ~/Desktop/est$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lselasticsearch elasticsearch-keystore.bat elasticsearch-plugin.bat elasticsearch-service-x64.exe elasticsearch-service.bat elasticsearch-translog elasticsearch.bat elasticsearch.in.shelasticsearch-keystore elasticsearch-plugin elasticsearch-service-mgr.exe elasticsearch-service-x86.exe elasticsearch-systemd-pre-exec elasticsearch-translog.bat elasticsearch.in.bat// 启动$ ./elasticsearch........[2018-08-28T20:10:54,296][INFO ][o.e.h.n.Netty4HttpServerTransport] [eCgKpl8] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[fe80::1]:9200&#125;, &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2018-08-28T20:10:54,296][INFO ][o.e.n.Node ] [eCgKpl8] started// 然后直接访问 127.0.0.1:9200, 结果如下:&#123; &quot;name&quot; : &quot;eCgKpl8&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;ExAhdS8VSnKNcr-waUCumA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;//curl直接请求renyimindeMacBook-Pro:config renyimin$ curl &apos;http://localhost:9200/?pretty&apos;&#123; &quot;name&quot; : &quot;eCgKpl8&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;ExAhdS8VSnKNcr-waUCumA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; Elasticsearch Head: Est 启动后, 访问127.0.0.1:9200可以查看版本等一些信息, 但这不是图形化的界面, 操作起来不是很方便, 所以希望能有一个可视化的环境来操作它, 可以通过安装 Elasticsearch Head 这个插件来进行管理; Elasticsearch Head是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装Elasticsearch Head必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体安装步骤如下: 12345678910cd ~/Desktop/est// 安装nodejs// github下载 Elasticsearch Head 源码:https://github.com/mobz/elasticsearch-head git clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install// 安装NodeJS之后, 使用npm安装gruntnpm install -g grunt -cli// 然后在目录下执行cnpm install 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi ~/Desktop/est/elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 grunt server; (在非elasticsearch-head目录中启动server会失败, 因为grunt需要读取目录下的Gruntfile.js) 最后访问 127.0.0.1:9100 注意, 目前为止 elasticsearch 和 elasticsearch-head 均为前台启动, 如果终端退出, 服务也都会随之关闭; Kibana: 为 ElasticSearch 提供的数据分析的 Web 接口, 可使用它对日志进行高效的搜索、可视化、分析等各种操作; 下载安装: https://www.elastic.co/downloads/past-releases (可选择需要的版本), 此处选择了5.5.3 (Mac版) 123cd ~/Desktop/estwget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-darwin-x86_64.tar.gztar -zxvf kibana-5.5.3-darwin-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;127.0.0.1&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12cd ~/Desktop/est/kibana-5.5.3-darwin-x86_64/bin./kibana 浏览器访问地址:http://127.0.0.1:5601/, 如下是因为 使用可参考: https://yq.aliyun.com/ziliao/310009 DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 术语 索引 一词在 Elasticsearch 语境中包含多重意思, 有必要说明一下: 索引(名词): 一个索引类似于传统关系数据库中的一个数据库, 是一个存储关系型文档的地方; 复数词为 indices 或 indexes 索引(动词): 索引一个文档, 就是存储一个文档到一个索引(名词)中, 以便它可以被检索和查询到;这非常类似于 SQL 语句中的 INSERT 关键词(不过在此处, 文档已存在时新文档会替换旧文档) 一个 Elasticsearch 集群可以包含多个索引(n), 每个索引可以包含多个类型, 这些不同的类型存储着多个文档, 每个文档又有多个属性, 简单理解如下: 索引对应mysql的数据库 类型对应mysql数据库中的表 文档对应mysql表中的一条记录 属性对应字段 简单操作有两种形式的搜索API: 一种是 轻量的 查询字符串版本, 要求在查询字符串中传递所有的参数; 另一种是更完整的 请求体 版本, 要求使用 JSON 格式和更丰富的查询表达式作为搜索语言 索引(v:存储)文档: 索引一个雇员文档, 每个文档都将是’employee’类型, 该类型位于索引(n)’megacorp’ 内, 该索引(n)保存在我们的 Elasticsearch 集群中 可以通过一条命令完成所有这些动作: 123456789101112# 路径 &apos;/megacorp/employee/1&apos; 包含了三部分的信息: # megacorp: 索引名称, # employee: 类型名称,# 1: 特定雇员的IDPUT /megacorp/employee/1 &#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 索引(v)更多员工信息 123456789PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;...... 检索文档 - 轻量搜索: 检索到单个雇员的数据, 在 Elasticsearch 中很简单, 执行一个HTTP GET 请求, 并指定文档的地址(使用索引库、类型和ID这三个信息即可返回原始的 JSON 文档) 返回结果包含了文档的一些 元数据, 以及 _source 属性(内容是John Smith雇员的原始JSON文档)123456789101112131415161718GET /megacorp/employee/1&#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 5, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 删除文档: 1234567891011121314DELETE /megacorp/employee/1&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 11, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 检查文档是否存在: 12HEAD /megacorp/employee/1404 - Not Found _search: 搜索 索引/类型 中所有雇员 可以看到, 我们仍然使用索引库 megacorp 以及类型 employee, 但与指定一个文档 ID 不同, 这次使用 _search, 返回结果包括了所有文档, 放在数组 hits 中;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647GET /megacorp/employee/_search&#123; &quot;took&quot;: 0, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125; ] &#125;&#125; 使用请求体方法的等价搜索是 (有两种形式的 搜索 API：一种是 “轻量的” 查询字符串 版本，要求在查询字符串中传递所有的 参数，另一种是更完整的 请求体 版本，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言) 1234curl -XPOST &quot;http://localhost:9200/megacorp/employee/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123;&quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;&apos; 搜索姓氏为”Smith”的雇员, 这里使用一个高亮搜索, 这个方法一般涉及到一个查询字符串 ?q= ： 仍然在请求路径中使用 _search 端点, 并将查询本身赋值给参数 q= , 返回结果给出了所有的 Smith: GET /megacorp/employee/_search?q=first_name:Jane","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/tags/Elasticsearch/"}]},{"title":"JWT(JSON Web Token)","slug":"jwt/2018-04-07","date":"2018-04-07T11:23:57.000Z","updated":"2018-08-21T03:25:46.000Z","comments":true,"path":"2018/04/07/jwt/2018-04-07/","link":"","permalink":"http://blog.renyimin.com/2018/04/07/jwt/2018-04-07/","excerpt":"","text":"https://www.jianshu.com/p/af8360b83a9f","categories":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/categories/杂项/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"http://blog.renyimin.com/tags/杂项/"}]},{"title":"PHP7新特性","slug":"php/2018-03-27-php7","date":"2018-03-27T13:26:39.000Z","updated":"2018-08-20T10:45:43.000Z","comments":true,"path":"2018/03/27/php/2018-03-27-php7/","link":"","permalink":"http://blog.renyimin.com/2018/03/27/php/2018-03-27-php7/","excerpt":"","text":"","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"32","slug":"linux/2018-03-25-32","date":"2018-03-25T12:43:26.000Z","updated":"2018-08-14T09:23:07.000Z","comments":true,"path":"2018/03/25/linux/2018-03-25-32/","link":"","permalink":"http://blog.renyimin.com/2018/03/25/linux/2018-03-25-32/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"31","slug":"linux/2018-03-25-31","date":"2018-03-25T08:50:21.000Z","updated":"2018-08-14T09:22:36.000Z","comments":true,"path":"2018/03/25/linux/2018-03-25-31/","link":"","permalink":"http://blog.renyimin.com/2018/03/25/linux/2018-03-25-31/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"30. Linux软件安装及管理","slug":"linux/2018-03-25-30","date":"2018-03-25T07:28:21.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/03/25/linux/2018-03-25-30/","link":"","permalink":"http://blog.renyimin.com/2018/03/25/linux/2018-03-25-30/","excerpt":"","text":"概述在Linux平台下, 软件包的类型可以划分为两类: 源码包、二进制包; 源码包: 软件的源代码(一般也叫Tarball, 即将软件的源码以tar打包后再压缩的资源包); 二进制包: 如 Red Hat发行版的 .rpm 包, Debian发行的.deb包; 源码包编译安装 源码包主要安装步骤大致如下: 获取程序源代码文件: 将tarball文件在/usr/local/src目录下解压 查看安装步骤流程: 进入解压好的源码包目录, 查阅INSTALL、README等相关文件内容 相关属性软件安装: 根据INSTALL、README的内容查看并安装好一些相关的软件 建立makefile: 以自动检测程序(configure或config)检测操作环境, 并建立Makefile文件 编译: 以make这个程序并使用该目录下的Makefile作为它的参数配置文件, 来进行make(编译或其他)的操作 安装: 以make这个程序并使用该目录下的Makefile作为它的参数配置文件, 根据install这个目标的指定来安装到正确的路径 tarball软件安装的命令执行方式简要如下: ./configure: 在源码包中运行 ./configure --prefix=/usr/local/XXX 进行环境检测, 并生成 Makefile文件该步骤比较重要, 安装信息都是这一步骤内完成的, 最好参考一下INSTALL、README的内容 makeclean: 这一步不一定需要执行, 但谁也不确定源码包中到底有没有包含上次编译过的目标文件(*.o), 所以清楚一下比较妥当 make: 会依据Makefile当中的默认工作进行编译的行为; make install: 通常是最后的步骤, 会依据Makefile文件里面关于install的选项, 将上一步骤所编译完成的数据安装到默认的目录中, 就完成安装! Tarball软件安装建议 考虑到管理用户所安装软件的便利性, 用户自行安装的软件建议放置在 /usr/local 里面; 源码则建议放在 /usr/local/src 下面;以nginx来说, 一般源码放在 /usr/local/src/ 下, 安装到 /usr/local/nginx 下;这样对于移除该软件来说就非常方便了, 直接将该软件的安装目录移除掉即可 不过, 如果要执行该软件的话, 要么使用绝对路径(如 /usr/local/nginx/sbin/nginx ), 要么需要将这个路径加到PATH里面; Nginx编译安装测试MySQL编译安装测试卸载使用源码包编译安装的软件, 只需要把整个安装路径删除掉就行了, 100%卸载, 不会像windows那样经常会残留那多注册表之类的垃圾; 但是根据你的安装时候选择的安装路径, 源码包的卸载又分为两种情况: 如果你安装时候指定的路径为 --prefix=/usr/local/XXX, 那么卸载的时候只需要把XXX这个文件夹删除即可, 因为该软件安装的所有文件都是放置在XXX这个文件夹; 但如果你进行源码包安装的时候没有指定位置(默认值一般为/usr/local/), 或指定的位置为 --prefix=/usr/local, 则这时候你安装软件生成的文件将分别存储在/usr/local/里面的bin、lib或ect等目录中, 如果用这种配置安装了很多软件, 那么这时候卸载某一个软件就相对麻烦很多了; 优缺点 源码包的优点: 开源, 如果有能力可以修改源代码 可以自由选择所需的功能 软件是编译安装, 所以更加适合自己的系统, 更加稳定也效率更高 卸载方便 源码包的缺点: 安装过程步骤较多, 尤其安装较大的软件集合时(如LAMP环境搭建) 编译过程时间较长, 安装比二进制安装时间长 因为是编译安装, 安装过程中一旦报错, 新手不太容易解决 二进制包yum rpm RedHat Package Manager, 简称为RPM 这个机制最早由Red Hat这家公司开发出来的, 后来实在很好用, 因此很多distributons就使用这个机制来作为软件安装的管理方式, 包括Fedora、CentOS、SuSE等知名的开发商都是用它; RPM最大的特点就是需要安装的软件已经编译过, 并已经打包成RPM机制的安装包, 通过里头默认的数据库记录这个软件安装时需要的依赖软件; 当安装在你的Linux主机时, RPM会先依照软件里头的数据查询Linux主机的依赖属性软件是否满足, 若满足则予以安装, 若不满足则不予安装; https://segmentfault.com/a/1190000011200461 优缺点 二进制包的优点: 包管理系统简单, 只通过几个命令就可以实现包的安装、升级、查询和卸载 安装速度比源码快得多 二进制包的缺点: 经过编译, 不可以再看到源码 功能选择不如源码包灵活 依赖性 最新更新2018/08/11","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"vim","slug":"linux/2018-03-16-21","date":"2018-03-16T02:05:32.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/03/16/linux/2018-03-16-21/","link":"","permalink":"http://blog.renyimin.com/2018/03/16/linux/2018-03-16-21/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"19. 查看进程信息","slug":"linux/2018-03-10-19","date":"2018-03-10T11:03:51.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/03/10/linux/2018-03-10-19/","link":"","permalink":"http://blog.renyimin.com/2018/03/10/linux/2018-03-10-19/","excerpt":"","text":"ps常用 ps aux |grep nginx 查看nginx进程 lsof lsof(list open files)是一个列出当前系统打开文件的工具, 在linux环境下, 任何事物都以文件的形式存在, 通过文件不仅仅可以访问常规数据, 还可以访问网络连接和硬件; 所以如传输控制协议(TCP)和用户数据报协议(UDP)套接字等, 系统在后台都为该应用程序分配了一个文件描述符, 无论这个文件的本质如何, 该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口; 常用: lsof -i:80 查看80端口进程的运行情况 nestat netstat命令是一个监控TCP/IP网络的非常有用的工具, 它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息; 常用 12345netstat -a # 列出所有端口netstat -at # 列出所有TCP端口netstat -au # 列出所有UDP端口netstat -antp | grep ssh # 查看端口和服务netstat -antp | grep 22","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"18. 查看磁盘使用情况","slug":"linux/2018-03-10-18","date":"2018-03-10T03:46:32.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/03/10/linux/2018-03-10-18/","link":"","permalink":"http://blog.renyimin.com/2018/03/10/linux/2018-03-10-18/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"17. 软链","slug":"linux/2018-03-10-17","date":"2018-03-10T02:05:32.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2018/03/10/linux/2018-03-10-17/","link":"","permalink":"http://blog.renyimin.com/2018/03/10/linux/2018-03-10-17/","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"06. Docker数据管理 - 数据卷(Volumes), 挂载主机目录/文件","slug":"docker/2017-12-17-06-docker","date":"2017-12-17T07:30:21.000Z","updated":"2018-08-20T10:22:48.000Z","comments":true,"path":"2017/12/17/docker/2017-12-17-06-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/17/docker/2017-12-17-06-docker/","excerpt":"","text":"容器中管理数据主要有两种方式：数据卷(Volumes) 和 挂载主机目录(Bind mounts) 数据卷(Volumes)简介 数据卷 是一个可供一个或多个容器使用的特殊目录, 它绕过UFS, 可以提供很多有用的特性: 数据卷可以在容器之间共享和重用 对数据卷的修改会立马生效 对数据卷的更新,不会影响镜像 数据卷默认会一直存在, 即使容器被删除 注意: 数据卷的使用, 类似于 Linux 下对目录或文件进行 mount, 镜像中被指定为挂载点的目录中的文件会隐藏掉, 能显示看的是挂载的数据卷; 数据卷操作 创建一个数据卷: docker volume create my-vol (其实还有一种方式就是在docker run的时候直接指定一个数据卷名, 就会自动帮你创建数据卷) 查看所有数据卷: docker volume ls 查看指定数据卷的信息: docker volume inspect my-vol 查看容器的数据卷挂载信息: docker inspect 容器名 删除数据卷 $ docker volume rm my-vol 数据卷 是被设计用来持久化数据的, 它的生命周期独立于容器, Docker不会在容器被删除后自动删除数据卷, 并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷; 如果需要在删除容器的同时移除数据卷, 可以在删除容器的时候使用 docker rm -v 这个命令, 这个命令貌似只是移除该容器和数据卷之间的关系, 除非这个数据卷没有任何容器引用了, 才可以使用下面介绍的命令来删除掉; 无主的数据卷可能会占据很多空间，要清理请使用命令 $ docker volume prune 可以看到清除时会提醒你 WARNING! This will remove all volumes not used by at least one container 清除的是没有被至少一个容器使用的数据卷! 启动容器时挂载数据卷 在使用 docker run 命令的时候, 还可以使用 --mount 参数来将 数据卷 挂载到容器里, 另外, 在一次 docker run 中可以挂载多个数据卷 下面创建一个名为 nginx_conf 的数据卷 123456789101112131415renyimindeMacBook-Pro:~ renyimin$ docker volume create nginx-default-html-rootnginx-default-html-rootrenyimindeMacBook-Pro:~ renyimin$ docker volume inspect nginx-default-html-root[ &#123; &quot;CreatedAt&quot;: &quot;2017-12-17T08:28:10Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/nginx-default-html-root/_data&quot;, &quot;Name&quot;: &quot;nginx-default-html-root&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;]renyimindeMacBook-Pro:~ renyimin$ 以 nginx:stable 镜像运行容器, 同时加载上面的数据卷’nginx-default-html-root’到容器内的 ‘/usr/share/nginx/html’ 目录 123renyimindeMacBook-Pro:~ renyimin$ docker run -d -p 8000:80 --name nginx --mount source=nginx-default-html-root,target=/usr/shar/nginx/html nginx:stabled9272900ba7f2a59e6ff402aeee642856679ac073787e08997baa4c97fff051crenyimindeMacBook-Pro:~ renyimin$ 另外值得注意的是: 如果容器中对应的目录不存在, 容器会自动创建目录; 如果运行容器时, 加载的数据卷不存在, 则会自动创建, 通过docker volume ls也可以看到自动创建的数据卷 数据卷只能挂载目录, 不能挂载文件 可以在主机里使用以下命令查看 nginx 容器的信息, 数据卷信息在 &quot;Mounts&quot; Key 下面 1234567891011121314$ docker inspect nginx&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;nginx-default-html-root&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/nginx-default-html-root/_data&quot;, &quot;Destination&quot;: &quot;/usr/shar/nginx/html&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;],... 挂载主机目录/文件 挂载一个主机目录作为数据卷: 使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去 1234567renyimindeMacBook-Pro:~ renyimin$ docker run -d -p 8001:80 --name nginx_t1 --mount type=bind,source=/Users/renyimin/Desktop/nginx_transfer,target=/transfer nginx:stablef5313881baa9bb4522552ba0b02dcad9f315d8a4e5245e2b362dc674ac9d4c4frenyimindeMacBook-Pro:~ renyimin$renyimindeMacBook-Pro:~ renyimin$ docker exec -it f5313881baa9bb4522552ba0b02dcad9f315d8a4e5245e2b362dc674ac9d4c4f /bin/sh// 下面可以看到容器自己创建的transfer目录# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp transfer usr var 查看容器的挂载信息, 发现和数据卷相比, Type 信息是 bind 而不是 volume, 并且没有数据卷的 name 信息, docker volume ls 也不会看到有新的数据卷被创建, 所以…可以认为只是一次简单的目录绑定 1234567891011renyimindeMacBook-Pro:testVip renyimin$ docker inspect testVip&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/renyimin/Desktop/nginx_transfer&quot;, &quot;Destination&quot;: &quot;/transfer&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 也可以挂载文件 注意, 如果你直接在本地新建一个nginx.conf就想在启动的时候直接挂载到容器中的nginx.conf, 那么nginx容器可能压根就启动不起来了, 后面会使用 docker cp 来解决这个问题123renyimindeMacBook-Pro:~ renyimin$ docker run -d -p 8002:80 --name nginx_t2 --mount type=bind,source=/Users/renyimin/Desktop/nginx_transfer1/index.html,target=/usr/share/nginx/html/index.html nginx:stableb85543ee9562647195ab094579b8e17fd8f439094ab5f0054215af435d33681crenyimindeMacBook-Pro:~ renyimin$ 选择 -v 还是 --mount 参数? Docker 新用户应该选择 --mount 参数, 经验丰富的 Docker 使用者对 -v 或者 --volume 已经很熟悉了, 但是推荐使用 --mount 参数; 可以理解为, --mount 参数应该可以挂载数据卷, 也可以代替-v来进行目录关联; Docker 挂载主机目录的默认权限是 读写, 用户也可以通过增加 readonly 指定为 只读 加了 readonly 之后, 就挂载为 只读 了, 如果你在容器内 /haha 目录新建文件, 会显示如下错误 1234567891011121314renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 8090:80 --name testVip --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha,readonly vipservice27863a3a8f70fa4bddb9c97fabfee2db7f35d5615d4b90ad0be13717dc23d092renyimindeMacBook-Pro:testVip renyimin$ docker exec -it 27863a3a8f70fa4bddb9c97fabfee2db7f35d5615d4b90ad0be13717dc23d092 /bin/shsh-4.2# sh-4.2# cd /sh-4.2# lsanaconda-post.log bin data dev etc haha home lib lib64 lost+found media mnt opt proc root run run.sh sbin srv sys tmp usr varsh-4.2# cd hahash-4.2# lsmyfirstregistry registry.tar// 可以看到报错了sh-4.2# touch a.txttouch: cannot touch &apos;a.txt&apos;: Read-only file systemsh-4.2# 查看数据卷的具体信息 $ docker inspect testVip 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/renyimin/Desktop/testVip&quot;, &quot;Destination&quot;: &quot;/haha&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: false, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 数据卷挂载的问题 上面提到的数据卷, 对很多容器都非常有用, 比如 mysql容器中存储数据文件的 /var/lib/mysql 目录你就需要挂载数据卷; mysql, php-fpm, nginx等容器中, 关于服务配置的目录你也需要挂载到数据卷, 这些配置你可能需要进行改动; 但是挂载数据卷有个问题, 一旦挂载之后, 容器中的目录就是空的, 原本服务的配置文件就被清空了, 也就导致有些容器在挂载数据卷之后, 无法正常启动; docker cp 命令 可以将本地目录/文件拷贝到容器, 也可以将容器中的目录/文件拷贝到本地; 格式: docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- 所以为了避免挂载导致容器无法正常启动, 挂载的可以这样来: 先确定你需要挂载的容器中目录的位置(比如: nginx容器中的配置文件在/etc/nginx/conf.d/default.conf ) 使用 docker cp 命令, 将需要映射的目录从容器复制到本地; (比如: docker cp nginx_test:/etc/nginx/ ./conf/) 然后再将本地default.conf文件挂载到nginx容器的/etc/nginx/conf.d/default.conf 启动nginx容器 先简单启动: 1234567renyimindeMacBook-Pro:html renyimin$ docker run -d -p 8005:80 --name nginx nginx:stable1b22387039746639bf5eec39c5eea597a078d0d6003c31c94bb65d82485f0da4renyimindeMacBook-Pro:html renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1b2238703974 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8005-&gt;80/tcp nginx983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 7 hours ago Up 3 hours 0.0.0.0:8088-&gt;80/tcp nginx_test0e7070854958 registry &quot;/entrypoint.sh /e...&quot; 5 months ago Up 5 days 0.0.0.0:5000-&gt;5000/tcp registry 从nginx容器copy 挂载主机目录/文件 需要的内容 1234567891011121314151617181920212223242526272829// 将nginx默认的root目录拷贝到宿主机renyimindeMacBook-Pro:nginx_test renyimin$ docker cp nginx:/usr/share/nginx/html ./htmlrenyimindeMacBook-Pro:nginx_test renyimin$ lshtmlrenyimindeMacBook-Pro:nginx_test renyimin$ cd htmlrenyimindeMacBook-Pro:html renyimin$ ls50x.html index.htmlrenyimindeMacBook-Pro:html renyimin$// 再将nginx相关配置拷贝到宿主机renyimindeMacBook-Pro:nginx_test renyimin$ docker cp nginx:/etc/nginx ./confrenyimindeMacBook-Pro:nginx_test renyimin$ lsconf htmlrenyimindeMacBook-Pro:nginx_test renyimin$ tree confconf├── conf.d│ └── default.conf├── fastcgi_params├── koi-utf├── koi-win├── mime.types├── modules -&gt; /usr/lib/nginx/modules├── nginx.conf├── scgi_params├── uwsgi_params└── win-utf1 directory, 10 filesrenyimindeMacBook-Pro:nginx_test renyimin$ 然后关掉nginx容器并删除 1234567891011121314renyimindeMacBook-Pro:conf renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1b2238703974 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:8005-&gt;80/tcp nginx983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 8 hours ago Up 3 hours 0.0.0.0:8088-&gt;80/tcp nginx_test0e7070854958 registry &quot;/entrypoint.sh /e...&quot; 5 months ago Up 5 days 0.0.0.0:5000-&gt;5000/tcp registryrenyimindeMacBook-Pro:conf renyimin$ docker stop nginxnginxrenyimindeMacBook-Pro:conf renyimin$ docker rm nginxnginxrenyimindeMacBook-Pro:conf renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 8 hours ago Up 3 hours 0.0.0.0:8088-&gt;80/tcp nginx_test0e7070854958 registry &quot;/entrypoint.sh /e...&quot; 5 months ago Up 5 days 0.0.0.0:5000-&gt;5000/tcp registryrenyimindeMacBook-Pro:conf renyimin$ 最后, 重新以为挂载主机目录/文件的方式启动nginx镜像 1234567renyimindeMacBook-Pro:conf.d renyimin$ docker run -d -p 8005:80 --name nginx --mount type=bind,source=/Users/renyimin/Desktop/nginx_test/html,target=/usr/share/nginx/html --mount type=bind,source=/Users/renyimin/Desktop/nginx_test/conf/nginx.conf,target=/etc/nginx/nginx.conf --mount type=bind,source=/Users/renyimin/Desktop/nginx_test/conf/conf.d/,target=/etc/nginx/conf.d nginx:stabledee610e0caefe889fcf7073c5679b4ab640e4620e74c5864bccb07b6718a1670renyimindeMacBook-Pro:conf.d renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdee610e0caef nginx:stable &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8005-&gt;80/tcp nginx983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 8 hours ago Up 4 hours 0.0.0.0:8088-&gt;80/tcp nginx_test0e7070854958 registry &quot;/entrypoint.sh /e...&quot; 5 months ago Up 5 days 0.0.0.0:5000-&gt;5000/tcp registry 尝试访问 http://locahost:8005, 成功; 在宿主机修改index.html文件, 直接刷新页面, 效果正常! 宿主机的挂载文件如下 (多余的哪些本地并没有进行映射的文件, 也可以直接删除) 修改nginx配置之后, 可以重启容器来使新配置生效 docker restart nginx 参考: https://yeasy.gitbooks.io/docker_practice/content/data_management/","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"05. 容器","slug":"docker/2017-12-16-05-docker","date":"2017-12-16T03:06:58.000Z","updated":"2018-08-20T10:22:48.000Z","comments":true,"path":"2017/12/16/docker/2017-12-16-05-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/16/docker/2017-12-16-05-docker/","excerpt":"","text":"容器 镜像(Image)和容器(Container)的关系, 就像是面向对象程序设计中的 类 和 实例 的关系一样, 镜像是静态的定义, 容器是镜像运行时的实体; 容器可以被 创建、启动、停止、删除、暂停等; 容器的实质是进程, 但与直接在宿主执行的进程不同, 容器进程运行于属于自己的独立的命名空间 因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间, 甚至自己的用户ID空间; 容器内的进程是运行在一个隔离的环境里, 使用起来, 就好像是在一个独立于宿主的系统下操作一样, 这种特性使得容器封装的应用比直接在宿主运行更加安全, 也因为这种隔离的特性, 很多人初学 Docker 时常常会把容器和虚拟机搞混; 前面讲过镜像使用的是分层存储, 容器也是如此, 每一个容器运行时, 是以镜像为基础层, 在其上创建一个当前容器的存储层, 这是为容器运行时读写而准备的存储层; 容器存储层的生存周期和容器一样, 容器消亡时, 容器存储层也随之消亡, 因此, 任何保存于容器存储层的信息都会随容器的删除而丢失; 按照 Docker 最佳实践的要求, 容器不应该向其存储层内写入任何数据, 容器存储层要保持无状态化 所有的文件写入操作, 都应该使用 数据卷(Volume)、或者 绑定宿主目录, 在这些位置的读写会跳过容器存储层, 直接对宿主(或网络存储)发生读写, 其性能和稳定性更高; 数据卷的生存周期独立于容器, 容器消亡, 数据卷不会消亡, 因此, 使用数据卷后, 容器可以随意删除、重新run, 数据却不会丢失; 容器操作启动 启动容器有两种方式：一种是基于镜像新建一个容器并启动; 另外一个是将在终止状态(stopped)的容器启动 创建并启动 因为 Docker 的容器实在太轻量级了, 很多时候用户都是随时删除和新创建容器; 新建并启动一个容器所需的命令主要为 docker run, 例如: $ docker run -d -p 5000:5000 --name myFirstRegistry registry, 是根据名为registry的镜像创建并运行一个名为myFirstRegistry容器; 当利用 docker run 来创建容器时, Docker 在后台运行的标准操作包括: 检查本地是否存在指定的镜像, 不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统, 并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 容器被启动后, 设置的挂载目录, 端口映射都会随着此容器, 容器stop后, 再次start, 这些设置都还在; 启动已终止容器: 可以利用 docker start [containerID or NAME] 命令, 直接将一个已经终止的容器启动运行; 守护态运行容器其实更多时候, 我们需要让容器在后台运行, 而不是直接运行容器并展示出结果, 此时只用在运行时加上 -d 参数即可; (在容器的第一种启动方式中已经介绍过了) 查看容器信息可以通过 docker ps 命令来查看正在运行的容器信息 可以通过 docker ps -a 命令来查看 正在运行的和终止的 容器信息 终止容器可以使用 docker stop [containerID or NAME] 来终止一个运行中的容器 重启容器可以使用 docker restart [containerID or NAME] 来重启一个运行中的容器 删除容器 可以使用 docker rm 容器ID/容器NAME 来删除一个处于终止状态的容器; 如果要删除一个运行中的容器，可以添加 -f 参数; 进入容器 可参考书中介绍 推荐使用 docker exec -it [containerID or NAME] /bin/sh 其中, /bin/bash 有可能是 /bin/sh，因为不一定所有的docker都安装了shell","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"03. php-fpm -- 慢日志, 错误日志","slug":"php/2017-12-03-php-03","date":"2017-12-03T12:32:17.000Z","updated":"2018-08-22T02:10:05.000Z","comments":true,"path":"2017/12/03/php/2017-12-03-php-03/","link":"","permalink":"http://blog.renyimin.com/2017/12/03/php/2017-12-03-php-03/","excerpt":"","text":"前言在生产环境中, 每隔一段时间(比如每周一次两次), 应该检查一些php脚本的运行状态, 观察那些php进程速度太慢, 或者有哪些错误日志; php-fpm 慢日志 对于php-fpm的慢日志, php-fpm.conf 中有两个配置项需要关注一下 request_slowlog_timeout: 默认值为0, 表示不进行慢日志记录;可以设置为n(n&gt;0)秒, 来打开慢查询日志的记录, 表示执行时间超过n秒的脚本将记录进slowlog日志中; slowlog: 用来设置慢日志的存放位置; 尝试如下配置 1234vi /usr/local/php/etc/php-fpm.d/www.conf// 如下: slowlog的位置如果是相对路径, 则默认是以php的安装位置为根目录的, slowlog = log/$pool.log.slowrequest_slowlog_timeout = 2 然后重启php-fpm (注意log目录必须存在且有权限, 不然会启动失败) 写如下php进行测试: 123&lt;?phpecho 123;sleep(3); 运行后, 果然记下了慢日志 123456tailf /usr/local/php/log/www.log.slow// 结果如下[17-Aug-2018 10:00:48] [pool www] pid 18139script_filename = /html/test.php[0x00007f2c36c140a0] sleep() /html/test.php:3 日志说明 script_filename: 是入口文件 sleep() 这一行是说: 是执行这个方法的时候超过执行时间的 每行冒号后面的数字是行号 另外, 在开启慢日志之后, 在phpfpm的错误日志中也有相关记录 php-fpm 错误日志 对于php-fpm的错误日志, 需要在全局配置文件 php-fpm.conf 中配置 error_log = log/error.log 注意该配置项不能放在进程池的配置文件中 另外, 配置文件中说明了, 如果错误日志的路径是相对路径, 是以/usr/local/php/var 为根目录的 继续运行之前的那个超时脚本, 会发现, php-fpm的错误日志中也会记录这个超时日志: 1234567tailf /usr/local/php/var/log/error.log// 如下[17-Aug-2018 12:11:31] WARNING: [pool www] child 19238, script &apos;/html/test.php&apos; (request: &quot;GET /test.php&quot;) executing too slow (2.001552 sec), logging[17-Aug-2018 12:11:31] NOTICE: child 19238 stopped for tracing[17-Aug-2018 12:11:31] NOTICE: about to trace 19238[17-Aug-2018 12:11:31] NOTICE: finished trace of 19238 但是可以测试, 当PHP程序出问题时, 比如少一个分号, 会发现, php-fpm的error.log中并不会记录该错误! php-fpm与php.ini的error_log 目前为止, php相关的错误日志既不会记录到php-fpm的error_log中, 也不会记录在php.ini中的error_log中; 要让php.ini记录错误日志, 其实只用查看你的错误日志文件是否有权限, 只要设置好权限, php.ini就会记录错误日志; 要让php-fpm.conf的error_log可以记录错误日志, 配置如下: 这种情况下, 如果php.ini中规定的错误日志文件权限不够, 则php-fpm的错误日志会自动记录, 如果权限够,则php.ini的错误日志会记录, 而php-fpm的错误日志不会记录;1234// 在php-fpm.conf中配置[www]catch_workers_output = yes// 同时需要在&quot;[global]&quot;下配置error_log的路径 php.ini配置: 123log_errors = Onerror_log = &quot;/usr/local/php/log/php_errors.log&quot;error_reporting=E_ALL","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"04.定制镜像 - docker commit手动定制","slug":"docker/2017-12-03-04-docker","date":"2017-12-03T06:09:21.000Z","updated":"2018-08-20T10:22:48.000Z","comments":true,"path":"2017/12/03/docker/2017-12-03-04-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/03/docker/2017-12-03-04-docker/","excerpt":"","text":"前言镜像是容器的基础, 每次执行 docker run 的时候都需要指定哪个镜像作为容器运行的基础。在之前的例子中, 我们所使用的都是来自于 Docker Hub 的镜像, 直接使用这些镜像是可以满足一定的需求, 而当这些镜像无法直接满足需求时, 我们就需要定制这些镜像。 docker commit 手动定制镜像 当运行一个容器后(如果不使用数据卷的话), 你所做的任何文件修改都会被记录于容器存储层里, 注意: 容器存储层的生存周期和容器一样, 容器被删除后, 存储层中的内容也就会被删除掉, 而不会保留到镜像中 如果改动了容器的存储层, 我们可以通过 docker diff 命令看到具体的改动 但是如果改动的是数据卷挂载到容器对应目录下的内容, docker diff 看不到具体的改动 Docker提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像 换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像 以后我们运行这个新镜像的时候, 就会拥有原有容器最后的文件变化 docker commit 的语法格式为: docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]], 如下: 其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空1$ docker commit --author &quot;Tao Wang &lt;twang2218@gmail.com&gt;&quot; --message &quot;修改了默认网页&quot; webserver nginx:v2 手动定制镜像~~挂载数据卷问题 之前已经配置了docker中国加速镜像, 现在通过 docker pull nginx:stable 获取一个nginx基础镜像; 直接运行这个nginx基础镜像为一个容器 由于该镜像非常基础, 甚至没有像vi的工具, 因此在启动时可以将nginx的项目根目录 /usr/share/nginx/html 映射出来, 以便于测试 12345renyimindeMacBook-Pro:~ renyimin$ docker run -d -p 8088:80 --mount type=bind,source=/Users/renyimin/Desktop/nginx_test,target=/usr/share/nginx/html --name nginx_test nginx:stable983a6495386490f36e58d194a15d3dabbf86ccbadf2e44bab67d841e2abd0eferenyimindeMacBook-Pro:~ renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 20 seconds ago Up 18 seconds 0.0.0.0:8088-&gt;80/tcp nginx_test 然后直接访问 localhost:8088 会发现nginx报错403 Forbidden, 这是因为挂载到容器中/usr/share/nginx/html目录的本地目录/Users/renyimin/Desktop/nginx_test中没有任何内容 接下来, 在/Users/renyimin/Desktop/nginx_test中创建一个index.html文件, 然后直接刷新localhost:8088, 就会看到效果! 现在修改了容器的文件，也就是改动了容器的存储层, 我们可以通过 docker diff 命令看到容器当前存储层的所有改动 12345678910renyimindeMacBook-Pro:~ renyimin$ docker diff nginx_testC /runA /run/nginx.pidC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temprenyimindeMacBook-Pro:~ renyimin$ 你会发现自己最直接的改动并没有体现出来, 其实这主要是因为你直接改动的文件是被挂载出来的, 如果不是挂载出来, 而是直接在容器中修改的话, 则会体现出来; 比如, 直接进入nginx容器, 在非挂载目录中创建一个新文件, 然后观察差异 1234567891011121314renyimindeMacBook-Pro:~ renyimin$ docker exec -it nginx_test /bin/shrenyimindeMacBook-Pro:~ renyimin$ docker diff nginx_testC /runA /run/nginx.pidC /tmp// 可以看到, 此次直接改动就体现出来了A /tmp/renyimin.htmlC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temprenyimindeMacBook-Pro:~ renyimin$ 手动定制镜像 docker commit --author &#39;renyimin&#39; --message &quot;在/tmp下touch了一个renyimin.html文件&quot; nginx_test nginx:test01 docker image ls 可以看到这个新定制的镜像 12345renyimindeMacBook-Pro:~ renyimin$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEnginx test01 ad0f6d04a963 4 seconds ago 109MBnginx stable 8ae4d16b741a 2 weeks ago 109MB...... 还可以用 docker history 具体查看镜像内的历史记录, 如果比较 nginx:v1 的历史记录, 我们会发现新增了我们刚刚提交的这一层 1234567891011121314renyimindeMacBook-Pro:~ renyimin$ docker history nginx:test01IMAGE CREATED CREATED BY SIZE COMMENTad0f6d04a963 57 seconds ago nginx -g daemon off; 2B 在/tmp下touch了一个renyimin.html文件8ae4d16b741a 2 weeks ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daem... 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) STOPSIGNAL [SIGTERM] 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) EXPOSE 80/tcp 0B&lt;missing&gt; 2 weeks ago /bin/sh -c ln -sf /dev/stdout /var/log/ngi... 22B&lt;missing&gt; 2 weeks ago /bin/sh -c set -x &amp;&amp; apt-get update &amp;&amp; a... 53.7MB&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) ENV NJS_VERSION=1.14.0.... 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.14.... 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) LABEL maintainer=NGINX ... 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) CMD [&quot;bash&quot;] 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) ADD file:919939fa0224727... 55.3MBrenyimindeMacBook-Pro:~ renyimin$ 新的镜像定制好后，就可以来尝试运行这个镜像 1234567renyimindeMacBook-Pro:~ renyimin$ docker run -d -p 8089:80 --name nginx_test01 nginx:test0188f45a479da70c07a0510edb4730773387541fd5dd16a8d379afc2d405f296bfrenyimindeMacBook-Pro:~ renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES88f45a479da7 nginx:test01 &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8089-&gt;80/tcp nginx_test01983a64953864 nginx:stable &quot;nginx -g &apos;daemon ...&quot; 19 minutes ago Up 19 minutes 0.0.0.0:8088-&gt;80/tcp nginx_testrenyimindeMacBook-Pro:~ renyimin$ 进入此容器, 会发现使用新的镜像启动容器后, 容器中的/tmp目录下包含我们提交的index.html个文件, 当然, localhost:8089和localhost:8088不同, 8089访问的还是默认欢迎页 renyimindeMacBook-Pro:~ renyimin$ docker exec -it nginx_test01 /bin/sh cd /tmpls renyimin.html # ``` 至此, 第一次使用 docker commit 命令完成了镜像定制, 手动操作给旧的镜像添加了新的一层, 形成新的镜像, 对镜像多层存储应该有了更直观的感觉; 慎用 docker commit 使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念, 但是实际环境中并不会这样使用; 因为如果仔细观察之前的 docker diff nginx_test 的结果, 会发现还有很多文件被改动或添加了, 但这些都是无关紧要的改动 这还仅仅是最简单的操作, 如果是安装软件包、编译构建, 那会有大量的无关内容被添加进来, 如果不小心清理, 将会导致镜像极为臃肿; 此外, 使用 docker commit 意味着所有对镜像的操作都是黑箱操作, 生成的镜像也被称为黑箱镜像, 换句话说, 就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像, 别人根本无从得知;而且, 即使是这个制作镜像的人, 过一段时间后也无法记清具体在操作的, 虽然 docker diff 或许可以告诉得到一些线索, 但是这种黑箱镜像的维护工作是非常痛苦的; 而且, 回顾之前提及的镜像所使用的分层存储的概念, 除当前层外, 之前的每一层都是不会发生改变的, 换句话说, 任何修改的结果仅仅是在当前层进行标记、添加、修改, 而不会改动上一层; 如果使用 docker commit 制作镜像, 每一次修改都会让镜像更加臃肿一次, 所删除的上一层的东西并不会丢失, 会一直如影随形的跟着这个镜像, 即使根本无法访问到, 这会让镜像更加臃肿;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"03. Docker Registry 仓库","slug":"docker/2017-12-02-03-docker","date":"2017-12-02T09:40:28.000Z","updated":"2018-08-20T10:22:48.000Z","comments":true,"path":"2017/12/02/docker/2017-12-02-03-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/02/docker/2017-12-02-03-docker/","excerpt":"","text":"公开 Docker Registry Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务 一般这类公开服务允许用户免费上传、下载公开的镜像, 并可能提供收费服务供用户管理私有镜像; 最常使用的Registry公开服务是官方的 Docker Hub, 这也是默认的 Registry，并拥有大量的高质量的官方镜像; 不过由于某些原因, 在国内访问这些服务可能会比较慢, 国内的一些云服务商提供了针对 Docker Hub 的镜像服务(Registry Mirror), 这些镜像服务被称为加速器; 但有时使用 Docker Hub 或其他公共仓库仍然不方便(比如, 有时候我们的服务器无法访问互联网 或者 你不希望将自己的镜像放到公网当中), 那就需要创建一个 本地私有仓库供; 私有 Docker Registry 除了使用公开服务外, 用户还可以在本地搭建私有Docker Registry, docker-registry是官方提供的工具, 可以用于构建私有的镜像仓库; 安装运行 docker-registry 你可以通过获取官方registry镜像来在本地运行一个自己的私有镜像仓库 (如 $ docker run -d -p 5000:5000 --restart=always --name registry registry, 将使用官方的registry镜像来启动一个私有仓库) 默认情况下, 仓库中的镜像会被创建在容器的 /var/lib/registry 目录下, 你可以通过 -v 参数来将镜像文件映射到本地的指定路径中; 另外, 可以将私有仓库的配置文件指定到本地的路径下 (如 ~/Desktop/registry-config/ 下 ) 我们大可不必这么麻烦, 只是简单运行一个私有仓库服务 $ docker run -d -p 5000:5000 --restart=always --name registry registry 查看私有仓库中镜像 用 curl 查看仓库中的镜像, 可以看到你的私有仓库暂时还是空的 123$ curl 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125;$ 还可以在浏览器中直接查看私有仓库中的镜像(并且内网其他机器也可以通过内网地址来访问你所搭建的私有仓库的镜像): 上传镜像到私有仓库中 之前我们已经通过获取官方 registry镜像 来创建好了自己的私有仓库, 接下来就可以使用 docker tag 来标记一个镜像, 然后推送它到仓库; 先查看一下本地已有的镜像 docker image ls : 12345678$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEregistry latest d1fd7d86a825 4 weeks ago 33.3MBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 使用 docker tag 将 registry:lates 这个镜像标记为一个新的本地镜像 127.0.0.1:5000/registry:latest ; 格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]12345678910$ docker tag registry:latest 127.0.0.1:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 使用 docker push 上传标记的镜像 12345678$ docker push 127.0.0.1:5000/registry:latestThe push refers to a repository [127.0.0.1:5000/registry]9113493eaae1: Pushed 621c2399d41a: Pushed 59e80739ed3f: Pushed febf19f93653: Pushed e53f74215d12: Pushed latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 然后查看仓库中的镜像，可以看到镜像已经被成功上传了 curl 查看 12curl 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;registry&quot;]&#125; 浏览器查看 查看某个镜像的tag列表 curl -XGET http://127.0.0.1:5000/v2/nginx/tags/list 上传私有仓库问题 如果上传的时候, 打包的镜像使用的是本机的内网地址, 最后在上传的时候, 你会发现如下报错信息: 1234$ docker push 192.168.1.3:5000/registry:latestThe push refers to a repository [192.168.1.3:5000/registry]Get https://192.168.1.3:5000/v2/: http: server gave HTTP response to HTTPS clientrenyimindembp:vipvip renyimin$ 此时, 你需要将内网地址配置到本机docker的 insecure registries 中, 如下: 之后, 无论本机还是在同一内网中的其他机器也都可以推送镜像到仓库中了(之前打包好的两个镜像, 都可以成功推送到私有仓库中): 1234567891011121314151617$ docker push 192.168.1.3:5000/registryThe push refers to a repository [192.168.1.3:5000/registry]9113493eaae1: Pushed 621c2399d41a: Pushed 59e80739ed3f: Pushed febf19f93653: Pushed e53f74215d12: Pushed latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 $ docker push 127.0.0.1:5000/registryThe push refers to a repository [127.0.0.1:5000/registry]9113493eaae1: Layer already exists 621c2399d41a: Layer already exists 59e80739ed3f: Layer already exists febf19f93653: Layer already exists e53f74215d12: Layer already exists latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 从私有仓库中下载镜像 先删除已有镜像 12345678910111213141516$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEregistry latest d1fd7d86a825 4 weeks ago 33.3MB127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MB192.168.1.3:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GB$ docker image rm 127.0.0.1:5000/registry:latest 192.168.1.3:5000/registry:latestUntagged: 127.0.0.1:5000/registry:latestUntagged: 127.0.0.1:5000/registry@sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cUntagged: 192.168.1.3:5000/registry:latestUntagged: 192.168.1.3:5000/registry@sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c 再尝试从私有仓库中下载这个镜像 (两个地址都可以下载, 也是因为之前配置了 Insecure registries, 这里最后才可以使用内网地址来下载) 123456789101112131415161718192021222324252627282930$ docker pull 127.0.0.1:5000/registry:latestlatest: Pulling from registryDigest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cStatus: Downloaded newer image for 127.0.0.1:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GB$ docker pull 192.168.1.3:5000/registry:latestlatest: Pulling from registryDigest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cStatus: Downloaded newer image for 192.168.1.3:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE192.168.1.3:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MB127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 几个简单问题 删除仓库镜像 自己的docker仓库中存放的镜像, 时间长了难免存在一些废弃的镜像在里面, 如果不删除就造成空间的浪费 需要对registry做适当配置, 可参考: https://docs.docker.com/registry/configuration/#override-specific-configuration-options 容器启动之后, 如果忘记挂载某个目录, 能否再进行挂载? 其实没有必要, 直接停止删除, 重开一个即可！","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"02. 镜像","slug":"docker/2017-12-02-02-docker","date":"2017-12-02T03:56:23.000Z","updated":"2018-08-20T10:22:48.000Z","comments":true,"path":"2017/12/02/docker/2017-12-02-02-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/02/docker/2017-12-02-02-docker/","excerpt":"","text":"前言镜像(Image)和容器(Container)的关系，就像是面向对象程序设计中的类和实例一样, 镜像是静态的定义, 容器是镜像运行时的实体; 所以, Docker运行容器前首先需要本地存在对应的镜像, 如果本地不存在该镜像, Docker会先尝试从镜像仓库下载该镜像; 镜像的获取 Docker Hub上有大量的高质量的镜像可以用, 如何获取这些镜像呢? 从Docker镜像仓库获取镜像的命令是 docker pull, 其命令格式为：docker pull [选项] [Docker Registry地址[:端口号]/]仓库名[:标签] docker pull命令的具体选项可以通过 docker pull --help 命令看到 Docker镜像仓库地址: 地址的格式一般是 &lt;域名/IP&gt;[:端口号] (默认地址是 Docker Hub 仓库地址) 仓库名: 仓库名是 两段式名称, 即 &lt;用户名&gt;/&lt;软件名&gt; (对于 Docker Hub, 如果不给出用户名, 则默认为 library, 也就是官方镜像) 比如 $ docker pull ubuntu:16.04: 由于没有给出Docker镜像仓库地址, 因此将会从Docker Hub获取镜像; 而仓库名称是 ubuntu(没有用户名), 因此将会去官方仓库 library/ubuntu 中, 获取标签为 16.04 的镜像; 另外, 如果从 Docker Hub 下载镜像非常缓慢，可以 配置镜像加速器。 配置镜像加速器 国内从 Docker Hub 镜像仓库拉取镜像有时会遇到困难, 此时可以配置镜像加速器, Docker 官方和国内很多云服务商都提供了国内加速器服务, 例如: Docker 官方提供的中国 registry mirror 阿里云加速器 DaoCloud 加速器 此处以 Docker 官方加速器为例进行介绍(由于本人使用macOS系统,下面只列出macOS上如何配置镜像加速器, 其他系统请参考) 在任务栏点击Docker for mac 应用图标 -&gt; Perferences… -&gt; Daemon -&gt; Basic -&gt; Registry mirrors 在列表中填写加速器地址即可, 修改完成之后，点击 Apply &amp; Restart 按钮, Docker 就会重启并应用配置的镜像地址了 如果在添加加速器地址后出现 registry-mirrors no certs for egistry.docker-.... 网上查找资料后, 有人说是证书问题, 尝试修改https为http后正常 检查加速器是否生效 配置加速器之后, 如果拉取镜像仍然十分缓慢, 请手动检查加速器配置是否生效, 在命令行执行 docker info 由于我配置的是docker hub提供的中国镜像站点, 所以如果从结果中看到了如下内容，说明配置成功(你看到的可能和我的不一样)123Registry Mirrors:http://registry.docker-cn.com///如果添加了多个加速站点, 此处也会有多个 镜像相关基础操作 列出已存在镜像: docker images：列表包含了 仓库名、标签、镜像ID、创建时间 以及 所占用的空间; 注意: 虽然 镜像ID 是镜像的唯一标识, 但是一个镜像可以打包出多个不同标签的镜像(如何打包,后面会学到), 所以有些镜像的ID一样, 但是tag会不一样12345renyimindeMacBook-Pro:testVip renyimin$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 7 weeks ago 33.3MBregistry latest d1fd7d86a825 7 weeks ago 33.3MBrenyimindeMacBook-Pro:testVip renyimin$ 删除镜像: 如果要删除本地的镜像, 可以使用 $ docker image rm [选项] &lt;镜像名1&gt; [&lt;镜像名2&gt; ...] 命令; (因为镜像ID可能会一样, 所以删除镜像用的是镜像名) 注意, 镜像名是 仓库名:标签, 如 docker image rm nginx:1.12.2 镜像更名: 镜像更改名称也很简单, 直接 $ docker tag 镜像名 新镜像名:标签 在 Docker 1.13+ 版本中推荐使用 docker image 来管理镜像 (比如 docker image ls 会列出所有镜像); 理解分层存储 镜像是多层存储，每一层是在前一层的基础上进行的修改; 而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层; 镜像构建时, 会一层层构建, 前一层是后一层的基础。每一层构建完就不会再发生改变, 后一层上的任何改变只发生在自己这一层; 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡, 这里的消亡是指容器被删除, 而不是stop容器, stop容器后, 容器中发生的改变不会被忽略, 除非容器被删除掉;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"01. 认识Docker","slug":"docker/2017-12-02-01-docker","date":"2017-12-02T03:03:06.000Z","updated":"2018-08-20T10:22:37.000Z","comments":true,"path":"2017/12/02/docker/2017-12-02-01-docker/","link":"","permalink":"http://blog.renyimin.com/2017/12/02/docker/2017-12-02-01-docker/","excerpt":"","text":"简介 Docker使用Google公司推出的Go语言实现; 属于操作系统层面的虚拟化技术, 也称其为容器; docker 与 传统虚拟机技术 对比 传统虚拟机技术是: 虚拟出一套硬件后; 在其上运行一个完整操作系统; 最后在该系统上再运行所需应用进程; 而容器内的应用进程直接运行于宿主的内核, 容器内没有自己的内核, 而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便; 如下图, 可以看到有 应用A 和 应用B 两个应用, 相比于传统虚拟技术, docker少了 Hypervisor(所有虚拟化技术的核心)和Guest OS这两层 为什么使用docker?作为一种新兴的虚拟化方式, Docker 跟传统的虚拟化方式相比具有众多的优势 更高效的利用系统资源由于容器不需要进行 硬件虚拟 以及 运行完整操作系统 等额外开销, 所以其实Docker对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 一致的运行环境开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。 更多好处请参考(https://yeasy.gitbooks.io/docker_practice/content/introduction/why.html) 对比传统虚拟机总结 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于原生 系统支持量 单机支持上千个容器 一般几十个 Docker三个基本概念理解了这三个概念, 就理解了 Docker 的整个生命周期 镜像 (Image)容器 (Container)仓库 (Repository) 《Docker从入门到实践》","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.renyimin.com/tags/Docker/"}]},{"title":"03. HTTP状态码详解","slug":"http/2017-11-30-HTTP-03","date":"2017-11-30T06:30:12.000Z","updated":"2018-07-20T13:42:17.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03/","excerpt":"","text":"1xx 101: 参考博文WebSocket简单示例分析 (做协议升级, 还会响应: Connection: Upgrade) 2xx Web API的设计与开发 P109 200 OK : 200码非常出名, 似乎没有对它进一步说明的必要; 201 Created : 当在服务器端创建数据成功时, 会返回201状态码; 也就是使用 POST 请求方法的场景 (如:用户登录后添加了新用户, 上传了图片等新创建数据的场景) 202 Accepted : 在异步处理客户端请求时, 它用来表示服务器端已经接受了来自客户端的请求, 但处理尚未结束; 在文件格式转换, 处理远程通知(Apple Push Notification等)这类很耗时的场景中, 如果等到所有处理都结束后才向客户端返回响应消息, 就会花费相当长的时间, 造成应用可用性不高; 这时采用的方法是服务器向客户端返回一次响应消息, 然后立刻开始异步处理。 202状态码就被用于告知客户端服务器端已经开始处理请求, 但整个处理过程尚未结束; 比如: 以LinkedIn的参与讨论的API为例如果成功参与讨论并发表意见, 服务器端通常会返回201状态码;但如果需要得到群主的确认, 那么所发表的意见就无法立即在页面显示出来, 这时服务器端就需要返回202状态码; 从广义上来看, 该场景也属于异步处理, 但和程序设计里的异步执行当然不同; 204 No Content : 正如其字面意思, 当响应消息为空时会返回该状态码。 其实就是告诉浏览器, 服务端执行成功了, 但是没什么数据返回给你, 所以你不用刷新页面, 也不用导向新的页面; 在用 DELETE 方法删除数据时, 服务器端通常会返回204状态码(阮一峰博文也提到过, 对DELETE适用); 除此之外, 也有人认为在使用 PUT或PATCH 方法更新数据时, 因为只是更新已有数据, 所以返回204状态码更加自然;书中建议 DELETE 返回204; PUT或PATCH返回200并返回该方法所操作的数据; 关于204状态码的讨论可以参考 p111; 205 Reset Content : 告诉浏览器, 页面表单需要被重置; 205的意思是服务端在接收了浏览器POST请求以后, 处理成功以后, 告诉浏览器, 执行成功了, 请清空用户填写的Form表单, 方便用户再次填写; 206 Partial Content : 成功执行了一个部分或Range(范围)的请求; 206响应中, 必须包含 Content-Range, Date 以及 ETag或Content-Location首部; 3xx300 Multiple Choices : 客户端驱动方式进行内容协商时, 服务器可能返回多个连接供客户端进行选择 (比如多语言网站可能会出现); 301 Moved Permanently : 在请求的URL已经被移除时使用, 响应的Location首部中应该包含资源现在所处的URL; (比较适合永久重定向) 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是301; 则即便稍后取消了location.php中的跳转(或者修改了跳转地址), 由于浏览器还是会认为你之前的跳转是永久性的, 再次访问www.test.com/location.php仍然会跳转到之前的跳转链接(除非清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 302 Found: 与301类似, 但是客户端应该使用Location首部给出的URL来进行临时定位资源, 将来的请求仍应该使用老的URL; 比如你从 www.test.com/location.php 中location跳转到 www.test.com/index.html 时, 如果响应的是302; 如果稍后取消了location.php中的跳转, 再次访问www.test.com/location.php, 会发现不会进行跳转, 而是访问到 location.php 修改后的代码 (不用清浏览器缓存); 另外, 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 会转成GET; 303 See Other : HTTP/1.1使用303来实现和302一样的临时重定向; 307 Temporary Redirect HTTP/1.1规范要求用307来取代302进行临时重定向; (302临时重定向留给HTTP/1.0) 所以他也具备302临时重定向的特点; 但是, 与 302, 303 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 308 Permanent Redirect 貌似不是rfc2616的标准 具备和301永久重定向的特点, 需要清除浏览器缓存才行; 但是, 与 301 不同, 它会将客户端的POST请求, 发送给location的目标页; 假设你之前是先访问www.test.com/test.html, 然后通过post提交到www.test.com/location.php, 最后由location.php再进行跳转的话, 仍然是POST; 304 Not Modified : 参考博文缓存相关 4xx Web API的设计与开发 P1134字头状态码主要用于描述因客户端请求的问题而引发的错误。也就是说, 服务器端不存在问题, 但服务器端无法理解客户端发送的请求, 或虽然服务器端能够理解但请求却没有被执行, 当遇到这些情况引发的错误时, 服务器端便会向客户端返回这一类别的状态码。因此, 当服务器端返回4字头的状态码时, 就表示客户端的访问方式发生了问题, 用户需要检查一下客户端的访问方式或访问的目标资源等。 400 Bad Request : 表示其他错误的意思, 即其他4字头状态码都无法描述的错误类型; 401 Unauthorized : 表示认证(Authentication)类型的错误 比如当需要先进行登录操作, 而却没有告诉服务器端所需的会话信息(比如token..), 服务器端就会返回401状态码, 告知客户端出错的大致原因; 403 Forbidden : 和401状态码比较相似, 所以也经常被混淆; 其实403表示的是授权(Authotization)类型的错误, 授权和认证的不同之处是: 认证表示”识别前来访问的是谁”, 而授权则表示”赋予特定用户执行特定操作的权限” 通俗地说: 401状态码表示”我不知道你是谁”, 403状态码表示”虽然知道你是谁, 但你没有执行该操作的权限” 404 Not Found : 表示访问的数据不存在, 但是 例如当客户端湿度获取不存在的用户信息时, 或者试图访问原本就不存在的端点时, 服务器就会返回404状态码; 所以, 如果客户端想要获取用户信息, 却得到服务器端返回的404状态码, 客户端仅凭”404 Not Found”将难以区分究竟是用户不存在, 还是端点URI错误导致访问了原本不存在的URI; 405 Method Not Allowed : 表示虽然访问的端点存在, 但客户端使用的HTTP方法不被服务器端允许; 比如客户端使用了POST方法来访问只支持GET方法的信息检索专用的API; 又比如客户端用了GET方法来访问更新数据专用的API等; 406 Not Acceptable : 服务器端API不支持客户端指定的数据格式时, 服务器端所返回的状态码; 比如, 服务器端只支持JSON和XML输出的API被客户端指定返回YAML的数据格式时, 服务器端就会返回406状态码; 408 Request Timeout : 当客户端发送请求至服务器端所需的时间过长时, 就会触发服务器端的超时处理, 从而使服务器端返回该状态码; 409 Conflict: 用于表示资源发生冲突时的错误 (est中就会有该错误码) 比如通过指定ID等唯一键值信息来调用注册功能的API时, 倘若已有相同ID的数据存在, 就会导致服务器端返回409状态码; 在使用邮箱地址及Facebook ID等信息进行新用户注册时, 如果该邮箱地址或者ID已经被其他用户注册, 就会引起冲突, 这时服务器端就会返回409状态码告知客户端该邮箱地址或ID已被使用; 410 Gone : 和 404状态码 相同, 都表示访问资源不存在, 只是410状态码不单表示资源不存在, 还进一步告知资源曾经存在, 只是目前已经消失了; 因此服务器端常在访问被删除的数据时返回该状态码, 但是为了返回该状态码, 服务器必须保存该数据已被删除的信息, 而且客户端也应该知晓服务器端保存了这样的信息; 但是在通过邮箱地址搜索用户信息的API中, 从保护个人信息的角度来说, 返回410状态码的做法也会受到质疑; (所以在此种资源不存在的情况下, 为了稍微安全一些, 返回410状态码需要慎重) 413 Request Entity Too Large : 413也是比较容易出现的一种状态码, 表示请求实体过大而引发的错误 请求消息体过长是指, 比如在上传文件这样的API中, 如果发送的数据超过了所允许的最大值, 就会引发这样的错误; 414 Request-URI Too Large : 414是表示请求首部过长而引发的错误 如果在进行GET请求时, 查询参数被指定了过长的数据, 就会导致服务器端返回414状态码 415 Unsupported Media Type : 和406比较相似 406我们知道是表示服务器端不支持客户端想要接收的数据格式 而415表示的是服务器端不支持客户端请求首部 Content-Type 里指定的数据格式, 也就是说, 当客户端通过POST,PUT,PATCH等方法发送的请求消息体的数据格式不被服务器支持时, 服务器端就会返回415状态码; 例如在只接收JSON格式的API里, 如果客户端请求时发送的是XML格式的数据去请求服务器端, 或者在 Content-Type 首部指定 application/xml, 都会导致该类型错误; 429 Too Many Requests : 是2012年RFC6585文档中新定义的状态码, 表示访问次数超过了所允许的范围; 例如某API存在一小时内只允许访问100次的访问限制, 这种情况下入股哦客户端视图进行第101次访问, 服务器便会返回该状态码; 表示在一定的时间内用户发送了太多的请求, 即超出了”频次限制”, 在响应中，可以提供一个 Retry-After 首部来提示用户需要等待多长时间之后再发送新的请求; 5xx 5字头状态码表示错误不发生在客户端, 而是由服务器自身问题引发的。 500 Internal Server Error : 是web应用程序开发里非常常见的错误, 当服务器代码里存在bug, 输出错误信息并停止运行等情况下, 就会返回该类型的错误; 因此, 不仅限于API, 对于5字头状态码的错误, 都要认真监视错误日志, 使系统在出错时及时告知管理员, 以便在错误发生时做好应对措施, 防止再次发生。 501 Not Implemented : ??? 502 Bad GateWay : ??? 503 Service Unavaliable : 用来表示服务器当前处于暂不可用状态 可以回送:响应首部 Retry-After 表示多久恢复; 不同的客户端与服务器端应用对于 Retry-After 首部的支持依然不太一致; 不过，一些爬虫程序，比如谷歌的爬虫程序Googlebot, 会遵循Retry-After响应首部的规则, 将其与503(Service Unavailable,当前服务不存在)响应一起发送有助于互联网引擎做出判断,在宕机结束之后继续对网站构建索引。 参考:https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After 504 Gateway Time-out: 复现这个错误码比较简单, 让你的php程序模拟耗时请求, 如下代码 123&lt;?phpsleep(70);//模拟耗时，睡70秒echo &quot;睡醒了&quot;; 就会返回 ``` 504 Gateway Time-out nginx/1.11.4 ``` 505 HTTP Version Not Supported: 服务器收到的请求, 使用的是它无法支持的HTTP协议版本; 参考:《HTTP权威指南》、《Web API的设计与开发》","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. HTTP请求方法","slug":"http/2017-11-30-HTTP-02","date":"2017-11-30T03:29:12.000Z","updated":"2018-07-20T13:20:39.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-02/","excerpt":"","text":"前言 HTTP/1.1 中实现的method, 参考RFC2616, 可以看到有: OPTIONS, HEAD, GET, POST, PUT, DELETE, TRACE, CONNECT RFC2616中提到: PATCH, LINK, UNLINK方法被定义, 但并不常见; (《图解http协议》中也提到 LINK, UNLINK 已经被http1.1废弃); 不同应用各自的实现不同, 有些应用会完整实现, 有些还会扩展, 有些可能只会实现一部分; PUT PUT: 替换资源; PUT 和 POST的区别: 在HTTP中, PUT被定义为 idempotent(幂等性) 的方法, POST则不是, 这是一个很重要的区别 应该用 PUT 还是 POST? 取决于这个REST服务的行为是否是idempotent(幂等)的假如发送两个请求, 希望服务器端是产生两个新数据，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用 POST 方法;但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的, 那就应该使用 PUT 方法; 虽然 POST 和 PUT 差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦; POST POST: 上面已经提过了, POST是非幂等的; POST 和 PUT 都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的; PATCHPATCH不是HTTP标准方法的，服务端需要考虑客户端是否能够支持的问题; 对已有资源的操作: 用于对资源的 部分内容 进行更新 (例如更新某一个字段, 具体比如说只更新用户信息的电话号码字段); 而 PUT 则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变; HEAD HEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回, 而仅仅返回HTTP头信息; 比如: 欲判断某个资源是否存在, 我们通常使用GET, 但这里用HEAD则意义更加明确; GET比较简单, 直接获取资源; OPTIONS这个方法使用比较少, 它用于获取当前URL所支持的方法;若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST;之前跨域相关博文 CORS方案 not-so-simple request 中的”预检”请求用的请求方法就是 OPTIONS; CONNECT要求用隧道协议连接代理, 如使用SSL TRACE~~未完待续 DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"02. php7常见扩展包的安装测试","slug":"php/2017-11-21-php-02","date":"2017-11-21T13:51:19.000Z","updated":"2018-08-20T10:56:34.000Z","comments":true,"path":"2017/11/21/php/2017-11-21-php-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/21/php/2017-11-21-php-02/","excerpt":"","text":"xdebug 扩展安装 去pecl上下载合适的xdebug扩展包 下载后解压, 进入解压目录, 运行 12345678910/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make install// 如下可以看到安装成功......Build complete.Don&apos;t forget to run &apos;make test&apos;.Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20151012/ 在php.ini文件中新增 12[xdebug]extension=xdebug.so 最后在phpinfo中即可看到redis扩展被成功安装, 或者运行 /usr/local/php/sbin/php-fpm -m |grep redis xhprof性能分析扩展包安装redis扩展包安装 去pecl上下载合适的redis扩展包 下载后解压, 进入解压目录, 运行 1234567891011/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make install// 如下可以看到安装成功......Build complete.Don&apos;t forget to run &apos;make test&apos;.Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20151012/[root@lant redis-4.1.1]# 在php.ini文件中新增 12[redis]extension=redis.so 最后在phpinfo中即可看到redis扩展被成功安装, 或者运行 /usr/local/php/sbin/php-fpm -m |grep redis MondoDB 扩展安装 去pecl下载合适的MongoDB扩展包 下载后解压, 并进入解压目录, 运行 123/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp;&amp; make install 在php.ini文件中新增 12[mongodb]extension=mongodb.so 最后在phpinfo中即可看到redis扩展被成功安装, 或者运行 /usr/local/php/sbin/php-fpm -m |grep redis AMQP 扩展安装 去pecl下载合适的AMQP扩展包 下载后解压, 并进入解压目录, 运行 123456/usr/local/php/bin/phpize./configure --with-php-config=/usr/local/php/bin/php-config// 貌似报错了: checking for amqp using pkg-config... configure: error: librabbitmq not found// 解决:make &amp;&amp; make install","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"01. PHP7编译安装","slug":"php/2017-11-20-php-01","date":"2017-11-20T12:58:13.000Z","updated":"2018-08-24T08:43:33.000Z","comments":true,"path":"2017/11/20/php/2017-11-20-php-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/20/php/2017-11-20-php-01/","excerpt":"","text":"编译安装 下载PHP7.0.15到 /usr/local/src 并解压缩 12wget http://cn2.php.net/distributions/php-7.0.15.tar.gztar -zxvf php-7.0.15.tar.gz 依赖安装 123yum install -y gcc gcc-c++ make automake autoconf gd file bison patch mlocate flex diffutils zlib zlib-devel pcre pcre-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel libcurl libcurl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers openldap-devellibxslt-devel kernel-devel libtool-libs readline-devel gettext-devel libcap-devel php-mcrypt libmcrypt libmcrypt-devel recode-devel gmp-devel icu libxslt libxslt-devel php-devel// 如果出现 No package libmcrypt available.// 解决方法: yum install epel-release //扩展包更新包 预编译, 编译, 安装 123// 这里设置的config-file-path是php.ini文件的, 因此在安装好之后要将php.ini文件放在此处./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-mysql-sock --with-mysqli --with-libxml-dir --with-openssl --with-mcrypt --with-mhash --with-pcre-regex --with-zlib --with-iconv --with-bz2 --with-curl --with-cdb --with-pcre-dir --with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --with-gettext --with-gmp --with-mhash --with-libmbfl --with-onig --with-pdo-mysql --with-zlib-dir --with-readline --with-libxml-dir --with-xsl --with-pear --enable-fpm --enable-soap --enable-bcmath --enable-calendar --enable-dom --enable-exif --enable-fileinfo --enable-filter --enable-ftp --enable-gd-native-ttf --enable-gd-jis-conv --enable-json --enable-mbstring --enable-mbregex --enable-mbregex-backtrack --enable-pdo --enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem --enable-sysvshm --enable-wddx --enable-zip --enable-mysqlnd-compression-supportmake &amp;&amp; make install 拷贝配置文件 12345cp /usr/local/src/php-7.0.15/php.ini-production /usr/local/php/etc/php.inicp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf// 把pid 改成 /run/php-fpm.pidvim /usr/local/php/etc/php-fpm.conf 将php-fpm加入到system中管理 12345678910111213141516cd /lib/systemd/systemvim php-fpm.service[Unit]Description=The PHP FastCGI Process ManagerAfter=syslog.target network.target[Service]Type=simplePIDFile=/run/php-fpm.pidExecStart=/usr/local/php/sbin/php-fpm --nodaemonize --fpm-config /usr/local/php/etc/php-fpm.confExecReload=/bin/kill -USR2 $MAINPIDExecStop=/bin/kill -SIGINT $MAINPID[Install]WantedBy=multi-user.target 启动php-fpm: systemctl start php-fpm.service 添加到开机启动: systemctl enable php-fpm.service 注意: php.ini 配置文件的路径是在编译时通过 —with-config-file-path 设置的; 而 php-fpm.conf 配置文件貌似是需要放在 安装目录下的etc目录中; php-fpm启动后, 会调用php-cgi解析器, 它会调用这两个配置文件; 如果nginx和php不在一台机器, 需要注意, php-fpm监听的不能是 127.0.0.1:9000, 而应该是对外ip:9000; 配置nginx 配置比较简单, 在对应的server块中设置: 12345678location ~ \\.php$ &#123; root /html; // 一般nginx和php-fpm都在一台机器上, 此处不在同一机器, 需要保证php-fpm的配置listen不是127.0.0.1:9000, 而是其对外ip:9000 fastcgi_pass 192.168.3.121:9000;#php-fpm的默认端口是9000 fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params;&#125; 注意, 如果php代码有错误, 但是访问只显示500状态码, 并不展示具体错误, 注意修改php-fpm.conf配置文件 123vi /usr/local/php/etc/php-fpm.d/www.con// 打开下面配置, 并设置为on即可;php_flag[display_errors] = off 增加扩展(phpize,php-config)如何确保php在编译的时候, 该有的扩展都打开了? 哪些是默认打开的? 如何确定都要开启哪些扩展??编译的时候打开的扩展和后面通过phpize编译的扩展有什么区别??为什么编译时候打开的扩展在php.ini中并没有显示 extension=xxx.so??phpfpm -m 找到的扩展, 和phpinfo中的不一样, 前者貌似是包含的扩展, 后者是已经打开正在使用的扩展! phpize 是用来扩展php模块的 在之前编译安装php时, ./configure 预编译阶段会有一些 --with 参数, 就是说明需要打开的PHP模块, 不过有些些模块在php安装的时候可能没有编译进来, 所以如果以后我们还想再增加些其他模块, 但是又不想重新编译php, 这个是时候就可以用 phpize 了; 当php编译完成后, phpize命令脚本一般会放在安装目录下的bin目录下; 主要是用来侦测环境, 在扩展模块中创建 configure文件; 比如想在php中加入Xhprof性能分析模块, memcache扩展模块 可以先到 pecl, 或者 github 上找到合适的扩展模块tarball; 然后就可以执行phpize来编译要添加的模块; 如果你的系统中有多个php版本, 需要选择正确的php版本下的phpize命令; 然后可以使用 ./configure 命令编译扩展模块, 此时可以使用 --with-php-config 选项指明相应php安装目录下的 php-config 命令路径, 如 ./configure --with-php-config=/usr/local/php/bin/php-config php-config是一个脚本文件, 用于获取所安装的php配置的信息 测试[xhprof性能分析扩展包安装] [redis扩展包安装] [rabbitmq扩展包安装] 小知识 如果机器上有很多个版本的php, 要想知道正在运行的是哪个版本, 直接 ps aux | grep php-fpm 即可看到主进程使用的配置文件, 如下: 12345678-bash-4.2$ ps aux | grep php-fpmwork 2670 0.0 0.4 740380 19060 ? S 13:15 0:04 php-fpm: pool wwwwork 2782 0.0 0.2 664000 9232 ? Ss Jul21 8:50 php-fpm: master process (/home/work/php70_15/etc/php-fpm.conf)work 4549 0.0 0.5 740644 23044 ? S 13:37 0:03 php-fpm: pool wwwwork 4911 0.0 0.4 740644 19248 ? S 13:41 0:04 php-fpm: pool wwwwork 5493 0.0 0.4 666536 18056 ? S 13:48 0:03 php-fpm: pool wwwwork 6819 0.0 0.5 740644 21168 ? S 14:03 0:02 php-fpm: pool wwwwork 9544 0.0 0.4 666604 18180 ? S 14:35 0:02 php-fpm: pool www 注意: 有时候会遇到nginx报500错误, 但是可能你正在使用的Laravel框架并不能捕获到错误, 此时, 不能单看代码层的错误日志, 因为有可能一个大数组就直接内存溢出了, 此时需要看 php-fpm/php.ini(一般是后者) 指定的错误日志, 如: 1234// less /home/work/php70_4/log/php_errors.log | grep &apos;error&apos;[24-Aug-2018 12:10:10 Asia/Shanghai] PHP Fatal error: Allowed memory size of 134217728 bytes exhausted (tried to allocate 20480 bytes) in /home/work/www/releases/c2b_car_dealer/201808042314lijian/unfrozenDeposit/rrc/vendor/illuminate/database/Connection.php on line 332","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.renyimin.com/tags/PHP/"}]},{"title":"23. R-B Tree (红黑树)","slug":"data-structure/2017-10-21-23-read-black-tree","date":"2017-10-21T02:57:09.000Z","updated":"2018-08-25T11:34:04.000Z","comments":true,"path":"2017/10/21/data-structure/2017-10-21-23-read-black-tree/","link":"","permalink":"http://blog.renyimin.com/2017/10/21/data-structure/2017-10-21-23-read-black-tree/","excerpt":"","text":"红黑树定义 上篇已经介绍了 2-3树, 现在要学习的是一种名为红黑二叉查找树的简单数据结构来表达并实现它; 红黑二叉树背后的基本思想是用 标准的二叉查找树(完全由2-结点构成) 和 一些额外的信息(替换 3-结点) 来表示2-3树; 我们将树中的链接分为两种类型: 红链接将两个 2-结点 链接起来构成一个 3-结点; 黑链接则是 2-3树中的普通链接; 确切地说: 将3-结点表示为一条左斜的红色链接(两个结点其中之一是另一个左子结点)相连的两个2-结点:这种表示法的一个优点是, 我们无需修改就可以直接使用标准的二叉查找树的get()方法, 对于任意的2-3树, 只要对结点进行转换, 我们都可以立即派生出一棵对应的二叉查找树。我们把用这种方式表示2-3树的二叉查找树称为红黑二叉查找树, 简称红黑树; 等价定义: 红黑树的另一种定义是含有红黑链接并满足下列条件的二叉查找树 红链接均为左链接 没有任何一个节点同时和两条红链接相连 该树是完美黑色平衡的, 即任意空链接到根节点的路径上的黑链接数量相同满足这样定义的红黑树和相应的2-3树是一一对应的 一一对应: 如果将一颗红黑树中的红链接画平, 那么所有的空链接到根节点的距离都将是相同的: 如果将有红链接相连的结点合并, 得到的就是一棵 2-3树;相反, 如果将一棵 2-3树中的 3-结点画作由红色左链接相连的两个2-结点, 那么不会存在能够和两条红链接相连的结点, 因为根据定义, 红黑树是完美黑色平衡的(即任意空链接到根节点的路径上的黑链接数量相同); 颜色表示 因为每个结点都只会有一条指向自己的链接(从它的父结点指向它), 为了方便, 我们将链接的颜色保存在表示结点的Node数据类型的布尔变量color中, 如果指向它的链接是红色的, 那么该变量为true, 黑色则为false; 我们约定空链接为黑色; 当我们提到一个结点的颜色时, 其实指的是指向该节点的链接的颜色, 反之亦然; 旋转 在我们实现的某些操作中可能会出现红色右链接或者两条连续的红链接, 但在操作完成前这些情况都会被小心地旋转并修复; 旋转操作会改变红链接的指向; 首先, 假设有一个红色右链接, 我们需要对其进行左旋转转化成左链接: 左旋转方法接受一条指向红黑树中的某个节点的 链接h 作为参数, 假设被指向的结点的右链接是红色的, 这个方法会对树进行必要的调整 左旋转方法会返回一个包含同一组键的子树, 且子树的左链接为红色的根节点的链接 其实操作很简单: 只是将 以两个键中的较小者作为根节点 变为 以较大者作根节点; 右旋转很好实现, 只需要将左右对调过来即可, 如下所示: 在插入新键时, 我们可以使用旋转操作帮助我们保证2-3树和红黑树之间的一一对应关系, 因为旋转操作可以保持红黑树的两个重要性质: 有序性 和 完美平衡性。 ¡¡https://www.cnblogs.com/songdechiu/p/6880485.html","categories":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/categories/Data-Structures/"}],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/tags/Data-Structures/"}]},{"title":"B-Tree(BTree),B+Tree","slug":"data-structure/2017-10-28-b-tree-b+tree","date":"2017-10-20T13:13:53.000Z","updated":"2018-08-25T11:33:01.000Z","comments":true,"path":"2017/10/20/data-structure/2017-10-28-b-tree-b+tree/","link":"","permalink":"http://blog.renyimin.com/2017/10/20/data-structure/2017-10-28-b-tree-b+tree/","excerpt":"","text":"B Tree 即B-Tree,点击链接进入, 发现就是B-Tree https://www.jianshu.com/p/da59af78ec59 https://github.com/PuShaoWei/arithmetic-php 大神: http://www.cnblogs.com/skywang12345/p/3603935.html","categories":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/categories/Data-Structures/"}],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/tags/Data-Structures/"}]},{"title":"B+Tree","slug":"data-structure/2017-10-28-b+tree","date":"2017-10-20T13:13:53.000Z","updated":"2018-08-23T07:47:01.000Z","comments":true,"path":"2017/10/20/data-structure/2017-10-28-b+tree/","link":"","permalink":"http://blog.renyimin.com/2017/10/20/data-structure/2017-10-28-b+tree/","excerpt":"","text":"","categories":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/categories/Data-Structures/"}],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/tags/Data-Structures/"}]},{"title":"22. 2-3Tree","slug":"data-structure/2017-10-19-22-2-3tree","date":"2017-10-19T03:37:08.000Z","updated":"2018-08-25T03:17:54.000Z","comments":true,"path":"2017/10/19/data-structure/2017-10-19-22-2-3tree/","link":"","permalink":"http://blog.renyimin.com/2017/10/19/data-structure/2017-10-19-22-2-3tree/","excerpt":"","text":"2-3树 2-3查找树 是最简单的 B-Tree 结构, 一棵2-3树 或为一棵空树, 或者由以下结点组成: 2-结点: 确切地说, 一棵标准的二叉树中的结点都是2-结点, 含有 一个键(及其对应的值) 和 两条链接 (左链接指向的结点的所有键都小于该结点的键, 右链接指向的结点的所有键都大于该结点的键) 3-结点: 含有 两个键(及其对应的值) 和 三条链接(左链接指向的结点的键都小于该结点的所有键, 右链接指向的结点的所有键都大于该结点的所有键, 中链接指向的结点的所有键, 都位于该结点的两个键的中间) 所以可以看出, 2-3树不是二叉树, 其结点可拥有3个孩子; 2-3树与满二叉树比较相似, 高为h的2-3树包含的结点数大于等于高度为h的满二叉树的结点数, 即至少有 $2^h-1$ 个结点 (换个角度分析, 包含n的结点的2-3树的高度不大于 $log_2{n+1}$(即包含n个结点的二叉树的最小高度, 也即包含n个结点的满二叉树的高度)) 若某棵 2-3树 不包含3-结点, 则看上去像满二叉树; 但是如果 2-3树 的一个内部结点确实有3个孩子, 那就比相同高度的满二叉树的结点更多 下面是一棵2-3查找树示意图: 一棵完美平衡的2-3Tree中的所有空链接到根结点的距离应该都是相同的; 为了简单起见, 接下来会用2-3Tree来指代一棵完美平衡的2-3Tree; 2-3Tree的查找过程: 向 2-结点 中插入新键 要在2-3树中插入一个新的结点, 可以像二叉查找树一样, 先进性一次未命中查找, 然后把新结点挂在树的底部; 但这样的话, 树无法保持完美平衡性; 而我们使用2-3树的原因就在于它能在插入后继续保持平衡: 如果未命中的查找结束于一个 2-结点, 那么将这个键加入这个 2-结点 中, 组成 3-结点; 而如果未命中的查找结束于一个 3-结点, 就比较麻烦一些; 向一棵只含有一个 3-结点 的树中插入新键 在考虑一般情况之前, 先假设需要向一棵只含有一个 3-结点 的树中插入一个新键; 由于 3-结点 只有两个键, 所以在它唯一的结点中已经没有可插入新键的空间了, 为了将新键插入, 我们先临时将新键存入该结点中, 使之成为一个 4-结点, 它很自然地拓展了以前的结点并含有 3个键 和 4条链接; 然后将 4-结点 转换为由3个 2-结点 组成的2-3树, 其中, 中间的键所在 2-结点 成为了其他两个键所在结点的父结点; 这是2-3树生长的基本方式 向一个父结点是 2-结点 的 3-节 点中插入新键 同样, 先将新键临时保存到该结点中, 组成4-结点并将其分解, 但此时不会为中键创建一个新结点; 而是将中键移到父结点中, 指向旧的 3-结点 的链接将被中键两边的两条链接替代, 并分别指向两个新的2-结点; 这次转换并不影响2-3树的主要性质, 树仍然是有序的, 仍然是完美平衡的(空链接到根结点的距离仍然相同) 向一个父结点是3-结点的3-结点中插入新键 同样, 先将新键临时保存到该结点中, 组成 4-结点 再将其分解, 将中键插入到父结点中, 同时指向 旧的3-结点的链接将被中键两边的两条链接替代, 父结点此时成为临时的4-结点; 对新的 4-结点 进行同样的处理, 即分解4-结点并将中键插入到父结点中; 推广到一般情况, 我们就这样一直向上不断分解临时的 4-结点 直到遇到一个 2-结点, 并将其替换成一个 3-结点, 或者一直到3-结点的根; 分解根结点 如果从插入结点到根结点一路都是 3-结点, 根结点最终将变成一个临时的4-结点; 这种情况可以照向一棵只有一个 3-结点 的树中插入新键的方法处理这个问题, 将根结点分解为3个2-结点, 使得树高加1 小结 局部变换 将2-3树中的一个 4-结点 分解, 有六种情况 1234564-结点是根结点4-结点是2-结点的左子结点4-结点是2-结点的右子结点4-结点是3-结点左子结点4-结点是3-结点中子结点4-结点是3-结点右子结点 2-3插入算法的根本在于这些变换都是局部的: 除了相关的结点和链接之外不必修改或者检查树的其他部分; 每次变换中, 变更的链接个数不会超过一个很小的常数;每个变换都会将 4-结点中 的一个键送入它的父结点并重构相应的链接而不用改树的其他部分 全局性质: 上述哪些局部变换不会影响树的全局有序性和平衡性: 任何空链接到根结点的路径长度都是相等的; 和标准的二叉树由上向下生长不同, 2-3树的生长是右下向上的, 可以研究一下下图, 就能很好滴理解 2-3树的构造方式 曾记否, 在二叉查找树中, 按照升序插入10个键会得到高度为9的一棵最差查找树, 但是使用2-3树, 树的高度是2; 分析 在一棵大小为N的2-3树中, 查找和插入操作访问的结点必然不超过 $log_2N$ 个; 证明: 一棵大小为N的2-3树中, 其高度在 $log_3N$(树中全是 3-节点) 和 $logN$(树中全是 2-节点) 之间; 所以可以确定, 2-3树 在最坏情况下仍有较好的性能, 查找和插入的性能被控制在对数时间级别; 完美平衡的 2-3树 要平展的多, 一棵含有10亿个结点的 2-3树, 其高度仅在19-30之间, 最多访问30个结点就能够在10亿个键中进行任意查找和插入操作, 这是相当惊人的; 完美平衡 Perfect balance: 实际上就是每条从根节点到叶节点的路径的高度都是一样的(Every path from root to leaf has same length) 但是这样直接实现2-3树不是很现实, 因为有很多情况需要处理 我们需要维护两种不同类型的结点, 将被查找的键和结点中的每个键进行比较, 将链接和其他信息从一种结点复制到另一种结点, 将结点从一种数据类型转换到另一种数据类型, 等等; 实现这些不仅需要大量的代码, 而且他们所产生的额外开销可能会使算法比标准的二叉查找树更慢; 平衡一棵树的初衷是为了消除最坏情况, 但我们希望这种保障所需的代码能够越少越好, 幸运的是你将看到, 我们只需要一点点代价就能用一种统一的方式完成所有变换。 参考:","categories":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/categories/Data-Structures/"}],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/tags/Data-Structures/"}]},{"title":"Tree","slug":"data-structure/2017-10-07-tree","date":"2017-10-07T08:56:39.000Z","updated":"2018-08-24T09:47:18.000Z","comments":true,"path":"2017/10/07/data-structure/2017-10-07-tree/","link":"","permalink":"http://blog.renyimin.com/2017/10/07/data-structure/2017-10-07-tree/","excerpt":"","text":"前言 树形结构 是一类重要的非线性数据结构; 其中以 树 和 二叉树 最为常用; 树形结构 除了在客观世界中广泛存在, 如形象地表示人类社会的族谱关系和各种社会组织机构; 在计算机领域也得到广泛应用, 比如在数据库系统中, 树形结构也是信息的重要组织形式之一; 树(tree)的定义和基本术语 树是n(n≥0)个结点的有限集, 在任意一个非空树中: 有且仅有一个特定的结点, 被称为树的根(Root)结点; 当n＞1, 除根结点之外的其余结点被分成m(m&gt;0)个互不相交的有限集 T1,T2,…,Tm ; 其中每一个集合本身又是一棵树, 称为根结点的子树; 如下图: (a) 是只有一个结点(根结点)的树; (b) 是有13个结点的树, 其中A是根, 其余结点分成3个互不相交的子集:T1 = {B,E,F,K,L}T2 = {C,G}T3 = {D,H,I,J,M} T1,T2,T3都是根节点A的子树, 且本身也是一颗树, 例如, T1的根为B, 其余结点分为2个互不相交的子集:T11 = {E,K,L}T12 = {F} T11和T12都是B的子树, 而T11中E是根, {K}和{L}是E的两棵互不相交的子树, 其本身又是只有一个结点(根结点)的树; 相关概念, 可参考 https://www.cnblogs.com/zsychanpin/p/7182467.html 结点(Node): 表示树中的数据元素, 是数据元素的别名; 结点的度(Degree of Node): 树中结点所拥有的子树的个数; (如图(b)中, 节点A的度为3) 树的度(Degree of Tree): 树中各结点度的最大值; (二叉树的度为2(非极端情况), 如图(b)中, 树的度为3) 叶子结点(Leaf Node): 也叫终端结点, 度为0的结点; ( 如图(b)中, 叶子节点为 {K.L,M}) 分支结点(Branch Node): 非终端结点, 度不为0的结点; (如图(b)中, 分支节点为 {A,B,C,D,E,F,G,H,I,J}) 结点的层次(Level of Node): 根结点的层数规定为1, 其余结点的层数等于它的双亲结点的层数+1； (如图(b)中, 节点E的层次为3) 树的深度(Depth of Tree): 树中各结点层次的最大值; (如图(b)中, 树的深度为4) 双亲(Parent): 树中一个孩子结点的上层结点(唯一); 孩子(Child): 树中一个结点的子树的根结点; 兄弟(Brother): 同一个双亲的孩子结点互为兄弟; (如图(b)中, 节点{B,C,D}互为兄弟, {H,I,J}也互为兄弟) 堂兄弟(Sibling): 双亲在同一层, 且双亲不同的结点互为堂兄弟; (如图(b)中, E和G, F和G, G和H, G和I, G和J … 就都互为堂兄弟) 祖先(Ancestor): 从根结点到该结点所经分支上的所有结点都是该节点的祖先; (如图(b)中, 叶子节点M的祖先为 {A,H,D} ) 子孙(Descendant): 一个结点的所有子树中的结点称之为该结点的子孙节点; (如图(b)中, 节点B的子孙节点为 {E,K,L,F} ) 有序树(Ordered Tree): 树中任意一个结点的各孩子结点有严格排列次序的树; 二叉树是有序树, 因为二叉树中每个孩子结点都确切定义为是该结点的左孩子结点还是右孩子结点; 如果一棵树中结点的各子树从左到右是有次序的, 即若交换了某结点各子树的相对位置则构成不同的树, 称这棵树为有序树; 反之, 则称为无序树(Unordered Tree); 森林: m(m≥0)棵不相交的树的集合称为森林; 自然界中树和森林是不同的概念, 但在数据结构中, 树和森林只有很小的差别, 任何一棵树, 删去根结点就变成了森林; 二叉树 二叉树是每个结点最多有两个子树的树结构; 通常子树被称作”左子树”(left subtree)和”右子树”(right subtree), 二叉树常被用于实现 二叉查找树 和 二叉堆; 二叉树 和 树的区别 一棵度为2的树 与 一棵二叉树的区别: 二叉树和度为2的树结构其实类似, 但是:二叉树有序, 度为2的树未必有序; 如下在网上找的, 具有3个结点的(普通无序)树 和 3个结点的二叉树的所有不同形态:在图中, 二叉树的1, 2，3，4形态对于树来说, 其实是相同的形态(与树的1形态一样, 因为树是无序的, 部分左右子节点); 二叉树 和 普通有序树 在只有一棵树的情况下, 二叉树有左右之分、有序树无左右之分; 所以, 二叉树可以理解为度为2的有序树, 这种说法并不正确:一棵度为二的有序树与一棵二叉树的区别在于: 如果有序树中的子树只有一个孩子时, 这个孩子结点就无须区分其左右次序; 而二叉树无论其孩子数是否为2, 均需确定其左右次序, 也就是说二叉树的结点次序不是相对于另一结点而言, 而是确定的; 二叉查找树 二叉排序树(Binary Sort Tree), 又称二叉查找树(Binary Search Tree), 亦称二叉搜索树; 它的高度决定了它的查找效率; 注意: 在极端情况下, 二叉查找树可能是线性的, 此时的查找时间复杂度为 $O(n)$ 二叉排序树 : 或者是一棵空树, 或者是具有下列性质的二叉树: 若左子树不空, 则左子树上所有结点的值均小于或等于它的根结点的值; 若右子树不空, 则右子树上所有结点的值均大于或等于它的根结点的值; 左、右子树也分别为二叉排序树; 尝试画几个二叉查找树, 可以使用旧金山大学的在线绘制工具 向二叉树中按顺序插入 10，9，8 三个节点: 向二叉树中按顺序插入 8，9，10 三个节点: 向二叉树中按顺序插入 9，8，10 或 9，10，8 三个节点: 满二叉树 一棵深度为k, 且有 $2^k-1$ 个节点的二叉树, 称为满二叉树 这种树的特点是每一层上的节点数都是最大节点数 第i层上的结点数为: $2^{i-1}$一个层数为k的满二叉树的叶子结点个数(也就是最后一层): $2^{k-1}$ 如下图: 完全二叉树 完全二叉树是由满二叉树而引出来的: 在一棵二叉树中, 除最后一层外, 若其余层都是满的, 并且最后一层或者是满的, 或者是在右边缺少连续若干节点, 则此二叉树为完全二叉树 满二叉树一定是完全二叉树, 完全二叉树不一定是满二叉树 深度为k的完全二叉树, 至少有 $2^(k-1)$ 个节点, 至多有 $2^k-1$ 个节点(即,完全二叉树最多会变成满二叉树) 如下图: 平衡二叉树 对于一般的二叉搜索树(Binary Search Tree), 其期望高度(即为一棵平衡树时)为 $log_2n$, 其各操作的时间复杂度 $O(log_2n)$ 同时也由此而决定; 但是在某些极端的情况下, 如该二叉树是一颗左斜树或者右斜树, 二叉搜索树将退化成近似链或链, 此时, 其操作的时间复杂度将退化成线性的，即 $O(n)$ , 这就意味着我们的二叉搜索树变成了顺序查找, 效率极为低下, 所以我们需要一种可以平衡二叉树的方法来避免出现极端情况, 方法有很多, 典型的有 AVL树, 红黑树等, 他们各有优势; 平衡二叉搜索树(Self-balancing binary search tree)又被称为AVL树(有别于AVL算法), 且具有以下性质 它是一棵空树或它的左右两个子树的高度差的绝对值不超过1, 并且左右两个子树都是一棵平衡二叉树; 最小二叉平衡树的节点的公式如下 F(n) = F(n-1)+F(n-2)+1 这个类似于一个递归的数列, 可以参考Fibonacci(斐波那契)数列, 1是根节点, F(n-1)是左子树的节点数量, F(n-2)是右子树的节点数量; 平衡二叉树的常用实现方法有 红黑树、AVL、替罪羊树、Treap、伸展树等; 而在平衡二叉搜索树中, 其高度一般都良好地维持在 $O(log_2n)$, 大大降低了操作的时间复杂度; AVL树 AVL是最先发明的自平衡二叉查找树算法 本身首先是一棵二叉搜索树; 带有平衡条件: 每个结点的左右子树的高度之差的绝对值(平衡因子)最多为1, 所以它也被称为高度平衡树; 也就是说, AVL树, 本质上是带了平衡功能的二叉查找树(二叉排序树, 二叉搜索树) 查找、插入和删除在平均和最坏情况下都是 $O(log_2n)$; AVL树的关键字查找操作与二叉树类似, 但为了做到自平衡, 其插入和删除操作需要做调整, 每次插入和删除节点都会破坏二叉树的平衡性, 为了保证AVL的平衡性, 每次插入和删除操作都需要进行自平衡调整, 失去平衡后的自平衡操作可归纳为下列四种情况: 单向右旋平衡处理LL:由于在a的左子树根结点的左子树上插入结点, a的平衡因子由1增至2, 致使以*a为根的子树失去平衡, 则需进行一次右旋转操作; 单向左旋平衡处理RR:由于在a的右子树根结点的右子树上插入结点, a的平衡因子由-1变为-2, 致使以*a为根的子树失去平衡, 则需进行一次左旋转操作； 双向旋转(先左后右)平衡处理LR:由于在a的左子树根结点的右子树上插入结点，a的平衡因子由1增至2，致使以*a为根的子树失去平衡，则需进行两次旋转(先左旋后右旋)操作; 双向旋转(先右后左)平衡处理RL:由于在a的右子树根结点的左子树上插入结点, a的平衡因子由-1变为-2, 致使以*a为根的子树失去平衡, 则需进行两次旋转(先右旋后左旋)操作; 测试, 可以使用旧金山大学的在线绘制工具看看 红黑树 普通的二叉查找树在极端情况下可退化成链表, 此时的增删查效率都会比较低下。为了避免这种情况, 就出现了一些自平衡的查找树, 比如 AVL, 红黑树等。这些自平衡的查找树通过定义一些性质, 将任意节点的左右子树高度差控制在规定范围内, 以达到平衡状态; 红黑树(Red Black Tree) 也是一种自平衡二叉查找树, 它是在1972年由Rudolf Bayer发明的, 当时被称为平衡二叉B树(symmetric binary B-trees); 后来, 在1978年被 Leo J. Guibas 和 Robert Sedgewick 修改为如今的 红黑树; 红黑树具有良好的效率, 它可在 $O(log_2N)$ 时间内完成查找、增加、删除等操作, 因此, 红黑树在业界应用很广泛, 比如 Java 中的 TreeMap, JDK 1.8 中的 HashMap、C++ STL 中的 map 均是基于红黑树结构实现的; 红黑树和AVL树类似, 都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡, 从而获得较高的查找性能; 一棵红黑树是指一棵满足下述性质的二叉搜索树(BST, binary search tree), 它通过如下的性质定义实现自平衡: 12345每个节点或者是黑色, 或者是红色根节点是黑色所有叶子都是黑色(叶子是NIL节点)如果一个节点是红色的, 则它的两个子节点必须是黑色的; (也就是说, 不能有两个相邻的红色结点)从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点(简称黑高) 有了上面的几个性质作为限制, 即可避免二叉查找树退化成单链表的情况; 但是仅仅避免二叉查找树退化成单链表还不够, 这里还要考虑某个节点到其每个叶子节点路径长度的问题, 如果某些路径长度过长, 那么, 在对这些路径上的结点进行增删查操作时, 效率也会大大降低; 这个时候性质4和性质5用途就凸显了, 有了这两个性质作为约束, 即可保证任意节点到其每个叶子节点路径最长不会超过最短路径的2倍; 当某条路径最短时, 这条路径必然都是由黑色节点构成; 当某条路径长度最长时，这条路径必然是由红色和黑色节点相间构成（性质4限定了不能出现两个连续的红色节点）。而性质5又限定了从任一节点到其每个叶子节点的所有路径必须包含相同数量的黑色节点。此时，在路径最长的情况下，路径上红色节点数量 = 黑色节点数量。该路径长度为两倍黑色节点数量，也就是最短路径长度的2倍。举例说明一下，请看下图： https://segmentfault.com/a/1190000012728513 二叉树的遍历规则 所谓树的遍历, 是指依照一定的规律不反复地访问(或取出节点中的信息, 或对节点做其它的处理)树中的每个节点, 其遍历过程实质上是将树这样的非线性结构按一定规律转化为线性结构; 二叉树的遍历顺序大体分为三种: 前序遍历(先根遍历、先序遍历), 中序遍历(中根遍历), 后序遍历(后根遍历) 先序遍历 先根遍历 的规则为: 若树为空,则退出; 否则按照 根节点—&gt;左子树—&gt;右子树 的顺序访问二叉树 比如, 下图所看到的树进行先根遍历 中序遍历 中根遍历 的规则为: 若树为空, 则退出; 否则按照 左子树—&gt;根节点—&gt;右子树 的顺序访问二叉树 比如, 下图所看到的树进行中根遍历 后序遍历 后根遍历 的规则为: 若树为空, 则退出; 否则按照 左子树—&gt;右子树—&gt;根节点 的顺序访问二叉树 比如, 下图所看到的树进行后根遍历 如下图: ?? 右旋操作 参考:https://www.sohu.com/a/201923614_466939https://blog.csdn.net/u012538536/article/details/50354604https://blog.csdn.net/qq_36525906/article/details/76890222https://blog.csdn.net/qq_36098284/article/details/80178336 伸展树(Splay Tree)哈夫曼树线索二叉树BTreeB+Tree参考https://github.com/xiufengcheng/DATASTRUCTURE/tree/master/Chapter_07_Tree#%E6%A3%AE%E6%9E%97https://blog.csdn.net/lemon89/article/details/50193891 https://www.cnblogs.com/kangjianwei101/p/5243404.html https://baike.baidu.com/item/2-3-4树/7689684?fr=aladdin 往下拉可以看到计算机中所有的树","categories":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/categories/Data-Structures/"}],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://blog.renyimin.com/tags/Data-Structures/"},{"name":"Tree","slug":"Tree","permalink":"http://blog.renyimin.com/tags/Tree/"}]},{"title":"28. 隔离级别 与 锁","slug":"mysql/2017-09-03-mysql-28","date":"2017-09-03T06:20:52.000Z","updated":"2018-09-01T12:47:44.000Z","comments":true,"path":"2017/09/03/mysql/2017-09-03-mysql-28/","link":"","permalink":"http://blog.renyimin.com/2017/09/03/mysql/2017-09-03-mysql-28/","excerpt":"","text":"前言 之前几篇博文已经介绍了Mysql事务, 高并发下事务将会面对的问题 及 MySQL的解决方案; MySQL主要采用 事务隔离性中的4种隔离级别 结合 MVCC机制 来进行解决; 而事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略; 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的; READ UNCOMMITTED 未提交读READ COMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料:-《高性能MySQL》 MySQL官方文档 美团技术博客","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"27. 幻读, 快照读(snapshot read), 当前读 (current read)","slug":"mysql/2017-09-02-mysql-27","date":"2017-09-02T11:25:07.000Z","updated":"2018-09-01T13:51:53.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-27/","excerpt":"","text":"RR + MVCC 虽然解决了 幻读 问题, 但要注意, 幻读针对的是读操作(对于其他操作就不一样了); 演示 打开 两个客户端 1,2 确保隔离级别为默认级别RR, 提供语句: 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 123456789101112mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) 回到 客户端2 的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了 select 幻读)!! 12345678910111213141516171819202122mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 但如果尝试在客户端2的事务中执行 insert/delete/update , 却会发现此类操作都可以感知到客户端1提交的新数据 123mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; 小结 虽然发现已经克服了幻读问题; 但当 在客户端2事务中 insert 插入一条id为4的新数据, 却发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 — 一致性非阻塞读中的一段介绍 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。如果一个事务去更新或删除其他事务提交的行, 则那些更改对当前事务就变得可见;但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 也就是RR隔离级别, 在同一事务中多次读取的话, 对 select 克服了 幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念: 当前读 和 快照读 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的 select 都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 快照读： 是通过MVVC(多版本控制)和 undo log 来实现的, 常见语句如下(貌似就是常见的悲观锁么): 1简单的select操作 (不包括: `select ... lock in share mode`, `select ... for update`) 而 当前读 根本不会创建任何快照, insert, update, delete都是当前读, 所以这几个操作会察觉到其他事务对数据做的更改(而普通select是察觉不到的): 12345select ... lock in share modeselect ... for updateinsertupdatedelete","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MySQL 高并发下常见的事务问题","slug":"mysql/2017-09-02-mysql-26","date":"2017-09-02T06:56:32.000Z","updated":"2018-09-01T13:48:20.000Z","comments":true,"path":"2017/09/02/mysql/2017-09-02-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/mysql/2017-09-02-mysql-26/","excerpt":"","text":"前言上一篇MySQL事务简介中对MySQL事务的 基本概念 及 特性 做了简单介绍; 接下来会分析在实际生产环境中面对高并发场景时, 事务会出现的一些常见问题; 高并发事务问题在并发量比较大的时候, 很容易出现 多个事务并行 的情况; 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境 过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以 最终导致了一些问题的出现; 脏读 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读 因为 事务A 此时读取到的是 并行事务B 尚未最终持久化的数据 (该数据还不具备事务的 持久性) 事务B 最终可能会因为其事务单元内部其他后续操作的失败 或者 系统后续突然崩溃等原因, 导致事务B最终整体提交失败而回滚, 那么最终 事务A 之前拿到就是 脏的数据 了(当然, 如果 事务A 在后续操作中继续读取的话, 无论事务B是否结束, 其每次的更新操作, 事务A都会及时读到新数据, 只不过这同时涉及到了下一个讨论的 不可重复读问题, 暂时可以不了解) 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身隔离性解决了脏读问题 : READ COMMITED 或 以上隔离级别(RC+); READ COMMITED 隔离级别保证了: 在事务单元中, 某条语句执行时, 只有已经被其他事务提交的持久性落地数据, 才对该语句可见; 不可重复读 之前 脏读问题 的解决了, 仅仅只意味着事务单元中的每条语句读取到的数据都是 具备持久性的落地数据而已; 之前在讨论脏读问题时, 有个问题也同时存在着, 那就是一个事务单元中 不可重复读 的问题; 显然, RC 隔离级别只解决了 脏读的问题 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了 解决方案 : RR+ 在MySQL中, 事务已经用自身隔离性解决了 不可重复读 问题 — REPEATABLE READ 或 以上隔离级别(RR+); REPEATABLE READ 级别保证了:在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的; ( READ COMMITED )在事务中, 多次读取同一个数据(在两次读取操作之间, 无论数据被 提交 多少次(即无论落地过多少遍), 每次读取的结果都应该是和事务中第一次读取的结果一样; 幻读 可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 不可重复读 和 幻读 这两个概念容易搞混 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录); 解决方案: RR + MVCC 其实对于 幻读 问题, 在Mysql的InnoDB存储引擎中, 是通过事务的 RR + MVCC机制 进行解决的;当然, 这里的幻读不涉及 具有当前读能力的那些语句; (也就是说只是解决幻读, 所谓幻写之类的就不在范围内了) 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 之所以 不可重复读 和 幻读 容易搞混, 可能是因为: 在mysql中, 由于默认就是RR隔离级别下, 该隔离级别已经解决了幻读, 所以无法模拟出幻读的场景; 而 退回到 RC隔离级别 的话, 虽然 幻读 和 不可重复读 都会出现, 但由于现象都是两次读取结果不一样, 容易分辨不出! 想了解更多, 可以参考下一篇幻读的延伸 高并发事务问题 之 更新丢失最后聊一下高并发事务的另一个问题, 也是最常遇到的问题: 丢失更新问题; 该问题和之前几个问题需要区分开: 该问题需要我们自己来解决;更新丢失问题分为两类 第一类丢失更新(回滚覆盖)简介 事务A 回滚时, 将 事务B 已经提交的数据覆盖了 需要注意的是: 这种情况在Mysql中不会出现; RU 级别演示 对于InnoDB事务的最低隔离级别 READ UNCOMMITED, 并行事务B的未提交数据都可以读到, 更别说已提交数据了 (所以回滚也会回滚到事务B提交的最新数据) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RC 级别演示 对于 READ COMMITTED: 在事务B提交之后, 事务A在T3阶段是可以select(快照读)到事务B最终提交的数据的, 更别说update(当前读)到了, 所以事务A最终的Rollback其实也是基于事务B提交后的数据的 (关于这里提到的快照读和当前读, 下一篇会介绍) 语句如下: 12345678SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age - 15 where id=2;commit; RR 级别演示 对于 REPEATABLE READ 可重复读, 事务A在T3阶段虽然select不到事务B最终提交的数据(快照读), 但是可以update(当前读)到事务B最终提交的数据的 (注意: RR与RC虽然都会有快照读, 但是快照读的结果却不一致, 其实是因为两者的MVCC机制快找时机不同导致的, 后面会讲解) 语句如下: 1234567SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;select * from test_transaction where id=2;update test_transaction set age = age+10 where id=2;rollback; 12345SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age-15 where id=2;commit; SERIALIZABLE 演示 SERIALIZABLE 串行化: 读写都加锁, 最容易出现死锁, 所以也不会出现第一类丢失更新的问题, 直接就死锁了 语句如下: 123456SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;SELECT @@SESSION.tx_isolation;begin;update test_transaction set age = age-10 where id=2;rollback; 1234567SELECT @@SESSION.tx_isolation;SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SELECT @@SESSION.tx_isolation;begin;select * from test_transaction where id=2;update test_transaction set age = age -15 where id=2;commit; 第二类丢失更新(提交覆盖) 直接上图 另外, 这里可以解释一下为什么 SERIALIZABLE级别 通常不会不被采用 其实 SERIALIZABLE 虽然做了串行化, 其实也就是对读写都加了锁, 但一旦事务并行, 如果将判断库存的读操作放在事务内就很容易会死锁而放在事务外, 由于更新操作仍然会依据上一个查询的结果, 所以仍然是避免不了第二类丢失更新问题的, 会造成超卖等问题; SERIALIZABLE 的串行化本身也太低效 另外, 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 解决第二类丢失更新的方案: 乐观锁 (在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制) 悲观锁 参考资料: 《高性能MySQL》 淘宝数据库内核6月报 美团技术博客 MySQL官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"09. Prototype 原型模式","slug":"oop/2017-09-02-09-Prototype","date":"2017-09-02T03:18:21.000Z","updated":"2018-08-03T03:37:13.000Z","comments":true,"path":"2017/09/02/oop/2017-09-02-09-Prototype/","link":"","permalink":"http://blog.renyimin.com/2017/09/02/oop/2017-09-02-09-Prototype/","excerpt":"","text":"场景举例 我们每周都会在wiki上创建文档写周报, 但其实每个人的周报格式都差不多, 只是内容细节不一样, 这样 如果每个人写周报都是在wiki中新建一个空白页, 然后再排格式、写内容、… 就会显得比较低效; 而如果每个人都可以根据已完成的周报 复制 一份作为模板, 在此基础上只用对周报内容做些修改, 既可以很容易完成工作, 又可以保证大家周报风格的一致; 这个问题映射到wiki系统的开发中, 其实也是类似的: 如果在wiki的系统设计中, 每次在创建文档时, 都只能 new 一个新的文档对象, 然后针对new出的每个新对象都需要设置 文档排版、内容 等, 这样就会显得很低效; 而如果能够以某个已经完成的文档对象为某种类型的文档对象模板(确定对象种类), 然后复制出新的文档对象, 用户仅需对此类对象做一些细节修改, 就会非常方便; 原型模式的引入 通过上面的问题场景分析, 你需要对wiki的文档模块进行重新设计 除了允许用户创建新文档外, 还允许用户对已经创建好的文档(比如周报文档)进行拷贝; 用户再次创建同类型文档(比如周报文档)时, 既可以创建全新的文档, 也可以选择合适的模板复制一份新的文档; 这种操作非常类似于常用的 复制和粘贴; 要一个面向对象系统中实现对象的复制和粘贴, 这就要引入原型模式了; 原型模式 原型模式(Prototype Pattern): 用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象; 原型模式是一种对象创建型模式; 工作原理很简单: 核心就在于如何实现克隆方法 UML图: Prototype(抽象原型类): 它是声明克隆方法的接口, 是所有具体原型类的父类, 可以是抽象类也可以是接口, 甚至还可以是具体实现类; ConcretePrototype(具体原型类): 它实现在抽象原型类中声明的克隆方法, 在克隆方法中返回自己的一个克隆对象; Client(客户类): 发起创建对象的动作, 让一个原型对象克隆其自身, 从而创建一个新的对象, 在客户类中只需要直接实例化或通过工厂方法等方式创建一个原型对象, 再通过调用该对象的克隆方法即可得到多个相同的对象; 由于客户类针对抽象原型类Prototype编程, 因此用户可以根据需要(的对象种类)选择具体原型类, 系统具有较好的可扩展性, 增加或更换具体原型类, 即增加或更换对象的种类(比如周报文档,或者建立文档..)都很方便; 简单代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&lt;?php/** * 原型接口或抽象原型 * Interface Prototype */interface Prototype&#123; public function copy();&#125;/** * 周报具体原型类 * Class ConcretePrototype1 */class WeeklyNewspaper implements Prototype&#123; public $title = &apos;统一周报模板&apos;; public $content = &apos;技术方面: php开发, java准备; 业务方面: 熟悉新业务&apos;; public function setTitle($title) &#123; $this-&gt;title = $title; &#125; public function setContent() &#123; return $this-&gt;content; &#125; public function display() &#123; // 页面展示你的周报信息 &#125; public function copy() &#123; return clone $this; &#125;&#125;/** * 简历具体原型类 * Class ConcretePrototype2 */class Resume implements Prototype&#123; public $title = &apos;PHP简历模板&apos;; public $content = &apos;技术: php, mysql; 工具: vagrant, docker;&apos;; public function setTitle($title) &#123; $this-&gt;title = $title; &#125; public function setContent() &#123; return $this-&gt;content; &#125; public function display() &#123; // 页面展示你的简历信息 &#125; public function copy() &#123; return clone $this; &#125;&#125;/** * Class Client */class Client&#123; /** * 尝试创建多份简历 */ public function copyResume(Prototype $prototype) &#123; // 不需要再new对象了 return $prototype-&gt;copy(); &#125;&#125;// 先实例化简历模板$weeklyNewspaper1 = new WeeklyNewspaper();$weeklyNewspaper1-&gt;title = &quot;这是一份统一的周报模板&quot;;// 复制周报模板, 不再需要new$client = new Client;$weeklyNewspaper2 = $client-&gt;copyResume($weeklyNewspaper1);$weeklyNewspaper3 = $client-&gt;copyResume($weeklyNewspaper1);$weeklyNewspaper4 = $client-&gt;copyResume($weeklyNewspaper1);// 两个对象相等var_dump($weeklyNewspaper1 == $weeklyNewspaper2);var_dump($weeklyNewspaper1 == $weeklyNewspaper3);var_dump($weeklyNewspaper1 == $weeklyNewspaper4);var_dump($weeklyNewspaper1 === $weeklyNewspaper2);// 重新编写周报$weeklyNewspaper2-&gt;title = &quot;这是PHP开发的周报模板&quot;;// 测试var_dump($weeklyNewspaper2-&gt;title); 深拷贝与浅拷贝 浅拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 而且对其他对象的引用仍然是指向原来的对象, 即浅拷贝只负责当前对象实例, 对引用的对象不做拷贝; 深拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 除了那些引用其他对象的变量, 那些引用其他对象的变量将指向一个被拷贝的新对象，而不再是原来那些被引用的对象;(即深拷贝把要拷贝的对象所引用的对象也拷贝了一次, 而这种对被引用到的对象拷贝叫做间接拷贝) 示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;?php/** * Interface Prototype */interface Prototype&#123; //浅拷贝 public function shallowCopy(); //深拷贝 public function deepCopy();&#125;/** * Class ConcretePrototype */class ConcretePrototype implements Prototype&#123; public $testObject = null; public function __construct($obj) &#123; $this-&gt;testObject = $obj; &#125; //浅拷贝 public function shallowCopy() &#123; return clone $this; &#125; //深拷贝 public function deepCopy() &#123; $deepObj = serialize($this); $cloneObj = unserialize($deepObj); return $cloneObj; &#125;&#125;class Demo&#123; public $title = &apos;测试类&apos;;&#125;/** * Class Client */class Client&#123; public function shallow(Prototype $prototype) &#123; return $prototype-&gt;shallowCopy(); &#125; public function deep(Prototype $prototype) &#123; return $prototype-&gt;deepCopy(); &#125;&#125;$client = new Client;$concretePrototype = new ConcretePrototype(new Demo());var_dump(&quot;浅拷贝:&quot;);$concretePrototype1 = $client-&gt;shallow($concretePrototype);var_dump($concretePrototype == $concretePrototype1);var_dump($concretePrototype === $concretePrototype1);var_dump($concretePrototype-&gt;testObject == $concretePrototype1-&gt;testObject);var_dump($concretePrototype-&gt;testObject === $concretePrototype1-&gt;testObject);var_dump(&quot;深拷贝:&quot;);$concretePrototype = new ConcretePrototype(new Demo());$concretePrototype2 = $client-&gt;deep($concretePrototype);var_dump($concretePrototype == $concretePrototype2);var_dump($concretePrototype === $concretePrototype2);var_dump($concretePrototype-&gt;testObject == $concretePrototype2-&gt;testObject);var_dump($concretePrototype-&gt;testObject === $concretePrototype2-&gt;testObject); 结果: 12345678910/Users/renyimin/Desktop/www.test.com/oop/prototype.php:65:string &apos;浅拷贝:&apos; (length=10)/Users/renyimin/Desktop/www.test.com/oop/prototype.php:67:boolean true/Users/renyimin/Desktop/www.test.com/oop/prototype.php:68:boolean false/Users/renyimin/Desktop/www.test.com/oop/prototype.php:69:boolean true/Users/renyimin/Desktop/www.test.com/oop/prototype.php:70:boolean true/Users/renyimin/Desktop/www.test.com/oop/prototype.php:72:string &apos;深拷贝:&apos; (length=10)/Users/renyimin/Desktop/www.test.com/oop/prototype.php:75:boolean true/Users/renyimin/Desktop/www.test.com/oop/prototype.php:76:boolean false/Users/renyimin/Desktop/www.test.com/oop/prototype.php:77:boolean true/Users/renyimin/Desktop/www.test.com/oop/prototype.php:78:boolean false 原型管理器 原型管理器(Prototype Manager)是将多个原型对象存储在一个集合中, 供客户端使用; 它是一个专门负责克隆对象的工厂, 其中定义了一个集合用于存储原型对象, 如果需要某个原型对象的一个克隆，可以通过复制集合中对应的原型对象来获得; 在原型管理器中针对抽象原型类进行编程, 以便扩展, 其结构如下图:","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"25. MySQL 事务简介","slug":"mysql/2017-08-27-mysql-25","date":"2017-08-27T11:31:07.000Z","updated":"2018-09-01T12:46:34.000Z","comments":true,"path":"2017/08/27/mysql/2017-08-27-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/08/27/mysql/2017-08-27-mysql-25/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的工作单元, 在这个独立的工作单元中, 可以有一组操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败 随处可见的例子: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，就需要打包在一个事务中作为 独立的工作单元 来执行。并且在 这个独立工作单元中的三个操作, 只要有任何一个操作失败, 则整体就应该是失败的, 那就必须回滚所有已经执行了的步骤; 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚 (这也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元;对于一个事务来说, 不能只成功执行其中的一部分操作, 整个事务中的所有操作要么全部成功提交, 要么有操作失败导致所有操作全部回滚, 这就是事务的原子性。 Consistency 一致性此一致性非彼一致性 你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性保证的是 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态, 而不是分布式中提到的数据一致性; 比如之前转账的例子: 转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15) 转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115) 转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取 Iron Man账户 的余额, 那么这个程序读到的应该是500才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务 隔离性 的四个 隔离级别, 到时候就知道这里为什么说 通常来说 ; (确实有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读)(注意: 和RR一样都采用了MVCC机制, 但与RR级别主要区别是快照时机不同, 暂时可不必了解, 后面文章会详解) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 只有该隔离级别才会读写都加锁 Durability 持久性 一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久保存到数据库中; 所谓永久, 也只是主观上的永久, 可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"22. 锁","slug":"mysql/2017-08-27-mysql-22","date":"2017-08-27T03:03:19.000Z","updated":"2018-09-01T12:53:16.000Z","comments":true,"path":"2017/08/27/mysql/2017-08-27-mysql-22/","link":"","permalink":"http://blog.renyimin.com/2017/08/27/mysql/2017-08-27-mysql-22/","excerpt":"","text":"锁 行锁 与 表锁 MyISAM只支持表锁, InnoDB可以支持行锁;MyISAM：执行读写SQL语句时，会对表加锁，所以数据量大，并发量高时，性能会急剧下降。InnoDB：细粒度行锁，在数据量大，并发量高时，性能比较优异 InnoDB的行锁是实现在索引上的，而不是锁在物理行记录上。潜台词是，如果访问没有命中索引，也无法使用行锁，将要退化为表锁。https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&amp;mid=2651961428&amp;idx=1&amp;sn=31a9eb967941d888fbd4bb2112e9602b&amp;chksm=bd2d0d888a5a849e7ebaa7756a8bc1b3d4e2f493f3a76383fc80f7e9ce7657e4ed2f6c01777d&amp;scene=21#wechat_redirect 锁与事务事务事务隔离级别与锁锁与索引索引系列锁与索引https://mp.weixin.qq.com/s/wGOxro3uShp2q5w97azx5A","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"07. Chain of Responsibility 责任链模式","slug":"oop/2017-08-18-OOP-07-Chain-of-Responsibility","date":"2017-08-18T13:13:56.000Z","updated":"2018-08-07T09:39:40.000Z","comments":true,"path":"2017/08/18/oop/2017-08-18-OOP-07-Chain-of-Responsibility/","link":"","permalink":"http://blog.renyimin.com/2017/08/18/oop/2017-08-18-OOP-07-Chain-of-Responsibility/","excerpt":"","text":"","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"06. Strategy 策略模式","slug":"oop/2017-08-15-06-Strategy","date":"2017-08-15T13:36:27.000Z","updated":"2018-08-03T02:16:28.000Z","comments":true,"path":"2017/08/15/oop/2017-08-15-06-Strategy/","link":"","permalink":"http://blog.renyimin.com/2017/08/15/oop/2017-08-15-06-Strategy/","excerpt":"","text":"场景举例 在软件开发中, 常常会遇到实现某个功能有多种途径, 每种途径对应着一种算法, 比如开发一个影院售票系统, 在该系统中需要为不同类型的用户提供不同的电影票打折方式, 具体打折方案如下: 学生凭学生证可享受票价8折优惠; 年龄在10周岁及以下的儿童可享受每张票减免10元的优惠(原始票价需大于等于20元); 影院VIP用户除享受票价半价优惠外还可进行积分, 积分累计到一定额度可换取电影院赠送的奖品; 在传统开发模式下, 为了满足上述需求, 你可能会设计一个类MovieTicket，其核心代码片段如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;?php/** * 电影票类 * Class MovieTicket */class MovieTicket&#123; private $doublePrice; private $StringType; public function setPrice($price) &#123; $this-&gt;price = $price; &#125; public function setType($type) &#123; $this-&gt;type = $type; &#125; public function getPrice() &#123; return $this-&gt;calculate(); &#125; /** * 计算打折后的票价 * @return string */ public function calculate() &#123; //学生票折后票价计算 if (&apos;student&apos; == $this-&gt;type) &#123; echo &apos;学生票:&apos;; return $this-&gt;price * 0.8; //儿童票折后票价计算 &#125; else if (&apos;children&apos; == $this-&gt;type &amp;&amp; $this-&gt;price &gt;= 20) &#123; echo &apos;儿童票:&apos;; return $this-&gt;price - 10; //VIP票折后票价计算 &#125; else if (&apos;vip&apos; == $this-&gt;type) &#123; echo &apos;VIP票:&apos;; echo &apos;增加积分!&apos;; return $this-&gt;price * 0.5; //不满足任何打折要求, 则返回原始票价 &#125; else &#123; return $this-&gt;price; &#125; &#125;&#125;class Client&#123; public function test() &#123; $movieTicket = new MovieTicket(); $originalPrice = 60.0; $currentPrice = 60.0; $movieTicket-&gt;setPrice($originalPrice); var_dump(&apos;原始价为:&apos; . $originalPrice); var_dump(&quot;---------------------------------&quot;); // 可以根据session中的用户类型来进行设置 $movieTicket-&gt;setType(&apos;student&apos;); //学生票 $currentPrice = $movieTicket-&gt;getPrice(); var_dump(&apos;折后价为:&apos; . $currentPrice); var_dump(&quot;---------------------------------&quot;); $movieTicket-&gt;setType(&apos;children&apos;); //儿童票 $currentPrice = $movieTicket-&gt;getPrice(); var_dump(&apos;折后价为:&apos; . $currentPrice); &#125;&#125;$client = new Client();$client-&gt;test(); 虽然该方案可以满足电影票打折问题, 该并不是一个好的解决方案, 它至少存在如下三个问题: MovieTicket类的 calculate() 方法非常庞大, 它包含各种打折算法的实现代码, 在代码中出现了较长的 if…else… 语句, 不利于测试和维护; 增加新的打折算法或者对原有打折算法进行修改时必须修改 MovieTicket 类的源代码, 违反了”开闭原则”, 系统的灵活性和可扩展性较差; 算法的复用性差, 如果在另一个系统(如商场销售管理系统)中需要重用某些打折算法, 只能通过对源代码进行复制粘贴来重用, 无法单独重用其中的某个或某些算法(重用较为麻烦); 策略模式的引入 通过分析, 导致上面问题的主要原因在于 MovieTicket 类职责过重, 它将各种打折算法都定义在一个类中, 这既不便于算法的重用, 也不便于算法的扩展; 因此你需要对 MovieTicket 类进行重构, 将原本庞大的 MovieTicket 类的职责进行分解, 将算法的定义和使用分离, 这就是策略模式所要解决的问题; 策略模式 策略模式(Strategy): 它定义了算法家族, 将每一个算法分别封装起来(每一个封装算法的类我们都可以称之为一种策略), 并让它们之间可以互相替换, 此模式让算法的变化独立于使用算法的客户; 也称为政策模式(Policy); 策略模式是一种对象行为模式; 策略模式的主要目的是将算法的行为和环境分开, 将算法的定义放在专门的策略类中, 每一个策略类封装了一种实现算法, 使用算法的环境类针对抽象策略类进行编程, 符合依赖倒转原则, 在出现新的算法时, 只需要增加一个新的实现了抽象策略类的具体策略类即可; 打折算法 为了实现之前打折算法的复用, 并能够灵活地向系统中增加新的打折方式, 可以使用策略模式对电影院打折方案进行重构, 重构后基本结构如下: 在向 MovieTicket 中注入具体策略对象时, 为了遵守 开闭原则, 可以根据用户session中的身份, 来决定使用的具体策略类, 比如vip用户使用VipDiscount类; 总结主要体现了对抽象编程的应用 主要优点 策略模式提供了对 开闭原则 的完美支持, 用户可以在不修改原有系统的基础上选择算法或行为, 也可以灵活地增加新的算法或行为; 提供了一种算法的复用机制, 由于将算法单独提取出来封装在策略类中, 因此不同的环境类可以方便地复用这些策略类; 主要缺点 客户端必须知道所有的策略类, 并自行决定使用哪一个策略类, 以便选择恰当的算法; 策略模式将造成系统产生很多具体策略类, 任何细小的变化都将导致系统要增加一个新的具体策略类; 无法同时在客户端使用多个策略类, 客户端每次只能使用一个策略类, 不支持使用一个策略类完成部分功能后再使用另一个策略类来完成剩余功能的情况; 适用场景 一个系统需要动态地在几种算法中选择一种, 那么可以将这些算法封装到一个个具体算法类中, 而这些具体算法类都是一个抽象算法类的子类; 不希望客户端知道复杂的与算法相关的数据结构, 在具体策略类中封装算法与相关的数据结构, 可以提高算法的保密性与安全性;","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"05. Factory Method 工厂方法模式","slug":"oop/2017-08-09-OOP-05-Factory","date":"2017-08-09T11:36:28.000Z","updated":"2018-08-01T02:23:15.000Z","comments":true,"path":"2017/08/09/oop/2017-08-09-OOP-05-Factory/","link":"","permalink":"http://blog.renyimin.com/2017/08/09/oop/2017-08-09-OOP-05-Factory/","excerpt":"","text":"前言 工厂模式是最常用的一类创建型设计模式, 通常所说的工厂模式是指 工厂方法模式, 它也是使用频率最高的工厂模式; 之前已经了解了简单工厂模式, 它只有一个工厂类, 在类中承担了各种Chart图形类的创建, 并且如果有其他系列类的创建, 那么工厂类就显得很臃肿; 正如之前学习的简单工厂模式, 存在问题: 工厂类过于庞大, 可能会包含了大量的if…else…代码, 导致维护和测试难度增大; 系统扩展不灵活，如果增加新类型的对象创建, 可能就需要修改静态工厂方法的业务逻辑, 违反了 开闭原则 ; 如何解决这两个问题, 这就是本文所介绍的 工厂方法模式 的动机之一; 工厂方法模式 在简单工厂模式中只提供一个工厂类, 该工厂类处于对产品类进行实例化的中心位置, 它需要知道每一个产品对象的创建细节, 并决定何时实例化哪一个产品类; 简单工厂模式最大的缺点是: 当有新类型的产品要加入到系统中时, 可能就需要修改工厂类, 需要在其中加入必要的业务逻辑, 这违背了 开闭原则; 此外, 在简单工厂模式中, 所有的产品都由同一个工厂创建, 工厂类职责较重, 业务逻辑较为复杂, 具体产品与工厂类之间的耦合度高, 严重影响了系统的灵活性和扩展性, 而工厂方法模式则可以很好地解决这一问题; 在工厂方法模式中, 不再提供一个统一的工厂类来创建所有的产品对象, 而是提供一个抽象工厂接口来声明一个 抽象的工厂, 而由其子类 具体工厂 来实现工厂方法, 你需要针对不同的产品提供不同的工厂; 在工厂方法模式结构图中包含如下几个角色: Factory(抽象工厂): 在抽象工厂类中, 声明了工厂方法(Factory Method), 用于返回一个产品, 抽象工厂是工厂方法模式的核心, 所有工厂类都必须实现该接口; ConcreteFactory(具体工厂): 它是抽象工厂类的子类, 实现了抽象工厂中定义的工厂方法, 并可由客户端调用, 返回一个具体产品类的实例; Product(抽象产品): 它是定义产品的接口; ConcreteProduct(具体产品): 它实现了抽象产品接口，某种类型的具体产品由专门的具体工厂创建, 具体工厂和具体产品之间一一对应; 代码 与简单工厂模式相比, 工厂方法模式最重要的区别是引入了抽象工厂角色, 抽象工厂可以是接口,也可以是抽象类, 其典型代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637&lt;?php$chartConfig = ['chartType' =&gt; 'PieChart'];abstract class Chart&#123; public abstract function display();&#125;class PieChart extends Chart&#123; public function display() &#123; var_dump('pieChart图'); &#125;&#125;abstract class Factory &#123; public static function factoryMethod($product) &#123;&#125;&#125;class ChartFactory extends Factory&#123; public static function factoryMethod($chart) &#123; return new $chart(); &#125;&#125;class Client&#123; public function chartTest($chartType) &#123; ChartFactory::factoryMethod($chartType)-&gt;display(); &#125;&#125;$client = new Client();$client-&gt;chartTest($chartConfig['chartType']); 在抽象工厂中声明了工厂方法, 具体产品对象的创建由其子类负责, 客户端针对抽象工厂编程, 可在运行时再指定具体工厂类, 具体工厂类实现了工厂方法, 不同的具体工厂可以创建不同的具体产品; 在客户端代码中, 只需关心抽象工厂类即可, 不同的具体工厂可以创建不同的产品; 可以通过配置文件来存储具体工厂类ConcreteFactory的类名, 更换新的具体工厂时无须修改源代码, 系统扩展更为方便; 小结工厂方法模式是简单工厂模式的延伸, 它继承了简单工厂模式的优点, 同时还弥补了简单工厂模式的不足;工厂方法模式是使用频率最高的设计模式之一, 是很多开源框架和API类库的核心模式;","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"04. Simple Factory 简单工厂模式 (创建较少的对象)","slug":"oop/2017-08-09-OOP-04-Factory","date":"2017-08-09T07:08:11.000Z","updated":"2018-08-01T01:58:48.000Z","comments":true,"path":"2017/08/09/oop/2017-08-09-OOP-04-Factory/","link":"","permalink":"http://blog.renyimin.com/2017/08/09/oop/2017-08-09-OOP-04-Factory/","excerpt":"","text":"场景举例 假设公司开发的CRM系统可以显示饼状图、柱状图等效果, 原始设计方案如下: 123456789101112131415161718192021222324class Client&#123; public $chartObject = null; public function __construct($type) &#123; switch ($chartType) &#123; case 'pie' : $this-&gt;chartObject = new PieChart(); break; case 'bar' : $this-&gt;chartObject = new BarChart(); break; default: //TODO break; &#125; &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125;&#125; 客户端代码通过调用 Client类 的构造函数来创建图表对象, 根据参数 type 可以得到不同类型的图表，然后再调用show()方法来显示相应的图表; 传统设计存在问题: 不难看出，Client类是一个巨大的类 Client类中包含很多 if…else… / switch…case… 代码块, 整个类的代码相当冗长; Client类的职责过重, 它将各种图表对象的创建和使用集中在一个类中实现, 违反了单一职责原则, 不利于类的重用和维护; 当需要增加新类型的图表时，必须修改Client类的源代码，违反了开闭原则; 客户端只能通过 new 关键字来直接创建图像对象, 图像类与客户端Client类耦合度较高 (比如一旦类的名字或参数发生变更, 你也必须修改Client代码的源代码), 也会违反开闭原则; 初步改进 为了不让Client类看起来过于冗长, 可以通过配置来决定使用哪个图形类: 123456789101112131415class Client &#123; public $chartObject = null; public function __construct($type) &#123; // 一行搞定 Config::get(&apos;chartType&apos;) 可以为 pie, bar $this-&gt;chartObject = new Config::get(&apos;chartType&apos;) . Chart(); &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125; &#125; 经过上面改进, 不仅可以简化代码, 而且可以保证扩展性, 当有新的图形类时, 只用增加配置和新图形类的名字对应好即可, 也不用去修改 Client 类; 已经基本解决了之前设计方案的诸多问题! 仍然存在的问题是: Client类的职责过重, 它将各种图表对象的创建和使用集中在一个类中实现, 违反了单一职责原则; 如果Client类不仅展示图表, 还做日志记录, 那么日志对象的创建也是在Client中创建并使用, 久而久之, 你的Client类中可能会创建大量的对象, 并且会依赖冗长的配置关系来在某类对象中选择所需的对象; 简单工厂模式 为了将图像对象的创建和使用分离, 使用简单工厂模式对图表库进行重构, 重构后的结构如下图所示： Chart接口充当抽象产品类, 其子类 PieChart 和 BarChart 充当具体产品类, Factory充当工厂类; 如果只是在Factory类中依然使用如下代码的话, 其实还是避免不了增加新对象对Factory类造成的修改, 虽然Factory的出现将类的创建和使用分离开来, 但是仍然会违反开闭原则; 1234567891011switch ($chartType) &#123; case &apos;pie&apos; : $this-&gt;chartObject = new PieChart(); break; case &apos;bar&apos; : $this-&gt;chartObject = new BarChart(); break; default: //TODO break;&#125; 所以这里的工厂模式其实是结合了之前的初步改进中的方法! 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?php$config = [&apos;chartType&apos; =&gt; &apos;PieChart&apos;];abstract class Chart&#123; public abstract function display();&#125;class PieChart extends Chart&#123; public function display() &#123; var_dump(&apos;pieChart图&apos;); &#125;&#125;class BarChart extends Chart&#123; public function display() &#123; var_dump(&apos;barChart图&apos;); &#125;&#125;class SimpleFactory&#123; public static function createChartObj($chart) &#123; return new $chart; &#125;&#125;class Client&#123; public function display($chartType) &#123; $chart = SimpleFactory::createChartObj($chartType); $chart-&gt;display(); &#125;&#125;$client = new Client();$client-&gt;display($config[&apos;chartType&apos;]); 问题虽然工厂类使得Chart类的创建和使用分离开了, 但仍然存在的问题是: Factory类可能是过于繁重的, Client所有想创建的对象都想使用Factory进行创建, 这样 要么你需要维护较繁重的类配置映射关系来在多种类型的对象中, 选择你需要的对象; 要么就得在Factory类中通过繁琐的 if…else 来区分不同种类的对象; 小结 简单工厂模式提供了 专门的工厂类 用于创建对象, 将对象的创建和对象的使用分离开, 它作为一种最简单的工厂模式在软件开发中得到了较为广泛的应用; 主要优点 工厂类包含必要的判断逻辑(当然也可以使用配置的方法来避免违反开闭原则), 可以决定在什么时候创建哪一个产品类的实例, 客户端可以免除直接创建产品对象的职责, 而仅仅消费产品, 简单工厂模式实现了对象创建和使用的分离; 客户端无须知道所创建的具体产品类的类名，只需要知道创建具体产品类所需要对应的参数即可, 对于一些复杂的类名, 通过简单工厂模式可以在一定程度减少使用者的记忆量; 主要缺点 由于工厂类集中了所有产品的创建逻辑, 职责过重, 一旦不能正常工作, 整个系统都要受到影响; 使用简单工厂模式, 势必会增加系统中类的个数(引入了新的工厂类), 增加了系统的复杂度和理解难度; 系统扩展困难，一旦添加新产品就不得不修改工厂逻辑, 在产品类型较多时, 有可能造成工厂逻辑过于复杂, 不利于系统的扩展和维护, (此处还是违反开放-封闭原则);不过这一点可以使用上面使用过的, 采用配置的方式来避免 简单工厂模式由于使用了静态工厂方法, 造成工厂角色无法形成基于继承的等级结构; 适用场景, 在以下情况下可以考虑使用 简单工厂模式: 工厂类负责创建的对象比较少, 由于创建的对象较少, 不会造成工厂方法中的业务逻辑太过复杂; 客户端只知道传入工厂类的参数, 对于如何创建对象并不关心;","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"0. S.O.L.I.D 五大基本原则","slug":"oop/2017-08-06-00-SOLID","date":"2017-08-06T13:36:27.000Z","updated":"2018-07-29T04:46:23.000Z","comments":true,"path":"2017/08/06/oop/2017-08-06-00-SOLID/","link":"","permalink":"http://blog.renyimin.com/2017/08/06/oop/2017-08-06-00-SOLID/","excerpt":"","text":"单一职责原则 SRP: The Single Responsibility Principle 单一职责原则是最简单的面向对象设计原则, 用于控制类的粒度大小; 此原则的核心就是 解耦 和 增强内聚性; 单一职责原则定义为: 一个类或者模块应该有且只有一个被改变的原因; 如果一个类承担的职责过多(耦合度就越大), 它被复用的可能性就越小; 一个职责的变化可能会影响其他的职责, 这种耦合会导致脆弱的设计, 当发生变化时, 设计会遭受到意想不到的破坏(因此要将这些职责进行分离, 将不同的职责封装在不同的类中); 开放封闭原则 OCP: The Open/Closed Principle 开放-封闭原则: 一个软件实体应当对扩展开放, 对修改关闭(即软件实体应尽量在不修改原有代码的情况下进行扩展); 如果一个软件系统设计符合开闭原则, 则可以非常方便地对系统进行扩展, 而且在扩展时无须修改现有代码。 随着软件规模越来越大, 运行时间越来越长, 软件维护成本也越来越高, 设计满足开闭原则的软件系统也变得越来越重要。 此原则的核心是 对抽象编程, 而不对具体编程; 为了满足开闭原则, 需要对系统进行抽象化设计, 抽象化 是开闭原则的关键; 例子, 假设系统可以显示各种类型的图表, 如 饼状图 和 柱状图 等, 为了支持多种图表显示方式 原始设计方案如下 代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?phpclass PieChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public $chartObject = null; public function __construct() &#123; //TODO &#125; public function display($chartType) &#123; switch ($chartType) &#123; case 'pie' : $piechart = new PieChart(); $piechart-&gt;display(); break; case 'bar' : $barchart = new BarChart(); $barchart-&gt;display(); break; default: //TODO break; &#125; &#125;&#125; 问题: 现在如果需要增加一个折线图 LineChart, 你就要需要修改 ChartDisplay 类的 display() 方法的源代码, 增加新的判断逻辑, 这就违反了开闭原则; 现对该系统进行重构, 使之符合开闭原则: 引入抽象图表类 AbstractChart, 并且让ChartDisplay针对抽象图表类进行编程(依赖抽象), 再在 ChartDisplay 的 display() 方法中调用具体 chart对象的display()方法显示图表 接下来, 只需要将LineChart作为AbstractChart的子类, 在客户端向ChartDisplay中注入一个LineChart对象即可, 无须修改现有类库的源代码 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?phpabstract class AbstractChart&#123; protected function display() &#123; &#125;&#125;class PieChart extends AbstractChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart extends AbstractChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public function __construct() &#123; //TODO &#125; public function display(AbstractChart $chart) &#123; $chart-&gt;display(); &#125;&#125;$cd = new ChartDisplay();$cd-&gt;display(new PieChart());$cd-&gt;display(new BarChart()); 里氏替换原则 LSP: The Liskov Substitution Principle 所有引用基类的地方, 必须能透明地使用其子类对象: 子类可以实现父类的抽象方法, 但是不能覆盖父类的非抽象方法, 也就是子类可以扩展父类的功能, 但是不能改变父类原有的功能; 主要就是说, 如果依赖的类将来有可能被扩展, 你最好设计一个抽象父类或接口, 子类继承、实现父类; 所以, 对比之前的开闭原则, 可以发现, 里氏代换原则是实现开闭原则的重要方式之一, 由于使用基类对象的地方都可以使用子类对象, 因此在程序中尽量使用基类类型来对对象进行定义, 而在运行时再确定其子类类型, 用子类对象来替换父类对象。 接口分离原则 ISP: The Interface Segregation Principle 该原则比较好理解: 不要定义过于臃肿的接口, 接口中不要有很多不相关的逻辑方法(否则一定也违背单一职责原则); 过于臃肿的接口可能会强迫用户去实现接口内部用户并不需要的方法, 换句话说, 使用 多个专门的接口 比使用 一个臃肿的总接口 要好很多; 如果你在类中实现的接口中有你不需要使用方法, 估计也是重写为空方法, 这其实已经违背了 接口分离原则 也就是说, 一个接口或者类应该拥有尽可能少的行为, 就是少到恰好能完成它自身的职责, 这也是保证软件系统模块的粒度尽可能少, 以达到高度可重用的目的; 依赖反转原则 DIP: The Dependency Inversion Principle 要针对接口编程, 而不是针对实现编程 如果说 开闭原则是面向对象设计的目标 的话, 那么依赖倒转原则就是面向对象设计的主要实现机制之一, 它是系统抽象化的具体实现; 上层不用去定义自己要依赖哪个具体的类, 而是定义自己依赖哪个 抽象; 然后让底层代码根据上层的要求, 去实现相应的 抽象; 这样就变成了底层对上层的依赖, 底层代码需要去 实现 上层代码定义的抽象; 在实现依赖倒转原则时, 我们需要针对抽象层编程, 将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象发生依赖关系时，通过抽象来注入所依赖的对象; 常用的注入方式有三种，分别是：构造注入，设值注入(Setter注入) 和 接口注入。 构造注入是指通过构造函数来传入具体类的对象 设值注入是指通过Setter方法来传入具体类的对象 而接口注入是指通过在接口中声明的业务方法来传入具体类的对象 小结在大多数情况下, 开闭原则、里氏代换原则 和 依赖倒转原则 这三个设计原则会同时出现: 开闭原则是目标, 里氏代换原则是基础, 依赖倒转原则是手段, 它们相辅相成, 相互补充, 目标一致, 只是分析问题时所站角度不同而已;","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"大O表示法","slug":"data-structure/2017-08-05-Big-O-notation","date":"2017-08-05T06:12:37.000Z","updated":"2018-08-08T10:00:29.000Z","comments":true,"path":"2017/08/05/data-structure/2017-08-05-Big-O-notation/","link":"","permalink":"http://blog.renyimin.com/2017/08/05/data-structure/2017-08-05-Big-O-notation/","excerpt":"","text":"前言计算机科学中, 大O表示法被用来描述一个算法的性能或复杂度; 大O表示法可以用来描述一个算法的最差情况, 或者一个算法执行的耗时或占用空间(例如内存或磁盘占用);相信许多人读过《Programming Pearls》(《编程珠玑》)或者其他计算机科学书籍时, 在看到大O符号或者其他奇怪的语法符号时都会感觉到自己遇到了一堵无法翻越的高墙。那这篇文章将会带领大家对大O表示法以及对数级算法有一个最简单的认识;首先作为一个程序员, 其次作为一个数学家, 我(Rob Bell)发现透彻的理解大O表示法的最好方式就是来尝试一些代码示例。下面按照算法的增长级别, 展示了一些常见的算法描述, 同时针对每个算法给出了一个简单的示例。 O(1) O(1)表示该算法的执行时间(或执行时占用空间)总是为一个常量, 不论输入的数据集是大是小 O(N) O(N)表示一个算法的性能会随着输入数据的大小变化而线性变化; 下面的例子表明了大O表示法其实是用来描述一个算法的最差情况的: 在for循环中, 一旦程序找到了输入数据中与第二个传入的string匹配时, 程序就会提前退出, 然而大O表示法却总是假定程序会运行到最差情况(在这个例子中, 意味着大O会表示程序全部循环完成时的性能) 1 O(N^2) O(N^2)表示一个算法的性能将会随着输入数据的增长而呈现出二次增长; 最常见的算法就是对输入数据进行嵌套循环, 如果嵌套层级不断深入的话, 算法的性能将会变为O(N^3), O(N^4), 以此类推; 例子: 1 O(2^N) O(2^N)表示一个算法的性能将会随着输入数据的每次增加而增大两倍; O(2^N)的增长曲线是一条爆炸式增长曲线——开始时较为平滑, 但数据增长后曲线增长非常陡峭; 一个典型的O(2^N)方法就是裴波那契数列的递归计算实现 12 对数 转载自CSDN https://github.com/PuShaoWei/arithmetic-php 大O表示法: 大O表示法初学者指南","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://blog.renyimin.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://blog.renyimin.com/tags/algorithm/"}]},{"title":"12. MySQL5.7.23 编译安装?","slug":"mysql/2017-07-21-mysql-12","date":"2017-07-21T12:43:26.000Z","updated":"2018-08-31T10:23:26.000Z","comments":true,"path":"2017/07/21/mysql/2017-07-21-mysql-12/","link":"","permalink":"http://blog.renyimin.com/2017/07/21/mysql/2017-07-21-mysql-12/","excerpt":"","text":"本地下载mysql5.7.23, vagrant挂载目录到虚拟机内部 /test, 将Tarball包移动到 /usr/local/src 目录下, 并 tar -zxvf mysql-5.7.23.tar.gz 解压; mysql源码编译所需的依赖安装, 可以参考 https://dev.mysql.com/doc/refman/5.7/en/source-installation.html https://dev.mysql.com/doc/refman/5.7/en/binary-installation.html#binary-installation-layout: 依赖安装 1yum -y install gcc gcc-c++ ncurses ncurses-devel bison libgcrypt perl make cmake mysql5.7系列还必须要求安装boost_1_59_0(必须是该版本) 本地下载boost_1_59_0, 并且将Tarball包移动到 /usr/local/src 目录下, 并 tar -zxvf mysql-5.7.23.tar.gz 解压; 直接移动解压目录到 /usr/localboost : cp -r boost_1_59_0 /usr/local/boost (不用编译安装) 开始编译安装 123cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/usr/local/mysql/data -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DMYSQL_TCP_PORT=3306 -DMYSQL_USER=mysql -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_BLACKHOLE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DENABLE_DOWNLOADS=1 -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/boostmake // 花费时间可能会比较长make install 已经在/usr/local/生成mysql目录 编译安装完成后, 需要创建运行mysql程序的相关账户及目录相关权限 123groupadd mysqluseradd -r -g mysql -s /bin/false mysqlchown -R mysql:mysql /usr/local/mysql/ 接下来需要设置mysql的配置文件my.cnf 注意: mysql5.7.18之后, 貌似已经不在解压包的support-files目录中提供my-default.cnf文件 参考在Unix和类Unix系统上，MySQL程序按照指定的顺序从下表中显示的文件中读取启动选项（首先列出的文件首先读取，后面读取的文件优先） 你会发现在centos7-minimal系统的/etc下就有my.cnf文件1234567891011121314151617181920212223242526cp my.cnf my.cnf.bakvi my.cnf // 对文件进行编辑[client]port = 3306default-character-set = utf8socket = /usr/local/mysql/mysql.sock[mysql]port = 3306default-character-set = utf8socket = /usr/local/mysql/mysql.sock[mysqld]user = mysqlbasedir = /usr/local/mysqldatadir=/usr/local/mysql/dataport = 3306character_set_server=utf8pid-file=/usr/local/mysql/mysqld.pidsocket=/usr/local/mysql/mysql.sockserver-id=1## include all files from the config directory#!includedir /etc/my.cnf.d 准备mysql用户权限等 12[root@lant src]# groupadd mysql[root@lant src]# useradd -r -g mysql -s /bin/false mysql 设置环境变量 123echo &apos;PATH=/usr/local/mysql/bin:/usr/local/mysql/lib:$PATH&apos; &gt;&gt; /etc/profile // 设置环境变量, 并开机运行echo &apos;export PATH&apos; &gt;&gt; /etc/profile //把PATH设为全局变量source /etc/profile 此时就可以全局使用/usr/local/mysql/bin下的命令 1234567891011121314[root@lant vagrant]# mysqld --helpmysqld Ver 5.7.23 for Linux on x86_64 (Source distribution)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Starts the MySQL database server.Usage: mysqld [OPTIONS]For more help options (several pages), use mysqld --verbose --help.[root@lant vagrant]# 初始化数据库 (会自动在/usr/local/mysql下生成data目录, 并且是mysql用户身份) 1234567cd /usr/local/mysql/bin/mysqld \\--initialize-insecure \\--user=mysql \\--basedir=/usr/local/mysql \\--datadir=/usr/local/mysql/data 将mysql添加到systemctl系统服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344cp /usr/local/src/mysql-5.7.23/scripts/systemd/mysqld.service.in /usr/lib/systemd/system/mysqld.servicevi mysqld.service[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/usr/local/mysql/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables // 这里没找到这个脚本, 就先注释掉了#ExecStartPre=/usr/local/mysql/bin/mysqld_pre_systemd# Start main serviceExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysql# Sets open_files_limitLimitNOFILE = 5000Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false 然后设置开机自动启动: 123456789systemctl daemon-reloadsystemctl start mysqldnetstat -anpt | grep 3306systemctl enable mysqld[root@lant mysql]# ps aux |grep mysqlmysql 2833 0.1 9.8 1768144 179708 ? Sl 02:40 0:00 /usr/local/mysql/bin/mysqld --daemonize --pid-file=/usr/local/mysql/mysqld.pidroot 2867 0.0 0.0 112704 972 pts/0 S+ 02:43 0:00 grep --color=auto mysql[root@lant mysql]# 默认没有密码, 所以直接设置新密码后即可登录 12mysqladmin -u root -p password &quot;renyimin&quot;;mysql -uroot -p 外部客户端连接? 最后更新时间 2018/08/12","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"","slug":"data-structure/2017-07-12","date":"2017-07-12T13:31:52.000Z","updated":"2018-08-27T12:45:51.000Z","comments":true,"path":"2017/07/12/data-structure/2017-07-12/","link":"","permalink":"http://blog.renyimin.com/2017/07/12/data-structure/2017-07-12/","excerpt":"","text":"大纲数据结构算法大O表示法时间复杂度空间复杂度常见的复杂度类型https://blog.csdn.net/mbh_1991/article/details/9774561O(1)O(N)O(N^2)O(2^N)O(logN) 参考:https://github.com/nonstriater/Learn-Algorithms","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://blog.renyimin.com/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://blog.renyimin.com/tags/algorithm/"}]},{"title":"13","slug":"nginx/2017-05-11-13","date":"2017-05-11T11:18:57.000Z","updated":"2018-08-22T03:18:42.000Z","comments":true,"path":"2017/05/11/nginx/2017-05-11-13/","link":"","permalink":"http://blog.renyimin.com/2017/05/11/nginx/2017-05-11-13/","excerpt":"","text":"","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"12. 反向代理, 负载均衡","slug":"nginx/2017-05-07-12","date":"2017-05-07T12:08:05.000Z","updated":"2018-08-22T02:26:40.000Z","comments":true,"path":"2017/05/07/nginx/2017-05-07-12/","link":"","permalink":"http://blog.renyimin.com/2017/05/07/nginx/2017-05-07-12/","excerpt":"","text":"反向代理 nginx反向代理的一些作用: 保护网站安全: 任何来自Internet的请求都必须先经过代理服务器; 通过配置缓存功能加速Web请求: 可以缓存真实Web服务器上的某些静态资源, 减轻真实Web服务器的负载压力; 实现负载均衡: 充当负载均衡服务器均衡地分发请求, 平衡集群中各个服务器的负载压力; 2. https://blog.csdn.net/kangshuo2471781030/article/details/79194973 负载均衡","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"11. HTTP 升级到 HTTPS","slug":"nginx/2017-05-02-11","date":"2017-05-02T13:06:27.000Z","updated":"2018-08-22T02:26:40.000Z","comments":true,"path":"2017/05/02/nginx/2017-05-02-11/","link":"","permalink":"http://blog.renyimin.com/2017/05/02/nginx/2017-05-02-11/","excerpt":"","text":"获取证书 升级到 HTTPS 协议的第一步, 就是要获得一张证书(一个二进制文件, 里面包含经过认证的网站公钥和一些元数据), 要从经销商购买; 有关 SSL 的介绍可以参阅维基百科的传输层安全协议和阮一峰先生的《SSL/TLS协议运行机制的概述》; SSL 证书主要有两个功能: 加密和身份证明; 通常需要购买, 也有免费的, 通过第三方 SSL 证书机构颁发, 常见可靠的第三方 SSL 证书颁发机构有: GeoTrust:是全球第二大数字证书颁发机构，已被Symantec收购。该品牌证书由工信部许可设立的电子认证服务机构“天威诚信”提供鉴证服务 GlobalSign: GMO GlobalSign是全球最早的数字证书认证机构之一，一直致力于网络安全认证及数字证书服务，是一个备受信赖的 CA 和 SSL 数字证书提供商 … SSL 证书按大类一般可分为 DV SSL 、OV SSL 、EV SSL 证书。 域名型SSL证书 ( DV SSL (Domain Validation)): 最低级别认证, 可以确认申请人拥有这个域名, 对于 DV SSL 证书, 能起到最基本的信息传输加密功能, 验证最基本的域名管理权, 适用个人, 小微企业等网站, 当天签发; 浏览器有https和小锁标示 企业型 SSL 证书 ( OV SSL (Company Validation)): 对于 OV SSL 证书(企业型SSL证书), 增加了验证企业身份的功能, 在证书内容中能显示中文或英文公司名称。适用企业官网、门户等网站, 1-2天签发; 浏览器有https和小锁标示是要购买者提交组织机构资料和单位授权信等在官方注册的凭证, 证书颁发机构在签发 SSL 证书前不仅仅要检验域名所有权, 还必须对这些资料的真实合法性进行多方查验, 只有通过验证的才能颁发 SSL 证书; 增强型 SSL 证书 (EV SSL (Extended Validation): 最高级别的认证, 对于 EV SSL 证书, 它是企业型证书的升级版, 在原有加密性及验证身份的基础上, 加强了防假冒网站功能。拥有独一无二的绿色地址栏及地址栏公司名称显示; 也因此具有能帮助网站提升形象以及增加网站体验度的功能, 在功能和效果上较之企业型更强大更多样化, 适用金融、电商等网站; 1-2天签发与其他 SSL 证书一样, 都是基于 SSL/TLS 安全协议, 但是验证流程更加具体详细, 验证步骤更多, 这样一来证书所绑定的网站就更加的可靠、可信;它跟普通 SSL 证书的区别也是明显的, 安全浏览器的地址栏变绿, 如果是不受信的 SSL 证书则拒绝显示, 如果是钓鱼网站, 地址栏则会变成红色, 以警示用户; 常见 CA 厂商对比: 国内提供SSL证书服务的云厂商列表: DV免费SSL证书 又拍云免费提供 Let’s Encrypt 和 Symantec 签发的两款 DV SSL 证书, 也是业内唯一一家提供两种免费证书的服务商; 免费DV SSL申请 然后点击”补全”, 对你想要设置https的域名进行配置, 注意不支持泛域名; 并且需要简单在又拍上随便绑定个cdn; 之后就处于待审核中; 参考: 又拍云博客 http://www.ruanyifeng.com/blog/2016/08/migrate-from-http-to-https.html https://aotu.io/notes/2016/08/16/nginx-https/index.html https://www.zhihu.com/question/19578422 https://blog.csdn.net/cloume/article/details/78252319 https://www.cnblogs.com/tianhei/p/7726505.html","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"08. try_files","slug":"nginx/2017-04-29-08","date":"2017-04-29T03:45:32.000Z","updated":"2018-08-22T08:26:41.000Z","comments":true,"path":"2017/04/29/nginx/2017-04-29-08/","link":"","permalink":"http://blog.renyimin.com/2017/04/29/nginx/2017-04-29-08/","excerpt":"","text":"","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"07. Rewrite ??","slug":"nginx/2017-04-28-07","date":"2017-04-28T12:05:59.000Z","updated":"2018-08-22T07:46:24.000Z","comments":true,"path":"2017/04/28/nginx/2017-04-28-07/","link":"","permalink":"http://blog.renyimin.com/2017/04/28/nginx/2017-04-28-07/","excerpt":"","text":"前言 Rewrite是Nginx服务器提供的一个重要的基本功能, 其在web服务器产品中几乎是必备的功能, 用于实现URL的重写 ; Nginx服务器的Rewrite功能的实现依赖于PCRE(Perl Compatible Regular Expressions, Perl兼容的正则表达式)的支持, 因此在编译安装Nginx服务器之前, 需要安装PCRE库 ; Nginx服务器使用ngx_http_rewrite_module模块解析和处理Rewrite功能的相关配置 , 该模块是Nginx服务器的标准HTTP模块; 在Nginx配置中, 有关Rewrite的配置指令不多, 但是已经能够提供比较完整的功能了! if 指令 有关Rewrite配置的if指令: 用来支持条件判断, 并根据条件判断的结果选择不同的Nginx配置, 可以在server块或者location块中配置该指令, 其语法结构为: if (condition) {....}; 注意: if关键字 和 () 之间要有空格; {} 代表一个作用域, 形成一个if配置块, 是条件为真时的Nginx配置; condition 为判断条件(true/false), 它可以支持以下几种设置方法: 变量名, 如下: 123if ($slow) &#123; # 注意if后面有空格...#Nginx配置&#125; 注意: 如果变量的值为 空字符串 或者 以”0”开头的任意字符串, if指令认为条件为false , 其他情况认为条件为true 使用 =(等号) 和 != (不等号)比较变量和字符串是否相等, 相等时if指令认为条件为true, 反之为false 123if ($request_method = POST) &#123; # 注意if后面有空格 return 405;&#125; 注意: 这里的字符串不要加引号 ; 使用正则表达式对变量进行匹配, 匹配成功时, if条件为true, 否则为false ;变量与正则表达式之间用 ~ , ~*, !~ 或 !~* 链接 ;~表示匹过程中对大小写敏感, ~* 表示匹配过程中对大小写不敏感;!~ 或 !~* 在匹配失败时, if指令认为条件为true, 否则为false ;在正则表达式中可以使用小括号对变量进行截取, 在花括号中使用 $1…$9 引用截取的值; 1234567if ($httpd_user_agent ~ MSIE) &#123; #$httpd_user_agent的值中是否含有MSIE字符串, 如果包含, 则为true ;&#125;if ( $http_cookie ~* &quot;id=([^;]+)(?:;|$)&quot; ) &#123; # Nginx配置, 可以使用$1和$2获取截取到的值, 如: # set $id $1 ;将截取到的id赋值给$id变量以备后用 ;&#125; 注意: 整个正则表达式字符串一般不需要加引号, 但如果含有右花括号}, 或者分号;字符时, 必须要给整个正则表达式添加引号 判断请求的文件是否存在使用 -f 和 !-f :当使用-f时, 如果请求文件存在, if指令认为条件为true, 如果请求文件不存在则为false ;使用!-f时, 如果请求的文件不存在, 但该文件所在的目录存在, if指令认为条件为true; 如果该文件和它所在的目录都不存在, 则为false; 如果仅请求的文件存在, 也为false; 123if (-f $request_filename) &#123; #判断请求的文件是否存在 ...&#125; 123if (!-f $request_filename) &#123; #判断请求的文件是否不存在 ...&#125; 判断请求的目录是否存在使用-d和!-d :当使用-d时, 如果请求的目录存在, if指令认为条件为true, 如果请求的目录不存在则为false ;使用!-d时, 如果请求的目录不存在, 但该目录所在的上级目录存在, if指令认为条件为true; 如果该目录和它的上级目录都不存在, 则为false; 如果请求的目录存在, 也为false;判断规则和-f和!-f类似, 使用方法参见-f和!-f的使用 ; 判断请求的目录或文件是否存在使用-e和!-e :当使用 -e 时, 如果请求的目录或文件存在, if指令认为条件为true, 否则为false ;使用 !-e 时, 如果请求的文件和该文件所在路径上的目录都不存在, 为true , 否则为false ;使用方法参见-f和!-f的使用 判断请求的文件是否可执行使用-x和!-x:当使用-x时, 如果请求的文件可执行, if指令认为条件为true, 否则为false ;使用!-x时, 如果请求的文件不可执行, 为true , 否则为false ;使用方法参见-f和!-f的使用 break 指令","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"06. location块的配置详解","slug":"nginx/2017-04-28-06","date":"2017-04-28T06:56:27.000Z","updated":"2018-08-22T02:27:10.000Z","comments":true,"path":"2017/04/28/nginx/2017-04-28-06/","link":"","permalink":"http://blog.renyimin.com/2017/04/28/nginx/2017-04-28-06/","excerpt":"","text":"简介 location 有”定位”的意思, 在虚拟主机的配置中, 是不可缺少的一部分, 可以根据用户访问某个虚拟主机时URI中的资源标志符来将本次访问定位到不同的处理方式上; 比如碰到 .php, 如何调用PHP解释器? 这时就需要location Location的语法结构为 : location [= | ~ | ~* | ^~ ] patt { ... } , 中括号[]中可以不写任何参数, 此时称为一般匹配, 也可以写参数; 其实Location的匹配可以划分为三种类型: location = patt {} // 精准匹配 location patt {} // 一般匹配 location ~ patt {} //正则匹配 ( location ~ patt {} )(如果patt包含正则表达式, 就必须使用~或者~) 那么location的这三种匹配类型在 nginx提供服务的时候是如何发挥作用的? 当一个URI访问到本虚拟主机时(即访问到某个Server块中时), 这个Server块中可能有多个Location块来决定这个URI应该如何进行访问, 接下来一一讨论; 精准匹配 这里只分析精准匹配, 所以所有的location都是设置的是精准匹配; 当用户url不包含uri部分时, 如果有精准匹配的 patt 为 / 系统首先会匹配到patt为/的精准匹配Location块 (如果没有匹配到, 则access_log会报错403 Forbidden, error_log日志也有详情) 重点: 然后会判断当前location的root选项指定的目录下, index选项指定的默认文件是否存在, 不存在则返回403;存在, 则给用户的URL加上index选项指定的默认文件名构建一个新的URI, 然后重新从头开始进行location匹配定位; 如果第二轮匹配不到location, 也是报404; 并且此时可以在错误日志中看到比较全的访问路径(目录/文件名); 如果匹配到了新的location最后需要判断在新匹配到的location中的root指定目录下有没有新URI中这个文件, 没有的话, 返回404，并且此时可以在错误日志中看到比较全的访问路径(目录/文件名); 测试: 用户访问 www.vhostnginx.com nginx配置如下, 第一轮匹配不到, 则会报错 304 Forbidden如果是访问 www.vhostnginx.com/index.html 则会报错 404, 因为第一次匹配到了, 第二次匹配 时又匹配到了, 但是文件不存在 123456789 server &#123; listen 80; server_name www.vhostnginx.com; root /html/test; # 该目录下并没有 index.html 文件 location =/index.html &#123; // 1. 先精准匹配到此处, url变为 www.vhostnginx.com/index.html, 结果 /html/jingzhun 下就没有index.html文件, 直接报错 403 Forbidden root /html/jingzhun; # 该目录下没有 index.html 文件 index index.html index.htm; &#125;&#125; 第一轮匹配到了, 第二轮没有匹配到 123456789101112server &#123; listen 80; server_name www.vhostnginx.com; root /html/test; location =/ &#123; // 1. 先精准匹配到此处, url变为 www.vhostnginx.com/index.html, 开始下一轮匹配; 而第二轮发现匹配不到任何location, 所以报错404 root /html/jingzhun; index index.html index.htm; &#125; location =/test.html &#123; index index.html; &#125;&#125; - 第一轮匹配到了, 第二轮也匹配到了, 但是没有文件 123456789101112 server &#123; listen 80; server_name www.vhostnginx.com; root /html/test; # 该目录下并没有 index.html 文件 location =/ &#123; // 1. 先精准匹配到此处, url变为 www.vhostnginx.com/index.html, 开始下一轮匹配 root /html/jingzhun; index index.html index.htm; &#125; location =/index.html &#123; // 2. 第二轮匹配, 匹配到了此location, 但是/html/test/下没有 index.html 文件, access_log日志报404, 错误日志中可以看到详情 open /html/test/index.html 失败, 通过路径可以看到已经匹配到了该location(使用了全局的root配置) index index.html; # 和这行配置没关系 &#125;&#125; 当用户的url包含uri时, 此时不会匹配 patt为/的location, 因此不存在上面的403问题, 而是正常的匹配, 匹配不到就是404 not found; 一般匹配 如果精准匹配匹配不到, 假设有个一般匹配为 /, 则匹配不到精准匹配的uri, 都会走一般匹配: 用户访问 www.vhostnginx.com/haha.html 12345678910111213141516server &#123; listen 80; server_name www.vhostnginx.com; root /html/test; location =/ &#123; root /html/jingzhun; index index.html index.htm; &#125; location =/test.html &#123; index index.html; &#125; location / &#123; # 前两个精准匹配都匹配不到, 则走location为/的一般匹配, 因为/html下无haha.html文件, 所以报错404, 错误日志也可以看到 open() /html/haha.html 失败, 通过路径可以看到已经匹配到了该location root /html; # 无haha.html文件 index index.html; &#125; &#125; nginx一开始的配置文件中就会有这种类型的配置, 感觉像是兜底的 如果精准匹配匹配不到, 并且一般匹配不为/, 则: 如果uri也匹配不到一般匹配, 仍然是报错 404 not found; 正则匹配 除了 精准匹配 和 一般匹配, 如果 正则匹配 也同时存在的时候: 先会匹配精准匹配 ; 然后会先匹配一般匹配, 多个一般匹配中, 会把匹配度最高的先记录下来 ; 然后再去匹配正则匹配, 如果正则匹配中有可以匹配成功的, 则使用之, 并且停止继续往下匹配(即使下面有匹配度更高的正则表达式, 也不会往下继续匹配了);如果正则匹配匹配不到, 则使用之前记录下来的匹配度最高的那个”一般匹配” ; 常见正则匹配符号: ~ : 区分大小写(大小写敏感)匹配成功 ~* : 不区分大小写匹配成功用户访问 www.vhostnginx.com/HAHA.html 1234567891011121314151617181920server &#123; listen 80; server_name www.vhostnginx.com; root /html/test; location =/ &#123; // 精准匹配, 匹配不到 root /html/jingzhun; index index.html; &#125; location =/test.html &#123; // 精准匹配, 匹配不到 index index.html; &#125; location / &#123; // 一般匹配, 匹配到了 root /html; # 该目录下没有haha.html index index.html; &#125; location ~* /haha &#123; // 正则匹配, 由于不区分大小写, 所以也匹配到了, 并且匹配的程度比一般匹配高, 所以会走这个location root /html/haha; # 该目录下有haha.html, 但是没有HAHA.HTML, 所以access_log中报404, 但是error_log中报错为: open() /html/haha/HAHA.html 失败, 证明不区分大小写, 已经找到正确的location了; &#125; // 注意, 如果这里是 ~, 而不是 ~* 的话, 则正则匹配会匹配失败, 那么会走一般匹配, 由于没有haha.html，所以也会报404, 但是error_log中的错误信息为: open() /html/HAHA.html 失败, 证明确实是走到了一般匹配的location块;&#125; !~ : 区分大小写匹配失败 !~* : 不区分大小写匹配失败 注意: 精准匹配和一般匹配和配置块的顺序无关, 系统会走完所有配置块; 而正则匹配和顺序有关, 有可能走了一般发现匹配到了, 就不往下走了! 测试(下面使用较早之前的笔记图):","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"05. nginx 配置文件简析","slug":"nginx/2017-04-27-05","date":"2017-04-27T12:56:27.000Z","updated":"2018-08-06T09:06:44.000Z","comments":true,"path":"2017/04/27/nginx/2017-04-27-05/","link":"","permalink":"http://blog.renyimin.com/2017/04/27/nginx/2017-04-27-05/","excerpt":"","text":"nginx 服务器配置文件都存放在安装目录conf中, 主配置文件名为 nginx.conf , 本篇介绍其内容和基本配置方法; nginx.conf 整体结构 nginx.conf主要由三部分组成: 全局块, events块, http块; 在 http块 中又包含 http全局块 和 多个 server块; 每个 server块 中, 又包含 server全局块 和 多个 location块; 总共5块区域可以进行配置, 总体结构如下图: 全局块全局块主要设置一些影响nginx整体运行的配置指令, 通常包括 nginx服务器的用户(组) 允许生成的 worker process 数 nginx进程PID存放路径 错误日志存放路径和级别 配置文件引入 ….nginx服务器的用户(组) 只能在全局块中配置 其所用指令为 user, 配置语法为 user user [group]; 只有被设置的用户或者用户组成员才有权限启动 Nginx 进程 如果是其他用户尝试启动, 将会报错; 如果希望所有用户都可以启动nginx:一种方法是将该行注释掉;另一种方法是 user nobody nobody(将用户或者用户组设置为nobody); worker process数配置 只能在全局块中配置 worker process 是nginx服务器实现并发处理服务的关键所在, 理论上来说值越大则可以支持的并发量就越高, 但实际上也受到操作系统本身资源和硬件设备(CPU和磁盘驱动)等的制约; 配置允许生成 worker process 数的指令是 worker_process, 语法格式为 worker_process number|auto number: 指定nginx进程最多可以产生的worker process数量 auto: nginx进程将自动检测 默认配置文件中, 设置的number为1, 所以启动nginx之, 会看到除了 master process 主进程之外, 还生成了1个 worker process;尝试将number改为3, 重启ngix之后, 会看到除了 master process 主进程之外, 还生成了3个 worker process; rrc的线上配置是 auto; nginx进程PID存放路径 只能在全局块中配置 nginx进程作为系统的守护进程运行时, 需要在某文件中保存当前运行进程的主进程号, nginx支持对它的存放路径进行自定义配置, 指令是 pid, 语法格式为 pid file file指定的是path及filename (默认的路径在nginx安装路径的logs目录下, 文件名为nginx.pid; 指定file时, 必须设置文件名, 如果只设置路径path, 则会报错)path可以是绝对路径, 也可以是以nginx安装目录为根目录的相对路径 比如要把nginx.pid放到nginx安装目录下的sbin目录下, 文件名为 nginx_web: pid sbin/nginx_web 错误日志存放路径和级别 可以在全局块, http全局块, server全局块, location块 中配置, 只是作用域不同; 配置错误日志使用的指令是 error_log, 语法格式是 error_log file|stderr [debug|info|notice|warn|rror|crit|alert|emerg] 可以看到, nginx服务器的日志支持输出到某一固定文件file, 或者输出到标准输出stderr; 日志的级别从左到右是从低到高, 设置某一级别后, 比这一级别高的日志都会被记录下来 nginx默认的存放和日志级别设置为 error_log logs/error error (注意: 路径如果不是绝对路径的话, 都是以nginx安装目录为根目录的相对路径) 注意: 指定的文件对于运行nginx进程的用户需要有写权限 配置文件引入 在一些情况下, 可能需要将其他的nginx配置或者第三方模块的配置引用到当前的主配置文件中;一般nginx服务有上配置了多个虚拟站点的话, 每个站点都是各自独立配置自己的配置文件, 然后引入到主配置文件中, 一般会写成 include conf.d/*.conf;; 语法结构 include file, file是要引入的配置文件, 支持相对路径 注意: 新引入进来的配置文件要求nginx进程用户对其有写权限; events块设置网络连接序列化 只能在events块 中配置 为了防止惊群效应, nginx包含了指令 accept_mutex, 当其设置为开启时, 将会对多个nginx进程接收连接进行序列化, 防止多个进程对连接的争抢, 语法格式为 accept_mutex on|off; 此指令默认为开启状态; 是否同时接收多个连接 只能在events块 中配置 nginx的每个worker process都有能力同时接收多个新到达的网络连接, 但是这需要在配置文件中进行设置, 指令为 multi_accept, 语法为 multi_accept on|off; 此指令默认为关闭状态, 即每个 worker process 一次只能接收一个新到达的网络连接; 事件驱动模型选择 只能在events块 中配置 nginx提供了多种事件驱动模型来处理网络消息, 可以使用 use 指令来强制nginx服务器选择哪种事件驱动模型进行消息处理, 语法格式为 use method; method的可选内容有: elect, poll, kqueue, epoll, rtsig, dev/poll, eventport, 关于事件驱动模型, 后面再细聊 一般使用 useepoll 配置最大连接数 只能在events块 中配置 指令 worker_connections 主要用来设置允许每一个 worker process 同时开启的最大连接数, 语法格式为 worker_connections number; 默认 number为512 注意: number不仅仅包括和前端用户建立的连接数, 而是包括所有可能的连接数; 另外, 最大连接数不能大于操作系统支持打开的最大文件句柄数量; http全局块自定义服务日志 log_format指令: 只能在 http全局块 中进行配置 access_log指令: 可以在 http全局块, server全局块, location块 进行配置 server全局块location块12345http &#123; include mime.types; default_type application/octet-stream; server_tokens off; # 不显示 nginx 版本","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"04. error.log 和 access.log","slug":"nginx/2017-04-24-04","date":"2017-04-27T08:36:25.000Z","updated":"2018-08-06T09:22:25.000Z","comments":true,"path":"2017/04/27/nginx/2017-04-24-04/","link":"","permalink":"http://blog.renyimin.com/2017/04/27/nginx/2017-04-24-04/","excerpt":"","text":"概述 nginx日志主要分为两种: 错误日志 和 自定义服务日志(也叫做访问日志); nginx.conf中, 虽然两个日志的相关配置默认都是注释的, 但并不意味着是关闭的, 除非显示地关闭; error.log 配置错误日志使用的指令是 error_log, 其用于配置nginx进程运行时的日志存放和级别; 在 全局块, http全局块, server全局块, location块 中都可以使用 error_log 指令对nginx的错误日志进行相关配置, 只是作用域不同; error_log 语法格式是 error_log file|stderr [debug|info|notice|warn|rror|crit|alert|emerg] nginx服务器的日志支持输出到某一固定文件file, 或者输出到标准输出stderr; 日志的级别从左到右是从低到高, 设置某一级别后, 比这一级别高的日志都会被记录下来; nginx默认的存放和日志级别设置为 error_log logs/error error (注意: 路径如果不是绝对路径的话, 都是以nginx安装目录为根目录的相对路径); 注意: 指定的文件对于运行nginx进程的用户需要有写权限; 关闭日志记录 (一般不做此操作) 需要注意的是: error_log off 并不能关闭错误日志, 而是会将错误日志记录到以nginx安装目录为根目录, 下的一个文件名为off的文件中 ; 正确的关闭 error.log 记录功能的方为 error_log /dev/null ; (表示将存储日志的路径设置为”垃圾桶”) nginx的错误日志主要记录客户端访问nginx出错时的日志, 格式不支持自定义; access.log 和常规的error.log日志不同, error.log主要记录nginx出错时的日志, 格式不支持自定义; 而access.log是记录nginx服务器提供服务过程应答前端请求的日志, 格式是支持自定义的, 因此也称为 自定义服务日志; nginx服务器支持对服务日志的 格式, 大小, 输出 等进行配置, 需要用到两个指令 access_log 和 log_format access_log指令 access_log 指令语法: access_log path [format[buffer=size]] path:配置服务日志文件的存放路径和文件名 format: 可选项, 自定义服务日志的格式字符串, 也可以通过 格式串的名称 使用log_format指令定义好的格式, 格式串的名称 在 log_format 指令中定义; size: 配置临时存放日志的内存缓冲区大小; 可以在 http全局块, server全局块, location块 进行配置 如果想关闭自定义服务日志的记录功能, 可以 access_log off; log_format指令 和 access_log 联合使用的另一个指令是 log_format 专门用来定义服务日志的格式 并且可以为格式字符串定义一个名称, 以便access_log指令可以直接调用 其语法格式为 log_format name string.... name: 为格式字符串定义的名字, 默认为 从 combined; string: 服务日志的格式字符串; (在定义过程中, 可以使用nginx配置预设的一些变量获取相关内容, 变量的名称使用双引号括起来, string整体使用单引号括起来) 在string中可以使用的变量: $request: 请求的URI和HTTP协议, 这是整个PV日志记录中最有用的信息, 记录服务器收到一个什么样的请求; $status: 记录请求返回的http状态码, 比如成功是200 ; $request_time: 整个请求的总时间, 指的就是从接受用户请求的第一个字节到发送完响应数据的时间, 即包括 接收请求数据时间、程序响应时间、输出响应数据时间; $upstream_response_time: 是指从Nginx向后端(php-cgi)建立连接开始到接受完数据然后关闭连接为止的时间;从上面的描述可以看出, $request_time 肯定大于等于 $upstream_response_time, 特别是使用POST方式传递参数时, 因为Nginx会把request body缓存住, 接收完毕后才会把数据一起发给后端;所以如果用户网络较差, 或者传递数据较大时, $request_time会比$upstream_response_time大很多;所以如果使用nginx的accesslog查看php程序中哪些接口比较慢的话, 记得在log_format中加入$upstream_response_time; $remote_addr: 客户端的IP地址; $http_user_agent: 客户端浏览器信息, 配置 ua=[$http_user_agent] 可能如下: 1ua=[Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1] rrc 示例: 123log_format main &apos;remote_addr=[$remote_addr] http_x_forward=[$http_x_forwarded_for] time=[$time_local] request=[$request] &apos; &apos;status=[$status] byte=[$bytes_sent] elapsed=[$request_time] refer=[$http_referer] body=[$request_body] &apos; &apos;ua=[$http_user_agent] cookie=[$http_cookie] gzip=[$gzip_ratio]&apos;; 只能在 http全局块 中进行配置 自定义服务日志切割logrotatehttp://blog.51cto.com/wn2100/2074048https://www.jianshu.com/p/514a9715de46 shell nginx日志落盘http://wiki.shanyishanmei.com/pages/viewpage.action?pageId=8989682","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"02. nginx编译安装 & https 服务支持","slug":"nginx/2017-04-21-02","date":"2017-04-21T14:26:11.000Z","updated":"2018-08-14T09:22:04.000Z","comments":true,"path":"2017/04/21/nginx/2017-04-21-02/","link":"","permalink":"http://blog.renyimin.com/2017/04/21/nginx/2017-04-21-02/","excerpt":"","text":"编译安装 nginx官网下载源码包到 /usr/local/src 目录下: 1234wget http://nginx.org/download/nginx-1.14.0.tar.gztar -zxvf nginx-1.14.0.tar.gzcd nginx-1.14.0// nginx 的README文件直接建议查看 http://nginx.org, 可以参考 http://nginx.org/en/docs/configure.html 参考官网文档, 可以看到nginx源码包其实默认指定的安装路径就是 /usr/local/nginx nginx编译安装需要提前准备的环境 由于nginx的一些模块需要依赖第三方库, 通常有pcre库(支持rewrite模块), zlib库(支持gzip模块) 和 openssl库(支持ssl (Secure Sockets Layer 安全套接层) 模块)等 :（可以使用yum安装即可） yum -y install gcc gcc-c++ automake pcre pcre-devel zlib zlib-devel open openssl-devel 继续 12./configure // 因为默认就是安装目录就是 /usr/local/nginx, 所以也可以不用指定 --prefix=/usr/local/nginx 参数make &amp;&amp; make install 到此就安装完毕, 可以 /usr/local/nginx/sbin/nginx 启动, 并正常访问! 整体安装过程非常简单; 设置环境变量 123echo &apos;PATH=/usr/local/nginx/sbin:$PATH&apos; &gt;&gt; /etc/profileecho &apos;export PATH&apos; &gt;&gt; /etc/profilesource /etc/profile 设置开机自启动 123456789101112131415161718192021// 如果是使用yum安装的nginx, 则会自动创建/lib/systemd/system/nginx.service文件// 由于此处是使用编译安装, 所以需要手动在系统服务目录里创建nginx.service文件vi /lib/systemd/system/nginx.service[Unit]Description=nginxAfter=network.target [Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true [Install]WantedBy=multi-user.target// 设置开机启动[root@lant system]# systemctl enable nginx.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. 测试 systemctl 命令 1234567891011121314151617// 运行如下命令, nginx被成功启动systemctl start nginx.service// 查看服务当前状态[root@lant system]# systemctl status nginx.service● nginx.service - nginx Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2018-08-12 04:41:25 UTC; 1min 21s ago Process: 2249 ExecStart=/usr/local/nginx/sbin/nginx (code=exited, status=0/SUCCESS) Main PID: 2250 (nginx) CGroup: /system.slice/nginx.service ├─2250 nginx: master process /usr/local/nginx/sbin/nginx └─2251 nginx: worker processAug 12 04:41:25 lant systemd[1]: Starting nginx...Aug 12 04:41:25 lant systemd[1]: Started nginx.// 重启服务systemctl restart nginx.service 最后重启虚拟机, nginx会自启动; 动态添加模块 Nginx在编译时, 我并没有去使用 --with- 参数去显示开启任何模块, 因为Nginx默认会开启一些常用模块, 比如 fastcgi模块、proxy模块、gzip模块、rewrite模块(需要安装pcre库)、upstream模块等 (可以参考《Nginx高性能Web服务器详解》P15-18) 查看nginx开启的模块 (nginx -V 貌似只能查看你在编译nginx时手动开启的模块) 不过需要注意的是, 让nginx支持HTTPS请求的ssl模块(需要安装openssl库)默认并没有被开启, 所以在编译nginx的时候, 应该加上参数 --with-http_ssl_module, 就像官方文档那样 12345678./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module // 后面两项, 因为之前已经通过yum进行安装了, 所以就不需要了 --with-pcre=../pcre-8.41 --with-zlib=../zlib-1.2.11 那现在问题来了, 如果遇到需要使用的模块在nginx编译时为手动开启, 此时nginx已经编译安装好了, 这就需要动态地来添加模块了? 进入解压好的nginx源码包 cd /usr/local/src/nginx-1.14.0 重新进行配置, 并编译 123./configure --with-http_ssl_modulemake// 注意千万不要执行make install 备份原有的已经安装好的nginx的启动文件 1cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak 停止nginx, 并使用刚刚编译好的nginx覆盖掉原来的nginx 123456[root@lant objs]# systemctl stop nginx.service[root@lant objs]# ps aux | grep nginxroot 10373 0.0 0.1 112704 972 pts/0 S+ 09:46 0:00 grep --color=auto nginx[root@lant objs]# cp /usr/local/src/nginx-1.14.0/objs/nginx /usr/local/nginx/sbin/cp: overwrite ‘/usr/local/nginx/sbin/nginx’? y[root@lant objs]# 然后运行nginx -V, 会发现模块已经添加, 最后启动nginx 123456789101112[root@lant objs]# nginx -Vnginx version: nginx/1.14.0built by gcc 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017TLS SNI support enabledconfigure arguments: --with-http_ssl_module[root@lant objs]# systemctl start nginx.service[root@lant objs]# ps aux | grep nginxroot 10383 0.0 0.2 45936 1120 ? Ss 09:48 0:00 nginx: master process /usr/local/nginx/sbin/nginxnobody 10384 0.0 0.3 46372 1892 ? S 09:48 0:00 nginx: worker processroot 10386 0.0 0.1 112704 968 pts/0 S+ 09:48 0:00 grep --color=auto nginx[root@lant objs]# 本地配置HTTPS访问 首先确保机器上安装了 openssl 和 openssl-devel 配置 HTTPS 要用到 私钥example.key文件 和 example.crt证书文件 使用 OpenSSL 生成 SSL Key 和 CSR 文件 申请证书文件的时候要用到 example.csr 文件 OpenSSL 命令可以生成 example.key文件 和 example.csr 证书文件 使用 OpenSSl命令可以生成 example.key 和 example.csr 文件: 1234567891011121314151617181920212223mkdir /usr/local/nginx/conf/sslopenssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /usr/local/nginx/conf/ssl/nginx.key -out /usr/local/nginx/conf/ssl/nginx.crt// 创建了有效期100年，加密强度为RSA2048的SSL密钥key和X509证书文件// 参数说明:req: 配置参数 -x509指定使用 X.509证书签名请求管理(certificate signing request (CSR)).&quot;X.509&quot; 是一个公钥代表that SSL and TLS adheres to for its key and certificate management.-nodes: 告诉OpenSSL生产证书时忽略密码环节.(因为我们需要Nginx自动读取这个文件, 而不是以用户交互的形式)-days 36500: 证书有效期, 100年-newkey rsa:2048: 同时产生一个新证书和一个新的SSL key(加密强度为RSA 2048)-keyout: SSL输出文件名-out: 证书生成文件名// 它会问一些问题Enter pass phrase for root.key: ← 输入前面创建的密码 Country Name (2 letter code) [AU]:CN ← 国家代号，中国输入CN State or Province Name (full name) [Some-State]:BeiJing ← 省的全名，拼音 Locality Name (eg, city) []:BeiJing ← 市的全名，拼音 Organization Name (eg, company) [Internet Widgits Pty Ltd]:MyCompany Corp. ← 公司英文名 Organizational Unit Name (eg, section) []: ← 可以不输入 Common Name (eg, YOUR name) []: ← 此时不输入 Email Address []:admin@mycompany.com ← 电子邮箱，可随意填Please enter the following ‘extra’ attributes to be sent with your certificate request A challenge password []: ← 可以不输入 An optional company name []: ← 可以不输入 基础配置 要开启 HTTPS 服务, 在配置文件server块中, 必须使用监听命令 listen 的 ssl 参数 和 定义服务器证书文件和私钥文件, 如下所示: 12345678910111213server &#123; #ssl参数 listen 443 ssl; server_name example.com; #证书文件 ssl_certificate example.com.crt; #私钥文件 ssl_certificate_key example.com.key; # 由于下面这两个命令的默认值已经好几次发生了改变，因此不建议显性定义，除非有需要额外定义的值， ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; #...&#125; 证书文件会作为公用实体发送到每台连接到服务器的客戶端, 私钥文件作为安全实体, 应该被存放在具有一定权限限制的目录文件, 并保证 Nginx 主进程有存取权限; 具体配置可参考如下 (为了防止在servername的最终可能访问默认的server, 因此将默认访问指定到另外一个端口加以区分): 12345678910111213server &#123; # listen 80; listen 443 ssl; ssl_certificate /usr/local/nginx/conf/ssl/nginx.crt; ssl_certificate_key /usr/local/nginx/conf/ssl/nginx.key; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_ciphers HIGH:!aNULL:!MD5; server_name www.vhostnginx.com; location / &#123; root html; index index.html index.htm; &#125;&#125; https://www.vhostnginx.com可以访问, 可以忽略浏览器的安全提示 要让http和https共存, 可以开启上面配置中的 listen 80; 即可 而且貌似是支持泛解析的, 也就是如果你在生成证书的时候配置的域名是 www.vhostnginx.com, 那么 blog.vhostnginx.com 也可以配置一个server块来使用https协议; 但是 blog.vhostnginx1.com 就不可以! 参考: https://aotu.io/notes/2016/08/16/nginx-https/index.html https://blog.csdn.net/w410589502/article/details/72833283 最后更新时间 2018/08/12","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]},{"title":"01.","slug":"nginx/2017-04-21-01","date":"2017-04-21T10:16:31.000Z","updated":"2018-08-13T02:21:23.000Z","comments":true,"path":"2017/04/21/nginx/2017-04-21-01/","link":"","permalink":"http://blog.renyimin.com/2017/04/21/nginx/2017-04-21-01/","excerpt":"","text":"安装系列配置文件系列日志文件系列upstream模块 大数据如何根据nginx日志统计信息","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.renyimin.com/tags/Nginx/"}]}]}