{"meta":{"title":"Lant's Blog","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Linux 负载相关基本命令","slug":"linux/2018-07-02-load","date":"2018-07-02T11:26:17.000Z","updated":"2018-07-19T12:39:51.000Z","comments":true,"path":"2018/07/02/linux/2018-07-02-load/","link":"","permalink":"http://blog.renyimin.com/2018/07/02/linux/2018-07-02-load/","excerpt":"","text":"uptime 命令 测试 12[work@trade-sandbox ~]$ uptime 16:47:10 up 91 days, 23:01, 10 users, load average: 1.55, 1.27, 1.45 说明: 123416:47:10 // 系统当前时间up 91 days, 23:01, // 主机已运行时间,时间越大,说明你的机器越稳定10 users //用户连接数, 是总连接数而不是用户数load average: 1.55, 1.27, 1.45 // 系统平均负载,统计最近1,5,15分钟的系统平均负载 w命令 w 命令用于显示已经登陆系统的用户列表, 并显示用户正在执行的指令; 执行这个命令可得知目前登入系统的用户有哪些人, 以及他们正在执行的程序; 单独执行w命令会显示所有的用户, 您也可指定用户名称, 仅显示某位用户的相关信息; 选项 12345-h：不打印头信息；-u：当显示当前进程和cpu时间时忽略用户名；-s：使用短输出格式；-f：显示用户从哪登录；-V：显示版本信息。 测试: (第一行其实就是 uptime 命令的执行效果) 12345678910111213[work@trade-sandbox ~]$ w 16:41:35 up 91 days, 22:55, 10 users, load average: 0.36, 0.87, 1.40USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATwork pts/0 10.51.64.132 10:50 5:49m 0.03s 0.00s tail -f push_remind-2018-07-19.logwork pts/2 10.51.64.132 Wed15 0.00s 0.01s 0.00s wwork pts/3 10.51.64.132 Wed17 23:08m 1:39 1:39 -bashwork pts/4 10.51.64.132 Wed17 23:10m 3:39 3:39 topwork pts/5 10.51.64.132 Wed17 2:16m 0.20s 0.00s tail -f solr.request-2018-07-19.logwork pts/6 10.51.64.132 14:22 1:46m 0.00s 0.00s -bashwork pts/7 10.162.220.93 14:34 36:17 0.05s 0.05s -bashwork pts/8 10.162.220.93 14:42 1:51m 0.02s 0.00s tail -f AgainPushToSolr-2018-07-19.logwork pts/9 10.162.220.93 14:43 30:45 0.07s 0.07s -bashwork pts/10 10.51.64.132 15:21 34:16 0.07s 0.07s -bash top top 命令能够清晰的展现出系统的状态，而且它是实时的监控，按q退出; 测试 12345678910111213141516[work@trade-sandbox ~]$ toptop - 19:16:30 up 92 days, 1:30, 6 users, load average: 0.21, 0.10, 0.09Tasks: 265 total, 1 running, 256 sleeping, 0 stopped, 8 zombieCpu(s): 3.0%us, 1.8%sy, 0.0%ni, 94.9%id, 0.3%wa, 0.0%hi, 0.0%si, 0.0%stMem: 8057768k total, 5748564k used, 2309204k free, 812724k buffersSwap: 0k total, 0k used, 0k free, 548248k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4497 work 20 0 253m 22m 10m S 1.0 0.3 0:06.57 php 3911 work 20 0 253m 22m 10m S 0.7 0.3 0:06.99 php 3916 work 20 0 253m 22m 10m S 0.7 0.3 0:06.98 php 6738 root 20 0 263m 16m 3624 S 0.7 0.2 163:41.71 ilogtail13226 work 20 0 253m 22m 10m S 0.7 0.3 0:00.77 php13714 work 20 0 253m 22m 10m S 0.7 0.3 0:00.30 php20559 work 20 0 255m 25m 10m S 0.7 0.3 1:04.16 php20562 work 20 0 255m 25m 10m S 0.7 0.3 1:04.10 php 说明 第一行仍然是系统运行时间 Tasks行: 展示了目前的进程总数, 处于运行状态的进程数, 处于睡眠状态的进程数, 处于停止状态的进程数, 要注意zombie,表示僵尸进程,不为0则表示有进程出现问题 Cpu(s)行:3.0%us 用户空间占用CPU百分比,1.0%sy 内核空间占用CPU百分比,0.0%ni 用户进程空间内改变过优先级的进程占用CPU百分比,94.9%id 空闲CPU百分比,0.3%wa 等待输入输出的CPU时间百分比,0.0%hi ,0.0%si ,0.1%st Mem 行:4147888k total 物理内存总量,2493092k used 使用的物理内存总量,1654796k free 空闲内存总量,158188k buffers 用作内核缓存的内存量 Swap 行:5144568k total 交换区总量 ,56k used 使用的交换区总量 ,5144512k free 空闲交换区总量 ,2013180k cached 缓冲的交换区总量 负载值分析 load average: 0.21, 0.10, 0.09 表示的是 1分钟, 5分钟, 15分钟的CPU负载情况 (一般5和15分钟才具有参考意义) 对于单核处理器来说(值的大小和cpu的核数有关系), 可以把值分为3个级别 小于1.0 如果值小于1, 那么说明系统cpu处理很流畅, 不会出现等待, 堵塞 等于1.0 说明cpu能力刚刚满负荷 大于1.0 说明cpu已经超负荷, 进程处理需要等待了, 效率低下 对于多核处理器说(假设双核), 等于说处理能力增加了一倍, 比较的值就是2了, 小于2.0才不用担心 如下是四核服务器, 所以比较的值就是4123456[work@trade-sandbox ~]$ cat /proc/cpuinfo | grep &apos;model name&apos;model name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHzmodel name : Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz[work@trade-sandbox ~]$ 也可以在运行top命令之后, 直接按 1, 查看 各cpu的负载 (再按1, 又回到现实CPU总负载) 123456789101112131415[work@trade-sandbox ~]$ toptop - 20:38:47 up 92 days, 2:53, 7 users, load average: 1.31, 0.84, 0.53Tasks: 245 total, 1 running, 237 sleeping, 0 stopped, 7 zombieCpu0 : 8.4%us, 2.0%sy, 0.0%ni, 89.0%id, 0.3%wa, 0.0%hi, 0.3%si, 0.0%stCpu1 : 3.6%us, 2.3%sy, 0.0%ni, 93.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.3%stCpu2 : 0.3%us, 0.7%sy, 0.0%ni, 99.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu3 : 1.3%us, 0.0%sy, 0.0%ni, 98.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 8057768k total, 5505852k used, 2551916k free, 816568k buffersSwap: 0k total, 0k used, 0k free, 470568k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1091 root 20 0 2446m 65m 3200 S 4.7 0.8 1653:45 java 1484 root 20 0 212m 14m 1100 S 0.7 0.2 550:15.91 supervisord 3911 work 20 0 253m 22m 10m S 0.7 0.3 0:41.20 php 4497 work 20 0 253m 22m 10m S 0.7 0.3 0:40.92 php","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.renyimin.com/tags/Linux/"}]},{"title":"32. RabbitMQ集群 -- 单机部署伪集群","slug":"rabbitmq/2018-06-26-rabbitmq-32","date":"2018-06-26T07:28:16.000Z","updated":"2018-07-19T08:29:41.000Z","comments":true,"path":"2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","link":"","permalink":"http://blog.renyimin.com/2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","excerpt":"","text":"前言当你需要在生产环境中部署RabbitMQ时, 需要注意的是, 单实例在生产环境虽然部署起来很容易, 但是当你的rabbitmq服务器遇到内存崩溃或者断电的情况时, 这款高性能的产品就要成为你的耻辱了, 将会为你造成极大的问题!因此你需要将你的RabbitMQ变成高可用的才行; 内建集群简介 RabbitMQ最优秀的功能之一就是其内建集群, 这款消息队列中间件产品本身是基于Erlang编写, Erlang语言天生具备分布式特性(通过同步Erlang集群各节点的magic cookie来实现), 因此, RabbitMQ天然支持Clustering, 这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。 RabbitMQ内建集群用来完成两个目标: 允许生产者和消费者在RabbitMQ节点崩溃的情况下继续运行;你可以失去一个RabbitMQ节点, 同时客户端可以重新连接到集群中的任何其他节点并继续生产或者消费消息, 就像什么都没有发生一样; 通过增加更多的节点来线性扩展消息吞吐量;如果RabbitMQ正疲于应对庞大的消息通信量的话, 那么线性地增加更多的节点则会增加更多性能; 了解内部元数据RabbitMQ内部会始终同步四种类型的内部元数据: 队列元数据: 队列名称和它的属性 (是否可持久化, 是否自动删除); 交换器元数据: 交换器名称、类型和属性 (可持久化等); 绑定元数据: 一张简单的表格展示了如何将消息路由到队列; vhost元数据: 为vhost内的队列、交换器和绑定提供命名空间和安全属性; 内存or磁盘节点每个Rabbitmq节点, 不管是单一节点系统或者是庞大集群的一部分, 要么是内存节点(RAM node), 要么是磁盘节点(disk node): 内存节点将所有的队列、交换器、绑定、用户、权限和vhost的元数据定义都仅存储在内存中; 而磁盘节点则将元数据存储在磁盘中; 非集群单一节点在单一节点的非集群环境中, RabbitMQ默认会将元数据都存放在内存中;但是, 会将标记为可持久化的队列和交换器(以及它们的绑定)存储到硬盘上, 存储到硬盘上可以确保队列和交换器在重启Rabbitmq节点后重新被创建; 单节点类型只允许磁盘类型的节点? 集群节点而当你引入Rabbitmq集群后, RabbitMQ需要追踪的元数据类型包括: 集群节点位置, 以及节点与已记录的其他类型的元数据的关系, 在这里, 集群对元数据的存储提供了选择: 将元数据存储到磁盘上 (集群中创建节点时的默认设置) 或者仅存储到RAM内存中 集群的类型Rabbit集群模式大概分为三种: 普通模式、镜像模式 普通模式 普通模式(也就是默认的集群模式), 对于该集群模式, 当你将多个节点组合成集群后, 需要注意的是: 不是每一个节点都有所有队列的完全拷贝 在非集群的单一节点中, 所有关于队列的信息(元数据、状态、内容)都完全存储在该节点上; 但是如果在普通集群模式下创建队列的话, 集群只会在当前节点而不是所有节点上创建完整的队列信息(元数据、状态、内容); 而其他非所有者的节点, 只知道队列的元数据和指向该队列存在的哪个节点的指针; 因此当集群中队列所有者的节点崩溃时, 该节点的队列和关联的绑定就都消失了, 并且附加在这些队列上的消费者就会无法获取其订阅的信息, 并且生产者也无法将匹配该队列绑定信息的消息发送到队列中; 接下来需要了解的一个问题是: 为什么在默认的集群模式下, RabbitMQ不将队列内容和状态复制到所有的节点上? 其实有两个原因 存储空间: 如果每个集群节点都拥有所有Queue的完全数据拷贝, 那么每个节点的存储空间会非常大, 集群的消息积压能力会非常弱(无法通过集群节点的扩容提高消息积压能力); 性能: 消息的发布者需要将消息复制到每一个集群节点, 对于持久化消息来说, 网络和磁盘的负载都会明显增加, 最终只能保持集群性能平稳(甚至更糟); 所以, 通过设置集群中的唯一节点来负责特定队列, 只有该负责节点才会因队列消息而遭受磁盘活动的影响所有其他节点需要将接受到的该队列的消息传递给该队列的所有者节点, 因此, 往RabbitMQ集群添加更多的节点意味着你将拥有更多的节点来传播队列, 这些新增节点为你带来了性能的提升; 但是有人可能会想: 是否可以让消费者重新连接到集群上, 这样不就可以重新创建队列了? 但需要注意的是: 因为一般如果我们的队列设置的是持久化的, 而在该队列的主节点挂掉之后, 重新连接到队列时, 一般也不会修改队列的持久化属性; 这就需要注意一个问题, 仅当你之前创建的队列为非持久化时, 你才可以重新创建该队列为持久化, 因为这是为了保证你之前的持久化队列节点在重新被恢复启动后, 其中的消息还会被恢复, 而如果你创建一个新的持久化队列, 如果覆盖之前的持久化队列, 那消息不就丢了!!所以如果之前是持久化队列, 而且还是以持久化的方式创建该队列, 集群就会报404 NOT FOUND错误 待尝试~~ 本机配置集群 在开始配置集群前, 首先要确保现存的Rabbitmq没有运行, 因此需要关闭节点 (本机为mac, 关闭操作如下) 123renyimindeMacBook-Pro:~ renyimin$ brew services stop rabbitmqStopping `rabbitmq`... (might take a while)==&gt; Successfully stopped `rabbitmq` (label: homebrew.mxcl.rabbitmq) 可以发现一个问题, 就是停止Rabbitmq服务之后, 貌似 RabbitMQ Management 的Web UI界面还是可以正常打开运行; 所以正确的关闭节点貌似是 rabbitmqctl stop 开始配置集群前需要注意: 通常来讲, 使用 rabbitmq-server 命令启动节点之后就大功告成了, 但是如果不用额外参数的话, 该命令会使用默认的节点名称 rabbit 和监听端口 5672;所以如果你想用该命令在一台机器上同时启动5个节点的话, 那么第2，3，4，5个节点都会因为节点名称和端口号冲突而导致启动失败; 因此, 为了在本机正常启动5个节点, 可以在每次调用 rabbitmq-server前, 通过设置环境变量 RABBITMQ_NODENAME, RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口号!在此处做实验时, 将会采用 rabbit, rabbit_1,…,4 命名节点名; 端口号为5612，5613，…5615 注意, 到目前为止, 虽然尚未谈论RabbitMQ的插件, 不过你有可能已经启用了一部分插件了; 如果确实如此的话, 你需要在启动集群节点前将插件禁用!这是因为像 RabbitMQ Management 这样的插件会监听专门的端口来提供服务(例如 Management 插件的 Web UI), 目前还没讲到如何设置插件监听不同的端口, 所以当第二个节点和之后的节点启动了它们的插件后, 就会和第一个启动节点的c插件相冲突, 然后节点就都崩溃了;可以先不禁用插件, 这样在启动多个节点时, 可以根据报错一个个关闭插件也可以; (rabbitmq-plugins disable 插件名) RabbitMQ集群的搭建 启动节点 注意: 启动的时候, 直接加上 -detached 参数的话, 可能会有些报错信息比如 error : cannot_delete_plugins_expand_dir, 这就是因为需要使用root权限才可以, 你可以使用 pa aux | grep rabbitmq 查看是否三个进程都成功启动了 注意: 启动时, 貌似不能像书上那样, RABBITMQ_NODENAME 只设置节点名, 最好设置上节点host 如下: 1234567renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit_1@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5674 RABBITMQ_NODENAME=rabbit_2@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ 然后可以查看个节点状态 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost status 现在启动了三个节点 rabbit, rabbit_1, rabbit_2, 并且每个节点都会有系统的主机名在@后; 但是每个节点仍然是独立节点, 拥有自己的元数据, 并且不知道其他节点的存在; 集群中的第一个节点rabbit,将初始元数据带入集群, 并且无需被告知加入; 而第二个和之后的节点, 将加入第一个节点rabbit, 并获取rabbit节点的元数据; 要将rabbit_1和rabbit_2节点加入rabbit, 要停止该Erlang节点上运行的rabbitmq应用程序, 并重设它们的元数据, 这样它们才可以被加入rabbit节点并且获取rabbit节点的元数据; 可以使用 rabbitmqctl 来完成这些工作 停止rabbit_1节点上的应用程序12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost stop_appStopping rabbit application on node rabbit_1@renyimindeMacBook-Pro ... - 重设rabbit_1节点的元数据和状态为清空状态 123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost resetResetting node rabbit_1@renyimindeMacBook-Pro ...``` - 这样你就准备好了一个 停止运行的并且清空了的 rabbit 应用, 现在可以准备好将其加入到集群中的第一个节点rabbit中:注意书上的 `cluster` 命令好像已经不用了, 换成了 `join_cluster` renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost join_cluster rabbit@localhost Clustering node rabbit_1@localhost with rabbit@localhost renyimindeMacBook-Pro:~ renyimin$ 12- 最后, 可以重启第二个节点的应用程序 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_app Starting node rabbit_1@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ 12- 节点rabbit_2加入集群的步骤同上, 具体操作如下: renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_app Starting node rabbit_1@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_app Stopping rabbit application on node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost reset Resetting node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost join_cluster rabbit@localhost Clustering node rabbit_2@localhost with rabbit@localhost renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_app Starting node rabbit_2@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ 124. 查看集群状态, 可以在任意一个节点通过 `rabbitmqctl cluster_status` 进行查看 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_status Cluster status of node rabbit@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_status Cluster status of node rabbit_1@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit@localhost,[]}, {rabbit_1@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost cluster_status Cluster status of node rabbit_2@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit_2@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ 12345675. 注意: - 上面使用比较多的 `rabbitmqctl` 命令的关键参数是 `-n`, 这会告诉rabbitmqctl命令, 你想在指定节点而非默认节点`rabbit@`上执行命令; - 记住, Erlang节点间通过Erlang cookie的方式来允许互相通信。因为rabbitmqctl使用Erlang OPT通信机制来和Rabbit节点通信, 运行rabbitmqctl的机器和所要连接的Rabbit节点必须使用相同的Erlang cookie, 否则你会得到一个错误; 当然, 上面的集群是在本机做伪集群, Erlang cookie 自然也都是一致的!6. 将节点从集群中删除 `forget_cluster_node` renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_1@localhost Removing node rabbit_1@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_2@localhost Removing node rabbit_2@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_3@localhost Removing node rabbit_3@localhost from the cluster renyimindeMacBook-Pro:~ renyimin$ 1234567 ## [RabbitMQ集群节点类型](http://www.rabbitmq.com/clustering.html#change-type)设置与修改1. 可以在将节点加入集群时, 设定节点的类型 ([参考](http://www.rabbitmq.com/clustering.html#creating-ram)) 比如 `rabbitmqctl -n rabbit_3@localhost join_cluster --ram rabbit@localhost` 2. 之前已经通过 `rabbitmqctl cluster_status` 查看了集群的状态, 里面比较重要的是 `nodes` 部分 - 下面告诉你有三个节点加入了集群, 并且三个节点都是 disc 磁盘节点! [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit_2@localhost,[]}]}] 12345 - running_nodes 部分告诉你集群中的哪些节点正在运行; 3. 现在你可以连接到这三个running_nodes中的任何一个, 并且开始创建队列, 发布消息或者执行任何其他AMQP任务; 4. 你也可以对节点类型进行修改, 如下将rabbit_2节点类型修改为内存节点 (注意: 修改节点类型, 需要先停止节点应用) renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_app Stopping rabbit application on node rabbit_2@localhost ... renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost change_cluster_node_type ram Turning rabbit_2@localhost into a ram node renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_app Starting node rabbit_2@localhost ... completed with 1 plugins. renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_status Cluster status of node rabbit_1@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost]}, {ram,[rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit@localhost,[]}, {rabbit_1@localhost,[]}]}] renyimindeMacBook-Pro:~ renyimin$ 1234## 测试1. 运行[生产者代码](https://github.com/rymuscle/Laravel-RabbitMQ/blob/master/app/Http/Controllers/Demo/LocalClusterController.php), 在集群中的rabbit节点中创建持久化队列 - 初始集群状态 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_status Cluster status of node rabbit@localhost ... [{nodes,[{disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]}]}, {running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]}, {cluster_name,&lt;&lt;&quot;rabbit@renyimindemacbook-pro.rrcoa.com&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit_2@localhost,[]}, {rabbit_1@localhost,[]}, {rabbit@localhost,[]}]}] 12- 运行生产者, 查看创建的队列(已经有一条msg放入队列中) renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... prefetchCountQueue 0 localClusterQueue 1 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... prefetchCountQueue 0 localClusterQueue 1 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... prefetchCountQueue 0 localClusterQueue 1 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... prefetchCountQueue 0 localClusterQueue 1 renyimindeMacBook-Pro:~ renyimin$ 1232. kill掉该持久化队列localClusterQueue所在的主节点rabbit - 查看节点进程 renyimindeMacBook-Pro:~ renyimin$ ps aux | grep rabbitmq root 2656 0.4 0.3 4150148 58156 ?? S 三01下午 5:09.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [{nodelay,true}] -rabbit tcp_listeners [{&quot;127.0.0.1&quot;,5672}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost&quot; -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672 -noshell -noinput renyimin 28537 0.0 0.0 2423384 232 s007 R+ 3:12下午 0:00.00 grep rabbitmq root 72516 0.0 0.5 4143168 79400 ?? S 1:03下午 0:16.71 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_2@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [{nodelay,true}] -rabbit tcp_listeners [{&quot;127.0.0.1&quot;,5674}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost&quot; -kernel inet_dist_listen_min 25674 -kernel inet_dist_listen_max 25674 -noshell -noinput root 71841 0.0 0.5 4138448 77104 ?? S 1:01下午 0:15.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_1@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [{nodelay,true}] -rabbit tcp_listeners [{&quot;127.0.0.1&quot;,5673}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost&quot; -kernel inet_dist_listen_min 25673 -kernel inet_dist_listen_max 25673 -noshell -noinput 12345678910 - `sudo kill 2656` 3. 将生产者改连 rabbit_1 节点, 重新运行生产者 - 报错: &lt;img src=&quot;/img/rabbitmq/cluster-error01.png&quot;/&gt; - 挂掉的主节点中已存在该持久化队列, 如果在主节点挂掉后, 你能直接连接其他节点创建该队列的话, 此时创建的是个新队列, 要知道, 宕机的主节点中的持久化队列还在等待恢复呢, 它内部可能让然有很多msg需要恢复并被处理; 所以Rabbit集群的这个问题是有原因的!!4. 可以重新启动该节点 `sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detached` - 会发现之前的持久化队列会被恢复 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... localClusterQueue 1 prefetchCountQueue 0 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... localClusterQueue 1 prefetchCountQueue 0 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... localClusterQueue 1 prefetchCountQueue 0 renyimindeMacBook-Pro:~ renyimin$ 1235. 此时即使生产者连接着 rabbit_1 也可以创建该同名持久化队列了 - 重新运行刚才连接到 rabbit_1 的生产者, 不会报错了, 而是正确往队列发布了一条消息 renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queues Timeout: 60.0 seconds ... Listing queues for vhost / ... localClusterQueue 2 prefetchCountQueue 0 renyimindeMacBook-Pro:~ renyimin$ ```","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"22. Priority Queues - queue(x-max-priority) + message(priority)","slug":"rabbitmq/2018-06-19-rabbitmq-22","date":"2018-06-19T11:21:32.000Z","updated":"2018-06-30T05:52:46.000Z","comments":true,"path":"2018/06/19/rabbitmq/2018-06-19-rabbitmq-22/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/rabbitmq/2018-06-19-rabbitmq-22/","excerpt":"","text":"Priority Queues 官网文档: https://www.rabbitmq.com/priority.html 译文 从3.5.0版本开始, RabbitMQ已经在核心中实现了 优先级队列任何队列都可以使用客户端提供的可选参数 $arguments 变成优先的 (但与其他功能不同, 此处可以使用可选参数 $arguments, 但是不支持使用 策略)该实现支持有限的优先级数量: 255, 建议1至10之间的值; 使用客户端提供的可选参数 $arguments要声明优先级队列, 请使用queue的可选参数 $arguments 的 x-max-priority 选项, 该参数应该是1到255之间的正整数, 表示队列应该支持的最大优先级; AMQP 0-9-1规范对于优先级的工作方式有些模糊, 它认为所有的队列中必须至少有两个是支持优先级的, 并且可以支持多达10个, 它没有定义 “没有优先级属性的消息该如何被处理”; 与AMQP 0-9-1规格相比, 默认情况下, RabbitMQ队列不支持优先级, 在创建优先级队列时, 您可以根据需要指定任意数量的优先级别, 不过需要注意:队列的每个优先级会有一些 内存 和 磁盘 方面的成本, 还会有额外的CPU成本, 特别是在消费时, 所以你可能不希望创建大量的优先级别; 消息的 priority 字段被定义为无符号字节, 所以实际上优先级应该在0到255之间;没有 priority 属性的消息被视为优先级为0, 优先级高于队列最大值的消息被视为以最高优先级发布; 资源使用注意事项如果需要优先级队列, 建议使用1到10之间的级别, 目前使用更多优先级将消耗更多资源(Erlang进程); 与消费者互动理解消费者在处理优先级队列时的工作方式非常重要, 默认情况下, 消费者在做任何应答之前, 可能已经被发送了大量的消息;因此, 如果一个饥饿的消费者连接一个消息稍后才会被发布到的空队列, 这样, 消息可能不会在队列中等待任何时间, 在这种情况下, 优先队列将不会有机会对它们做优先级;在大多数情况下, 你会希望在消费者手动确认模式下使用 basic.qos, 以限制可以随时发送的消息数量, 从而允许优先化消息; 其他功能的互动通常, 优先级队列具有标准RabbitMQ队列的所有功能: 它们支持持久性, 分页, 镜像等; 有几点需要注意的互动:应该过期的消息仍然只会从队列的头部过期, 这些消息将永远不会传递, 但它们将显示在队列统计信息中;设置了 max-length 的Queues, 通常情况下, 将从队列头部丢弃消息以强制执行限制, 这意味着可能会丢弃更高优先级的消息, 为低优先级的消息让路, 这可能不是你所期望的; 为什么 策略 定义是不可能的为队列定义可选参数最方便的方法是通过策略, 策略是配置队列长度限制, TTL等的推荐方式;但是, 策略不能用于配置优先级, 因为策略是动态的, 并且可以在声明队列后进行更改, 优先级队列在队列声明之后永远不会改变它们支持的优先级的数量, 因此策略不是一个安全的选项; Rabbit中的消息可以按 发送顺序, 优先级高低 被依次消费 默认不设置 队列,消息 优先级 的情况下, 如果只有一个消费者, 消息会按照其发送顺序被依次消费 如果设置了 队列,消息 优先级, 如果只有一个消费者, 消息会按照 全部按照发送顺序, 发送顺序+优先级顺序, 全部按照优先级顺序 3种顺序依次被消费;下面会有案例展示; 要想使用优先级, 需要使用 队列的 $arguments的 x-max-priority 选项 和 消息的 $arguments的 priority 选项; 分析当有多个consumer时, 是无法保证消息按发送顺序或者优先级顺序被消费的, 因为每个consumer视自身能力, 都有自己的消费速度, 而且也不一定稳定; 当queue只有一个consumer时, 如果producer先发送msg到queue, 然后再启动consumer, 此时和consumer的Qos是否设置或者设置大小无关, msg会按照优先级由高到低依次输出 (不会按照producer的发送顺序输出) 像之前译文中 与消费者互动 中提到的, 这是因为 msg 有足够的时间(消费者启动后连接到server的时间), 在Rabbit Server中按照优先级做好排序; 当queue只有一个consumer时, 如果先启动consumer, 然后producer再发送msg到queue 如果没有设置Qos, 即便consumer处理每条消息的速度很慢, 但是由于运行producer之后, 消息会在第一时间被尽可能全量发送到consumer, 所以queue中貌似没什么时间对msg按照优先级进行排序, 所以consumer拿到的msg只是按照发送顺序排列的消息;先启动consumer的情况下, 消费者是处于饥饿状态; 如果设置了Qos的prefetch_count=1, 先启动consumer:理论上, 第一个msg是按照发送顺序, 但是之后的9个msg都会按照优先级顺序;但实际测试发现, 有时候msg还会全部按照优先级顺序来, 这是个比较疑惑的问题, 后来通过观察WebUI管理界面可以发现, 在启动消费者之后, 并不一定队列可以立马识别到该consumer (因为队列的 Consumers 这列显示还是0), 如果你此时运行了producer, 那么相当于先运行了producer, 然后再启动了消费者, 所以msg将会全部按照优先级循序被消费; 所以要为了确保消费者被先启动, 最好确保队列的Consumers这列显示为1, 表示消费者已经连接上来了;并且即使你prefetch_count=1的同时设置了 sleep(5), 还是会出现上面的问题(也是由于虽然你先执行了consumer, 但其实producer先被执行的原因)! 如果设置了Qos, 但是设置的prefetch_count&gt;1(比如为2)理论上也是前两个msg按照发送顺序, 而后面8个按照优先级顺序;实际测试发现, 如果出现上面的意外, 也会导致10条消息全部按照优先级顺序被消费; 小结 先运行生产者生产消息, 然后再启动消费者, 消息会有时间按照优先级在queue中做好排序, 然后发送给消费者; (这种条件类似消费能力不足的情况, 类似于consumer消费过慢并产生很多unack, 导致queue中的msg有时间按照优先级排序) 先启动消费者, 然后运行生产者生产消息, 消息会根据消费者设置的Qos, 优先会有prefetch_count数量的消费者被直接发送到consumer, 这些msg会按照发送先后顺序被消费, 之后的剩余消费者则会按照优先级顺序被消费; 总之, 消息想要按照设置的优先级别来被消费, 在将消息发出去之前, Server必须有足够的时间对Queue中的消息按照优先级高低做好排序; 所以消息优先级貌似对应用场景比较苛刻(消费者貌似要唯一, 而且需要消费者阻塞产生unacked之后, server中的消息貌似才有时间排序), 所以这个优先级感觉挺鸡肋, 如果需要消息有顺序, 还不如就 保证消费者唯一, 然后按照发送顺序消费即可! 测试 针对上面的分析进行测试 在生产者中准备好10条优先级为0-10的消息, 然后乱序发送, 代码参考此处 消费者使用三种Qos做测试 1. 默认Qos, 不做限制 2. Qos设置为1, 3. Qos设置为2, 代码参考此处 先运行生产者, 然后启动消费者, 会发现乱序的消息会按照优先级高低依次输出 而如果先启动消费者, 之后再运行生产者, 你会发现, 由于会有prefetch_count的msg被发送给消费者, 所以这部分消息会按照发送先后顺序被消费, 而其余消息会按照优先级高低依次输出;如果没有设置Qos, 或者Qos设置的数量大于消息条数, 那么msg都会按照发送的先后顺序被消费, 不会按照优先级顺序被消费; 如果消息设置了选项 priority, 而队列没有设置 x-max-priority, 效果会怎样? 1234567renyimindeMacBook-Pro:Rabbit renyimin$ php artisan msgPriorityConsumerIn AMQPChannel.php line 188: PRECONDITION_FAILED - inequivalent arg &apos;x-max-priority&apos; for queue &apos;msgPrioityQueue&apos; in vhost &apos;/&apos;: received none but current is the value &apos;10&apos; of type &apos;signedint&apos; renyimindeMacBook-Pro:Rabbit renyimin$ 队列设置了优先级选项 x-max-priority, 如果消息不设置优先级选项 priority, 效果会怎样? 无论设置适当的Qos先启动消费者, 还是先运行生产者生产消息, msg都是按照发送顺序被消费的! 如果有多个消费者, 消息被发送给不同的消费者, 这样还会按照优先级顺序进行么? 多个消费者是无法保证Msg被消费的顺序的; 集群模式下, 发送顺序和优先级顺序如何保证? ~~ 待续","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"21. Consumer Priorities 消费者优先级","slug":"rabbitmq/2018-06-15-rabbitmq-21","date":"2018-06-15T05:04:26.000Z","updated":"2018-06-20T04:47:52.000Z","comments":true,"path":"2018/06/15/rabbitmq/2018-06-15-rabbitmq-21/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/rabbitmq/2018-06-15-rabbitmq-21/","excerpt":"","text":"Consumer Priorities简介 消费者优先级允许你确保高优先级的消费者在活跃时接收消息, 并且消息只有在高优先级消费者阻塞时才会转向低优先级的消费者; 通常, 连接到队列的活跃消费者, 是以循环方式接收来自它的消息; 当使用 “消费者优先级” 时, 如果存在多个活跃的消费者具有相同的高优先级, 消息也会被循环传送给它们; 定义活跃消费者 活跃的消费者是可以无需等待就能收到消息的消费者. 假如一个消费者无法接收消息, 它就会变成阻塞状态: 因为它的 channel 在发布 basic.qos 之后已达到 unack消息的最大数量, 或者仅仅是因为网络拥塞; 当消费者的 “优先级” 被使用时, 你可以期望你的最高优先级的消费者接收所有的消息, 直到它们被阻塞, 然后较低优先级的消费者才将开始接收一些消息; 理解RabbitMQ仍然会优先传递消息是很重要的: 如果有一个活跃着的低优先级消费者已经准备好了, Rabbitmq是不会等待高优先级的阻塞消费者变成非阻塞的(它会将消息发送给已经准备好的低优先级的活跃消费者); 使用消费者 优先级别 将 basic.consume() 方法中的 $arguments 参数的 x-priority 属性设置为整数值, 未指定值的消费者优先级为0, 更大的数字表示更高的优先级, 并且可以使用正数和负数; 注意事项: 多个消费者共同绑定同一个队列时, 可以给消费者设置优先级, 这样, 优先级高的消费者会优先拿到消息并进行处理, 除非优先级高的都处于阻塞状态(unack达到Qos设置的上限值), 否则优先级低的消费者不会拿到消息;通过运行下面示例(没有设置Qos时)可以发现, 同时启动两个消费者, 然后刷新生产者, 会发现在高优先级的消费者在默认不设置Qos时, 即不阻塞的情况下, 低优先级消费者一条消息也不会收到(没有消费结果); 接下来可以模拟高优先级的Consumer阻塞, 会发现, 高优先级的Consumer在拿到第一条消息之后, 优先级低的Consumer会立刻开始进行处理注意, 模拟消费者阻塞时, 不能只通过比如 sleep(50) 这种方式, 因为这样只是消费者的处理速度变慢, 但是消费者的预取量还是默认的(即不受限制), 所以消息还是会被发送给优先级高的消费者, 这样造成的结果就是 优先级高的消费者一直在缓慢地消费, 而优先级低的消费者一直在闲置;所以还需要给优先级高的消费者设定 Qos=1 来进行测试(设定为1是为了测试方便, 也可以设置其他值, 不过这样的话, 就会有qos所设置的数量的消息被优先推送给高优先级的consumer, 直到达到qos量之后才会阻塞), 这样才能使得优先级高的消费者在没有急时ack的情况下被阻塞, 从而让优先级低的Consumer进行消费实例运行(刷新生产者10次尝试一下)可以发现, 高优先级的消费者在拿到第一条消息后, 后面的消息都给了 低优先级的消费者, 但随着高优先级的消费者进行ack而恢复活跃状态, 又会优先拿到消息 示例可查看: 生产者 TestPriorityConsumerController.php 消费者 priorityConsumer1.php, priorityConsumer2.php 扩展https://www.rabbitmq.com/blog/2013/12/16/using-consumer-priorities-with-rabbitmq/","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"20. 消费者预取 Consumer Prefetch","slug":"rabbitmq/2018-06-13-rabbitmq-20","date":"2018-06-13T11:23:36.000Z","updated":"2018-07-19T02:06:14.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","excerpt":"","text":"Consumer Prefetch 作为限制 unacked 消息数量的更自然有效的方法; AMQP 0-9-1 指定了 basic.qos 方法, 以便你在消费者进行消费时, 可以限制channel(或connection)上未确认消息的数量; 但是值得注意的是: channel 并不是理想的设定范围, 因为单个channel可能从多个队列进行消费, channel和queue需要为每个发送的消息相互协调, 以确保它们不会超出限制, 这在单台机器上会慢, 而在整个集群中使用时会非常慢; 此外, 对于许多用途, 指定适用于每个消费者的预取计数更会简单一些; 因此, RabbitMQ在 basic.qos 方法中重新定义了全局标志的含义 (在php-amqplib中basic_qos()的第三个参数a_global): 请注意, 在大多数API中, 全局标志的默认值为false; (php-amqplib的basic_qos()方法的第三个参数a_global默认也为false) 简要分析 在使用RabbitMQ时, 如果完全不配置QoS, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是采用默认方式, 将队列中的所有消息按照网络和客户端允许的速度尽快轮发到与队列绑定的consumers端; 而consumers会在本地缓存所有投递过来的messages, 这样的话, 就可能会导致 如果某个消费者的业务逻辑处理比较复杂(将会在较长时间之后才会操作完成并进行ack), 这也就导致消费慢的Consumer将会在本地堆积很多消息, 从而导致内存不足或者对其他进程造成影响 (消费者可能被撑到假死); 而其他消费能力强的Consumers, 可能已经很快地消费完成处于闲置状态, 从而造成资源浪费; 同时, 新启的消费者也无法分担已经被之前消费者缓存到其本地的消息, 所以此时即便启动更多消费者, 也无力缓解大量的 unacked 消息积压, 让你产生疑惑; 而当你设置了Qos之后, RabbitMQ虽然也是将队列中的消息尽快轮发到Consumers中, 但是因为消费者具有的 prefetch_count 消息预取值上限, 所以RabbitMQ在轮发消息的时候, 如果发现消费者的 unacked 消息达到了 prefetch_count 的值, 即使rabbitmq中有很多ready的就绪消息, 也不会给该Consumer继续投递消息了(只有消费者的 unacked 消息小于prefetch_count的值时, 才会继续通过轮发方式给该consumer投递ready消息), 如果此时有新的消费者加入, 它也将会拿到未投递出去的ready消息! 可以通过启动 prefetchCountConsumer1，prefetchCountConsumer2 两个消费者(prefetch_count 均为10), 然后使用下面测试中的生产者发送100条消息, 前期观察会发现队列中消息的最大 unacked 为20, 并且你会发现队列中处于ready状态的消息会每次2个的递减, 这就预示着, 每次这两个消费者只要 unacked 的消息书小于prefetch_count(10), Rabbitmq才会给这两个consumer各自发送一条msg; 之后如果启动了 prefetchCountConsumer3(prefetch_count为20), 此时会发现队列中消息的最大 unacked 会为40, prefetchCountConsumer3的加入会使得队列中处于ready状态的消息直接骤减20个, 最后rabbitmq中的ready消息已经为0, 每个消费者还在继续消费各自未 unacked 的消息, 最终消费完成后, 整个队列中的 unacked 消息为0; Qos的设置只有在开启手动ack后才会生效 (即, prefetch_count 在 no_ask=false 的情况下生效) 测试 一般情况下, 同一队列绑定的多个消费者都是处理同一个业务, 而且如果在同一台机器启动, 消费能力应该都差不多, 但也难免出现如: 消费者资源分配不均 或者 两个消费者在处理业务时所请求的服务端机器配置有差异(假设SLB后又2台配置不均的机器), 这种情况还是应该考虑进来的! 本测试比较简单, 主要测试在默认不设置Qos的情况下, 两个消费能力不同的消费者在处理消息时存在的问题之一: 由于这种情况下, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是只顾自己轮发消息, 这样就会导致消息被轮发完成后, 消费能力高的消费者可能很快消费完消息并处于闲置状态, 而消费能力低的消费者却在很慢地进行消费, 这样就造成了资源的浪费; 准备 创建消费者1 ‘qosCustomer1’ (简单打印消息内容) , 代码参考, 启动消费者 php artisan qosConsumer1 创建消费者2 ‘qosCustomer2’ (sleep 5秒, 模拟处理能力比较差) , 代码参考, 启动消费者 php artisan qosConsumer2 创建生产者一次向队列 ‘qosQueue’ 中推送10条消息 , 代码参考, 请求一次生产者 http://www.rabbit.com/testQos 注意需要先启动消费者, 再请求生产者; (如果先请求了生产者, 可能在启动第一个消费者之后, 其会迅速消费完10条消息, 这样就无法模拟效果了) 测试发现 qosCustomer1 : 迅速打印出结果(1,3,5,7,9), 然后就处于闲置状态了 qosCustomer2 : 还在缓慢打印(2,4,6,8,10) 可以看到, 如果不设置Qos, Rabbitmq会尽快将消息从队列中轮发投递出去, 不会对消费者的消费能力进行任何评估! 所以: 为了避免这种浪费资源的情况, 你可能就需要根据上一篇讲解的 prefetch_count 来针对不同消费者进行设置; 问题答疑测试 根据上面的描述, 有个疑问: 在默认不设置Qos的情况下, 既然生产者发布的消息会尽可能全部推送给消费者进程, 队列中会尽可能将消息全部推出, 缓存在消费者本地, 那当消费者断开时, 消息是如何恢复到队列中的? 或者不会恢复到队列中? 为了答疑, 下面进行测试 准备测试代码 创建消费者1 ‘prefetchCountConsumer1’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 创建消费者2 ‘prefetchCountConsumer2’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 生产者一次向队列 ‘prefetchCountQueue’ 中推送100条消息 , 代码参考 测试: 在生产者请求一次之后(http://www.rabbit.com/prefetchCount), ready : 100, unacked: 0, total : 100, 表示队列中已经有100条消息已经就绪, 等待发出 运行第一个php artisan prefetchCountConsumer1之后, ready : 0, unacked : 100, total : 100 (也就是说, queue中已经没有 ready状态, 即准备好待发送的消息了, 消息都传递给消费者1了) 随着消费者的缓慢消费, ready : 0, unacked : 94, total : 94 () 如果模拟 挂掉第一个消费者之后, 会发现, ready : 83， unacked : 0, total : 83 (也就是说消费者意外宕掉之后, 队列中的消息会重新处于就绪状态, 等待着新的消费者来消费) 再次启动消费者2 php artisan testQosConsumerPrefetchCount2之后, ready : 0, unacked : 80, total : 80 (消息又会被全量发送给消费者2) 注意: 如果此时启动消费者1, 你会发现, 它是无法帮助消费者2进行消费的, 因为消息都在消费者2的本地, 所以队列中并没有 ready状态的就绪消息; 测试注意: 上述测试过程如果先启动两个消费者, 然后再发布消息进行测试, 你会发现, 由于两个消费者都设置了预取值, 而且相等, 所以消息仍然会快速轮发给这两个消费者; 如果将两个消费者的 prefetch_count 都设置为10, 那么你会发现, unacked 最多也就是两个消费者的prefetch_count和, 即20个 小结 消费者的 unacked 消息数量如果未达到Qos设置的 prefetch_count 量, Rabbit不会顾及消费者的消费能力, 会尽可能将queue中的消息全部推送出去给消费者; 因此, 当你发现消费者消费缓慢, 产生大量 unacked 消息时, 即便增加新的消费者, 也无法帮助之前的消费者分担消息(除非消费者1的 unacked 达到了 prefetch_count 限制), 只能分担队列中处于 ready 状态的消息; 除非你断开之前的消费者, 然后启动一个新的消费者, 消费者中积压的消息才会重新放入队列中 (因为之前的消费者挂掉之后, 其处理后的剩余消息在 queue中会恢复为 ready 状态) 但是注意: 新启动的这个消费者如果设置额prefetch_count不合理的话, 假设与之前消费者的 预取值 设置一样大, 它很快也会产生大量 unacked 消息 所以, 在新启消费者的时候, 需要设计好 prefetch_count 的大小, 然后可以启动多个消费者来共同进行消费; 扩展 rabbitmq对 basic.qos 信令的处理 首先, basic.qos 是针对 channel 进行设置的, 也就是说只有在channel建立之后才能发送basic.qos信令; RabbitMQ只支持通道级的预取计数, 而不是connection级的 或者 基于大小的预取;预取 在rabbitmq的实现中, 每个channel都对应会有一个rabbit_limiter进程, 当收到basic.qos信令后, 在rabbit_limiter进程中记录信令中prefetch_count的值, 同时记录的还有该channel未ack的消息个数; 在php-amqplib中, 可以使用 channel 的 basic_qos() 方法来进行控制, basic_qos() 有三个参数: prefetch_size : 限制预取的消息大小的参数, rabbitmq暂时没有实现 (如果prefetch_size字段不是默认值0, 则会通知客户端出错, 通知客户端RabbitMQ系统没有实现该参数的功能, 还可以参考此文)当你设置prefetch_size大于0的时候, 会出现如下报错 prefetch_count : 预取消息数量 global: 在3.3.0版本中对global这个参数的含义进行了重新定义, 即glotal=true时表示在当前channel上所有的consumer都生效(包括已有的), 否则只对设置了之后新建的consumer生效;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"19. 消费者预取 Consumer Prefetch - RabbitMQ关于 吞吐量, 延迟 和 带宽 的一些理论","slug":"rabbitmq/2018-06-12-rabbitmq-19","date":"2018-06-12T03:26:55.000Z","updated":"2018-06-30T05:51:29.000Z","comments":true,"path":"2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","link":"","permalink":"http://blog.renyimin.com/2018/06/12/rabbitmq/2018-06-12-rabbitmq-19/","excerpt":"","text":"RabbitMQ关于吞吐量,延迟和带宽的一些理论 译文: 网上虽然有很多版本的译文, 在此处还是重新做了翻译 你在Rabbit中有一个队列, 然后有一些客户端从这个队列中进行消费;如果你根本没有设置QoS(basic.qos), 那么Rabbit将尽可能快地按照网络和客户端允许的速度将所有队列的消息推送到客户端; 因此, 消费者所占用的内存将会激增, 因为它们将所有消息都缓存在自己的RAM中;同时, 值得注意的是: 此时如果你询问Rabbit, 队列可能会显示为空, 但是会有大量的未确认消息在客户端中, 准备被客户端应用程序处理;并且此时如果你添加新的消费者, 由于没有消息留在队列中, 所以队列也无法将消息发送给新的消费者的;尽管有其他消费者可用于更快地处理消息, 但由于消息已经在现有的客户端中缓存, 并且可能在那里很长一段时间, 所以这是相当次优的! 所以，默认的QoS预取给客户端(consumer)设置了无限的缓冲区, 这可能导致不良的行为和性能; 那么, 应该将QoS预取缓冲区大小设置为多少呢?目标是让消费者保持工作饱和状态, 但要尽量减少客户端的缓冲区大小, 以便让更多的消息保留在Rabbit的队列中, 这样就可以供新消费者来消费; 比方说, Rabbit从这个队列中拿出一条消息, 把它放到网络上，然后到达消费者, 需要50ms; 客户端处理消息需要4ms;一旦消费者处理了消息, 它就会发送一个ack给Rabbit, 这将再次花费50ms发送给Rabbit并被Rabbit进行处理; 所以我们总共有104ms的往返时间。如果我们消息设置了QoS预取值为1, 那么直到这个往返行程完成之前, Rabbit是不会发送下一个消息给客户端的;因此, 每次往返的104ms中, 客户端只有4ms,或者说只有3.8％的时间忙碌, 而我们希望百分之百的时间都在忙碌中; 如果我们在每个消息的客户端上执行 总的往返时间/处理时间, 会得到 104/4 = 26如果我们设置消息的QoS预取值为26, 那就解决了我们的问题: 假设客户端具有26个消息缓冲, 等待处理(这是一个明智的假设:一旦你设置了basic.qos, 然后从一个队列中进行消费, Rabbit将会尽可能多的将消息发送到你订阅该队列的客户端中, 直到QoS的限制; 如果你认为消息不是很大, 带宽也很高, 那么Rabbit很可能更快地发送消息到你的客户端, 因此, 从完整的客户端缓冲区的角度来做所有的数学运算是合理的(也更简单的) 如果每条消息需要4ms的处理来处理, 那么总共需要 26×4 = 104ms 来处理整个缓冲区(中的消息);第一个4ms是第一个消息的客户端处理消息的时间, 处理完成后, 客户端然后发出一个确认(这一点需要50ms才能到达代理), 然后继续处理缓冲区中的下一条消息, 代理向客户端发出一条新消息, 这需要50ms的时间, 所以到了104ms时间, 客户端已经完成缓冲区的处理, 代理的下一条消息已经到达, 并准备好等待客户端来处理它;因此, 客户端始终处于忙碌状态: 具有较大的QoS预取值也不会使其更快了, 但是我们最大限度地减少了缓冲区的大小, 并且减少了客户端消息的延迟; 事实上, 客户端能够在下一条消息到达之前完全排空缓冲区, 因此缓冲区实际上保持为空; 如果处理时间和网络行为保持不变, 此解决方案绝对没问题但考虑一下如果网络突然间速度减半会发生什么情况(rymuscle:显然, 网络传输时间就加长了): 此时你的预取缓冲区(也就是你设置的prefetch预取值)就不够大了, 现在客户端会就会闲置, 等待新消息到达, 因为客户端能够处理消息的速度比Rabbit能够提供新消息的速度要快; 为了解决这个问题, 我们可能会决定将QoS预取大小加倍(或接近两倍), 如果我们从26开始将它推到51, 那么如果客户端处理保持在每个消息4ms, 我们现在在缓冲区中会有51 4 = 204ms的消息处理时间, 其中4ms将用于处理消息, 而200ms用于发送消息回复rabbit并收到下一条消息, 因此, 我们现在可以应对网络速度的减半;但是, 如果网络正常运行, 现在将QoS预取加倍, 意味着每个消息都会驻留在客户端缓冲区中一段时间​​, 而不是在到达客户端时立即处理;再次分析: 从现在51条消息的完整缓冲区开始, 我们知道新消息将在客户端完成处理第一条消息之后的100ms处开始出现在客户端, 但在这100毫秒内, 客户只能处理100/4 = 25个消息, 这意味着当新消息到达客户端时, 它会在客户端从缓冲区头部移除时被添加到缓冲区的末尾;而缓冲区将始终保持(50 - 25 = 25)个消息长度, 因此每个消息将在缓冲区中保持 25 4 = 100ms所以有时候你会看到你的消费者虽然活着没有假死, 但是却有大量的unacked! 可以考虑一下这个原因!! 因此, 我们看到, 增加预取缓冲区大小, 以便客户端可以应对恶化的网络性能, 同时保持客户端繁忙, 大大增加网络正常运行时的延迟!! 同样, 如果不是网络性能的恶化, 而是客户端开始花费40ms来处理每条消息而不是之前的4ms, 会发生什么情况?假设原始的预取缓冲区大小设置的是26条消息, 客户端现在需要花40ms处理第一条消息, 然后将确认消息发送回Rabbit并移至下一条消息;ack仍然需要50ms才能到达Rabbit, 而Rabbit发出一条新的消息需要50ms, 但在100ms内, 客户端只处理了 100/40 = 2.5 条消息, 而不是剩余的25条消息;因此当新消息到来时, 缓冲区在这一点上仍然是有 25 - 3 = 22 个消息, 这样的话, 来自Rabbit的新消息就不会被立即处理, 而是位于第23位, 落后于其他22条仍在等待处理的消息;客户端(Consumer)将会有 22 * 40 = 880ms 的时间都不会触及到那个新到的消息, 鉴于从Rabbit到客户端的网络延迟仅为50ms, 这个额外的880ms延迟现在为延迟的95％ (880 / (880 + 50) = 0.946); 当你决定尝试通过添加更多消费者来处理这种增长的积压时, 需要注意, 现在有消息正在被现有客户端缓冲, 并不是说你增加消费者就能缓解这部分的压力! 更糟糕的是, 如果我们将缓冲区大小设置为可以预取51条消息以应对网络性能下降,会发生什么?处理第一条消息后, 将在客户端缓冲另外50条消息, 100ms后(假设网络运行正常), 一条新消息将从Rabbit到达客户端, consumer在100ms中只能处理这50条消息中的两条消息(缓冲区现在为47条消息长),因此新消息将会在缓冲区中是第48位, 这样的话, 知道 47 40 = 1880ms 之后, 消费者才会开始处理新来的消息, 同样, 考虑到向客户端发送消息的网络延迟仅为50ms, 现在这个1880ms的延迟意味着客户端缓冲占延迟的97％(1880/(1880 + 50)= 0.974);这可能是不可接受的: 数据只能在客户端收到后2秒内立即处理, 才能有效且有用！*如果其他消费客户端空闲, 他们无能为力: 一旦Rabbit向客户端发送消息, 消息就是客户端的责任, 直到他们拒绝或拒绝消息; 消息发送到客户端后，客户端不能窃取彼此的消息;您希望客户端保持繁忙状态, 但客户端尽可能少地缓存消息, 以便客户端缓冲区不会延迟消息, 因此新消费客户端可以快速接收来自Rabbit队列的消息; 因此, 如果网络变慢, 缓冲区太小会导致客户端空闲; 但如果网络正常运行, 缓冲区太大会导致大量额外的延迟;如果客户端突然开始花费更长时间来处理每个缓冲区, 则会导致大量额外的延迟;很明显, 你真正想要的是可以变化的缓冲区大小, 这些问题在网络设备中很常见, 并且一直是很多研究的主题;主动队列管理算法试图尝试放弃或拒绝消息，以避免消息长时间处于缓冲区。当缓冲区保持空闲时（每条消息只遭受网络延迟，并且根本不在缓冲区中），缓冲区在那里吸收峰值，从而实现最低延迟。从网络路由器的角度来看，Jim Gettys一直在研究这个问题：局域网和广域网性能之间的差异会遇到完全相同的问题。实际上，无论何时，在生产者（在我们的例子中为Rabbit）和消费者（客户端应用程序逻辑）之间都有一个缓冲区，双方的性能可以动态变化，您将会遇到这些问题。最近发布了一种名为Controlled Delay的新算法，该算法似乎在解决这些问题方面效果很好。 … 未完待续","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"13. Queue Length Limit (队列长度限制) - x-max-length, x-max-length-bytes, x-overflow","slug":"rabbitmq/2018-06-09-rabbitmq-13","date":"2018-06-09T07:26:36.000Z","updated":"2018-06-21T07:36:26.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-13/","excerpt":"","text":"理论 官方文档: https://www.rabbitmq.com/maxlength.html 译文: 一个队列的最大长度可以通过设置 消息的数量 或者 消息的字节数(所有消息体长度的总和, 忽略消息属性 和 任何杂项开销), 或者两者都设置, 来限制; 对于任何给定的队列, 其最大长度(包括以上说的两种类型), 可以使用队列的$arguments参数, 或者在服务器端使用policies策略来进行定义;在 policies策略 和 $arguments 都指定最大长度的情况下, 会应用这两个值中的最小值; 在所有情况下, 使用就绪(Ready状态)消息的数量, 未确认的(Unacked)消息不计入限制的数量, 如下简单展示了 queue中的几种消息状态:从 rabbitmqctl list_queues 中的 messages_ready 和 message_bytes_ready 字段 以及 管理API 会展示受到限制的值 123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...testQosQueue 162msgPrioityQueue 0renyimindeMacBook-Pro:~ renyimin$ - 默认最大队列长度行为 当设置最大队列 长度 或 大小 并达到最大值时, RabbitMQ的默认行为是将 队列前面(即队列中最早的消息) **丢弃** 或 **死信** 消息; 要修改此行为, 请使用下面描述的溢出设置; - 队列溢出行为 使用 `overflow`溢出设置 来配置队列溢出行为, 如果 `overflow` 设置为`reject-publish`, 则最近发布的消息将被丢弃; 否则, 如果publisher confirms(发布者确认)被启用, 则会通过 `basic.nack` 消息向发布者通知 reject拒绝; 如果一条消息被路由到多个队列并被至少一个队列拒绝, 该channel将通过 `basic.nack` 通知发布者, 该消息仍然会发布到所有其他可以入队的队列中; - 使用 策略 定义最大队列长度 要使用`策略`指定最大长度, 需要添加key `max-length` 和/或 `max-length-bytes` 到策略定义, 例如: 123456rabbitmqctl : rabbitmqctl set_policy my-pol &quot;^one-meg$&quot; &apos;&#123;&quot;max-length-bytes&quot;:1048576&#125;&apos; --apply-to queuesrabbitmqctl (Windows) : rabbitmqctl.bat set_policy my-pol &quot;^one-meg$&quot; &quot;&#123;&quot;&quot;max-length-bytes&quot;&quot;:1048576&#125;&quot; --apply-to queues``` &quot;my-pol&quot;策略确保 `one-meg` 队列包含不超过1MiB的消息数据, 达到1MiB限制时, 最旧的消息将从队列头部丢弃;- 要定义溢出行为 - 是从头删除消息还是拒绝新发布, 请将`overflow` key 添加到策略定义, 例如: rabbitmqctl : rabbitmqctl set_policy my-pol &quot;^two-messages$&quot; &apos;{&quot;max-length&quot;:2,&quot;overflow&quot;:&quot;reject-publish&quot;}&apos; --apply-to queues rabbitmqctl (Windows) : rabbitmqctl.bat set_policy my-pol &quot;^two-messages$&quot; &quot;{&quot;&quot;max-length&quot;&quot;:2,&quot;&quot;overflow&quot;&quot;:&quot;&quot;reject-publish&quot;&quot;}&quot; --apply-to queues ``` &quot;my-pol&quot;策略确保 &quot;two-messages&quot; 队列包含不超过2条消息, 并且只要队列中包含2条消息并启用发布商确认, 所有其他发布都会发送 `basic.nack` 响应; 还可以使用管理插件定义策略, 有关更多详细信息，请参阅[策略文档](https://www.rabbitmq.com/parameters.html#policies) - 使用 $arguments 定义最大队列长度 通过提供带有非负整数值的 `x-max-length`($ageuments中的选项) 队列声明参数, 可以设置最大消息数; 非负整数值的 `x-max-length-bytes` 则可以设置字节的最大长度; 如果两个参数都设置了, 那么两者都适用, 无论哪个限制首先被实施; 通过为队列声明参数$arguments的 `x-overflow` 选项提供一个字符串值, 可以设置溢出行为, 可能的值有 `drop-head`(默认) 或 `reject-publish`; 测试 默认溢出行为下, 当队列中消息放满时, 消息什么时候丢弃, 什么时候被死信? 测试代码参考;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"12. Lazy Queues 惰性队列(x-queue-mode) + 持久化","slug":"rabbitmq/2018-06-09-rabbitmq-12","date":"2018-06-09T02:14:30.000Z","updated":"2018-06-30T06:50:51.000Z","comments":true,"path":"2018/06/09/rabbitmq/2018-06-09-rabbitmq-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/rabbitmq/2018-06-09-rabbitmq-12/","excerpt":"","text":"Lazy Queues 从RabbitMQ 3.6.0开始m, Broker有了 Lazy Queues 的概念 – queues尽可能早地将其内容移动到磁盘的队列, 并且只在消费者请求时将其加载到RAM中, 因此命名为 Lazy Queues; Lazy Queues的主要目标之一是能够支持非常长的队列(数百万条消息), 出于各种原因, 队列可能变得很长: 消费者脱机/已经崩溃/因维护而停机 突然有消息进入高峰, 生产者超过消费者 消费者比平时慢 默认情况下, 消息被发布到RabbitMQ时, 队列将其保存在内存缓存中, 这能够尽可能快地向消费者传递消息; 请注意, 持久性消息可以在进入RabbitMQ时写入磁盘, 并同时保存在RAM中; 每当代理认为需要释放内存时, 缓存中的消息将被分页到磁盘; 将一批消息分页到磁盘会花费时间并阻塞队列进程, 从而无法在分页时收到新消息, 尽管最近版本的RabbitMQ改进了分页算法, 但对于队列中有数百万条消息可能需要分页的用例, 情况仍然不理想;而懒惰队列会尝试将消息尽可能早地移动到磁盘上, 这意味着在正常操作的大多数情况下, RAM中的消息数量明显减少, 这是以增加磁盘I / O为代价的; 队列具备两种模式: default 和 lazy 默认的为default模式, 在3.6.0之前的版本无需做任何变更 lazy模式即为惰性队列的模式, 可以通过调用 channel.queueDeclare 方法的时候在参数中设置 x-queue-mode 为 lazy 或者 default 来指明队列为惰性或者正常 也可以通过Policy的方式设置, 如果一个队列同时使用这两种方式设置的话, 那么队列参数优先于策略值 只能通过删除队列, 重新创建队列并使用不同的参数重新声明它, 来更改队列模式 当你想要优先保持节点内存使用率较低, 而较高的磁盘I/O和磁盘利用率可以接收时，所以惰性队列就比较合适; 惰性队列和持久化消息可谓是最佳拍档 如果消息是持久化的, 那么这样的I/O操作不可避免, 同时内存也会过高; 注意如果 惰性队列 中存储的是 非持久化 的消息, 内存的使用率会一直很稳定, 但是重启之后消息又会丢失; 所以如果惰性队列结合持久化消息, 这就比较合适了要持久化, 磁盘I/O是无法避免的, 但可以通过惰性队列保证内存降低! 代码参考 未完待续~~","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"11. Dead Letter Exchanges (DLX) 死信交换器","slug":"rabbitmq/2018-06-07-rabbitmq-11","date":"2018-06-07T06:36:51.000Z","updated":"2018-07-19T02:06:48.000Z","comments":true,"path":"2018/06/07/rabbitmq/2018-06-07-rabbitmq-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/07/rabbitmq/2018-06-07-rabbitmq-11/","excerpt":"","text":"关于死信 死信: 来自队列的消息可能会变成 “dead-lettered”(死信), 即, 当出现以下情况的时候, 会被重新发布到另一个Exchange中:123消息被拒绝(`basic.reject` 或 `basic.nack`) 并且 `requeue=false(即没有进行重排)`;消息TTL过期队列达到最大长度(队列满了, 无法再添加数据到mq中) 死信交换器(也叫死信邮箱): 在定义业务队列的时候, 你需要考虑指定一个死信交换器, 死信交换器可以和任何一个普通的队列进行绑定, 然后在 业务队列 出现 死信 的时候就会将数据发送到 死信队列; 死信Exchange(DLX)其实也是正常的Exchange, 它们可以是任何常见的type, 并且可以像通常那样进行声明; 对于任何给定的队列, 客户端可以使用队列的 $arguments 可选参数 或 在服务器中使用 策略 来定义DLX;(在 策略 和 $arguments 都指定的情况下, $arguments 将覆盖 策略) 要为队列设置死信交换, 需要队列 $arguments 参数的 x-dead-letter-exchange 选项来指定交换器的名称, testDlxRoutingKey 指定 死信路由键: 123456789$channel-&gt;exchange_declare(&apos;testDlxExchange&apos;, &apos;direct&apos;, false, true, false, false, false);$arguments = new AMQPTable([ // 指定死信交换器 &apos;x-dead-letter-exchange&apos; =&gt; &apos;testDlxExchange&apos;, // 指定死信路由键 &apos;x-dead-letter-routing-key&apos; =&gt; &apos;testDlxRoutingKey&apos;]);// 如下就可以把上面的普通交换器testDlxExchange设置为队列testDlxQueue的死信队列了$channel-&gt;queue_declare(&apos;testDlxQueue&apos;, false, true, false, false, false, $arguments); 使用策略配置DLX可参考官网文档 如果DLX不存在, 则出现死信之后, 这些死信将会被丢弃; 像上面那样, 你也可以指定一个死信routingkey, 以便在死信消息被重发时使用; 如果没有设置, 则会使用消息自己原本的routingkey: 假如你使用 routingkey=foo 将消息发布到交换器, 然后该消息被dead-lettered, 它将被发布到routingkey为 foo 的 死信exchange; 而如果消息最初着陆的队列已声明了 x-dead-letter-routing-key=bar, 则该消息将被发布到它的(routingkey为 “bar”)死信exchange; 死信队列 : 死信队列实际上就是一个普通的队列, 只是这个队列跟死信交换器进行了绑定, 用来存放死信而已; 当一个死信exchange被指定时, 除了声明队列上的通常配置权限之外, 用户还需要对该队列具有读权限, 并对死信exchange有写入权限, 权限在队列声明时被验证 如果死信消息被重发时, publisher confirms 是开启的, 则消息在从原始队列中删除之前, 必须得到 其最终到达的死信队列(DLX路由的目标) 的确认消息; 换句话说, “发布”(消息过期的那个)队列将不会在死信队列确认接收消息之前删除消息; 请注意, 如果不干净的代理关闭, 可能会在 原始队列 和 死信目标队列 上复制相同的消息; Dead-Lettered Messages 死信被re-publish后, 会在消息的header中增加一个叫做 x-death 的数组内容, 包含了以下字段内容: 123456queue 消息被重新路由前所在的queue的名称reason 消息成为死信的原因，有如下几种: 消息被拒绝并且requeue=false, message的TTL过期, 超过了queue允许的最大消息长度time 消息成为死信的日期时间exchange 消息被重新路由到exchange的名称routing-keys 消息被发布时的 routing-keysoriginal-expiration 消息的原始expiration属性 问题 一个queue可以绑定一个DLX, 那是否可以有多个DLX? 还是说所有的死信只能扔到一个DLX下的死信queue中? 如果queue既设置了 x-dead-letter-routing-key, 也进行了绑定并且设置了 bindingKey, 那么消息会如何路由? 死信队列如果也有超时时间, 此时消息再次超时之后, 还可以再次通过另一个 EXL 放入新的 死信队列么? 常见问题 ##https://www.cnblogs.com/williamwsj/p/8108970.html 可以设置消息的存活时间;https://my.oschina.net/u/3015099/blog/781970https://blog.csdn.net/qq_29778131/article/details/52536965 疑问死信消息重新发布，发布者确认已在内部启用，因此消息最终着陆的“死信队列”（DLX路由目标）必须在消息从原始队列中删除之前确认消息。换句话说，“发布”（邮件过期的队列）队列不会在死信队列确认接收邮件之前删除邮件（请参阅确认以了解有关所做保证的详细信息）。请注意，如果代理程序不正常关闭，则可能会在原始队列和死文字目标队列上复制相同的消息。有可能形成消息死书的循环。例如，当一个队列在没有指定死信路由密钥的情况下将消息发送到默认交换器时，会发生这种情况。如果在整个周期内没有拒绝，这些周期中的消息（即达到相同队列两次的消息）将被丢弃。","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"10. TTL (x-expires,x-message-ttl, expiration)和 延迟队列","slug":"rabbitmq/2018-06-07-rabbitmq-10","date":"2018-06-07T06:28:56.000Z","updated":"2018-07-19T02:06:48.000Z","comments":true,"path":"2018/06/07/rabbitmq/2018-06-07-rabbitmq-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/07/rabbitmq/2018-06-07-rabbitmq-10/","excerpt":"","text":"Time-To-Live Extensions RabbitMQ允许你为 Messages 和 Queues 设置 TTL(生存时间): 如果通过 队列的可选参数 设置队列/消息的TTL, 则队列中所有消息都有相同的过期时间 如果通过 消息的属性 为每条消息单独设置TTL, 则每条消息的TTL都可以不同; 如果上述两种方法同时使用, 则消息的过期时间以两者之间TTL较小的那个数值为准; Queue TTL 通过在队列声明时, 设置 $arguments 参数的 x-expires 选项 或 通过设置 expires 策略, 可以为给定队列设置过期时间 这可以控制队列在被自动删除之前有多长时间可以不被使用不被使用的意思是: 队列没有consumer, 队列尚未重新声明, 并且至少在有效期内未调用 basic.get, 待测试??例如, 这可以用于RPC的回复队列, 其中可以创建许多可能永远不会被耗尽的队列?? x-expires 以毫秒为单位, 与消息ttl不同, 它不能为0 (1000表示1秒内未使用的队列将会被删除) 此功能可以与自动删除队列属性一起使用 queue.declare 命令中的 x-expires 参数控制 queue 被自动删除前可以处于未使用状态的时间;未使用的意思是 queue 上没有任何 consumer, queue 没有被重新声明, 并且在过期时间段内未调用过 basic.get 命令;通俗的说就是: 一个被设置为自动删除的队列, 在没有消费者对其进行消费的情况下, 它可以存活的时间;该方式可用于, 例如, RPC-style 的回复 queue, 其中许多 queue 会被创建出来, 但是却从未被使用 ?? 这句话还未领会 服务器会确保在过期时间到达后 queue 被删除, 但是不保证删除的动作有多么的及时。在服务器重启后，持久化的 queue 的超时时间将重新计算。 队列过期后, 消息不会进入死信 Message TTL通过队列 设置消息TTL 消息TTL可以通过设置 队列的 $arguments 参数的 x-message-ttl 选项来设置队列中消息的TTL; 已经在队列中超过配置的TTL的消息被认为是dead(死亡的) 请注意: 路由到多个队列的消息可能会在其驻留的每个队列的不同时间死亡, 或根本不会死亡; 一个队列中消息的死亡对其他队列中同一消息的生命没有影响; 服务器保证死亡的消息不会使用 basic.deliver 进行投递(发送给消费者) 或 包含在 basic.get-ok 响应中(用于一次性提取操作);此外, 服务器将尝试在基于TTL的到期时或之后不久删除 “消息”; TTL参数 或 策略 的值必须是非负整数(0 &lt;= n), 以毫秒为单位因此, 值1000意味着添加到队列的消息将在队列中存在1秒 或者 直到它被传递给消费者; 使用 策略 定义消息TTL 通过消息 设置TTL 通过在使用 basic.publish 发布消息时, 在消息的属性中设置 expiration 字段, 这样就可以基于每条消息指定TTL; expiration 字段的值描述了以毫秒为单位的TTL周期, 与 x-message-ttl 的设置相同由于 expiration 字段必须是字符串, 因此代理将(仅)接受数字的字符串表示形式。 当同时指定 每个队列 和 每条消息 的TTL时, 将选择两者之间的较低值; 对比 对于第一种设置队列TTL属性的方法, 一旦消息过期, 就会从队列中抹去; 而第二种方法里, 即使消息过期, 也不会马上从队列中抹去, 因为每条消息是否过期是在即将投递到消费者之前判定的; 为什么两者得处理方法不一致? 因为第一种方法里, 队列中已过期的消息肯定在队列头部, RabbitMQ只要定期从队头开始扫描是否有过期消息即可; 而第二种方法里, 每条消息的过期时间不同, 如果要删除所有过期消息, 势必要扫描整个队列, 所以不如等到此消息即将被消费时再判定是否过期, 如果过期, 再进行删除; 因此, 如果使用了第二种方式, 建议让消费者在线以确保消息更快地被丢弃; 测试中会有体现 测试 Publisher代码参考 内容为业务队列绑定一个死信队列(死信队列可以先不存在, 即 你可以不用先运行Consumer来创建dlx) Consuemr代码参考 内容为死信队列与死信交换器的绑定 延迟队列TTL+DLX 首先, 应该清楚的是, 延迟队列存储的对象对应的肯定是延时消息, 所谓 延时消息 是指当消息被发送以后, 并不想让消费者立即拿到消息, 而是等待指定时间后, 消费者才拿到这个消息进行消费; 比如: 在订单系统中, 一个用户下单之后通常有30分钟的时间进行支付, 如果30分钟之内没有支付成功, 那么这个订单将进行一场处理, 这是就可以使用延时队列将订单信息发送到延时队列; 用户希望通过手机远程遥控家里的智能设备在指定的时间进行工作, 这时候就可以将用户指令发送到延时队列, 当指令设定的时间到了再将指令推送到只能设备; RabbitMQ延迟队列的实现: TTL+DLX https://blog.csdn.net/u014308482/article/details/53036770 死信延时的问题: https://blog.csdn.net/qq315737546/article/details/66475743 插件方式参考: https://www.cnblogs.com/haoxinyue/p/6613706.html","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"08. Publisher Confirms 发布者确认机制","slug":"rabbitmq/2018-06-05-rabbitmq-08","date":"2018-06-05T11:20:56.000Z","updated":"2018-06-30T05:50:02.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","excerpt":"","text":"前言 在RabbitMQ中, 为了 保证消息能安全发布到Broker, 保证消息被消费者成功处理, 因此, publishers 和 conmusers 需要有用于 delivery(交付)确认 和 处理确认 的机制; 从 consumers 到 RabbitMQ 的递送处理确认被称为 AMQP 0-9-1中的确认; 从 broker 到 publishers 的确认是一个扩展协议, 被称为 publisher confirms; 这两个功能都基于相同的想法, 并受TCP的启发; 它们对于 从publishers到RabbitMQ节点 以及 从RabbitMQ节点到consumers 的可靠传送都是至关重要的; 之前已经介绍了 consumers ack, 下面介绍 publisher confirm Publisher Confirms 当你向Broker(Rabbit Server)发布消息时, 由于网络常出现的不确定因素, 客户端(Publisher)需要有机制来确认该消息已到达服务器; 使用标准的AMQP 0-9-1, 保证消息不会丢失的唯一方法是使用事务 – 使channel事务化, 然后为 每个消息 或 消息集 发布和提交; 不过, 由于事务非常重量级, 会将吞吐量降低250倍; 为了解决这个问题, 引入了 Publisher Confirms, 它模仿协议中已经存在的 消费者确认机制; 要启用这个确认机制，客户端可以通过使用channel的 confirm.select 方法 如果设置了 confirm.select 方法的 no-wait, 代理会用 confirm.select-ok 进行响应, 不过这点你貌似也只能通过抓包来观察: 这里说的 confirm.select 和 php-amqplib包中的 confirm_select_ok() 方法可不是一个意思, 而且php-amqplib也没对confirm_select_ok做实现 上面也提到了, 该确认机制是模仿已经存在的 消费者确认机制, 所以, Broker也会使用类似 ack, nack 来响应Publisher: 可以通过为 set_ack_handler , set_nack_handler 设置回调, 来监测消息是否成功到达服务器, 成功则会触发 set_ack_handler, 失败则会触发 set_nack_handler 只有在负责队列的Erlang进程中发生内部错误时才会回应nack, 所以这个在测试中也一直没有使用set_nack_handler捕获到错误, 但是对于nack的消息, 可以设置进行重发; 注意: 这两监听函数是监听 publisher confirm 应答的, 可不是监听 consumer ack 应答的; 一旦在channel上使用 confirm.select 方法, 就说它处于确认模式, 事务通道不能进入确认模式, 一旦通道处于确认模式, 就不能进行事务处理; 也就是说 事务 和 Publisher Confirm 不能同时使用; 一旦通道处于确认模式, 代理和客户端都会对消息进行计数(在第一次confirm.select时从1开始计数), 然后, broker通过在相同channel上发送 basic.ack 来处理它们, 从而确认消息; delivery-tag 字段包含确认消息的序列号; 最大 Delivery Tag, 递送标签是一个64位长的值，因此其最大值为9223372036854775807.由于递送标签的范围是按每个通道划分的，因此发布商或消费者在实践中不太可能运行该值 Publisher Confirms 的顺序考虑 在大多数情况下, RabbitMQ将按发布顺序向publisher确认消息(这适用于在单个频道上发布的消息); 但是, 发布者确认是异步发出的, 并且可以确认一条消息或一组消息;由于消息确认可以以不同的顺序到达, 所以, 应用程序应尽可能不取决于确认的顺序; 未完~~~ https://yq.aliyun.com/articles/42206 测试代码publisher confirm 不需要消费者参与, 代码参考 预习本篇还涉及到了 Qos预取相关的内容: 针对Qos的提前预习(译文) 信道预取设置(QoS)由于消息是异步发送(推送)给客户端的, 因此在任何给定时刻通常都有不止一条消息在信道上运行; 此外, 客户的手动确认本质上也是异步的, 所以有一个 未确认的交付标签的滑动窗口, 开发人员通常会倾向于限制此窗口的大小, 以避免消费者端无限制的缓冲区问题。这是通过使用 basic.qos 方法设置 预取计数 值完成的, 该值定义了channel上允许的最大未确认递送数量, 一旦数字达到配置的计数, RabbitMQ将停止在通道上传送更多消息, 除非至少有一个未确认的消息被确认;例如, 假设在通道 “Ch” 上有未确认的交付标签5,6,7和8, 并且通道 “Ch” 的预取计数(后面会学到是prefetch_count)设置为4, 则RabbitMQ将不会在 “Ch” 上推送更多交付, 除非至少有一个未完成的交付被确认(当确认帧在 delivery_tag=8 的频道上到达时, RabbitMQ将会注意到并再发送一条消息) QoS预取设置对使用 basic.get(pull API) 获取的消息没有影响, 即使在手动确认模式下也是如此; 消费者确认模式, 预取和吞吐量(译文) 确认模式 和 QoS预取值 对消费者吞吐量有显着影响, 一般来说, 增加预取值将提高向消费者传递消息的速度, 当然, 自动确认模式可以产生最佳的传送速率 但是, 在上面两种情况下, 尚未完成交付处理的消息(unacked)数量也会增加, 从而增加消费者RAM消耗; 自动确认模式或带无限预取的手动确认模式应谨慎使用, 消费者在没有确认的情况下消耗大量消息将导致其所连接的节点上的内存消耗增长; 预取值1是最保守的, 但这将显着降低吞吐量, 特别是在消费者连接延迟较高的环境中, 对于许多应用来说, 更高的价值是合适和最佳的; 100到300范围内的Qos(prefetch_count)预取值通常提供最佳的吞吐量, 并且不会面临压垮consumer的重大风险, 而更高的值往往会遇到效率递减的规律;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"07. Consumer Acknowledgements 消费者确认机制","slug":"rabbitmq/2018-06-03-rabbitmq-07","date":"2018-06-05T09:45:51.000Z","updated":"2018-07-04T06:02:28.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-03-rabbitmq-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-03-rabbitmq-07/","excerpt":"","text":"前言 在RabbitMQ中, 为了 保证消息能安全发布到Broker, 保证消息被消费者成功处理, 因此, publishers 和 conmusers 需要有用于 delivery(交付)确认 和 处理确认 的机制; 从 consumers 到 RabbitMQ 的递送处理确认被称为 AMQP 0-9-1中的确认; 从 broker 到 publishers 的确认是一个扩展协议, 被称为 publisher confirms; 这两个功能都基于相同的想法, 并受TCP的启发; 它们对于 从publishers到RabbitMQ节点 以及 从RabbitMQ节点到consumers 的可靠传送都是至关重要的; Consumer Acknowledgements Delivery Identifiers: Delivery Tags(交付标签) 当消费者(订阅)被注册时, 消息将被RabbitMQ使用 basic.deliver 方法递送(推送), 该方法带有一个 delivery tag, 它唯一标识 channel 上的一个delivery(投递); 由于投放标签的范围是按每个channel划分的, 因此交付必须在同一channel上被确认; 确认交付的客户端库方法将交付标签作为参数, 如下, 可以在consumer中使用如下几种消息应答123$channel-&gt;basic_ack(8, false); // 有2个参数可用 delivery_tag, multiple$channel-&gt;basic_nack(8, false, true); // 有4个参数可用 delivery_tag, multiple, requeue$channel-&gt;basic_reject(8, false); // 有3个参数可用 delivery_tag, requeue 消费者确认模式和数据安全注意事项 当节点向consumer投递消息时, 它必须确定消息是被消费者处理(或至少是被接收到了); 由于这个过程中会有很多导致失败的情况, 比如consumer应用程序失败, 网络原因等; 消息传递协议通常提供一种确认机制, 允许消费者给他们所连接的节点发送 确认应答, 该机制是否被使用, 是在消费者订阅时决定的; RabbitMQ可以在消息被发出去之后立即就认为它已成功投递, 也可以在客户端手动发送确认之后才认为消息被成功投递, 可以使用以下协议方法之一来手动发送确认, 确认可以是 positive正面 或 negative负面: basic.ack is used for positive acknowledgements basic.nack is used for negative acknowledgements (note: this is a RabbitMQ extension to AMQP 0-9-1) basic.reject is used for negative acknowledgements 但是与 basic.nack 相比有一个限制 Positive acknowledgements 是告诉Rabbitmq消息已被成功投递并处理, 并且消息可以被丢弃了; 使用 basic.reject 的 Negative acknowledgements 也是相同的效果, 它与 Positive acknowledgements 主要是语义上的差异 positive acknowledgements 假定一条消息已被成功处理; 而 negative acknowledgements 则暗示交付未被处理, 但消息仍应被删除; (不过, negative acknowledgements 的两个方法, 都有个 requeue 参数可以将消息重新放到队列中) 发后即忘模式 而在 自动确认模式 下, 消息在发送后会立即被认为已成功delivery(投递), 这种模式可以实现更高的吞吐量(只要消费者能够跟上), 但会降低 delivery(交付) 和 消费者处理 的安全性; (无法保证消息能真正到达, 也无法保证被消费) 这种模式通常被称为 发后即忘模式, 与手动确认模式不同, 如果消费者的TCP连接或通道在成功交付之前关闭, 则服务器发送的消息将丢失, 因此, 自动消息确认应被视为不安全的; 使用自动确认模式时需要考虑的另一件事是消费者过载, 手动确认模式通常与 有限的通道预取(Qos-prefetch_count,后面会讲到)一起使用, 这限制了通道上unacked消息的数量; 然而, 对于自动确认, 根据定义默认没有Qos限制的, 消费者因此可能被delivery(投递)速度所压垮, 可能导致内存积压, 或者操作系统终止进程; 因此, 仅建议可以高效且稳定处理消息的消费者使用 发后即忘 模式; Negative AckNegative Ack 和 requeue 有时候, Consumers 并不能立即处理投递来的消息, 但其他实例可能能够处理, 在这种情况下, 你可能就需要重新安排并让其他消费者接收并处理它, basic.reject() 和 basic.nack() 是两种用于这种场景的方法; 这些方法通常用于 negatively ack (应答)一个投递, 这个投递可以被broker丢弃或重新入队列(默认是被丢弃的), 此行为由 requeue 字段控制(如 $channel-&gt;basic_nack(8, false, true);); 当该字段设置为true时, 代理将使用指定的 delivery tag 重新进行交付 如果可能, 当消息被 requeue(重排)时, 它将被放置在其队列中的原始位置, 如果不是(由于多个消费者共享队列时来自其他消费者的并发递送和确认), 则该消息将被重新排队到更接近队列头的位置; 已重排的消息可能会立即准备好重新发送, 具体取决于它们在队列中的位置, 以及具有活跃consumer的channel使用的预取值, 这意味着, 如果所有消费者都因为暂时状况 requeue 时, 它们将创建一个 requeue/redelivery(重新发货/重新递送)的循环, 就网络带宽和CPU资源而言, 这样的循环可能是昂贵的; 对于这种情况, 有建议说是在出现无法正确消费的消息时不要采用requeue的方式来确保消息可靠性, 而是重新投递到新的队列中,比如设定的死信队列中, 以此可以避免前面所说的死循环而又可以确保相应的消息不丢失, 对于死信队列中的消息可以用另外的方式来消费分析, 以便找出问题的根本; Nack 和 Reject 消费者实现可以跟踪重新传送的次数并拒绝消息(丢弃它们)或延迟计划重新计划; 可以使用 basic.nack() 方法可以一次性 拒绝 或 requeue 多个消息, 这是区别于 basic.reject()的; basic.nack() 接受一个额外的参数 multiple, 这两个方法的所有参数对比之前也已经介绍过了, 如下: 消费者失败或失去连接时: 自动requeue 使用手动确认时, 任何未收到应答的投递将在发送递送的通道(或连接)关闭时自动requeue 由于这种行为, 消费者可能已经执行了, 但是确认消息在发送途中由于网络问题, publisher没收到应答, 这就会导致消息被requeue(重发), 这样消费者必须做好准备来处理重新来的投递，这就得考虑到幂等性; 请注意, 因为有requeue机制, 所以消费者可以收到先前传送给其他消费者的消息。 一次确认多次交付 可以批量手动确认以减少网络流量, 这是通过将确认方法的 multiple 字段设置为true来完成的; 请注意: basic.reject() 在历史上并没有这个字段, 这就是为什么 basic.nack() 被RabbitMQ作为协议扩展引入的原因; 当 multiple 字段设置为true时, RabbitMQ将确认 到本次确认中指定的标签为止 所有未完成交付的标签 (如 $channel-&gt;basic_ack(8, true);) ; 与其他确认相关的内容一样, 这是在每个channel的范围的;例如, 假设在通道 “Ch” 上有未确认的交付标签 5,6,7,8, 当设置确认方法的 delivery_tag 为8并且 multiple 设置为true的情况下到达该channel时, 将确认从5到8的所有标签; 如果 multiple 设置为false, 那么值确认标签8; 客户端错误: 双重ack 和 未知Tags 如果消费者不止一次确认同一个递送标签, RabbitMQ将会出现channel错误, 例如 PRECONDITION_FAILED - unknown delivery tag 100, 如果使用未知的delivery tag(投递标签), 则会抛出同样的通道异常; Broker报出 “unknown delivery tag” 的另一种情况是, 无论是positive 还是negative 的应答, 在与收到投递的channel不同的channel上尝试确认; (不过这个一般我们都是使用当前channel应答当前channel) 扩展 - Negative Ack AMQP 0-9-1 中的消费者可以选择使用 手动确认机制; AMQP 0-9-1 规范定义了 basic.reject() 方法, 该方法允许consumer应答单条negatively确认消息，指示broker丢弃或重新发送消息; 不幸的是，basic.reject 不支持批量应答 negatively确认消息; 为了解决这个问题, RabbitMQ支持了 basic.nack() 方法, 该方法提供了 basic.reject 的所有功能, 同时也允许批量应答消息 要批量拒绝消息, 客户端将 basic.nack() 方法的 multiple 标志设置为true; 在这方面, basic.nack 补充了 basic.ack 的批量确认语义; 扩展 - consumer-cancle 当Consumer通过channel从queue中消费消息时, 有很多原因会导致消费停止; 其中一个很明显的原因是, consumer在该channel中发出 basic.cancel, 这将导致consumer被取消, 并且 Rabbit Server 回应 basic.cancel-ok; 其他事件(如queue被删除)或集群场景中队列所在的节点失败, 将导致消费被取消, 但不会通知客户端通道, 这通常是无益的; 测试 消费者代码参考 生产者代码参考","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]}]}