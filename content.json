{"meta":{"title":"Lant's","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"13. mapping","slug":"elasticsearch/2018-06-19-13","date":"2018-06-19T11:21:52.000Z","updated":"2018-11-20T05:13:34.000Z","comments":true,"path":"2018/06/19/elasticsearch/2018-06-19-13/","link":"","permalink":"http://blog.renyimin.com/2018/06/19/elasticsearch/2018-06-19-13/","excerpt":"","text":"mapping核心数据类型 es的文档中, 每个字段都有一个数据类型, 可以是: 一个简单的类型, 如 text, keyword, date, long, double, boolean 或 ip-支持JSON的分层特性的类型,如对象或嵌套 或者像 geo_point, geo_shape 或 completion 这样的特殊类型 为不同目的以不同方式索引相同字段通常很有用, 例如, 字符串字段可以被索引为用于全文搜索的文本字段, 并且可以被索引为用于排序或聚合的关键字字段, 或者, 可以使用标准分析器, 英语分析器和法语分析器索引字符串字段; 之前已经了解过: 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping;为了更准确方便地让es理解我们的意图, 一般我们会对index的type手动来创建mapping mapping操作 GET /products/_mapping/computer 只能在创建index时手动创建mapping, 或者新增field mapping, 但是不能 update filed mapping; 手动创建index并设置mapping 1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot; : &quot;date&quot; &#125;, # 如果不想进行分词, 就设置为 keyword &quot;publisher_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 尝试修改某个字段的mapping, 会报错 1234567891011121314151617181920212223242526272829303132PUT /website&#123; &quot;mappings&quot;:&#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; &#125;&#125;# 结果报错&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/RwXzLP7UTOGUQ_BYMInedw] already exists&quot;, &quot;index_uuid&quot;: &quot;RwXzLP7UTOGUQ_BYMInedw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 测试mapping 测试1 123456789101112131415161718192021222324GET website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 注意 只能在不同的索引中对相同的字段设定不同的datatype, 即便是在同一个index中的不同type, 也不能对相同的field设置不同的datatype;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"12. 分词器","slug":"elasticsearch/2018-06-17-12","date":"2018-06-17T09:05:52.000Z","updated":"2018-11-20T05:12:52.000Z","comments":true,"path":"2018/06/17/elasticsearch/2018-06-17-12/","link":"","permalink":"http://blog.renyimin.com/2018/06/17/elasticsearch/2018-06-17-12/","excerpt":"","text":"之前在介绍mapping时, 已经了解到, ES会根据文档的字段类型, 来决定该字段是否需要进行分词和倒排索引, 而分词器的主要工作就是对字段内容进行分词, 通过分词器处理好的结果才会拿去建立倒排索引; ES内置的分词器: standard analyzer simple analyzer whitespace analyzer language analyzer 测试分词器: 12345678910111213141516171819202122232425262728293031GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, # 切换进行测试 &quot;text&quot; : &quot;Test to analyze&quot;&#125;# 结果&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;test&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125; ]&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"11. 诡异的搜索结果 引出 mapping","slug":"elasticsearch/2018-06-16-11","date":"2018-06-16T11:23:16.000Z","updated":"2018-11-20T05:11:27.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-11/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-11/","excerpt":"","text":"诡异的搜索结果 构造数据 1234567891011121314151617181920DELETE /websiteGET /website/_mappingPUT /website/article/1&#123; &quot;post_date&quot;: &quot;2018-06-20&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;php is the best language&quot;&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2018-06-21&quot;, &quot;title&quot;: &quot;java&quot;, &quot;content&quot;: &quot;java is the second&quot;&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2018-06-22&quot;, &quot;title&quot;: &quot;php&quot;, &quot;content&quot;: &quot;C++ is third&quot;&#125; 诡异的搜索结果 1234GET /website/article/_search?q=2018 # 3条查询结果GET /website/article/_search?q=2018-06-21 # 3条查询结果GET /website/article/_search?q=post_date:2018-06-22 # 1条查询结果GET /website/article/_search?q=post_date:2018 # 0条查询结果 结果分析 前两个查询之所以能匹配到所有文档, 是因为查询时并没有指定字段进行匹配, 所以默认查询的是_all字段, 而_all是经过分词的并且有倒排索引对于第一个查询来说, 2018 这个值进行分词后还是2018, 自然是可以匹配到所有文档的而对于第二个查询来说, q=2018-06-21 进行分词后也包含2018, 所以也可以匹配到所有文档 对于第三个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 可以匹配到是比较正常的情况 而对于第四个查询, 由于 q参数 指定了字段, 所以不会去查询_all字段, 而是去查指定的post_data字段, 但却没有结果 这就要引出 ES 的mapping机制了 Mapping映射 在ES中, 当我们手动去创建一个文档到索引中的时候, ES其实默认会自动为每个文档的type创建一个mapping, 这种创建mapping的方式称为 dynamic mapping; mapping就是index的type的元数据, 每个type都有一个自己的mapping, 决定了该type下文档中每个field的数据类型, 分词及建立倒排索引的行为 以及 进行搜索的行为; ES在自动创建mapping时, 会根据字段值去自行猜测字段的类型, 不同类型的field, 有的是full-text, 有的就是exact-value 对于 full-text型的field, es会对该filed内容进行分词, 时态转换, 大小写转换, 同义词转换等一系列操作后, 建立倒排索引; 对于 exact-value型的field, es则不会进行分词等处理工作 full-text型 和 exact-value型 的不同, 也决定了当你进行搜索时, 其处理行为也是不同的 如果指明要搜索的field, ES也会根据你要搜索的字段的类型, 来决定你发送的搜索内容是否先需要进行全文分析…等一些列操作 当然, 如果你搜索时不指定你具体字段, 则搜索的是 _all, 是会先对你的发送的搜索内容进行分词等操作的 之前诡异的例子中, 其实就是因为在创建文档时, 由于 post_date 字段的值被ES自认为是date类型(exact-value精确值), 所以es不会对这种类型做分词及倒排索引, 所以 GET /website/article/_search?q=post_date:2018 在搜索时候, 其实是去精准匹配post_date字段了, 所以匹配不到; 查看你索引type的默认mapping: 123456789101112131415161718192021222324252627282930313233GET /website/_mapping&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 引出手动创建mapping 除了让es自动为我们创建mapping, 一般我们都是在创建文档前, 先手动去创建index和type, 以及type对应的mapping 为了能够将时间域视为时间, 数字域视为数字, 字符串域视为全文或精确值字符串, ES 需要知道每个域中数据的类型 而很显然我们比ES更了解我们的字段类型, ES根据值去判断的话, 很容易出现误判 比如你如果字段是个日期, 可能形式为 2018-06-20 12:13:15 但ES可能不会认为这是个date类型, 如果是 2018-06-20 它又认为是date类型, 所以还是自己手动设置比较好 虽然映射是index的type的, 但事实上, 如果在相同的index中, 即使在不同的type, 你也不能对相同字段做不同的类型指定, 可参考类型和映射 只能在不同的索引中对相同的字段设定不同的类型","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"10. 了解 `_all` field","slug":"elasticsearch/2018-06-16-10","date":"2018-06-16T06:29:39.000Z","updated":"2018-11-20T05:09:59.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-10/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-10/","excerpt":"","text":"_all 对于在学习 query-string 搜索时, 一般这样来用 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc 这种查指定字段进行筛选的方式; 其实ES还可以直接 GET /products/computer/_search?q=diaonao 来进行检索, 这种检索方式会对文档中的所有字段进行匹配; 之所以可以对文档中的所有字段进行匹配, 是 _all 元数据的作用 当你在ES中索引一个document时, es会自动将该文档的多个field的值全部用字符串的方式连接起来, 变成一个长的字符串, 作为 _all field值, 同时对_all分词并建立索引; 之后在搜索时, 如果没有指定对某个field进行搜索, 默认就会搜索 _all field, 而你传递的内容也会进行分词后去匹配 _all 的倒排索引 练习 1234567891011DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 下面的检索都可以搜索到上面的文档GET /products/computer/_search?q=4500GET /products/computer/_search?q=xuhang","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"09. 组合多条件搜索","slug":"elasticsearch/2018-06-16-09","date":"2018-06-16T02:07:26.000Z","updated":"2018-11-20T05:42:11.000Z","comments":true,"path":"2018/06/16/elasticsearch/2018-06-16-09/","link":"","permalink":"http://blog.renyimin.com/2018/06/16/elasticsearch/2018-06-16-09/","excerpt":"","text":"查询 虽然 Elasticsearch 自带了很多的查询, 但经常用到的也就那么几个 match_all : 简单的匹配所有文档, 在没有指定查询方式时(即查询体为空时), 它是默认的查询 match : 无论你在任何字段上进行的是全文搜索还是精确查询, match 查询都是你可用的标准查询如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 keyword 字符串字段，那么它将会精确匹配给定的值 不过, 对于精确值的查询，你可能需要使用 filter 过滤语句来取代查询语句，因为 filter 将会被缓存 multi_match 查询可以在多个字段上执行相同的 match 查询 range 查询找出那些落在指定区间内的数字或者时间 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者 keyword字符串term 查询对于输入的文本不分析, 所以它将给定的值进行精确查询 terms 查询和 term 查询一样, 但它允许你指定多值进行匹配, 如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件 需要注意的是: term 和 terms 是不会对输入文本进行分析, 如果你的搜索如下虽然索引中存在 first_name 为 John 的文档, 但是由于该字段是全文域, 分词后可能就是 john, 而使用 terms 或者 term 的话, 由于不会对查询语句中的’John’进行分词, 所以它去匹配分词后的’John’的话, 实际上就是去匹配’john’, 由于大小写不匹配, 所以查询不到结果; 如果查询改为john反而却能匹配到更多term查询的奇葩例子可以查看term 查询文本 12345678GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot; : &#123; &quot;first_name&quot; : [&quot;John&quot;] &#125; &#125;&#125; exists 查询和 missing 查询被用于查找某个字段是否存在, 与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性;注意: 字段存在和字段值为””不是一个概念, 在ES中貌似无法匹配一个空字符串的字段; 可以参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/_dealing_with_null_values.html 这些查询方法都是在 HTTP请求体中作为 query参数 来使用的; constant_score : 可以使用它来取代只有 filter 语句的 bool 查询, 在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助; 当你的查询子句只有精确查询时, 可以将 term 查询被放置在 constant_score 中，转成不评分的 filter, 这种方式可以用来取代只有 filter 语句的 bool 查询 组合多查询 现实的查询需求通常需要在多个字段上查询多种多样的文本, 并且根据一系列的标准来过滤; 为了构建类似的高级查询, 你需要一种能够将多查询组合成单一查询的查询方法; 可以用 bool查询 来实现需求; bool查询将多查询组合在一起, 成为用户自己想要的布尔查询, 它接收以下参数: must : 文档 必须 匹配这些条件才能被包含进来 must_not : 文档 必须不 匹配这些条件才能被包含进来 should : 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分 上面的每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 filter(带过滤器的查询) : 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档 例子1: should只是针对结果进行加分, 并不会决定是否有匹配结果 (不过, 这只是当should不在must或should下的时候) 只有 must 和 must_not 中的子句是决定了是否能查询出数据 而 should 只是在针对查询出的数据, 如果对还能满足should子句的文档增加额外的评分 (如果should之外的其他语句不能查询出结果, 即便should可以匹配到文档, 整体查询最终也不会有匹配结果)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970DELETE /test/PUT /test/cardealer/1&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 206425533, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/2&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 91, &quot;action_operator_name&quot; : &quot;王玥91&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/3&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 301, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/4&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;affirm_deal_tinspection&quot;, &quot;action_time&quot; : &quot;2018-09-27 10:13:40&quot;, &quot;action_operator&quot; : 42, &quot;action_operator_name&quot; : &quot;王玥42&quot;, &quot;action_target&quot; : 200, &quot;action_note&quot; : &quot;确认成交：竞拍成功，车辆状态：待验车&quot;&#125;PUT /test/cardealer/5&#123; &quot;record_type&quot; : &quot;c2b_car_action&quot;, &quot;action_point&quot; : &quot;abortive_married_deal&quot;, &quot;action_time&quot; : &quot;2018-08-22 17:11:53&quot;, &quot;action_note&quot; : &quot;撮合失败，系统自动流拍，车辆状态：销售失败&quot;, &quot;action_target&quot; : 600, &quot;action_operator&quot; : 83, &quot;action_operator_name&quot; : &quot;王玥83&quot;&#125;GET /test/cardealer/_searchGET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, # 增加评分 &quot;should&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 例2: 如果不想因为某个字段的匹配而增加评分, 可以将该匹配放在 filter 过滤语句中; 当然, filter 子句 和 查询子句 都决定了是否有匹配结果, 这是它两 和 上面那种 should 用法的不同之处 如下可以看到 filter 过滤子句 和 查询子句的 区别, 虽然结果一样, 但是结果的评分有差异 12345678910111213141516171819202122232425262728293031323334# 查询语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123;&quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123; &quot;action_operator&quot; : 42 &#125; &#125; ], &quot;must_not&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125;# 过滤语句GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125; &#125;, &quot;must_not&quot; : &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125; &#125;, &quot;filter&quot; : [ &#123;&quot;match&quot; : &#123;&quot;action_operator&quot; : 42&#125;&#125;, &#123;&quot;match&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125; &#125;&#125; 将 bool 查询包裹在 filter 语句中, 还可以在过滤标准中增加布尔逻辑 constant_score 查询 a AND (b OR c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ...FROM ...WHERE ... = &quot;...&quot; AND ( ... = &quot;...&quot; OR ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句此时的用法和一开始那种不同, 它不仅仅是提升结果评分, 而是直接决定了结果是否匹配 12345678910111213141516GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 1234567891011121314151617181920GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略, 语句会更简化 &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal1&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 600&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; a OR (b AND c) 型 传统SQL经常会有如下形式的查询条件组合 12345SELECT ... FROM ... WHERE ... = &quot;...&quot; OR ( ... = &quot;...&quot; AND ... = &quot;...&quot; ) es 中写法如下 (下面展示了用 查询语句 和 过滤语句两种写法) 可以看到, 在这种写法下, should子句不仅仅是提升结果评分, 而是直接决定了结果是否匹配; 可参考组合查询—控制精度中的介绍 所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，但有多少 should 语句应该匹配呢？ 默认情况下，没有 should 语句是必须匹配的，只有一个例外：那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。 123456789101112131415161718192021222324GET /test/cardealer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; # 不带评分的过滤查询写法只用把这里换成 filter, 当然, 如果不写filter时, 这一层及下一层都可以省略 &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;match&quot; : &#123;&quot;record_type&quot; : &quot;c2b_car_action&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;action_point&quot; : &quot;abortive_married_deal&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;action_target&quot; : 200&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125; &#125; 组合过滤和组合查询类似, 主要是对组合查询子句的搭配, 基本上都是如下构造, 然后就是放进 filter 或者 must 的区别, 之前例子已经给过了 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; 组合查询可参考 https://www.elastic.co/guide/cn/elasticsearch/guide/cn/bool-query.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"08. 全文检索,结构化精确检索,短语检索,统计 预习","slug":"elasticsearch/2018-06-15-08","date":"2018-06-15T10:56:31.000Z","updated":"2018-11-11T05:46:39.000Z","comments":true,"path":"2018/06/15/elasticsearch/2018-06-15-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/15/elasticsearch/2018-06-15-08/","excerpt":"","text":"查询和过滤 在es中检索文档时候, 对文档的筛选分为 查询 和 过滤, 这两种方式是不太一样的 练习, 搜索商品desc字段中包含 ‘diannao’, 并且售价大于5000的商品 1234567891011121314151617GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;: &#123; &quot;must&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot; : &#123;&quot;gt&quot;: 5000&#125; &#125; &#125; &#125; &#125;&#125; 注意: 结构化检索(精确类型字段的检索) 一般会被放到filter过滤语句中, 不会进行分词和相关度排名, 但会对过滤进行缓存 全文检索(全文类型字段的检索) 一般用查询语句进行筛选, 会进行分词和相关度排名 full-text 检索 ES可以进行全文检索并可以进行相关度排名 重新准备数据 1234567891011121314151617181920212223242526272829DELETE /productsPUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer&quot;, &quot;desc&quot; : &quot;gaoqing hongji diannao&quot;, &quot;price&quot; : 4870, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;chaobao&quot;, &quot;gaoqing&quot;] &#125;PUT /products/computer/3&#123; &quot;name&quot; : &quot;dell&quot;, &quot;desc&quot; : &quot;daier chaoji diannao&quot;, &quot;price&quot; : 5499, &quot;tag&quot; : [&quot;shishang&quot;, &quot;gaoqing&quot;, &quot;gaoxingneng&quot;] &#125;POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125; 练习, 全文检索 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;&#125; 练习 全文高亮检索 12345678910111213GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot;:&quot;gaoqing diannao&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;desc&quot; : &#123;&#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWbE6HmlWC0s-aachNUv&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;huawei&quot;, &quot;desc&quot;: &quot;china best diannao gaoqing&quot;, &quot;price&quot;: 6080, &quot;tag&quot;: [ &quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;desc&quot;: [ &quot;china best &lt;em&gt;diannao&lt;/em&gt; &lt;em&gt;gaoqing&lt;/em&gt;&quot; ] &#125; &#125;, ...... 结构化精确检索phrase search(短语搜索) 与全文索引不同, 全文索引会对你发送的 查询串 进行拆分(做分词处理), 然后去倒排索引中与之前在存储文档时分好的词项进行匹配, 只要你发送的查询内容拆分后, 有一个词能匹配到倒排索引中的词项, 该词项所对应的文档就可以返回; phrase search(短语搜索)则不会对你发送的 查询串 进行分词, 而是要求在指定查询的字段中必须包含和你发送的查询串一模一样的内容 才算是匹配, 否则该文档不能作为结果返回; 短语搜索 和 结构化搜索还是不一样 结构化搜索是 你的查询串 和 指定的文档字段内容 是完全一致的, 查询串和字段本身都不会做分词, 一般该字段也是精确类型的字段类型; 而 短语搜索 则是, 你的 查询串 不会做分词, 但是你查询的字段可能会做分词, 你的查询串需要包含在 指定字段中; 搜索商品desc字段中包含 ‘gaoqing diannao’短语 的文档 123456789# 短语检索GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;desc&quot;: &quot;diannao gaoqing&quot; &#125; &#125;&#125; 结果发现, 虽然还是查询的全文字段desc, 但是结果却只有一个 提前了解ES统计语法 统计商品 每个tag下的商品数量, 即, 根据商品的tag进行分组 12345678GET /products/computer/_search&#123; &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 初次运行报错 1234567891011121314151617181920212223242526&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;products&quot;, &quot;node&quot;: &quot;eCgKpl8JRbqwL3QY0Vuz3A&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [tag] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.&quot; &#125; &#125; ] &#125;, &quot;status&quot;: 400&#125; 解决方案: 将文本field的 filedata 属性设置为true (现在不用知道这玩意儿, 先尽快解决, 看到聚合分析的预发和效果, 后面讲在详聊该问题) 123456789PUT /products/_mapping/computer&#123; &quot;properties&quot;: &#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 重新执行统计语句, 发现返回中除了分析的结果, 还包含了查询的文档内容; 如果只想显示聚合分析的结果, 可以如下设置size为0: 123456789GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 针对名称中包含”china”的商品, 计算每个tag下的商品数 12345678910111213GET /products/computer/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;gaoqing&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot; : &#123; &quot;terms&quot; : &#123;&quot;field&quot;: &quot;tag&quot;&#125; &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格 (先分组, 再计算每组的平均值) 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot; : &#123; &quot;avg&quot; : &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 计算每个tag下商品的平均价格, 并且按照平均价格进行排序 1234567891011121314GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;:&quot;tag&quot;, &quot;order&quot;: &#123;&quot;avg_by_price&quot;:&quot;desc&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125; &#125; &#125; &#125; &#125;&#125; 结果: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_tag&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5789.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5483 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 3, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5150 &#125; &#125;, &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;&#125; 练习, 按照指定的价格范围区间进行分组, 然后再每个分组内再按照tag进行分组, 最后在计算每组的平均价格 1234567891011121314151617181920212223242526GET /products/computer/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ &#123;&quot;from&quot;:4500, &quot;to&quot;:5000&#125;, &#123;&quot;from&quot;:5000, &quot;to&quot;:5500&#125;, &#123;&quot;from&quot;:5500, &quot;to&quot;:6100&#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;tag&quot;&#125;, &quot;aggs&quot;:&#123; &quot;avg_by_price&quot;: &#123; &quot;avg&quot;: &#123;&quot;field&quot;:&quot;price&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 结果: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 16, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price_range&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;4500.0-5000.0&quot;, &quot;from&quot;: 4500, &quot;to&quot;: 5000, &quot;doc_count&quot;: 2, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;chaobao&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 2, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4685 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4870 &#125; &#125;, &#123; &quot;key&quot;: &quot;xuhang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 4500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5000.0-5500.0&quot;, &quot;from&quot;: 5000, &quot;to&quot;: 5500, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125;, &#123; &quot;key&quot;: &quot;shishang&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 5499 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;5500.0-6100.0&quot;, &quot;from&quot;: 5500, &quot;to&quot;: 6100, &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;gaoqing&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;gaoxingneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125;, &#123; &quot;key&quot;: &quot;jieneng&quot;, &quot;doc_count&quot;: 1, &quot;avg_by_price&quot;: &#123; &quot;value&quot;: 6080 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"07. 查询小优化","slug":"elasticsearch/2018-06-14-07","date":"2018-06-14T02:50:39.000Z","updated":"2018-11-11T05:45:58.000Z","comments":true,"path":"2018/06/14/elasticsearch/2018-06-14-07/","link":"","permalink":"http://blog.renyimin.com/2018/06/14/elasticsearch/2018-06-14-07/","excerpt":"","text":"由于你的每个查询操作都可能会被转发到不同node的shard去执行, 现在假设你的查询, 会打到不同的10个shard上, 每个shard上都要花费1秒钟才能出结果, 这样你总共10s后才会给用户响应, 如果是个商品列表, 用户体验就会非常差 假设本来需要在10秒钟拿到100条数据(每个shard上10条), 现在你可以设置让es在1秒钟就让请求返回, 只拿到部分数据即可 此时可以在查询请求时跟上 timeout 参数(10ms, 1s， 1m): GET /_search?timeout=1ms (可灌入大量数据做测试) 深度分页问题: 假设你的列表每页展示20条数据, 总共1万页, 当我们在使用ES进行分页搜索时, 你想查询第9900页的那20条数据当你的请求到达第一个协调节点后, 它会要求ES给你返回所有该索引对应的primary-shard上的前9900页的数据, 然后es在内存中排序后, 这样会大量占用当前协调节点的计算机资源, 所以尽量避免出现这种深度分页的查询;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"06. mget, bulk 批量操作","slug":"elasticsearch/2018-06-10-06","date":"2018-06-10T09:06:39.000Z","updated":"2018-11-11T05:50:11.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-06/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-06/","excerpt":"","text":"mget 批量查询 批量查询可以只发送一次网络请求, 返回多条查询结果, 能大大缩减网络请求的性能开销 练习 : 12345678GET /_mget&#123; &quot;docs&quot; : [ &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:1&#125;, &#123;&quot;_index&quot;:&quot;products&quot;,&quot;_type&quot;:&quot;computer&quot;,&quot;_id&quot;:2&#125;, &#123;&quot;_index&quot;:&quot;blogs&quot;,&quot;_type&quot;:&quot;php&quot;,&quot;_id&quot;:1&#125; ]&#125; bulk 语法: 每个操作要两个json串, 语法如下: 12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; 可以执行的操作类型如: delete: 删除一个文档, 只要一个json串就可以了 create: PUT /index/type/id/_create 创建, 存在会报错 index: 即普通的 put 操作, 可以是创建也可以是全量替换文档 update: 执行部分字段更新 练习: 12345678910111213141516171819DELETE /productsPUT /products/computer/1 # 先创建一个文档&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;GET /products/computer/_searchPOST /products/_bulk&#123;&quot;delete&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 1&#125;&#125; # 删除id为1的文档 (1行json即可)&#123;&quot;create&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 2&#125;&#125; # 创建id为2的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-create-test2&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;&#125;&#125; # 创建一个文档 (es生成id, 2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;index&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3&#125;&#125; # 创建一个id为3的文档 (2行json)&#123;&quot;test_field&quot; : &quot;_bulk-index-test3&quot;, &quot;test_field2&quot; : &quot;_bulk-index-test3&quot;&#125;&#123;&quot;update&quot; : &#123;&quot;_type&quot; : &quot;computer&quot;, &quot;_id&quot; : 3, &quot;_retry_on_conflict&quot;: 3 &#125;&#125; # 更改id为3的文档中的test_field字段&#123;&quot;doc&quot; : &#123;&quot;test_field&quot; : &quot;_bulk-index-update-test3&quot;&#125;&#125; bulk操作中, 任何一个操作失败, 不会影响其他的操作, 但是在返回结果里会有异常日志 bulk的请求会被加载到内存中, 所以如果太大的话, 性能反而会下降, 因此需要通过反复测试来获取一个比较合理的bulk size, 一般从1000~5000条数据开始尝试增加数据; 如果看大小的话, 最好在5-15M之间;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"05. ES的搜索方式 Query-string 与 query DSL, multi-index, multi-type搜索模式","slug":"elasticsearch/2018-06-10-05","date":"2018-06-10T06:33:46.000Z","updated":"2018-11-20T04:56:07.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-05/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-05/","excerpt":"","text":"Query-string 搜索 之所以叫 query-string, 是因为search的参数都是以http请求的 query-string 来传递的 练习, 搜索全部商品 GET /products/computer/_search 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 GET /products/computer/_search?q=desc:diannao&amp;sort=price:desc query-string这种搜索比较适合在命令行使用curl快速地发一个请求来检索信息, 如果查询比较复杂, 一般不太适用, 正式开发中比较少用; query DSL DSL(Domain Specified Language): 领域特定语言 (这里即 ES的领域特定语言), 是在HTTP的请求体中通过json构建查询语法, 比较方便, 可以构建各种复杂语法; 练习, 查询所有商品 123456GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 练习, 搜索商品desc字段中包含 ‘diannao’, 并按照售价排序 1234567891011GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match&quot;: &#123; &quot;desc&quot;:&quot;diannao&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &quot;desc&quot;&#125; ]&#125; 练习, 分页查询商品 12345678GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot; : 0, &quot;size&quot; : 2&#125; 练习, 指定需要返回的字段 (使用_source元数据: 可以指定返回哪些field) 1GET /products/computer/1?_source=name,price 1234567GET /products/computer/_search&#123; &quot;query&quot; : &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot; : [&quot;name&quot;, &quot;desc&quot;, &quot;tag&quot;]&#125; query DSL 可以在HTTP请求体中构建非常复杂的查询语句, 所以比较常用; 更多复杂用法后面会聊到; multi-index, multi-type搜索模式 GET /_search : 检索所有index, 所有type下的数据 GET /index/_search : 指定一个index, 搜索其下所有type的数据 GET /index1,index2/_search : 指定多个index, 搜索他们下面所有type的数据 GET /index1,index2/type1,type2/_search : 指定多个index, 搜索他们下面指定的多个type的数据 _all/type1,type2/_search : 搜索所有index下指定的多个type的数据","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"04. 简单尝试 CURD","slug":"elasticsearch/2018-06-10-04","date":"2018-06-10T02:36:57.000Z","updated":"2018-11-11T05:52:21.000Z","comments":true,"path":"2018/06/10/elasticsearch/2018-06-10-04/","link":"","permalink":"http://blog.renyimin.com/2018/06/10/elasticsearch/2018-06-10-04/","excerpt":"","text":"Cat Api ES提供的 Cat Api 可以用来查看 集群当前状态, 涉及到 shard/node/cluster 几个层次 尝试使用 GET /_cat/health?v 查看 时间戳、集群名称、集群状态、集群中节点的数量 等等 12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1540815645 20:20:45 elasticsearch yellow 1 1 6 6 0 0 6 0 - 50.0% 返回信息 和 集群健康API(GET _cluster/health) 返回都一样 索引文档ES 中可以使用 POST 或 PUT 来索引一个新文档, 熟悉HTTP协议的话, 应该知道 PUT是幂等的, 而POST是非幂等的, ES也遵循了这一点 PUT PUT 创建文档的时候需要手动设定文档ID (类似已知id, 进行修改) 如果文档不存在, 则会创建新文档; 如果文档存在, 则会覆盖整个文档 (所以需要留意) 虽然使用PUT可以防止POST非幂等引起的多次创建, 但也要留意使用PUT带来的文档覆盖问题 练习: 12345678910111213141516171819202122# 此处创建一个 索引为 products , 类型为 computer, 文档ID为1的商品 PUT /products/computer/1&#123; &quot;name&quot; : &quot;lenovo&quot;, &quot;desc&quot; : &quot;lianxiang diannao chaobao&quot;, &quot;price&quot; : 4500, &quot;tag&quot; : [&quot;jieneng&quot;, &quot;xuhang&quot;, &quot;chaobao&quot;] &#125;# 返回&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, # 表示应该写入的有两个分片(1个主分片和1个副本分片, 但注意: 这里代表的可不是总分片数, 显然es的索引默认对应5个主分片, 每个主分片又对应一个副本分片, 总共会有10个分片) &quot;successful&quot;: 1, # 表示成功写入一个分片, 即写入了主分片, 但是副本分片并未写入, 因为目前只启了一个节点 &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 另外, 注意: 使用PUT创建文档时, 如果不指定ID, 则会报错 POST POST 创建文档时不需要手动传递文档ID, es会自动生成全局唯一的文档ID 练习 12345678910111213141516171819202122POST /products/computer/&#123; &quot;name&quot; : &quot;huawei&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;] &#125;# 返回, 可以看到文档ID是自动生成的, 其他字段和使用`PUT`时返回的信息相同&#123; &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;AWa_MgAhWC0s-aachNUS&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 检索文档先尝试最简单的一种 query-string 查询方式: GET /products/computer/_search : 查询/products/computer/下的所有文档 更新文档 PUT、POST PUT 对整个文档进行覆盖更新 1234PUT /products/computer/2&#123; &quot;name&quot; : &quot;acer-hongji&quot;&#125; partial update: 如果只是想更新文档的部分指定字段, 可以使用 POST 结合 _update : (partial update内置乐观锁并发控制) 123456POST /products/computer/2/_update?retry_on_conflict=5&#123; &quot;doc&quot;: &#123; &quot;name&quot; : &quot;acer-hongji-鸿基&quot; &#125;&#125; 这里注意一下_update的内部机制其实是: es先获取整个文档, 然后更新部分字段, 最后老文档标记为deleted, 然后创建新文档此时在标记老文档为deleted时就可能会出现并发问题, 如果线程1抢先一步将老文档标注为deleted, 那么线程2在将新文档标注为deleted时就会失败(version内部乐观锁机制)此时在es内部会做处理, 他内部完成了对乐观锁的实现, 如果失败后, 其实也是进行重试, 你可以手动传递 retry_on_conflict参数来决定其内部的重试次数 PUT如何只创建不替换: 由于创建文档与全量替换文档的语法是一样的, 都是 PUT, 而有时我们只是想新建文档, 不想替换文档 可以使用 op_type=create 来说明此命令只是用来执行创建操作的PUT /index/type/id?op_type=create 或 PUT /index/type/id/_create 可以看到, 此时, 如果文档已经存在, 会进行报错提示冲突, 而不会帮你直接替换1234567PUT /products/computer/1?op_type=create&#123; &quot;name&quot; : &quot;huawei create&quot;, &quot;desc&quot; : &quot;china best diannao gaoqing create&quot;, &quot;price&quot; : 6080, &quot;tag&quot; : [&quot;gaoxingneng&quot;, &quot;gaoqing&quot;, &quot;jieneng&quot;, &quot;create&quot;] &#125; 删除文档 ES的文档替换: 上面已经了解过, 其实就是PUT创建文档, 如果传递的文档id不存在, 就是创建, 如果文档id已经存在, 则是替换操作; 注意: es在做文档的替换操作时, 会将老的document标记为deleted, 然后新增我们给定的那个document, 当后续创建越来越多的document时, es会在适当的时机在后台自动删除标记为delete的document; ES的删除: 不会直接进行物理删除, 而是在数据越来越多的时候, es在合适的时候在后台进行删除 练习: 123456789101112131415DELETE /products/computer/2# 返回&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;products&quot;, &quot;_type&quot;: &quot;computer&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"03. ES 一些基本概念","slug":"elasticsearch/2018-06-09-03","date":"2018-06-09T10:23:07.000Z","updated":"2018-11-11T05:49:17.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-03/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-03/","excerpt":"","text":"近实时 从文档被索引到可以被检索会有轻微延时, 约1s Index(索引 n) 这里的Index是个名词, 类似于传统RDS的一个数据库, 是存储document的地方 一个Index可以包含多个 type (索引的复数词为 indices 或 indexes) index 名称必须是小写, 不能用下划线开头, 不能包含逗号 一般将不同的项目数据放到不同的index中 每个index会物理地对应多个分片, 这样, 每个项目都有自己的分片, 互相物理地独立开, 如果有项目是做复杂运算的, 也不会影响其他项目的分片 索引(v) : ES中的还会提到 索引一个文档, 这里的 索引 是动词, 存储文档并建立倒排索引的意思; Type(类型) 一个Index中可以有多个type 代表document属于index中的哪个类别(type 可以对同一个index中不同类型的document进行逻辑上的划分,可以粗略地理解成传统数据库中的数据表?) 名称可以是大小写, 不能用下划线开头, 不能包含逗号 注意: type是对index做的逻辑划分, 而shard是对index做的物理划分 Document(文档) ES中的最小数据单元, ES使用 JSON 作为文档的序列化格式 (ES中的文档可以通俗地理解成传统数据库表中的一条记录) _id: 文档id 可以手动指定, 也可以由es为我们生成; 手动指定id: 根据应用情况来判断是否符合手动指定 document id, 一般如果是从某些其他的系统中导入数据到es, 就会采用这种方式, 就是使用系统中已有的数据的唯一标识作为es中的document的id;比如从数据库中迁移数据到es中, 就比较适合采用数据在数据库中已有的primary key;put /index/type/id 自动生成id: 如果说我们目前要做的系统主要就是将数据存储到es中, 数据产生出来以后直接就会存放到es, 所以不需要手动指定document id的形式, 可以直接让es自动生成id即可;post /index/typees自动生成的id长度为20个字符, URL安全, base64编码, GUID, 分布式并行生成时, es会通过全局id来保证不会发生冲突; Cluster(集群) 集群是由一个或者多个拥有相同 cluster.name 配置项的节点组成, 一个ES节点属于哪个集群, 是由其配置中的 cluster.name 决定的; 节点启动后, 其默认name是elasticsearch, 因此如果在一个机器中启动一堆节点, 那它们会自动组成一个es集群(因为它们的cluster.name都是elasticsearch) 这些节点共同承担数据和负载的压力; 当有节点加入集群中或者从集群中移除节点时, 集群将会重新平均分布所有的数据; Shard(分片): type是对index做的逻辑划分, 而shard是对index做的物理划分 一个分片就是一个 Lucene 的实例, 它是一个底层的工作单元, 其本身就是一个完整的搜索引擎; 分片是数据的容器, 文档其实是保存在分片中的: 当我们将很多条document数据添加到索引中时, 索引实际上是指向一个或者多个物理分片; 因此, 你要存储到索引中的数据其实会被分发到不同的分片中, 而每个分片也仅保存了整个索引中的一部分文档; 当你的集群规模扩大或者缩小时(即增加或减少节点时), ES 会自动的在各节点中迁移分片, 而数据是存放在shard中的, 所以最终会使得数据仍然均匀分布在集群里 shard 可以分为 primary shard(主分片), replica shard(副本分片) replica shard 可以容灾, 水平扩容节点时, 还可以自动分配来提高系统负载 默认情况下, 每个index有5个parimary shard, 而每个parimary shard都有1个replica shard, 即每个index默认会对应10个shard 另外, ES规定了, 每个index的 parimary shard 和 replica shard 不能在全部都在同一个节点上, 相同内容的 replica shard 也不能在同一节点上, 不然起不到容灾作用; 集群状态 yellow 在ES中, 每个索引可能对应多个主分片, 每个主分片也都可能对应多个副本分片 对于每个索引, 要保证不会导致es集群为 yellow, 需要注意: es节点数 &gt;= number_of_replicas+1 当索引的 `number_of_replicas=1` 时, 无论 `number_of_shards` 为多少, 2个节点 (`es节点数 = number_of_replicas+1`) 就可以保证集群是 green; 当索引的 `number_of_replicas&gt;1` 时, 只有当 `es节点数 = number_of_replicas+1` 时, 集群才会变为green; 对于任何一个索引, 由于任何具有相同内容的分片(相同主分片的两个副本分片, 或者主分片和其某个副本分片)不会被放在同一个节点上, 所以如果节点数量不够的话, 有些replica-shard分片会处于未分配状态, 集群状态就不可能是green而是yellow; 比如索引 test 有 3个主分片, 每个主分片对应3个副本分片(该索引总共 3+3*3=12 个分片), 那么至少得4(number_of_replicas+1)个节点, 才能保证每个节点上都不会出现具有相同内容的分片, 即可以保证集群是green;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"02. ES 版本选择及简单安装","slug":"elasticsearch/2018-06-09-02","date":"2018-06-09T06:56:01.000Z","updated":"2018-11-11T05:48:59.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-02/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-02/","excerpt":"","text":"版本选择 ES 的版本迭代比较快, 目前(06/2018)为止, 已经到6.X了, 可参考官网文档, 可能很多公司还在用2.X, 或者刚切到5.X; 此处之所以选用5.5.3来学习调研, 主要是因为公司选用的阿里云服务提供的是 ES 5.5.3版本 (所以你在选择版本时, 也可以根据 自建、购买云服务 来决定) 安装 安装Java, 推荐使用Java 8 : yum install java-1.8.0-openjdk* -y ES 下载 123456$ cd /usr/local/src$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz$ tar -zxvf elasticsearch-5.5.3.tar.gz$ cd elasticsearch-5.5.3$ lsbin config lib LICENSE.txt modules NOTICE.txt plugins README.textile 启动 ES: es不能使用root权限启动, 所以需要创建新用户 123456$ adduser es$ passwd es$ chown -R es /usr/local/src/elasticsearch-5.5.3/$ cd /usr/local/src/elasticsearch-5.5.3/bin$ su es$ ./elasticsearch 验证es是否安装成功 可以在浏览器中打开 127.0.0.1:9200 (此处使用的是vagrant设定了虚拟主机的ip, 所以访问 http://192.168.3.200:9200/, 不过有些小坑下面会介绍 ) 或者可以 curl -X GET http://192.168.3.200:9200 启动坑点启动可能会报一些错(调研使用的是 centos7-minimal 版) 每个进程最大同时打开文件数太小 123456789101112131415[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]``` 解决方案: 切换到root, 可通过下面2个命令查看当前数量``` $ ulimit -Hn4096$ ulimit -Sn1024// 编辑如下文件vi /etc/security/limits.conf// 增加如下两行配置* soft nofile 65536* hard nofile 65536 elasticsearch用户拥有的内存权限太小, 至少需要262144 12ERROR: [1] bootstrap checks failed[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方案, 切换到root 123vi /etc/sysctl.conf 添加 vm.max_map_count=262144执行 sysctl -p 默认9200端口是给本机访问的, 因此es在成功启动后, 如果使用 192.168.3.200:9200 来访问, 可能失败, 因此需要在es配置文件elasticsearch.yml中增加 network.bind_host: 0.0.0.0, 重启后则可以正常访问 12345678910111213&#123; &quot;name&quot; : &quot;rjAFeY9&quot;, # node 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, # 节点默认的集群名称 (可以在es节点的配置文件elasticsearch.yml中进行配置) &quot;cluster_uuid&quot; : &quot;zaJApkNPRryFohhEMEVH5w&quot;, &quot;version&quot; : &#123; # es 版本号 &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 上面未解释的信息暂时先不用了解 如果想启动多个结点, 还可能会报如下几个错 尝试启动第二个节点, 报错 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000080000000, 174456832, 0) failed; error=&apos;Cannot allocate memory&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 174456832 bytes for committing reserved memory.# An error report file with more information is saved as:# /usr/local/src/elasticsearch-5.5.3/bin/hs_err_pid8651.log 解决方案: 其实这是因为我给虚拟机分配了2G的内存, 而elasticsearch5.X默认分配给jvm的空间大小就是2g, 所以jvm空间不够, 修改jvm空间分配 1234567vi /usr/local/src/elasticsearch-5.5.3/config/jvm.options将:-Xms2g-Xmx2g修改为:-Xms512m-Xmx512m 再次启动又报错 123...maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])... 解决方案: 在 elasticsearch.yml 配置文件最后添加 node.max_local_storage_nodes: 256, 然后重新添加第二个节点 Elasticsearch Head 安装es 启动后, 访问 127.0.0.1:9200 可以查看版本和集群相关的信息, 但如果能有一个可视化的环境来操作它可能会更直观一些, 可以通过安装 Elasticsearch Head 这个插件来进行管理;Elasticsearch Head 是集群管理、数据可视化、增删改查、查询语句可视化工具, 在最新的ES5中安装方式和ES2以上的版本有很大的不同, 在ES2中可以直接在bin目录下执行 plugin install xxxx 来进行安装, 但是在ES5中这种安装方式变了, 要想在ES5中安装则必须要安装NodeJs, 然后通过NodeJS来启动Head, 具体过程如下: nodejs 安装 123// 更新node.js各版本yum源(Node.js v8.x)curl --silent --location https://rpm.nodesource.com/setup_8.x | bash -yum install -y nodejs github下载 Elasticsearch Head 源码 1234cd /usr/local/srcgit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm install // (可能会有一些警告) 修改Elasticsearch配置文件, 编辑 elasticsearch-5.5.3/config/elasticsearch.yml, 加入以下内容: 12http.cors.enabled: true // 注意冒号后面要有空格http.cors.allow-origin: &quot;*&quot; 编辑elasticsearch-head-master文件下的Gruntfile.js, 修改服务器监听地址, 增加hostname属性, 将其值设置为 * : 123456789101112vi elasticsearch-head/Gruntfile.jsconnect: &#123; hostname: &quot;*&quot;, // 此处 server: &#123; options: &#123; port: 9100, base: &apos;.&apos;, keepalive: true &#125; &#125;&#125; 编辑elasticsearch-head-master/_site/app.js, 修改head连接es的地址，将localhost修改为es的IP地址 (注意:如果ES是在本地,就不要修改,默认就是localhost) 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://localhost:9200&quot;; 在启动elasticsearch-head之前要先启动elasticsearch, 然后在elasticsearch-head-master/目录下运行启动命令 1npm run start 最后验证 http://192.168.3.200:9100/ Kibana安装Kibana 是一个开源的分析和可视化平台, 属于 Elastic stack 技术栈中的一部分, Kibana 主要提供搜索、查看和与存储在 Elasticsearch 索引中的数据进行交互的功能, 开发者或运维人员可以轻松地执行高级数据分析, 并在各种图表、表格和地图中可视化数据;接下来主要就是使用Kibana的DevTools提供的控制台进行ES的学习 下载, 此处选择了5.5.3 12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gztar -zxvf kibana-5.5.3-linux-x86_64.tar.gz 修改config/kibana.yml文件, 加入以下内容: 1234server.port: 5601 server.name: &quot;kibana&quot; server.host: &quot;0.0.0.0&quot; elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 然后启动kibana服务: 12 cd /usr/local/src/kibana-5.5.3-linux-x86_64/bin./kibana 浏览器访问地址:http://192.168.3.200:5601/ DevTools 与 5.x之前版本的Sense Sense 是一个 Kibana 应用它提供交互式的控制台, 通过你的浏览器直接向 Elasticsearch 提交请求, 操作es中的数据 现在不用安装了, 可以直接使用Kibana提供的 DevTools 注意此时, 之前的es集群变成yellow状态了 (因为kibana有个副本分片并没有处于正常状态, 因为当前只有一个节点, 副本分片无法被分配到其他节点, 具体细节先不用着急, 后面会进行分析) 小结到此为止, 应该对ES有了最基础的了解, 且基本环境已经安装完毕, 对于后续的练习暂时就够了","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"01. 初识 Elasticsearch","slug":"elasticsearch/2018-06-09-01","date":"2018-06-09T06:24:25.000Z","updated":"2018-11-11T05:48:36.000Z","comments":true,"path":"2018/06/09/elasticsearch/2018-06-09-01/","link":"","permalink":"http://blog.renyimin.com/2018/06/09/elasticsearch/2018-06-09-01/","excerpt":"","text":"可以通过如下几个特点来认识ES: 开源 基于 Lucene, 提供比较简单的Restful APILucene 可以说是当下最先进、高性能、全功能的搜索引擎库, 由Apache软件基金会支持和提供(更多细节自行了解)但Lucene非常复杂, ES的目的是使全文检索变得简单, 通过隐藏 Lucene 的复杂性, 取而代之的提供一套简单一致的 RESTful API 高性能全文检索和分析引擎, 并可根据相关度对结果进行排序 可以快速且 近实时 地存储,检索(从文档被索引到可以被检索只有轻微延时, 约1s)以及分析 海量数据检索及分析: 可以扩展到上百台服务器, 处理PB级 结构化 或 非结构化 数据 面向文档型数据库, 存储的是整个对象或者文档, 它不但会存储它们, 还会为它们建立索引 应用场景 当你的应用数据量很大, 数据结构灵活多变, 数据之间的结构比较复杂, 如果用传统数据库, 可能不仅需要面对大量的表设计及数据库的性能问题, 此时可以考虑使用ES, 它不仅可以处理非结构化数据, 而且可以帮你快速进行扩容, 承载大量数据; 具体比如多数据源聚合大列表页: 微服务架构是目前很多公司都采用的架构, 所以经常会面对 多数据源聚合的 大列表页, 一个列表中的筛选字段,展示字段可能会来自多个服务, 同时涉及到分页, 所以传统方案可能比较吃力, 而且也得不到比较好的效果; (RRC这边目前是使用 ES 做 数据视图服务, 对这种大列表页所用到的数据源字段做统一配置和聚合) 日志数据分析, RRC 使用 ElasticStack 技术栈来很方便地对各服务的日志进行查询,分析,统计; 站内搜索(电商, 招聘, 门户 等等)都可以使用 ES 来做全文检索并根据相关性进行排名, 高亮展示关键词等;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://blog.renyimin.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch(一)","slug":"Elasticsearch-一","permalink":"http://blog.renyimin.com/tags/Elasticsearch-一/"}]},{"title":"02. 环境准备","slug":"swoole/2017-11-03-02","date":"2017-11-03T10:36:52.000Z","updated":"2018-11-11T06:23:52.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-02/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-02/","excerpt":"","text":"","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]},{"title":"01. swoole简介 及 知识扫盲","slug":"swoole/2017-11-03-01","date":"2017-11-03T02:45:17.000Z","updated":"2018-11-11T06:23:14.000Z","comments":true,"path":"2017/11/03/swoole/2017-11-03-01/","link":"","permalink":"http://blog.renyimin.com/2017/11/03/swoole/2017-11-03-01/","excerpt":"","text":"简介引用swoole官网对swoole的介绍 Swoole:面向生产环境的 PHP 异步网络通信引擎 使 PHP 开发人员可以编写高性能的异步并发 TCP、UDP、Unix Socket、HTTP，WebSocket 服务。Swoole 可以广泛应用于互联网、移动通信、企业软件、云计算、网络游戏、物联网（IOT）、车联网、智能家居等领域。 使用 PHP + Swoole 作为网络通信框架，可以使企业 IT 研发团队的效率大大提升，更加专注于开发创新产品。 特性 Swoole 使用纯 C 语言编写，提供了 PHP 语言的异步多线程服务器，异步 TCP/UDP 网络客户端，异步 MySQL，异步 Redis，数据库连接池，AsyncTask，消息队列，毫秒定时器，异步文件读写，异步DNS查询。 Swoole内置了Http/WebSocket服务器端/客户端、Http2.0服务器端。 除了异步 IO 的支持之外，Swoole 为 PHP 多进程的模式设计了多个并发数据结构和IPC通信机制，可以大大简化多进程并发编程的工作。其中包括了并发原子计数器，并发 HashTable，Channel，Lock，进程间通信IPC等丰富的功能特性。 Swoole2.0 支持了类似 Go 语言的协程，可以使用完全同步的代码实现异步程序。PHP 代码无需额外增加任何关键词，底层自动进行协程调度，实现异步。 典型应用场景 移动互联网API服务器 物联网(IOT) 微服务(Micro Service) 高性能Web服务器 游戏服务器 在线聊天系统 很多用户案例可以参考 https://wiki.swoole.com/wiki/page/p-case.html","categories":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/categories/Swoole/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.renyimin.com/tags/Swoole/"}]}]}