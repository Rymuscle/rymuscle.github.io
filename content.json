{"meta":{"title":"Lant's Blog","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"<HTTP权威指南>学习--第17章 内容协商与转码 (300)","slug":"http/2017-12-06-HTTP-06-内容协商与转码 ","date":"2017-12-06T10:50:27.000Z","updated":"2018-02-06T14:30:24.000Z","comments":true,"path":"2017/12/06/http/2017-12-06-HTTP-06-内容协商与转码 /","link":"","permalink":"http://blog.renyimin.com/2017/12/06/http/2017-12-06-HTTP-06-内容协商与转码 /","excerpt":"","text":"前言 一个URL常常需要代表若干不同的资源: 例如那种需要以多种语言提供其内容的网站站点。 如果某个站点有 ‘说法语的’ 和 ‘说英语的’ 两种用户, 它可能想用这两种语言提供网站站点信息; 理想情况下，服务器应当向英语用户发送英文版，向法语用户发送法文版; 而用户只要访问网站主页就可以得到相应语言的内容。 HTTP提供了 内容协商 方法，允许客户端和服务器作这样的决定。 通过这些方法，单一的URL就可以代表不同的资源(比如，同一个网站页面的法语版和英语版)，这些不同的版本称为变体。 除了根据 内容协商 来决定URL代表的是那种版本的资源。另外, 对于有些特定的URL来说, 服务器还可以根据一些其他原则来决定发送什么内容给客户端最合适。在有些场合下, 服务器甚至可以自动生成定制的页面。比如，服务器可以为手持设备把HTML页面转换成WML页面，这类动态内容变换被称为转码。这些变换动作是HTTP客户端和服务器之间进行内容协商的结果。 内容协商技术 共有3种不同的方法可以决定服务器上哪个页面最适合客户端: 让客户端来选择, 服务器自动判定, 或 让中间代理来选。这3种技术分别称为客户端驱动的协商、服务器驱动的协商 以及 透明协商 内容协商技术摘要如下: 客户端驱动 (300)1.对于服务器来说，收到客户端请求时只是发回响应，在其中列出可用的页面，让客户端决定要看哪个，这是最容易的事情。 很显然，这是服务器最容易实现的方式，而且客户端很可能选择到最佳的版本(只要列表中有让客户端选择的足够信息)。 不利之处是每个页面都需要两次请求: 第一次获取列表，第二次获取选择的副本。这种技术速度很慢且过程枯燥乏味，让用户厌烦。 2.从实现原理上来说，服务器实际上有两种方法为客户端提供选项: 一是发送回一个HTML文档，里面有到该页面的各种版本的链接和每个版本的描述信息; 另一种方法是发送回HTTP/1.1响应时，使用 300 Multiple Choices 响应代码。客户端浏览器收到这种响应时，在前一种情况下(发回html文档的情况)，会显示一个带有链接的页面; 在后一种情况下，可能会弹出对话窗口，让用户做选择。不管怎么样，决定是由客户端的浏览器用户作出的 3.除了增加时延并且对每个页面都要进行繁琐的多次请求之外, 这种方法还有一个缺点: 它需要多个URL, 公共页面要一个, 其他每种特殊页面也都要一个。 服务器驱动1.之前已经知道了客户端驱动的协商存在的若干缺点。大部分缺点都涉及客户端和服务器之间通信量的增长, 这些通信量用来决定什么页面才是对请求的最佳响应。 2.而减少额外通信量的一种方法是让服务器来决定发送哪个页面回去，但为了做到这一点，客户端必须发送有关客户偏好的足够信息，以便服务器能够作出准确的决策。服务器通过 客户端请求的首部集 来获得这方面的信息(客户偏好)!! 有以下两种机制可供HTTP服务器评估发送什么响应给客户端比较合适： 检査 客户端请求中的内容协商首部集: 服务器察看客户端发送的 Accept内容协商首部集, 设法用相应的响应首部与之匹配; 根据其他(非内容协商)首部进行变通, 例如，服务器可以根据客户端发送的 User-Agent 首部来发送响应 客户端内容协商首部集1.客户端可以用下面列出的HTTP首部集发送用户的偏好信息 Accept : 告知服务器发送何种媒体类型Accept-Language : 告知服务器发送何种语言Accept-Charset : 告知服务器发送何种字符集Accept-Encoding : 告知服务器采用何种编码 2.实体首部集 和 内容协商首部集 注意: 内容协商首部集与实体首部非常类似(比如 Accept-Encoding 和 Content-Encoding)。不过, 这两种首部的用途截然不同: 实体首部集,像运输标签,它们描述了把报文从服务器传输给客户端的过程中必须的各种报文主体属性; 如下列出的实体首部集来匹配客户端的Accept内容协商首部集 12345Accet首部 实体首部Accept Content-TypeAccept-Language Content-LanguageAccept-Charset Content-TypeAccept-Encoding Content-Encoding （由于HTTP是无状态的协议，表示服务器不会在不同的请求之间追踪客户端的偏好，所以客户端必须在每个请求中都发送其偏好信息） 而内容协商首部集是由客户端发送给服务器用来告知其偏好信息的, 以便服务器可以从文档的不同版本中选择出最符合客户端偏好的那个来提供服务; 3.如果两个客户端都发送了 Accept-Language 首部来描述它们感兴趣的语言信息, 服务器就能够决定发送www.joes-hardware.com的何种版本给哪个客户端了。让服务器选择发送回去的文档，减少了往返通信的时延，这种时延是客户端驱动模型中无法避免的。 4.然而, 假设某个客户端偏好西班牙文，那服务器应当回送哪个版本的页面呢？英语还是法语？服务器只有两种选择：猜测 或 回退到客户端驱动模型,问客户端选择哪个。假如这个西班牙人碰巧懂一点英语，他可能会选择英文页面，这不是最理想的，但它能解决问题。在这种情况下，这个西班牙人需要有办法传达更多与其偏好有关的信息，也就是他的确对英语略知一二，在没有西班牙语的时候，英语也行。 5.幸运的是，HTTP提供了一种机制，可以让与这个西班牙人情况类似的客户端更详细地描述其偏好。这种机制就是质量值(简称q值) 内容协商首部中的质量值1.HTTP协议中定义了质量值，允许客户端为每种偏好类别列出多种选项，并为每种偏好选项关联一个优先次序。 例如，客户端可以发送下列形式的Accept-Language首部：Accept-Language: en; q=0.5, fr; q=0.0 , nl; q=1.0, tr; q=0.0 其中q值的范围从0.0-1.0(0.0是优先级最低的，而1.0是优先级最高的)。 上面列出的那个首部，说明该客户端最愿意接收荷兰语(缩写为nl)文档，但英语(缩写为en)文档也行; 无论如何，这个客户端都不愿意收到法语(缩写为fr)或土耳 其语(缩写为tr)的版本; 2.注意: 偏好的排列顺序并不重要，只有与偏好相关的q值才是重要的; 客户端其它请求首部集1.服务器也可以根据客户端其他请求首部集来匹配响应, 比如 User-Agent 首部。例如, 服务器知道老版本的浏览器不支持JavaScript语言，这样就可以向其发送不含有JavaScript的页面版本。 2.在这种情况下，没有q值机制可供査找”最近似”的匹配。服务器或者去找完全匹配，或者简单地有什么就给什么，这取决于服务器的实现。 3.由于缓存需要尽力提供所缓存文档中正确的”最佳”版本，HTTP协议定义了服务器在响应中发送的 Vary 首部。这个首部告知缓存,客户端,和所有下游的代理, 服务器根据哪些首部来决定发送响应的最佳版本。 透明协商(vary首部)1.了支持透明内容协商，服务器必须有能力告知代理，服务器需要检査哪些请求首部，以便对客户端的请求进行最佳匹配。但是HTTP/1.1规范中没有定义任何透明协商机制, 不过却定义了 Vary 首部。服务器在响应中发送了Vary首部，以告知中间节点需要使用哪些请求首部进行内容协商2.代理缓存可以为通过单个URL访问的文档保存不同的副本, 如果服务器把它们的决策过程传给代理,这些代理就能代表服务器与客户端进行协商。（缓存同时也是进行内容转码的好地方，因为部署在缓存里的通用转码器能对任意服务器，而不仅仅是一台服务器传来的内容进行转码） 3.对内容进行缓存的时候是假设内容以后还可以重用。然而，为了确保对客户端请求回送的是正确的已缓存响应, 缓存必须应用服务器在回送响应时所用到的大部分决策逻辑; 4.之前我们已经了解了客户端发送的Accept内容协商首部集; 也了解到, 为了给每条请求选择最佳的响应, 服务器使用了哪些与这些首部集匹配的相应实体首部集。其实, 代理缓存也必须使用相同的首部集来决定回送哪个已缓存的响应。 5.下图展示了涉及缓存的正确及错误的操作序列。 缓存把第一个请求转发给服务器，并存储其响应。 对于第二个请求，缓存根据URL査找到了匹配的文档。但是，这份文档是法语版的，而请求者想要的是西班牙语版的。如果缓存只是把文档的法语版本发给请求者的话，它就犯了错误; 像上面2中提到的, 代理缓存也必须要根据客户端发送来的内容协商首部来给客户端返回正确的响应 Vary首部1.下面是浏览器和服务器发送的一些典型的请求及响应首部: 2.然而, 如果服务器的决策不是依据Accept首部集，而是比如User-Agent首部的话，情况会如何？例如, 服务器可能知道老版本的浏览器不支持JavaScript语言, 因此可能会回送不包含JavaScript的页面版本。如果服务器是根据其他首部来决定发送哪个页面的话, 和Accept首部集一样, 缓存也必须知道这些首部是什么, 这样才能在选择回送的页面时做出同样的逻辑判断。 3.HTTP的 Vary 响应首部中列出了所有客户端请求首部, 服务器可用这些首部来选择文档或产生定制的内容(在常规的内容协商首部集之外的内容)。例如, 若所提供的文档取决于User-Agent首部, Vary首部就必须包含User-Agent; 小结 当新的请求到达时, 代理缓存会根据内容协商首部集来寻找最佳匹配。但在把文档提供给客户端之前, 它还必须检査服务器有没有在已缓存响应中发送Vary首部。 如果有Vary首部, 那么新请求中那些首部的值必须与旧的已缓存的响应的请求首部相同。(也就是说,代理缓存也会保存旧的请求的请求首部和响应首部, 下面一句话更加肯定这一点) 因为服务器可能会根据客户端请求的首部来改变响应, 为了实现透明协商, 代理缓存就必须为每个已缓存变体保存客户端请求首部和相应的服务器响应首部) 如果某服务器的Vary首部看起来像 Vary: User-Agent, Cookie 这样，大量不同的User-Agent和Cookie值将会产生非常多的变体, 而代理缓存必须为每个变体保存其相应的文档版本。当缓存执行査找时，首先会对内容协商首部集进行内容匹配，然后比较请求的变体与缓存的变体。如果无法匹配，缓存就从原始服务器获取文档 转码 我们已经讨论了一个机制, 该机制可以让客户端和服务器从某个URL的一系列文档中挑选出最适合客户端的文档。但是, 实现这些机制的前提是，存在一些满足客户端需求的文档—不管是完全满足还是在一定程度上满足; 然而, 如果服务器没有能满足客户端需求的文档会怎么样呢？服务器可以给出一个错误响应。但理论上，服务器可以把现存的文档转换成某种客户端可用的文档, 这种选项称为转码; 下面列出了一些假设的转码 1234567转换之前 转换之后HTML文档 WML文档高分辨率图像 低分辨率图像彩色图像 黑白图像有多个框架的复杂页面 没有很多框架或图像的简单文本页面有Java小应用程序的HTML页面 没有Java小应用程序的HTML页面有广告的页面 去除广告的页面 有3种类别的转码: 格式转换、信息综合以及内容注入 格式转换 格式转换是指将数据从一种格式转换成另一种格式, 使之可以被客户端査看。通过HTML到WML的转换, 无线设备就可以访问通常供桌面客户端査看的文档了。通过慢速连接访问Web页面的客户端并不需要接收高分辨率图像, 如果通过格式转换降低图像分辨率和颜色来减小图像文件大小的话, 这类客户端就能更容易地査看图像比较丰富的页面了。 格式转换可以由如下内容协商首部集来驱动, 但也能由 User-Agent 首部来驱动。注意: 内容转换或转码 与 内容编码 或 传输编码 是不同的, 后两者一般用于更高效或安全地传输内容, 而前两者则可使访问设备能够査看内容; Accet首部 实体首部 Accept Content-Type Accept-Language Content-Language Accept-Charset Content-Type Accept-Encoding Content-Encoding 信息综合 从文档中提取关键的信息片段称为信息综合(information synthesis), 这是一种有用的转码操作。这种操作的例子包括根据小节标题生成文档的大纲，或者从页面中删除广告和商标 根据内容中的关键字对页面分类是更精细的技术, 有助于总结文档的精髓。这种技术常用于Web页面分类系统中，比如门户网站的Web页面目录 内容注入参见P423 转码与静态预生成的对比 转码的替代做法是在Web服务器上建立Web页面的不同副本, 例如一个是HTML, 一个是WML, 一个图像分辨率高，一个图像分辨率低；一个有多媒体内容，一个没有。 但是，这种方法不是很切合实际，原因很多： 某个页面中的任何小改动都会牵扯很多页面，需要很多空间来存储各页面的不同版本，而且使页面编目和Web服务器编程(以提供正确的版本)变得更加困难。 有些转码操作，比如广告插入(尤其是定向广告插入)，就不能静态实现, 因为插入什么广告和请求页面的用户有关 对单一的根页面进行即时转换，是比静态的预生成更容易的解决方案。 但这样会在提供内容时增加时延。不过有时候其中一些计算可以由第三方进行，这样就减少了Web服务器上的计算负荷——比如可以由代理或缓存中的外部Agent完成转换 下图显示了在代理缓存中进行的转码 参考","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP -- 实体和编码 (400, 411, 206)","slug":"http/2017-12-04-HTTP-04-Entity-Header-Fields ","date":"2017-12-04T04:50:27.000Z","updated":"2018-02-07T11:35:54.000Z","comments":true,"path":"2017/12/04/http/2017-12-04-HTTP-04-Entity-Header-Fields /","link":"","permalink":"http://blog.renyimin.com/2017/12/04/http/2017-12-04-HTTP-04-Entity-Header-Fields /","excerpt":"","text":"主要讲解一些实体首部字段的相关知识 前言1.要知道, 每天都有数以万计的各种媒体对象经由HTTP传送, 如图像, 文本, 影片以及软件程序等。只要你能叫出名字, HTTP就可以传送。2.更重要的是: HTTP还会确保它的报文被正确传送, 识别, 提取以及适当处理 3.具体来说, HTTP要确保它所承载的”货物”满足以下条件: 符合用户的需要(基于Accept系列的内容协商首部) 在网络上可以快速有效地传输(通过范围请求, 差异编码以及其他数据压缩方法) 完整到达, 未被篡改(通过传输编码首部和Content-MD5校验和首部) 可以被正确识别(比如:通过Content-Type首部说明媒体格式, Content-Language首部说明语言), 以便浏览器和其他客户端能正确处理内容。 可以被正确地解包(比如:通过Content-Length首部和Content-Encoding首部)。 是最新的(通过实体验证码和缓存过期控制, 这一点第7章 缓存相关知识点已经介绍过) 可以看到, HTTP要保证它所承载的货物完好的话, 会用到各种首部字段; 报文实体1.HTTP响应报文中的报文实体由 实体首部 和 实体主体 组成; 实体首部和实体主体之间以一个空白的CRLF行分隔; 实体主体中是原始数据, 所以需要 实体首部 来描述数据的意义;(例如 Content-Type实体首部 告诉我们数据的媒体格式…)。 2.如下一个简单的HTTP响应报文: 3.上面报文中: 实体首部Content-Type:text/plain指出这是一个纯文本文档 Content-Length首部指出它只有18个字节 一行空白(CRLF)把首部字段 和 主体的开始部分分隔开来 实体首部字段 HTTP/1.1几个基本实体首部字段如下123456789Content-Type : 实体中所承载对象的类型Content-Length 所传送实体主体的长度或大小Content-Encoding 对象数据所做的任意变换(比如,压缩)Content-Location 一个备用位置, 请求时可通过它获得对象Content-Range 如果这是部分实体, 这个首部说明它是整体的哪个部分Content-MD5 实体主体内容的校验和Last-Modified 所传输内容在服务器上创建或最后修改的日期时间Expires 实体数据将要失效的日期时间（为了兼容HTTP/1.0, 1.1中可以使用Cache-Control:max-age=..）Allow 该资源所允许的各种请求方法, 例如：GET和HEAD 实体主体1.实体主体就是原始数据, 所以需要实体首部告诉我们如何去解释数据(content-type是图像还是文本,content-encoding是已被压缩或者重编码)2.实体首部字段最后一以一个空白的CRLF行结束, 随后就是实体主体的原始内容, 不管内容是什么, 文本或二进制,文档或图像,压缩的或未压缩的, 英语,法语或日语,都紧随这个CRLF之后。 Content-Length1.实体首部 Content-Length 指示出报文主中实体主体的字节大小。 2.注意: 如果对文本文件进行了gzip压缩的话, Content-Length首部就是压缩后的大小, 而不是原始大小(后面在介绍内容编码过程的时候还会提及)。 某些HTTP应用程序在这方面搞错了, 发送的是数据编码之前的大小, 这会导致严重的错误, 尤其是用在持久连接上。 3.除非使用了分块编码, 否则 Content-Length 实体首部就是带有实体主体的报文必须使用的; 当然, 短连接自然也用不上content-length, 它依靠关闭连接就可以知道实体的大小 使用Content-Length首部是为了能够检测出服务器崩溃而导致的报文截尾; 并对共享持久连接的多个报文进行正确分段。 检测截尾1.HTTP早期版本采用关闭连接的办法来划定报文的结束; 但是, 没有 Content-Length 的话, 客户端无法区分到底是报文结束而导致正常的连接关闭, 还是在报文传输中由于服务器崩溃而导致的连接关闭; 所以客户端需要 Content-Length 实体首部, 来监测报文截尾 2.报文截尾的问题 对缓存服务器来说尤其严重, 如果缓存服务器收到被截尾的报文却没有识别出截尾的话, 它可能会存储不完整的内容并多次使用它来提供服务。 缓存代理服务器通常不会为没有显示 Content-Length 首部的HTTP主体做缓存, 以此来减小缓存截尾报文的风险。 Content-Length与持久连接1.Content-Length首部对于持久链接是必不可少的: 如果响应通过持久连接传送, 就可能有另一条HTTP响应紧随其后。客户端通过Content-Length首部就可以知道报文在何处结束,下一条报文从何处开始。因为连接是持久的, 客户端无法依赖连接关闭来判断报文的结束。所以需要有Content-Length来说明实体大小 2.有一种情况, 使用持久连接可以没有Content-Length首部，即采用分块编码(chunked encoding)时。 在分块编码的情况下, 数据是分为一系列的块来发送的, 每块都有大小说明。 哪怕服务器在生成首部的时候不知道整个实体的大小(通常是因为实体是动态生成的), 仍然可以使用分块编码传输若干已知大小的块。 确定实体长度的规则 Content-Length如果存在并且有效的话，则必须和消息内容的传输长度完全一致; 如果存在Transfer-Encoding(重点是chunked), 则在header中不能有Content-Length，有也会被忽视; 如果采用短连接，则直接可以通过服务器关闭连接来确定消息的传输长度。（这个很容易懂） 在Http 1.0及之前版本中，content-length字段可有可无。因为这之前都不支持长连接. 在http1.1及之后版本, 如果是keep alive，则content-length和chunk必然是二选一 若是非keep alive，则和http1.0一样。content-length可有可无. 为了和使用HTTP/1.0的应用程序兼容，任何带有实体主体的HTTP/1.1请求都必须带有正确的Content-Length首部字段(除非已经知道服务器兼容HTTP/1.1)HTTP/1.1规范中建议对于带有主体但没有Content-Length首部的请求，服务器如果无法确定报文的长度，就应当发送 400 Bad Request响应 或 411 Length Required 响应，后一种情况表明服务器要求收到正确的Content-Length首部; 实体摘要Content-MD5 数据的校验和 尽管HTTP通常都是在像TCP/IP这样的可靠传输协议之上实现的, 但仍有很多因素会导致报文的一部分在传输过程中被修改;比如有不兼容的转码代理, 或者中间代理有误等等; 为检测实体主体的数据是否被不经意地修改, 发送方可以在生成初始的主体时, 生成一个数据的 校验和, 这样接收方就可以通过检査这个校验和来捕获所有意外的实体修改了; 服务器使用Content-MD5首部, 来发送对实体主体运行MD5算法的结果。 只有产生响应的原始服务器可以计算并发送Content-MD5首部, 中间代理和缓存不应当修改或添加这个首部, 否则就会与验证端到端完整性的这个最终目的相冲突。 Content-MD5实体首部是在对内容做了所有需要的内容编码(content-encoding)之后, 还没有做任何传输编码(transfer-encoding)之前计算出来的。 为了验证报文的完整性, 客户端必须先进行传输编码的解码, 然后计算所得到的未进行传输编码的实体主体的MD5; 如果一份文档使用gzip算法进行压缩, 然后用分块编码发送, 那么就对整个经gzip压缩的主体进行MD5计算 一般不常用到Content-MD5首部 作为对HTTP的扩展, 在IETF的草案中提出了其他一些摘要算法。这些扩展建议增加新的 Want-Digest 首部, 它允许客户端说明期望响应中使用的摘要类型，并使用质量值来建议多种摘要算法并说明优先顺序; 媒体类型和字符集简介Content-Type Content-Type实体首部字段说明了实体主体的MIME类型。MIME类型是标椎化的名字, 用于说明作为运载实体的基本媒体类型(比如:HTML文件, Microsoft Word文档或是MPEG视频等)。客户端应用程序使用MIME类型来解释和处理其内容。 Content-Type 的值就是标椎化的MIME类型, 都在互联网号码分配机构IANA中注册。MIME类型由一个主媒体类型(比如:text,image或audio等)后面跟一条斜线一级一个子类型组成, 子类型用于进一步描述媒体类型。 后面会详细讨论MIME类型 要注意: Content-Type 实体首部, 说明的是原始实体主体的媒体类型, 例如, 经过内容编码的实体, Content-Type 首部说明的仍然是编码之前的实体主体的类型。 文本的字符编码 Content-Type首部还支持可选的参数来进一步说明内容的类型。charset(字符集)参数就是个例子, 它说明把实体中的比特转换为文本文件中的字符的方法:Content-type:text/html;charset=iso-8859-4 后面会详细讨论字符集 内容编码 - Content-EncodingHTTP应用程序有时在发送之前需要对内容进行编码。例如会把很大的HTML文档发送给通过慢速连接连上来的客户端之前, 服务器可能会对它进行压缩。 这样有助于减少传输实体的时间。 服务器还可以把内容搅乱或加密,以此来防止未经授权的第三方看到文档的内容。 这种类型的编码是在发送方(可能是服务器也可能是代理缓存,下篇文章会看到 “缓存同时也是进行内容转码的好地方” 这句话)应用到内容之上的,当内容经过内容编码之后, 编好码的数据就放在实体主体中,像往常一样发送给接收方。 内容编码过程 网站服务器生成原始响应报文, 其中有原始的 Content-Type 和 Content-Length首部; 内容编码服务器(也可能就是原始的服务器或下行的代理)创建编码后的报文。(编码后的报文Content-Type仍然和编码前相同, 但是Content-Length可能不同,比如主体被压缩了)。 注意: Content-Type实体首部还需要出现在报文中, 因为它说明了实体的原始格式, 一旦实体被编码, 要显示的时候 ， 可能还是需要该信息才行的。、 Content-Length是编码之后的主体长度 接收程序得到编码后的报文, 进行解码, 或得原始报文。 可以参考下图: 内容编码类型Content-Encoding HTTP定义了一些标准的内容编码类型, 并允许用扩展的形式添加更多的编码。由互联网号码分配机构(IANA)对各种编码进行标准化, 它给每个内容编码算法分配了唯一的代号。 Content-Encoding 实体响应首部就用这些标准化的代号来说明编码时使用的算法; 编码算法(Content-Encoding值)如下: 1234gzip 表明实体采用GNU zip编码compress 表明实体采用Unix的文件压缩程序deflate 表明实体是用zlib的格式压缩的identity 表明没有对实体进行编码。当没有Content-Encoding header时， 就默认为这种情况 gzip, compress, 以及deflate编码都是无损压缩算法，用于减少传输报文的大小，不会导致信息损失。 其中gzip通常效率最高， 使用最为广泛。 Accept-Encoding 为了避免服务器使用客户端不支持的编码方式, 客户端就把自己支持的内容编码方式列表放在请求的 Accept-Encoding 首部里面发出去。 如果HTTP请求中没有包含 Accept-Encoding 首部, 服务器可以假设客户端能够接受任何编码方式(等价于发送Accept-Encoding:*) 如下展示了HTTP事务中对 Accept-Encoding 请求首部的使用 Accept-Encoding 字段包含用逗号分隔的的支持编码的列表, 下面是一些例子 12345Accept-Encoding: compress,gzipAccept-Encoding: Accept-Encoding: *Accept-Encoding: compress;q=0.5, gzip;q=1.0Accept-Encoding: gzip;q=1.0, identity;q=0.5, *;q=0 传输编码与分块编码已经在通用首部字段中进行过介绍了更多参考P375 范围请求 - 断点续传accept-ranges, content-range, range HTTP允许客户端只请求文档的一部分或者说某个范围。假设你正通过慢速的调制解调器连接下载最新的热门软件, 已经下了四分之三, 忽然因为一个网络故障, 连接中断了。你已经为等待下载完成耽误了很久, 而现在被迫要全部重头再来, 那多倒霉。 有了范围请求, HTTP客户端可以通过请求曾获取失败的实体的一个范围(或者说一部分), 来恢复下载该实体。当然这有一个前提，那就是从客户端上一次请求该实体到这次发出范围请求的时段内，该对象在服务器中没有改变过 例如: 1234GET /bigfile.html HTTP/1.1Host: www.joes-hardware.com Range: bytes=4000-User-Agent: Mozilla/4.61 [en] (WinNT; I) 在本例中, 客户端请求的是文档开头4000字节之后的部分(不必给出结尾字节数, 因为请求方可能还不知道文档的大小)。（在客户端收到了开头的4000字节之后就失败的情况下, 可以使用这种形式的范围请求） 还可以用Rang请求首部字段来请求多个范围(这些范围可以按任意顺序给出, 也可以相互重叠)。 例如,假设客户端同时连接到多个服务器, 为了加速下载文档而从不同的服务器下载同一个文档的不同部分。 对于客户端在一个请求内请求多个不同范围的情况, 返回的响应也是单个实体, 不过它会有一个多部分主体及Content-Type:multipart/byteranges首部。 并不是所有服务器都接受范围请求，但很多服务器可以。 服务器可以通过在响应中包含响应首部字段Accept-Ranges, 向客户端说明可以接受的范围请求。这个首部的值是计算范围的单位，通常是以字节计算的。例如： 1234HTTP/1.1 200 0KDate: Fri, 05 Nov 2016 22:35:15 GMTServer: Apache/1.2.4Accept-Ranges: bytes 服务器响应中的 响应首部字段 accept-ranges告诉客户端可以使用范围请求 对应的 客户端请求的 请求首部字段range 可以告知服务器此次请求的范围 (可以是多个范围)接收到客户端请求中包含range首部字段的服务器, 会在处理请求之后返回状态码为 206 Partial Content的响应;无法处理该范围请求时,则会返回状态码 200 ok响应及全部资源。 服务器响应中的 Content-Range实体首部字段 能告诉客户端作为响应返回的实体的哪个部分符合范围请求。字段值以字节为单位(和服务器之前发回的accept-ranges首部字段说明的单位一样,都是字节)还会告知当前发送的部分及整个实体大小；1Content-Range: bytes 5001-10000/1000 下图展示了范围请求的一系列HTTP事务的例子 小结 Range首部在流行的点对点(Peer-to-Peer，P2P)文件共享客户端软件中得到广泛应用, 它们从不同的对等实体同时下载多媒体文件的不同部分。 注意, 范围请求也属于一类实例操控, 因为它们是在客户端和服务器之间针对特定的对象实例来交换信息的。也就是说, 客户端的范围请求仅当客户端和服务器拥有文档的同一个版本时才有意义。** 扩展 – 差异编码 我们曾把网站页面的不同版本看作页面的不同实例。如果客户端有一个页面的已过期副本, 就要请求页面的最新实例。如果服务器有该页面更新的实例, 就要把它发给客户端, 哪怕页面上只有一小部分发生了改变,也要把完整的新页面实例发给客户端 若改变的地方比较少, 与其发送完整的新页面给客户端, 客户端更愿意服务器只发送页面发生改变的部分, 这样就可以更快地得到最新的页面。 差异编码是HTTP协议的一个扩展, 它通过交换对象改变的部分而不是完整的对象来优化传输性能。 差异编码和范围请求一样, 也是一类实例操控, 因为它依赖客户端和服务器之间针对特定的对象实例来交换信息。 详解 下图清楚地展示了差异编码的结构:包括请求、生成、接收和装配文档的全过程。 客户端必须告诉服务器: 它有页面的哪个版本 它愿意接受页面最新版的差异(A-IM:diffe) 它懂得哪些将差异应用于现有版本的算法 服务器必须: 检査它是否有这个页面的客户端现有版本 计算客户端现有版本与最新版之间的差异(有若干算法可以计算两个对象之间的差异), 然后服务器必须计算差异, 发送给客户端, 告知客户端所发送的是差异(IM:diffe), 并说明最新版页面的新标识(ETag), 因为客户端将差异应用于其老版本之后就会得到这个版本 客户端在If-None-Match首部中使用的是它所持有页面版本的唯一标识, 这个标识是服务器之前响应客户端时在ETag首部中发送的。 客户端是在对服务器说:”如果你那里页面的最新版本标识和这个ETag不同, 就把这个页面的最新版本发给我”。 如果只有If-None-Match首部，服务器将会把该页面的最新版本完整地发给客户端。(假设最新版和客户端持有的版本不同) 不过, 如果客户端想告诉服务器它愿意接受该页面的差异, 那发送A-IM首部就可以了。 A-IM是Accept-Instance-Manipulation(接受实例操控)的缩写。 形象比喻的话, 客户端相当于这样说:”哦对了, 我能接受某些形式的实例操控, 如果你会其中一种的话, 就不用发送完整的文档给我了”。 在A-IM首部中, 客户端会说明它知道哪些算法可以把差异应用于老版本而得到最新版本, 服务端发送回下面这些内容: 一个特殊的响应代码 —— 226 IM Used, 告知客户端它正在发送的是所请求对象的实例操控, 而不是那个完整的对象自身; 一个IM(Instance-Manipulation的缩写)首部, 说明用于计算差异的算法; 新的 ETag 首部 Delta-Base 首部, 说明用于计算差异的基线文档的ETag(理论上, 它应该和客户端之前请求里的if-None-Match首部中的ETag相同) 下表总结了差异编码使用的首部 实例操控,差异生成器,差异应用器 客户端可以使用A-IM首部说明可以接受的一些实例操控的类型。(比如有diffe) 而服务器在IM首部中说明使用的是何种实例操控。(比如是diffe) 不过到底哪些实例操控类型是可接受的呢？它们又是做什么的呢？下表中列出了一些在IANA注册的实例操控类型 更多差异编码相关的 实例操控类型, 可以查看P384 差异编码可以减少传输次数，但实现起来可能比较麻烦。 设想一下页面改动频繁，而且有很多不同的人都在访问的情形。支持差异编码的服务器必须保存页面随时间变化的所有不同版本，这样才能指出最新版本与所请求的客户端持有的任意版本之间的差异 如果文档变化频繁，而且有很多客户端都在请求文档，那它们就会获得文档的不同实例。随后当它们再向服务器发起请求时，它们将请求它们所持有的版本与最新版本之间的差异。为了能够只向它们发送变化的部分，服务器必须保存所有客户端曾经持有过的版本 要降低提交文档时的延迟时间，服务器必须增加磁盘空间来保存文档的各种旧的实例。实现差异编码所需的额外磁盘空间可能很快就会将减少传输量获得的好处抵消掉 学习–第15章","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP -- cookie机制 (401)","slug":"http/2017-12-02-HTTP-06-cookie","date":"2017-12-02T02:27:36.000Z","updated":"2018-02-28T08:56:07.000Z","comments":true,"path":"2017/12/02/http/2017-12-02-HTTP-06-cookie/","link":"","permalink":"http://blog.renyimin.com/2017/12/02/http/2017-12-02-HTTP-06-cookie/","excerpt":"","text":"前言 HTTP最初是一个 匿名, 无状态 的请求/响应协议; 服务器处理来自客户端的请求, 然后向客户端回送一条响应。Web服务器几乎没有什么信息可以用来判断是哪个用户发送的消息, 也无法记录来访用户的请求序列。 对于现代的Web站点我们, 我们知道, Web服务器可能会同时与成千上万个不同的客户端进行对话, 这些服务器通常都要记录下它们与谁交谈, 而不会认为所有的请求都来自匿名的客户端。因为现代的Web站点希望能够提供个性化的接触, 它希望对连接另一端的用户有更多的了解, 并且能在用户浏览页面时对其进行跟踪。 比如, Amazon.com可以通过以下几种方式实现站点的个性化: 个性化问候 (专门为用户生成的欢迎词和页面内容, 使购物体验更加个性化) 有的放矢的推荐 (通过了解客户的兴趣, 推荐一些他们认为客户可能会感兴趣的商品。还可以在临近客户生日或其他一些重要日子的时候提供生日特定的商品) 管理信息的存档 (站点可以将用户的如地址,信用卡信息保存在一个数据库中, 只要他们识别出用户, 就可以使用该用户存档的管理信息, 而不是用户在购物时一次次地填写繁琐的地址和信用卡信息) 记录会话 (HTTP事务是无状态的, 每条请求/响应都是独立进行的。而很多Web站点希望能在用户与站点交互的过程中, 比如使用在线购物车的时候, 构建增量状态。要做到这一功能, Web站点就需要有一种方式来区分来自不同用户的HTTP事务) 简而言之就是: HTTP无状态协议, 希望识别每个来自不同用户的HTTP事务; HTTP识别用户的几种技巧 承载用户身份信息的HTTP首部 客户端IP地址跟踪, 通过用户的IP按地址对其进行识别 用户登录, 用认证方式来识别用户 胖URL, 一种在URL中嵌入识别信息的技术 cookie, 一种功能强大且高效的持久身份识别技术 HTTP首部 下面给出了7种最常见的用来承载用户相关信息的HTTP首部, 此处先讨论前3个, 后面4个首部用于更高级的识别技术 1234Form请求首部字段, 用户的Email地址User-Agent请求首部字段, 用户的浏览器软件Refer请求首部字段, 用户是从这个页面上依照连接跳转过来的Authorization请求首部字段, 用户名和密码(稍后讨论) Form请求首部字段: 包含了用户的Email地址, 每个用户都有不同的Email地址, 所以在理想情况下, 可以将这个地址作为可行的源端来识别用户。 但是由于担心那些不讲道德的服务器会搜索这些E-mail地址, 用于垃圾邮件的发送, 所以很少有浏览器会发送Form首部; 实际上Form首部是由自动化机器人或蜘蛛发送的, 这样在出现问题时, 网管还有个地方可以发送愤怒的投诉邮件?? User-Agent请求首部字段：可以将用户所用的浏览器的相关信息告知服务器, 包括程序的名称和版本, 通常还包含操作系统的相关信息。 要实现定制内容与特定的浏览器及其属性间的良好互操作时, 这个首部是非常有用的, 但它并没有为识别特定的用户提供太多有意义的帮助。 Referer请求首部字段: 提供了用户来源页面的URL Referer首部自身并不能完全标识用户, 但它确实说明了用户之前访问过哪个页面。 通过它可以更好地理解用户的浏览器行为, 以及用户的兴趣所在, 比如, 如果你是从一个篮球网站抵达某个Web服务器的,这个Web服务器可能会推断你是个篮球迷。 总之, Form，User-Agent, Referer 这几个请求首部字段都不足以实现可靠的识别特定用户。 客户端IP地址 通常HTTP首部并不提供客户端的IP地址, 但Web服务器可以通过其他方法找到另一端的IP地址。 但是, 使用客户端IP地址来识别用户存在很多缺点, 限制了将其作为用户识别技术的效能: 客户端IP地址描述的是所用的机器，而不是用户。如果多个用户共享同一台计算机，就无法对其进行区分了； 很多因特网服务提供商都会在用户登录时为其动态分配IP地址。用户每次登录时，都会得到一个不同的地址，因此Web服务器不能假设IP地址可以在各登录会话之间标识用户； 为了提高安全性，并对稀缺的地址资源进行管理，很多用户都是通过网络地址转换(Network Address Translation, NAT)防火墙来浏览网络内容的。这些NAT设备隐藏了防火墙后面那些实际客户端的IP地址，将实际的客户端IP地址转换成了一个共享的防火墙IP地址和不同的端口号； HTTP代理和网关通常会打开一些新的、到原始服务器的TCP连接。Web服务器看到的将是代理服务器的IP地址，而不是客户端的。有些代理为了绕过这个问题会添加特殊的Client-IP或X-Forwarded-For扩展首部来保存原始的IP地址，但并不是所有的代理都支持这种行为 用户登录 Web服务器无需被动地根据用户的IP地址来猜测他的身份，它可以要求用户通过用户名和密码进行认证登录来显式地询问用户是谁。 为了使Web站点的登录更加简便，HTTP中包含了一种内建机制，可以用 www-Authenticate响应首部 和 Authorization请求首部 向Web站点传送用户的相关信息。 一旦登录，浏览器就可以不断地在每条发往这个站点的请求中发送这个登录信息了。这样，就总是有登录信息可用了 如果服务器希望在为用户提供对站点的访问之前，先行登录，可以向浏览器回送一条HTTP响应代码 401 Login Required。 然后，浏览器会显示一个登录对话框，并用Authorization首部在下一条对服务器的请求中提供这些信息 如下图 在图a中，浏览器对站点www.joes-hardware.com发起了一条请求； 站点并不知道这个用户的身份，因此在图b中，服务器会返回 401 Login Required HTTP响应码，并添加 www-Authentication响应首部，要求用户登录。这样浏览器就会弹出一个登录对话框；只要用户输入了用户名和密码(对其身份进行完整性检査)，浏览器就会继续原来的请求。这次它会添加一个 Authorization请求首部，说明用户名和密码。对用户名和密码进行加密，防止那些有意无意的网络观察者看到； 现在，服务器已经知道用户的身份了，今后的请求要使用用户名和密码时，浏览器会自动将存储下来的值发送出去，甚至在站点没有要求发送的时候也经常会向其发送。浏览器在每次请求中都向服务器发送Authorization首部作为一种身份的标识，这样，只要登录一次，就可以在整个会话期间维持用户的身份了 但是，登录多个Web站点是很繁琐的。从一个站点浏览到另一个站点的时候，需要在每个站点上登录。更糟的是，很可能要为不同的站点记住不同的用户名和密码。访问很多站点，喜欢的用户名可能已经被其他人用过了，而且有些站点为用户名和密码的长度和组成设置了不同的规则 胖URL 有些Web站点会为每个用户生成特定版本的URL来追踪用户的身份。通常, 会对真正的URL进行扩展, 在URL路径开始或结束的地方添加一些状态信息。 用户浏览站点时, Web服务器会动态生成一些超链, 继续维护URL中的状态信息. 改动后包含了用户状态信息的URL被称为胖URL(fat URL)。下面是Amazon.com使用的一些胖URL实例, 每个URL后面都附加了一个用户特有的标识码, 在这个例子中就是002-1145265-8016838，这个标识码有助于在用户浏览商店内容时对其进行跟踪 12&lt;a href=&quot;/exec/obidos/tg/browse/-/229220/ref=gr_gifts/002-1145265-8016838&quot;&gt;All Gifts&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;/exec/obidos/wishlist/ref=gr_pll_/002-1145265-8016838&quot;&gt;Wish List&lt;/a&gt;&lt;br&gt; 可以通过胖URL将Web服务器上若干个独立的HTTP事务捆绑成一个”会话”或”访问”。 用户首次访问这个Web站点时，会生成一个唯一的ID，用服务器可以识别的方式将这个ID添加到URL中去，然后服务器就会将客户端重新导向这个胖URL。 不论什么时候，只要服务器收到了对胖URL的请求，就可以去査找与那个用户ID相关的所有增量状态(购物车、简介等)，然后重写所有的输出超链，使其成为胖URL，以维护用户的ID 可以在用户浏览站点时，用胖URL对其进行识别。但这种技术存在几个很严重的问题： 丑陋的URL – 浏览器中显示的胖URL会给新用户带来困扰； 无法共享URL – 胖URL中包含了与特定用户和会话有关的状态信息。如果将这个URL发送给其他人，可能就在无意中将个人信息都共享出去了； 破坏缓存 – 为每个URL生成用户特有的版本就意味着不再有可供公共访问的URL需要缓存了； 额外的服务器负荷 – 服务器需要重写HTML页面使URL变胖； 逃逸口 – 用户跳转到其他站点或者请求一个特定的URL时，就很容易在无意中”逃离”胖URL会话, 只有当用户严格地追随预先修改过的链接时，胖URL才能工作。如果用户逃离此链接，就会丢失他的进展(可能是一个已经装满了东西的购物车)信息，得重新开始； 在会话间是非持久的，除非用户收藏了特定的胖URL，否则用户退出登录时，所有的信息都会丢失 cookie cookie是当前识别用户，实现持久会话的最好方式。 cookie最初是由网景公司开发的, 但现在所有主要的浏览器都支持它。 cookie非常重要，而且它定义了一些新的HTTP首部。 cookie的存在也影响了缓存，大多数缓存和浏览器都不允许对任何cookie的内容进行缓存。 cookie的类型 可以笼统地将cookie分为两类：会话cookie 和 持久cookie。 会话cookie 是一种临时cookie，它记录了用户访问站点时的设置和偏好，用户退出浏览器时，会话cookie就被删除了。 持久cookie 的生存时间更长一些，它们存储在硬盘上，浏览器退出，计算机重启时它们仍然存在。 通常会用 持久cookie 维护某个用户会周期性访问的站点的配置文件或登录名。 会话cookie 和 持久cookie 之间唯一的区别就是它们的过期时间。如果设置了 Discard 参数，或者没有设置 Expires 或 Max-Age 参数来说明扩展的过期时间，这个cookie就是一个会话cookie 工作机制 用户首次访问Web站点时, Web服务器对用户一无所知。Web服务器希望这个用户会再次回来，所以想给这个用户”拍上”一个独有的cookie, 这样以后它就可以识别出这个用户了。 cookie中包含了一个由 名字=值(name=value) 这样的信息构成的任意列表, 并通过 Set-Cookie 或 Set-Cookie2 HTTP响应(扩展)首部将其贴到用户身上去 cookie中可以包含任意信息, 但它们通常都只包含一个服务器为了进行跟踪而产生的独特的识别码。比如, 服务器会将一个表示id=”34294”的cookie贴到用户上去。 服务器可以用这个数字来查找服务器为其访问者积累的数据库信息(购物历史、地址信息等) 但是, cookie并不仅限于ID号, 很多Web服务器都会将信息直接保存在cookie中, 比如: Cookie： name &quot;Brian Totty&quot;; phone=&quot;555-1212&quot; 浏览器会记住从服务器返回的 Set-Cookie 或 Set-Cookie2 首部中的cookie内容，并将cookie集存储在浏览器的cookie数据库中。将来用户返回同一站点时，浏览器会挑中那个服务器贴到用户上的那些cookie，并在一个cookie请求首部中将其传回去。 cookie罐: 客户端的状态 cookie的基本思想就是让浏览器积累一组服务器特有的信息，每次访问服务器时都将这些信息提供给它。因为浏览器要负责存储cookie信息，所以此系统被称为客户端侧状态(client-side state)。 不同的浏览器会以不同的方式来存储cookie。（更多参考P218） 不同的站点使用不同的cookie 浏览器内部的 cookie罐 中可以有成百上千个cookie，但浏览器不会将每个cookie都发送给所有的站点。实际上，它们通常只向每个站点发送2-3个cookie, 原因如下： 对所有这些cookie字节进行传输会严重降低性能。浏览器实际传输的cookie字节数要比实际的内容字节数多; cookie中包含的是各服务器特有的名值对，所以对大部分站点来说，大多数cookie都是自己无法识别的无用数据; 将所有的cookie发送给所有站点会引发潜在的隐私问题，那些不信任的站点也会获得只想发给其他站点的信息; 总之，**浏览器只会向服务器发送由同一服务器产生的那些cookie。如：joes-hardware.com产生的cookie会被发送给joes-hardware.com, 不会发送给bobs-books.com 或 marys-movies.com 很多Web站点都会与第三方厂商达成协议，由其来管理广告。 这些广告被做得像Web站点的一个组成部分，而且它们确实发送了持久cookie。 用户访问另一个由同一广告公司提供服务的站点时，由于域是匹配的，浏览器就会再次回送早先设置的持久cookie。 营销公司可以将此技术与Referer首部结合，暗地里构建一个用户档案和浏览习惯的详尽数据集。 现代的浏览器都允许用户对隐私特性进行设置，以限制第三方cookie的使用 cookie的域属性 产生cookie的服务器可以向 Set-Cookie响应首部 添加一个 Domain属性 来控制哪些站点可以看到那个cookie。 比如，下面的HTTP响应首部就是在告诉浏览器将 cookie user= “maryl7” 发送给域为 “.airtravelbargains.com” 的所有站点： 1234567Set-cookie: user=&quot;maryl7&quot;; domain=&quot;airtravelbargains.com&quot;如果用户访问的是www.airtravelbargains.com 或者 specials.airtravelbargains.com或任意以.airtravelbargains.com结尾的站点，下列Cookie首部都会被发布出去：Cookie: user=&quot;maryl7&quot; cookie路径属性 cookie规范甚至允许用户将cookie与部分Web站点关联起来, 可以通过 Path属性 来实现这一功能, 在这个属性列出的URL路径前缀下所有cookie都是有效的 例如, 某个Web服务器可能是由两个组织共享的，每个组织都有独立的cookie, 站点www.airtravelbargains.com可能会将部分Web站点用于汽车租赁 比如，http://www.airtravelbargains.com`/autos/` – 用一个独立的cookie来记录用户喜欢的汽车尺寸, 可能会生成一个如下所示的特殊汽车租赁cookie: 12345678Set-cookie: pref=compact; domain=&quot;airtravelbargains.com&quot;; path=/autos/如果用户访问http://www.airtravelbargains.com/specials.html，就只会获得这个cookie：Cookie: user=&quot;maryl7&quot; (结合前面域属性的例子)但如果访问http://www.airtravelbargains.com/autos/cheapo/index.html，就会获得这两个cookie:Cookie: user=&quot;maryl7&quot;Cookie: pref=compact 因此，cookie就是由服务器贴到客户端上，由客户端维护的状态片段，只会回送给那些合适的站点。下面我们来更仔细地看看cookie的技术和标准。 cookie的成分现在使用的cookie规范有两个不同的版本: cookies版本0(有时被称为Netscape cookies) 和 cookies版本1(RFC 2965)。cookies版本1 是对cookies版本0的扩展，应用不如后者广泛。 Cookies版本0参考P284 Cookies版本1参考P285 版本1的cookie2请求首部和版本协商cookie与缓存cookie安全性和隐私","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP -- 缓存 (200, 304)","slug":"http/2017-11-30-HTTP-05-cache","date":"2017-11-30T06:27:36.000Z","updated":"2018-02-08T02:10:46.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-05-cache/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-05-cache/","excerpt":"","text":"Web缓存简介 Web缓存是指可以自动保存常见文档副本的HTTP设备。当Web请求抵达缓存设备时, 如果缓存设备本地有”已缓存的”副本, 就可以从本地设备而不是原始服务器中提取这个文档。 使用缓存的优点: 减少了冗余的数据传输, 节省了你的网络费用 有很多客户端访问一个流行的原始服务器页面时, 服务器会多次传输同一份文档, 每次传送给一个客户端。这样就会导致一些相同的字节在网络中一遍遍地传输, 这些冗余的数据传输会耗尽昂贵的网络带宽, 降低传输速度, 加重Web服务器的负载。有了缓存, 就可以保留第一条服务器响应的副本, 后继请求就可以由缓存的副本来应对了, 这样可以减少那些流入/流出原始服务器而被浪费掉了的重复流量。 缓解了网络本身的瓶颈问题, 不需要更多的带宽就能够更快地加载页面 很多网络为本地网络客户端提供的带宽比为远程度服务器提供的带宽要宽。(这个大家应该有深有体会, 局域网内传输是很快的!!!)如果客户端能够从一个快速局域网的缓存中得到一份副本, 那么缓存就可以提高性能—尤其是要传输比较大的文件时。 降低了对原始服务器的要求, 服务器可以更快地响应, 避免过载的出现 瞬间拥塞: 突发事件(爆发性新闻, 抢购等)使很多人几乎同时去访问一个Web文档时, 就会出现瞬间拥塞, 由此造成的过多流量峰值可能会使网络和Web服务器产生灾难性的崩溃。而缓存在应对瞬间拥塞时就显得非常重要。 降低了距离时延, 因为从较远的地方加载页面会更慢一些即使带宽不是问题, 距离也可能成为问题。每台网络路由器都会增加因特网流量的时延。即使客户端和服务器之间没有太多的路由器, 光速自身也会造成显著的时延。 比如波士顿到旧金山的直线距离大约为2700英里, 在最好的情况下, 以光速传输(186000英里/秒)的信号可以在大约15毫秒从波士顿传送到旧金山, 并在30毫秒内完成一个往返。假设某个Web页面包含了20个小图片, 都在旧金山的一台服务器上, 如果波士顿的一个客户端打开了4条到服务器的并行连接, 而且保持着连接的活跃状态, 光速自身就要消耗大约1/4秒(240毫秒)的下载时间。如果服务器位于(距离旧金山6700英里)的东京, 时延就会变成600毫秒。中等复杂的web页面会带来几秒钟的光速时延。况且实际应用中, 信号的传输速度会比光速低一些, 因此距离时延会更加严重。 而将缓存放在附近的机房里可以将文件传输距离从数千英里缩短为数十米。 ‘缓存命中’、’未命中’ 的概念 如果一些请求到达缓存设备时, 缓存设备可以用本地已有的副本为这些请求提供服务, 就被称为缓存命中。 如果一些请求到达缓存设备时, 缓存设备本地没有副本提供给这些请求, 而将请求转发给原始服务器, 这就被称为缓存未命中。 后面还有 再验证命中 和 再验证未命中 的概念; 引出文档过期 和 服务器再验证 已缓存的数据要与服务器数据保持一致: 缓存设备本地的副本 并不是时刻都与原始服务器上的文档一样, 毕竟服务器中的这些文档会随着时间发生变化(比如有些报告可能每个月都会变化, 而在线报纸每天都会变化, 财经数据可能每过几秒就会发生变化)。 所以, 如果缓存提供的总是老的数据, 就会变得毫无用处。 HTTP通过一些简单的机制, 可以做到: 在不要求服务器记住有哪些缓存设备拥有其文档副本的情况下, 保持已缓存数据与服务器数据之间的充分一致。 HTTP将这些简单的机制称为 文档过期(document expiration)(也就是缓存副本的过期时间) 和 服务器再验证(server revalidation)。 缓存副本的过期时间 原始服务器通过 老式的HTTP/1.0+的实体首部字段Expires 或 新式的HTTP/1.1的通用首部字段Cache-Control:max-age 可以向每个文档附加一个过期日期。 Expires 和 Cache-Control:max-age 所做的事情本质上是一直的, 但由于 Cache-Control 首部使用的是相对时间而不是绝对时间, 所以我们更倾向与使用比较新的 Cache-Control 首部。 Expires 绝对日期依赖于计算机时钟的正确设置 如下图: 在缓存文档过期之前, 缓存设备可以随意使用这些副本, 而且无需与服务器做任何联系!! 当然, 除非 客户端请求中包含 “阻止提供缓存” 的首部 Cache-Control:no-store; 或者客户端请求中包含”只有经过验证才能返回缓存副本”的首部Cache-Control:no-cache*), 但是一旦已缓存文档过期, 缓存设备就必须与服务器进行核对(当然, 除非你设置了Cache-Control:only-if-cached要求只使用缓存), 询问文档是否被修改过, 如果被修改过, 就要获取一份新鲜(带有新的过期日期)的副本。 注意: 不推荐使用Expires首部, 它指定的是实际的过期日期而不是秒数。HTTP设计者后来认为, 由于很多服务器的时钟都不同步, 或者不正确, 所以最好还是用剩余秒数, 而不是绝对时间来表示过期时间。 有些服务器还会回送一个Expires:0响应头,视图将文档置于永远过期的状态, 但这种语法是非法的, 可能给某个软件带来问题, 应该试着支持这种结构的输入, 但是不应该产生这种结构的输出。 而 Cache-Control 的 max-age 则可以设置 Cache-Control: max-age=0 另外, 注意 no-cache 和 must-revalidate 的区别 no-cache: 告诉浏览器、缓存服务器，不管本地副本是否过期，使用资源副本前，一定要到源服务器进行副本有效性校验。 must-revalidate：告诉浏览器、缓存服务器，本地副本过期前，可以使用本地副本；本地副本一旦过期，必须去源服务器进行有效性校验。(这应该是缓存系统的默认行为, 但must-revalidate指令使得这个要求是明确的参考) 可参考 副本过期算法测试 FireFox测试过期时间算法( Date + Expire/max-age - Age) 123456789101112131415161718192021222324&lt;?php/** * 主要测试浏览器确实是根据 expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) * 来计算失效时间的 * chrome好像不太正常(会交替显示 123 和 456789) * firefox 进行回车测试, 结果发现完全正常 */if(!isset($_SERVER['HTTP_IF_MODIFIED_SINCE'])) &#123; header(\"HTTP/1.1 200\"); header('Cache-Control: max-age=30'); // 放到下一行就不生效了(响应头还特么还有顺序?) header('Age:10'); // 发现浏览器确实拿着这个日期去判断有没有过期 header('Date:'. date('D, d M Y H:i:s', time()-10).' GMT'); header('Last-Modified:'. date('D, d M Y H:i:s', time()-20000).' GMT'); echo 123; exit;&#125;//算法: expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头)// 此处浏览器判断 date + 30 - 10 , 由于date是当前时间, 所以差不多有20秒的过期时间// 如果是Age:0的话, date + 30 -0, 由于date是当前时间, 所以差不多有30秒的过期时间// 如果是Date-10, Age:0的话, date-10 + 30 -10, 所以差不多有10秒的过期时间 (不太好抓, 不过肯定是10秒过期)//发现只有超时之后, 才会显示出下面的信息 (firefox准确无误地实现,在未过期之前, 是不会带If-Modified-Since头去请求的)echo 456789;die; 缓存副本过期后的”再验证” 原始服务器上的内容可能会发生变化, 缓存要不时地对其进行检测, 看看自己保存的副本是否仍是服务器上最新的副本。这种”新鲜度检测”就被称为HTTP再验证(revalidation)。 虽然缓存可以在任意时刻, 以任意的频率从对副本进行再验证, 但是由于缓存中通常会包含数百万的文档, 而且网络带宽是很珍贵的, 所以大部分缓存只有在客户端发起请求,并且副本旧的足以需要再次检测的时候, 才会对副本进行再验证。 副本旧的足以需要再次检测的时候? 也就是缓存副本的过期时间已到!! 但是仅仅是已缓存文档过期了, 还不能说明该过期文档和原始服务器上的文档有实际的区别, 这只是意味着到时间进行再验证了！ 再验证命中(缓慢命中) 缓存对副本进行再验证时, 会向原始服务器发送一个小的再验证请求。如果发现内容没有变化, 服务器会以一个小的 304 Not Modified 进行响应。 只要缓存知道副本仍然有效, 就会再次将副本标识为暂时新鲜的, 并将副本提供给客户端, 这被称为再验证命中(revalidate hit) 或 缓慢命中(slow hit)。 当然, 这种方式确实还是需要与原始服务器进行核对, 所以会比单纯的缓存命中要慢, 但是它并没有从服务器中获取对象数据, 所以要比缓存未命中要快一些。 再验证未命中 缓存对副本进行再验证时, 会向原始服务器发送一个小的再验证请求。如果缓存发现服务器对象与已缓存副本不同, 则服务器会向客户端发送一条普通的, 带有完整内容的 HTTP 200 OK 响应; 当然, 这种方式确实不仅需要与原始服务器进行核对, 而且会从服务器中获取对象数据, 所以理论上貌似要比缓存未命中要慢一些, 但其实差不多 再验证 – 服务器对象被删除如果再验证发现服务器对象已经被删除, 服务器就回送一个 404 Not Found 响应, 缓存也会将其副本删除。 小结成功的再验证 比 缓存未命中 要快失败的再验证 几乎和 缓存未命中 速度一样 再验证依靠 – 条件方法 为了有效地进行再验证, HTTP定义了一些特殊的请求, 不用从服务器上获取整个对象, 就可以快速检测出内容是否是最新的。 HTTP的条件方法可以高效地实现再验证。 HTTP允许缓存向原始服务器发送一个 “条件GET”, 请求只有在服务器文档与缓存中现有的副本不同时, web服务器才会回送对象主体; 通过这种方式, 将新鲜度检测和对象获取结合成了单个条件GET。 向GET请求报文中添加一些特殊的条件首部, 就可以发起条件GET。 HTTP定义了5个条件请求首部, 对 缓存再验证 来说有用的2个首部是 If-Mofified-Since 和 If-None-Match, 所有的条件首部都以前缀If-开头。 If-Modified-Since:Date 再验证 最常见的缓存再验证首部是 请求首部字段 If-Modified-Since, If-Modified-Since再验证请求通常被称为IMS请求。 如果自If-Modified-Since指定日期之后, 文档被修改了, If-Modified-Since 条件就为真, 通常GET就会成功执行, 携带新首部的新文档会被返回给缓存, 新首部除了其他信息之外, 还包含了一个新的过期日期; 如果自If-Modified-Since指定日期之后, 文档没被修改, If-Modified-Since 条件就为假, 会向客户端返回一个小的 304 Not Modified响应报文, 为了提高有效性, 不会返回文档主体。这些首部是放在响应中返回的, 但是只会返回哪些需要在源端更新的首部, 比如, Content-Type首部通常不会被修改, 所以通常不需要发送。一般会发送一个新的过期日期。 请求首部字段If-Modified-Since 和 实体首部字段Last-Modified 配合工作。 原始服务器会将最后的修改日期附加到所提供的文档上去, 当缓存要对已缓存文档进行再验证时, 就会包含一个If-Modified-Since首部, 其中携带有最后修改已缓存副本的日期: If-Modified-Since:&lt;cached last-modified date&gt; 如果在此期间原始服务器文档被修改了, 最后的修改日期就会不同了, 这样If-Modified-Since条件就为真, 原始服务器就会回送新的文档； 否则, 服务器会注意到缓存的最后修改日期与服务器文档当前的最后修改日期相符合, 则会返回一个 304 Not Modified 响应。 小结: 如果在验证发现原始服务器内容未发生变化, If-Modified-Since在验证会返回304响应, 如果发生了变化, 就返回带有新主体的200响应。 If-None-Match 实体标签再验证实体标签 有些情况下使用最后修改日期进行再验证是不够的: 有些文档可能会被周期性地重写, 但实际包含的数据常常却是一样的。尽管内容没有发生变化, 但是修改日期会发生变化。 有些文档可能内容被修改了, 但是所做的修改并不重要, 不需要让世界范围内的缓存都重装数据(比如对拼写或注释的修改)。 涉及到弱验证器 有些服务器无法准确地判定其页面的最后修改日期。 有些服务器提供的文档会在亚秒间隙发生变化(比如,实时监视器), 对这些服务器来说, 以秒为粒度的修改日期可能就不够用了。 为了解决上述问题, HTTP有一个被称为 实体标签(ETag) 的 版本标识符, 这个实体标签是附加到文档上的任意标签, 标签可能可能包含了文档序列号或版本名, 或是文档内容的校验及其他指纹信息。 当对文档进行修改时, 可以修改文档的实体标签来说明这个新的版本。这样, 如果实体标签被修改了, 缓存就可以用 If-None-Match 条件首部来GET文档的新副本了。 假设缓存中有一个文档已经过(Expires:, Cache-Control:max-age)期, 或者其他配置导致需要再次验证, 如果缓存中有一个实体标签为v2.6, 则它会与原始服务器进行再验证: 如果服务器上的实体标签已经发生了变化(可能变成了v3.0, 和v2.6不再匹配), 服务器则会在一个 200 OK 响应中返回新的内容以及新的Etag标签 ; 如果标签仍然与原始服务器标签匹配, 则会返回一条304 Not Modified响应; 弱验证器 只要原始服务器内容发生变化, 则实体标签就会变化, 正常情况下, 强验证器就会对比失败, 导致服务器会在一个 200 OK 响应中返回新的内容以及新的Etag标签; 有时, 服务器希望对文档进行一些不重要的修改, 并且不需要使所有已缓存副本都失效HTTP1.1支持的”弱验证器”, 就允许对一些内容做修改, 此时服务器会用前缀 W/ 来标识弱验证器。 不管相关的实体值以何种方式发生了变化, 强实体标签都要发生变化, 而相关实体在语义上发生了比较重要的变化时, 弱实体标签页应该发生变化。 实体标签 和 最近修改日期 如果服务器回送了一个实体标签, HTTP/1.1客户端就必须使用实体标签验证器。如果服务器只回送了一个Last-Modified值, 客户端就可以使用 If-Modified-Since 验证。如果实体标签和最后修改日期都提供了, 客户端就应该使用这两种再验证方案, 这样HTTP1.0和HTTP1.1换成你都可以正确响应了。 除非HTTP/1.1原始服务器无法生成实体标签验证器, 否则就应该发送一个出去, 如果使用弱实体标签有优势的话, 发送的可能就是个弱实体标签, 而不是强实体标签。而且最好同时发送一个最近修改值。如果HTTP/1.1缓存或服务器受到的请求既带有 If-Modified-Since, 又带有实体标签条件首部, 那么只有这两个条件都满足时, 才能返回 304 Not Modified 响应(也就是两个都做验证)。 缓存状态码 200 和 304参考P176: HTTP没有为用户提供一种手段来区分响应是缓存命中的, 还是访问原始服务器得到的。在这两种情况下, 响应状态码都是200OK, 说明响应有主体部分。 你从public公共缓存中可能直接得到未过期的资源, 此时会返回 200 ok; 你也可能到公共缓存后发现要再验证, 此时发现文本已变更, 服务器也会返回 200 ok; 缓存命中(这里指的是公共的代理缓存命中) 返回 200 ok 客户端第一次访问资源, 浏览器和服务器之间有代理服务器, 这样的话, 由于这个代理服务器是个公共代理, 所以里面可能已经有了服务器响应的资源副本, 所以代理服务器会直接响应 资源副本和200 ok给客户端; 访问原始服务器, 返回 200 ok 浏览器和服务器之间没有代理服务器, 这样的话, 客户端第一次请求资源, 则服务器直接 响应 200 ok 和 资源对象 给客户端; 客户端多次访问资源, 浏览器和服务器之间有代理服务器, 但是由于种种原因, 缓存需要再验证, 并且结果发现再验证未命中, 则服务器会响应资源对象和200 ok给代理缓存, 然后代理再响应”服务器响应的资源和200 ok副本”给客户端; 浏览器直接取的自己的本地缓存, 返回 200 ok (from disk/memory cache) 此处由于浏览器之前缓存了 代理缓存服务器cdn 上的缓存副本, 所以浏览器缓存的副本和上面cdn代理缓存的副本一样(age缓存时间都没变), 只不过会标注”已缓存”来表示没有响应主体部分 304 Not Modified 是缓存和服务器多确认了一次缓存有效性检测后, 发现缓存再验证命中, 但是用的还是缓存。 小结: 304 Not Modified 比 再验证未命中返回200 OK 快, 但是比 private缓存命中返回 200 ok from disk/memory cache 慢; 参考 http权威指南176页 公有和私有缓存 通用首部字段(general header fields)Cache-Control有两个缓存响应指令: public 和 private 缓存可以是单个用户专用的, 也可以是数千名用户共享的 ; 专用缓存被称为私有缓存(private cache), 私有缓存是个人的缓存, 包含了单个用户最常用的页面 ; 共享缓存被称为公有缓存(public cache), 公有缓存包含了某个用户团体常用页面 ; 私有缓存私有缓存不需要很大的动力或存储空间, 这样就可以将其做的很小, 很便宜。Web浏览器中就有内建的私有缓存—大多数浏览器都会将常用文档缓存在你个人电脑的磁盘和内存中, 并且允许用户去配置缓存的大小和各种设置; 公有缓存 公有缓存是特殊的共享代理服务器, 被称为缓存代理服务器(caching proxy server), 或者更常见地被称为代理缓存(proxy cache)。 代理缓存会从自己本地缓存中给用户提供缓存资源, 或者代表用户与服务器进行联系。公有缓存会接受来自多个用户的访问, 所以通过它可以更好地减少冗余流量。 如下图: 每个客户端都会重复地访问一个(还不在私有缓存中的)新的”热门”文档。每个私有缓存都要获取同一份文档, 这样它就会多次穿过网络。 而使用共享的公有缓存时, 对于这个流行的对象, 缓存只要取一次就行了, 它会用共享的副本为所有的请求服务, 以降低网络流量。 缓存控制 客户端可以用 Cache-Control 请求首部来 强化 或 放松 对过期时间的限制。 有些应用程序对文档的新鲜度要求很高, 对于这些应用程序, 客户端可以用 Cache-Control 首部使过期时间更严格; 另一方面, 为了提高性能, 可靠性或开支的一种折中方式, 客户端可能会放松新鲜度要求; 如下对 Cache-Control 的请求指令进行了小结:HTTP Cache-Control: max-age和max-stale=s的区别 Pagma:no-cache 和 Cache-Control:no-cache 一样, 不过是为了兼容HTTP/1.0; 参考:HTTP权威指南 – 第七章 缓存《图解HTTP协议》https://developer.mozilla.org/en-US/docs/Web/HTTPRFC 2616MDN Web docs","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP - General Header Fields","slug":"http/2017-11-30-HTTP-04-General-Header-Fields","date":"2017-11-30T03:50:02.000Z","updated":"2018-02-08T02:10:34.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-04-General-Header-Fields/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-04-General-Header-Fields/","excerpt":"","text":"通用首部字段 general-header Cache-Control Cache-Control 的指令是可选的,可以有多个, 多个指令之间通过 , 分隔; 缓存指令是单向的, 即请求中存在一个指令并不意味着响应中将存在同一个指令; 可以按请求和响应分为: 缓存请求指令 缓存响应指令 更多参考HTTP - 缓存 immutable属于缓存控制的一个扩展属性, http://www.jdon.com/performance/cache-control-immutable.html 当一个支持immutable的客户端浏览器看到这个属性时, 它应该知道, 如果没有超过过期时间，那么服务器端该页面内容将不会改变, 这样浏览器就不应该再发送有条件的重新验证请求(比如通过If-None-Match 或 If-Modified-Since等条件再向服务器端发出更新检查);也就是说, 通常过去我们使用304回复客户端该页面内容没有变化，但是如果用户按浏览器的刷新或F5键，浏览器会再次向服务器端发出该页面内容请求，服务器端如果确认该页面没有变化，那么发回304给客户端，不再发送该页面的实体内容，虽然这样节省了来回流量，但是如果大型网站的很多用户为了得到及时信息，经常会刷新浏览器，这就造成了大量刷新请求，向服务器端求证该页面是否改变，这会影响网站的带宽，也增加服务器端验证压力。而新的选项immutable可以杜绝这种现象。immutable可以节省HTTP请求,缩短请求时间,这是因为服务器不必再处理304响应了。 Date 简单来说就是HTTP报文的创建日期, 它会参与到缓存控制的 过期时间运算中; 公式: expirationTime = responseTime(Date头) + freshnessLifetime(max-age/Exprie值) - currentAge(Age头) 而响应首部字段中额 Age 头字段, 是告诉客户端, 源服务器在多久之前创建了响应, 字段的单位为秒; 如果创建该响应的服务器是缓存服务器, Age值是指缓存后的响应再次发起认证到认证完成的时间值, 代理创建响应时必须加上首部字段Age。 Pragma Pragma是HTTP1.1之前版本的历史遗留字段, 仅作为与HTTP1.0做向后兼容; 也属于和缓存相关的通用首部字段, 但只用在客户端发送的请求当中, 客户端会要求所有的中间服务器不返回缓存的资源(参考Cache-Control:no-cache), 但是并不专用于缓存, 最常用的是Pragma:no-cache; Connection 该首部字段具备两个作用: 控制不再转发给代理的首部字段 和 管理持久连接; 更多参考HTTP - 并行连接, 持久连接 未完待续… Via 使用首部字段Via, 是为了追踪客户端与服务器之间的请求和响应报文的传输路径;报文在经过代理或者网关时, 会先在首部字段Via中附加该服务器的信息, 然后再进行转发。 Via首部是为了追踪传输路径, 所以也经常会和TRACE方法一起使用, 比如代理服务器受到由TRACE方法发送过来的请求(其中Max-Forward:0)时, 代理服务器就不能再转发该请求了,这种情况下, 代理服务器会将自身的信息附加到Via首部后, 返回该请求的响应。 Via通用首部字段不仅用于报文的转发, 还可避免请求回环的发生, 所以必须在经过代理时附加该首部字段; 未完待续… Transfer-Encoding传输编码 该通用首部字段规定了传输报文主体时采用的编码方式; HTTP/1.1的传输编码方式仅对分块传输编码有效, 即只能设置为 Transfer-Encoding:chunked; MDN参考; Content-Encoding 和 Transfer-Encoding 二者经常会结合来用, 其实就是针对 Transfer-Encoding 的分块再进行 Content-Encoding压缩; 对比 请求首部字段Accept-encoding期望内容编码 请求首部字段Accept-encoding是将客户端用户代理(浏览器)所支持的内容编码方式(通常是某种压缩算法) 及 内容编码方式的优先级顺序, 通知给服务器; 通过内容协商, 服务端会选择一个客户端支持的方式, 使用并在 响应报文的实体首部字段 Content-Encoding 中通知客户端服务器选择了哪种内容编码方式; 另外, 可以一次性指定多种内容编码! 也可以使用权重q值来表示相对优先级; 正常情况下, 主要采用以下4种编码方式: gzip compress deflate identity: 不执行压缩或不会变化的默认编码格式; 即使客户端和服务器都支持某些相同的压缩算法，但如果Accept-encoding:identity, 表示客户端告诉服务器对响应主体不要进行压缩。导致这种情况出现的两种常见的情形是： 要发送的数据已经经过压缩, 再次进行压缩不会导致被传输的数据量更小, 一些图像格式的文件会存在这种情况; 服务器超载, 无法承受压缩需求导致的计算开销, 通常, 如果服务器使用超过80%的计算能力, 微软建议不要压缩; 只要 identity(表示不需要进行任何编码)没有被明确禁止使用, 即, 没有通过 identity;q=0或是*;q=0指令明确设置 identity 的权重值，则服务器禁止返回表示客户端错误的 406 Not Acceptable 响应。 也就是只有你通过设置identity的权重为0, 服务器才可以返回表示客户端错误的406 Not Acceptable MDN参考 对比 实体首部字段Content-encoding内容编码 实体首部字段’Content-encoding’会告诉客户端, 服务器对实体的主体部分选用的内容编码方式, 内容编码方式是指在不丢失实体信息的前提下所进行的压缩, 参考 请求首部字段Accept-encoding 所建议给服务器的编码方式: gzip compress deflate identity 对比 请求首部字段Accept-charset 请求首部字段’Accept-charset’会告诉服务器, 用户代理(浏览器)所支持的字符集和字符集的相对优先顺序。 可以一次性指定多种字符集; 也可以使用权重q值来表示相对优先级; 小结 请求首部字段Accept-encoding 和 实体首部字段Content-encoding来决定压缩方式; 通用首部字段Transfer-Encoding用来决定响应实体是否分块; 请求首部字段Accept-charset是客户端高速服务端自己能支持的编码方式; TrailerUpgradeWarning参考《HTTP权威指南》《图解HTTP协议》https://developer.mozilla.org/en-US/docs/Web/HTTPhttps://tools.ietf.org/html/rfc2616","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"HTTP - 请求Method","slug":"http/2017-11-30-HTTP-03-Method","date":"2017-11-30T03:25:12.000Z","updated":"2018-02-08T02:05:41.000Z","comments":true,"path":"2017/11/30/http/2017-11-30-HTTP-03-Method/","link":"","permalink":"http://blog.renyimin.com/2017/11/30/http/2017-11-30-HTTP-03-Method/","excerpt":"","text":"常见Method简要分类 HTTP/1.1 中实现的 method 见RFC2616 可以看到有: OPTIONS, GET, HEAD, POST, PUT, DELETE, TRACE, CONNECT RFC2616中提到: PATCH，LINK，UNLINK方法被定义，但并不常见, 在RFC 2068中实现; 《图解http协议》中LINK,UNLINK已经被http1.1废弃; 规范中虽然是上面那样定义的, 但具体还要看不同应用各自是如何去实现的, 有些应用会完整实现, 有些还会扩展, 有些可能会实现一部分 参考symfony中的 symfony/src/Symfony/Component/HttpFoundation/Request.php 12345678910const METHOD_HEAD = 'HEAD';const METHOD_GET = 'GET';const METHOD_POST = 'POST';const METHOD_PUT = 'PUT';const METHOD_PATCH = 'PATCH';const METHOD_DELETE = 'DELETE';const METHOD_PURGE = 'PURGE';const METHOD_OPTIONS = 'OPTIONS';const METHOD_TRACE = 'TRACE';const METHOD_CONNECT = 'CONNECT'; 而像postman这种工具, 实现的就比较多: PUT(对比POST) PUT: 对已有资源进行更新操作, 所以是 update 操作; 一个简单例子: 假设一个博客系统提供一个Web API(http://superblogging/blogs/post/{blog-name}), 可以使用PUT或者POST进行请求, HTTP的body部分就是博文内容，这是一个很简单的REST API例子。 put和post有什么区别呢? 在HTTP中, PUT被定义为 idempotent(幂等性)) 的方法，POST则不是，这是一个很重要的区别 我们应该用PUT还是POST？取决于这个REST服务的行为是否是idempotent(幂等)的, 假如发送两个请求, 希望服务器端是产生两个博客帖子，那就说明这个服务不是idempotent的, 因为多次使用产生了副作用了, 那就应该使用POST方法。但如果是希望后一个请求把第一个请求覆盖掉(这不正是修改么), 那这个服务就是idempotent的。 虽然POST和PUT差别不大, 用错了也没关系, 但是你的服务一放到internet上，如果不遵从HTTP协议的规范，就可能给自己带来麻烦 POST POST: 上面已经提过了, 所以POST是非幂等的; POST和PUT都可以上传文件或者创建新信息, 但主要看你的REST服务行为是否是幂等的 再比如, 在我们的支付系统中，一个api的功能是创建收款金额二维码，它和金额相关，每个用户可以有多个二维码，如果连续调用则会创建新的二维码，这个时候就用POST但如果用户的账户二维码只和用户关联，而且是一一对应的关系，此时这个api就可以用PUT，因为每次调用它，都将刷新用户账户二维码 PATCH(对比PUT) 对已有资源的操作:用于资源的部分内容的更新, 例如更新某一个字段。具体比如说只更新用户信息的电话号码字段。 而PUT则用于更新某个资源较完整的内容, 比如说用户要重填完整表单更新所有信息, 后台处理更新时可能只是保留内部记录ID不变。 当资源不存在时: PATCH 可能会去创建一个新的资源, 这个意义上像是 saveOrUpdate 操作。 参考: https://segmentfault.com/q/1010000005685904/ https://unmi.cc/restful-http-patch-method/ http://restcookbook.com/HTTP%20Methods/patch/ https://tools.ietf.org/html/rfc5789 HEADHEAD和 GET 本质是一样的, 区别在于如果使用HEAD, 响应体将不会被返回，而仅仅返回HTTP头信息。有的人可能觉得这个方法没什么用，其实不是这样的。想象一个业务情景: 欲判断某个资源是否存在, 我们通常使用GET, 但这里用HEAD则意义更加明确。 GET比较简单, 直接获取资源; OPTIONS 这个方法很有趣, 但极少使用。它用于获取当前URL所支持的方法。若请求成功, 则它会在HTTP头中包含一个名为 Allow 的头, 值是服务器所支持的方法, 如 GET, POST。 另外, 之前[介绍跨域]时(稍后奉上,近期正在重新修正), CORS方案 -- (not-so-simple request)中的”预检”请求用的请求方法就是 OPTIONS CONNECT要求用隧道协议连接代理, 如使用SSL TRACE书中说比较少用 TRACE_Method是HTTP（超文本传输）协议定义的一种协议调试方法，该方法会使服务器原样返回任意客户端请求的任何内容。TRACE和TRACK是用来调试web服务器连接的HTTP方式。支持该方式的服务器存在跨站脚本漏洞，通常在描述各种浏览器缺陷的时候，把”Cross-Site-Tracing”简称为XST。攻击者可以利用此漏洞欺骗合法用户并得到他们的私人信息。（这个命令好怕怕，无知好吓人啊）如何关闭Apache的TRACE请求虚拟主机用户可以在.htaccess文件中添加如下代码过滤TRACE请求:RewriteEngine onRewriteCond %{REQUEST_METHOD} ^(TRACE|TRACK)RewriteRule .* - [F]服务器用户在httpd.conf尾部添加如下指令后重启apache即可:TraceEnable off DELETE参考 PURGE非规范中定义的方法","categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://blog.renyimin.com/tags/HTTP/"}]},{"title":"15.实战compose编排集成环境","slug":"docker/2017-10-21-15-docker","date":"2017-10-21T03:50:08.000Z","updated":"2018-03-05T03:19:55.000Z","comments":true,"path":"2017/10/21/docker/2017-10-21-15-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/21/docker/2017-10-21-15-docker/","excerpt":"","text":"Docker-Composer编排nginx+phpfpm+mysql+redis+mongo集成环境Github仓库地址:https://github.com/rymuscle/docker-composer-lnmp 定义项目目录结构 为了在项目启动后, 能方便地对各项服务服务进行配置, 观察日志, 持久化数据, 并且保证容器不过于臃肿; 我们在项目中的各服务在启动时, 我们就需要将诸如: 服务的配置文件, 日志文件, 数据库服务的数据文件目录, Nginx服务的项目目录等从容器的存储层用挂载数据卷的方式持久化出来; 要对各服务进行数据挂载, 因此在使用Docker-Composer编排环境项目前, 要先定义好项目的目录结构, 本人目前的目录结构如下 (基本已做到见名知意): 1234567891011121314151617181920212223242526272829├── docker-compose.yml├── mongo│ ├── Dockerfile│ └── data├── mysql│ ├── Dockerfile│ ├── conf│ ├── data│ ├── env_config.env│ └── log├── nginx│ ├── Dockerfile│ ├── conf│ └── log├── php│ ├── php56│ │ ├── Dockerfile│ │ ├── conf│ │ └── log│ └── php72│ ├── Dockerfile│ ├── conf│ └── log├── redis│ ├── Dockerfile│ ├── conf│ ├── data│ └── log└── site 目录结构说明 每个服务下面都放置了一份属于容器自己的Dockerfile文件, 方便对容器进行单独定制 (比如php可能需要安装一些扩展, 就可以放在自己的Dockerfile中进行) 因为挂载会使用 本机文件/目录 覆盖 容器中的目录/文件, 所以各服务的配置文件需要提前准备好, 然后挂载到容器服务的对应目录下(可以直接对应文件,也可以对应目录), 各容器的配置文件, 日志等的路径, 可以到Docker Hub上参考具体容器的详情 拿到各容器的中的的配置文件路径后, 需要使用先启动容器, 然后使用 docker cp 将配置文件/目录拷贝到本地 注意在挂载项目根目录site时需要注意, 由于nginx和php-fpm为各自独立的服务(分布式部署方式), 所以 本机site目录挂载到nginx容器中的项目目录 和 本机site目录挂载到php-fpm容器中的项目目录应该一样, 比如都是 ./site:/www 在nginx配置时稍加注意, php-fpm才可以正确找到项目路径 准备各容器镜像1234567891011121314renyimindembp:php56 renyimin$ docker pull nginx:1.12.2renyimindembp:php56 renyimin$ docker pull mongo:3.2renyimindembp:php56 renyimin$ docker pull redis:3.2renyimindembp:php56 renyimin$ docker pull mysql:5.6renyimindembp:php56 renyimin$ docker pull php:5.6-fpmrenyimindembp:php56 renyimin$ docker pull php:7.2-fpmrenyimindembp:php56 renyimin$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmysql 5.6 0248eeb807c7 4 days ago 256MBnginx 1.12.2 a810e4ccb26c 2 weeks ago 108MBphp 5.6-fpm aaf299648ff3 2 weeks ago 367MBphp 7.2-fpm 60245f64ed12 2 weeks ago 367MBredis 3.2 3859b0a6622a 2 weeks ago 99.7MBmongo 3.2 56d7fa068c3d 2 weeks ago 300MB 从容器中获取配置文件启动上面所有容器容器的启动方式最好也参考一下Docker Hub上相关资料 (比如mysql在启动时就需要-e设置环境变量来提供root用户密码) nginx 容器启动: $ docker run -d --name nginx nginx:1.12.2 mysql 容器启动: $ docker run --name mysql -e MYSQL_ROOT_PASSWORD=renyimin -d mysql:5.6 php:fpm5.6容器的启动: docker run -d --name php-5.6-fpm php:5.6-fpm 即可启动 php:fpm7.2容器的启动: docker run -d --name php-7.2-fpm php:7.2-fpm 即可启动 mongo容器的启动: docker run -d --name mg mongo:3.2 redis容器的启动: docker run -d --name redis redis:3.2 结果如下: 12345678renyimindembp:php56 renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES145a629c84be nginx:1.12.2 &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 80/tcp nginx72390d1e2aa1 mysql:5.6 &quot;docker-entrypoint...&quot; 56 seconds ago Up 56 seconds 3306/tcp mysql6b5019214fc1 mongo:3.2 &quot;docker-entrypoint...&quot; About a minute ago Up About a minute 27017/tcp mg7524ffe44739 redis:3.2 &quot;docker-entrypoint...&quot; About a minute ago Up About a minute 6379/tcp redis8716df7a08fb php:7.2-fpm &quot;docker-php-entryp...&quot; About an hour ago Up 27 minutes 9000/tcp php-7.2-fpm2d2d5bb12a68 php:5.6-fpm &quot;docker-php-entryp...&quot; About an hour ago Up About an hour 9000/tcp php-5.6-fpm docker cp 获取配置文件各容器的配置文件, 日志等的路径, 可以到Docker Hub上参考具体容器的详情最后需要手动调整一下目录结构即可 nginx 拷贝nginx默认配置文件到设定的 nginx/conf 目录 :docker cp 8d572e88f735:/etc/nginx/ /Users/renyimin/Desktop/docker-compose/nginx/conf/ mysql 拷贝mysql默认数据库文件到设定的 mysql/data 目录docker cp 72390d1e2aa1:/var/lib/mysql /Users/renyimin/Desktop/docker-compose/mysql/data 拷贝mysql默认配置文件到设定的 mysql/conf 目录docker cp 72390d1e2aa1:/etc/mysql/ /Users/renyimin/Desktop/docker-compose/mysql/conf/ php56 拷贝php56默认配置文件到设定的 php56/conf 目录docker cp 2d2d5bb12a68:/usr/local/etc/php/ /Users/renyimin/Desktop/docker-compose/php/php56/conf/docker cp 2d2d5bb12a68:/usr/local/etc/php-fpm.d/ /Users/renyimin/Desktop/docker-compose/php/php56/conf/ php72 拷贝php72默认配置文件到设定的 php72/conf 目录docker cp 8716df7a08fb:/usr/local/etc/php/ /Users/renyimin/Desktop/docker-compose/php/php72/conf/docker cp 8716df7a08fb:/usr/local/etc/php-fpm.d/ /Users/renyimin/Desktop/docker-compose/php/php72/conf/ mongo 拷贝mongo默认数据库文件到设定的 mongo/data 目录 (mongo容器的/data有configdb和db两个目录)docker cp 6b5019214fc1:/data/ /Users/renyimin/Desktop/docker-compose/mongo/ redis 拷贝redis默认数据库文件到设定的 redis/data 目录docker cp 7524ffe44739:/data /Users/renyimin/Desktop/docker-compose/redis/data 拷贝redis默认配置文件到设定的 redis/conf 目录docker cp 7524ffe44739:/usr/local/etc/redis/ /Users/renyimin/Desktop/docker-compose/redis/conf/ 整理各个服务的Dockerfile 首先, 每个服务的 FROM 和 MAINTAINER 需要写在Dockerfile的开头; 这里主要定制了php-fpm的 Dockerfile, 安装哪些扩展主要是参考 Docker Hub上php容器的介绍 https://github.com/twang2218/docker-lnmp 歪麦博客 最后在Dockerfile中完成对扩展的安装 如何安装扩展可以参考Docker Hub 和 这里 注意如何清理?1234# 貌似源码安装的软件需要如下进行清理:RUN docker-php-source extract \\ # 此处开始执行你需要的操作 \\ &amp;&amp; docker-php-source delete 如果稍后需要安装其他扩展, 在Dockerfile文件中定制即可, 然后 docker-compose up -d --build php(docker-composer.yml中设定的服务名) 重新构建即可 编写docker-compose.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071version: &quot;3&quot;services: nginx: build: context: ./nginx/ dockerfile: Dockerfile #args: ports: - &quot;8090:80&quot; volumes: - ./site/:/www:rw - ./nginx/conf/conf.d:/etc/nginx/conf.d/:ro - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro - ./nginx/log/:/var/log/nginx/:rw depends_on: - php networks: - frontend php: build: ./php/php72/ expose: - &quot;9000&quot; volumes: - ./site/:/www:rw - ./php/php72/conf/php/:/usr/local/etc/php/ - ./php/php72/conf/php-fpm.d/:/usr/local/etc/php-fpm.d/:rw - ./php/php72/log/php-fpm/:/var/log/:rw depends_on: - mysql - redis - mongo networks: - frontend - backend mysql: build: ./mysql/ ports: - &quot;3307:3306&quot; volumes: - ./mysql/conf/:/etc/mysql/:rw - ./mysql/data/:/var/lib/mysql/:rw - ./mysql/log/:/var/log/mysql/:rw env_file: ./mysql/env_config.env environment: TZ: &apos;Asia/Shanghai&apos; MYSQL_ROOT_PASSWORD: &quot;renyimin&quot; networks: - backend mongo: build: ./mongo/ ports: - 27018:27017 volumes: - ./mongo/data:/data:rw - ./mongo/data:/var/log/mongodb/:rw redis: build: ./redis/ volumes: - ./redis/conf/:/usr/local/etc/redis - ./redis/data/:/data:rw - ./redis/log/:/var/log/ ports: - &quot;6380:6379&quot;networks: frontend: backend: 编排编排构建过程可能会有点慢(视网络快慢, 因为需要安装一些php扩展) 小细节问题 如果修改docker-compose.yml文件中某个容器的配置, 需要如何单独重新构建这个容器? docker-compose up --no-deps -d &lt;SERVICE_NAME&gt; , 如下:1234567# 当docker-compose.yml中nginx服务的相关配置没有改动时renyimindembp:docker-compose renyimin$ docker-compose up --no-deps -d nginxdockercompose_nginx_1 is up-to-date# 当docker-compose.yml中nginx服务的相关配置被改动后renyimindembp:docker-compose renyimin$ docker-compose up --no-deps -d nginxRecreating dockercompose_nginx_1 ... donerenyimindembp:docker-compose renyimin$ 如果修改了某个容器的配置文件, 如何单独重启这个容器? 注意, 此时docker-compose up --no-deps -d nginx重新构建会发现docker-compose.yml中nginx服务的相关配置并没有改动, 所以不会生效 此时应该重启这个容器: docker-compose restart nginx 123renyimindembp:docker-compose renyimin$ docker-compose restart nginxRestarting dockercompose_nginx_1 ... donerenyimindembp:docker-compose renyimin$ 每次 docker-compose up 会重新构建所有容器 但是只构建docker-compose.yml中配置发生变化的容器1234567renyimindembp:docker-compose renyimin$ docker-compose up -ddockercompose_mongo_1 is up-to-datedockercompose_redis_1 is up-to-datedockercompose_mysql_1 is up-to-datedockercompose_php_1 is up-to-datedockercompose_nginx_1 is up-to-daterenyimindembp:docker-compose renyimin$ pecl扩展安装问题代理问题: 参考https://www.awaimai.com/2120.html讨论未完待续~~ (安装mongo, redis, memcached 扩展) 参考:https://yeasy.gitbooks.io/docker_practice/content/compose/commands.html#up参考:https://github.com/twang2218/docker-lnmp","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"14.Compose模板文件","slug":"docker/2017-10-19-14-docker","date":"2017-10-19T10:50:08.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/19/docker/2017-10-19-14-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/19/docker/2017-10-19-14-docker/","excerpt":"","text":"docker-compose.yml","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"13.Compose命令学习","slug":"docker/2017-10-19-13-docker","date":"2017-10-19T06:27:53.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/19/docker/2017-10-19-13-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/19/docker/2017-10-19-13-docker/","excerpt":"","text":"Compose命令说明 命令作用的对象: 对于Compose来说, 大部分命令所作用的对象既可以是项目本身, 也可以指定为项目中的服务或者容器。如果没有特别的说明, 命令针对的对象将是项目, 这意味着项目中所有的服务都会受到命令影响。 命令格式: docker-compose 命令的基本的使用格式是: docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 执行docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。 命令选项: -f, --file FILE 指定使用的Compose模板文件,默认为docker-compose.yml, 可以多次指定。 -p, --project-name NAME 指定项目名称,默认将使用所在目录名称作为项目名。 --x-networking 使用Docker的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出。 命令使用说明version打印compose的版本信息 docker-compose version config验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。 images列出Compose模板文件中包含的镜像 docker-compose images build ?? 构建(重新构建)项目中的服务容器, 格式为 docker-compose build [options] [SERVICE...] 服务容器一旦构建后, 将会带上一个标记名, 例如对于web项目中的一个db容器, 可能叫web_db 可以随时在项目目录下运行 docker-compose build 来重新构建服务 选项包括： --force-rm 删除构建过程中的临时容器 --no-cache 构建镜像过程中不使用 cache(这将加长构建过程) --pull 始终尝试通过 pull 来获取更新版本的镜像。 up 该命令十分强大, 它将尝试自动完成包括构建镜像, (重新)创建服务, 启动服务, 并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动, 除非已经处于运行状态; (可以说，大部分时候都可以直接通过该命令来启动一个项目。) 格式为 docker-compose up [options] [SERVICE…]。 默认情况, docker-compose up 启动的容器都在前台, 控制台将会同时打印所有容器的输出信息, 可以很方便进行调试。当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d, 将会在后台启动并运行所有的容器, 一般推荐生产环境下使用该选项。 默认情况, 如果服务容器已经存在, docker-compose up将会尝试停止容器, 然后重新创建(保持使用 volumes-from 挂载的卷), 以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。 如果用户不希望容器被停止并重新创建, 可以使用 docker-compose up --no-recreate。 这样将只会启动处于停止状态的容器, 而忽略已经运行的服务; 如果用户只想重新部署某个服务, 可以使用 docker-compose up --no-deps -d &lt;SERVICE_NAME&gt; 来重新创建服务并后台停止旧服务, 启动新服务, 并不会影响到其所依赖的服务。 port打印某个容器端口所映射的公共端口, 格式为 docker-compose port [options] SERVICE PRIVATE_PORT --protocol=proto 指定端口协议, tcp(默认值)或者 udp --index=index 如果同一服务存在多个容器, 指定命令对象容器的序号(默认为 1) down此命令将会停止 up 命令所启动的容器，并移除网络 run在指定服务上执行一个命令, 格式为 docker-compose run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...] exec进入指定的容器 kill通过发送SIGKILL信号来强制停止服务容器, 格式为 docker-compose kill [options] [SERVICE...]支持通过 -s 参数来指定发送的信号, 例如, 通过$ docker-compose kill -s SIGINT指令发送SIGINT信号 top查看各个服务容器内运行的进程","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"12.Docker三剑客 之 Compose","slug":"docker/2017-10-19-12-docker","date":"2017-10-19T03:07:53.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/19/docker/2017-10-19-12-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/19/docker/2017-10-19-12-docker/","excerpt":"","text":"Compose简介 Compose 项目是Docker官方的开源项目, 负责快速在集群中部署分布式应用。 Compose定位是「定义和运行多个Docker容器的应用(Defining and running multi-container Docker applications)」。 通过之前的学习，已经知道使用一个Dockerfile模板文件, 可以让用户很方便的定义一个单独的应用容器。然而, 在日常工作中, 经常会碰到需要多个容器相互配合来完成某项任务的情况。 例如要实现一个Web项目, 除了 Web服务容器 本身, 往往还需要再加上后端的 数据库服务容器，甚至还包括 负载均衡容器 等。 Compose 恰好满足了这样的需求, 它允许用户通过一个单独的 docker-compose.yml 模板文件(YAML格式)来定义 一组相关联的应用容器 为一个项目(project)。 Compose 中有两个重要的概念: 服务(service): 一个应用容器，实际上可以包括若干运行相同镜像的容器实例。 项目(project): 由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose的默认管理对象是项目, 通过子命令对项目中的一组容器进行便捷地生命周期管理。Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 安装与卸载 Compose 目前支持 Linux、macOS、Windows 10 三大平台。 Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至直接运行在 Docker 容器中。 PIP 安装 这种方式是将 Compose 当作一个 Python 应用来从 pip 源中安装。 执行安装命令: pip install -U docker-compose 安装成功后，可以查看 docker-compose 命令的用法: $ docker-compose -h","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"11.高级网络配置","slug":"docker/2017-10-19-11-docker","date":"2017-10-19T02:20:31.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/19/docker/2017-10-19-11-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/19/docker/2017-10-19-11-docker/","excerpt":"","text":"Docker虚拟网络原理 当Docker启动时, 会自动在主机上创建一个docker0虚拟网桥, 实际上是 Linux 的一个 bridge, 可以理解为一个软件交换机。它会在 挂载到它上面的哪些网口 之间转发信息。 同时, Docker随机分配一个本地未占用的私有网段中的一个地址给docker0接口, 比如典型的172.17.42.1,掩码为255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段(172.17.0.0/16)的地址。 当创建一个Docker容器的时候,同时会创建了一对veth pair接口(当数据包发送到一个接口时, 另外一个接口也可以收到相同的数据包) 这对接口一端在容器内,即 eth0; 另一端在本地并被挂载到docker0网桥, 名称以 veth 开头(例如 vethAQI2QT); 通过这种方式, 主机可以跟容器通信, 容器之间也可以相互通信, Docker就创建了在主机和所有容器之间一个虚拟共享网络。 接下来的部分将介绍在一些场景中, Docker 所有的网络定制配置, 以及通过 Linux 命令来调整、补充、甚至替换 Docker 默认的网络配置。 未完待续…. 参考: https://yeasy.gitbooks.io/docker_practice/content/advanced_network/","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"10.Docker中的网络功能","slug":"docker/2017-10-17-10-docker","date":"2017-10-17T10:15:23.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/17/docker/2017-10-17-10-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/17/docker/2017-10-17-10-docker/","excerpt":"","text":"外部访问容器端口映射 容器中可以运行一些网络应用, 要让外部也可以访问这些应用, 可以通过 -P 或 -p 参数来指定端口映射。 1$ docker run -d -p 8090:80 --name testVip --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha,readonly vipservice 使用 docker ps 可以看到本地主机的 8090 被映射到了容器的 80 端口。 1234renyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES27863a3a8f70 vipservice &quot;/bin/sh -c &apos;/run....&quot; 39 minutes ago Up 39 minutes 3306/tcp, 15672/tcp, 0.0.0.0:8090-&gt;80/tcp testViprenyimindeMacBook-Pro:testVip renyimin$ 此时访问本机的 8090 端口即可访问容器内 web 应用提供的界面。 查看映射端口配置 使用 docker port 容器名 来查看当前映射的端口配置, 也可以查看到绑定的地址 123456renyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES27863a3a8f70 vipservice &quot;/bin/sh -c &apos;/run....&quot; 45 minutes ago Up 45 minutes 3306/tcp, 15672/tcp, 0.0.0.0:8090-&gt;80/tcp testViprenyimindeMacBook-Pro:testVip renyimin$ docker port testVip80/tcp -&gt; 0.0.0.0:8090renyimindeMacBook-Pro:testVip renyimin$ 注意: -p 可以多次使用来绑定多个端口 容器有自己的内部网络和 ip 地址(使用 docker inspect 可以获取所有的变量，Docker 还可以有一个可变的网络配置) 容器互联容器的连接(linking)系统是除了端口映射外, 另一种跟容器中应用交互的方式。该系统会在源和接收容器之间创建一个隧道, 接收容器可以看到源容器指定的信息。相对于端口映射, 这种方式主要用来做内部链接, 不暴露链接接口给外部 自定义容器命名 连接系统依据容器的名称来执行。因此，首先需要自定义一个好记的容器命名。虽然当创建容器的时候, 系统默认会分配一个名字。 自定义命名容器有2个好处: 自定义的命名，比较好记，比如一个web应用容器我们可以给它起名叫web 当要连接其他容器时候，可以作为一个有用的参考点，比如连接web容器到db容器 使用 --name 标记可以为容器自定义命名(使用 docker ps 来验证设定的命名) 注意:容器的名称是唯一的。如果已经命名了一个叫 web 的容器，当你要再次使用 web 这个 名称的时候，需要先用 docker rm 来删除之前创建的同名容器。–rm在执行 docker run 的时候如果添加 --rm 标记，则容器在终止后会立刻删除。注意, --rm 和 -d 参数不能同时使用。 容器互联使用 --link name:alias 参数可以让容器之间安全的进行交互, 其中 name 是要链接的容器的名称, alias 是这个连接的别名; 下面先创建一个新的数据库容器 123renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 3307:3306 --name db --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha,readonly vipservicefd29cafa699da868a57f559630c6fb7f5dcf30705f884c9fa7feb4a373bba37crenyimindeMacBook-Pro:testVip renyimin$ 然后创建一个新的 web 容器，并将它连接到 db 容器 123renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 8090:80 --name web --link db:db --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha,readonly vipservice0e20f11d0f57fedb9d5fee96c8d9d56b67a7c62ded101fd92abf12bba04d09e4renyimindeMacBook-Pro:testVip renyimin$ 使用 docker ps 来查看容器的连接 12345renyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0e20f11d0f57 vipservice &quot;/bin/sh -c &apos;/run....&quot; About a minute ago Up About a minute 3306/tcp, 15672/tcp, 0.0.0.0:8090-&gt;80/tcp webfd29cafa699d vipservice &quot;/bin/sh -c &apos;/run....&quot; About a minute ago Up About a minute 80/tcp, 15672/tcp, 0.0.0.0:3307-&gt;3306/tcp dbrenyimindeMacBook-Pro:testVip renyimin$ (理论上可以看到自定义命名的容器 db 和 web, db 容器的 names 列有 db 也有 web/db。这表示 web 容器链接到 db 容器，web 容器将被允许访问 db 容器的信息。) 但实际上貌似没有在names列看到有web/db Docker 在两个互联的容器之间创建了一个安全隧道，而且不用映射它们的端口到宿主主机 上。在启动 db 容器的时候可以不用使用 -p 和 -P 标记，从而避免了暴露数据库端口到外部网络上。 Docker 通过 2 种方式为容器公开连接信息: 环境变量 更新 /etc/hosts 文件 使用 env 命令来查看 web 容器的环境变量 1$ docker run --rm --name web2 --link db:db vipservice env 更多参考书上… 除了环境变量, Docker 还添加 host 信息到父容器的 /etc/hosts 的文件。下面是父容器 web 的 hosts 文件 这里有 2 个 hosts，第一个是 web 容器，web 容器用 id 作为他的主机名，第二个是 db 容器 的 ip 和主机名。 123456789101112131415renyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES297c83c40bc4 vipservice &quot;/bin/sh -c &apos;/run....&quot; 9 minutes ago Up 8 minutes 3306/tcp, 15672/tcp, 0.0.0.0:8090-&gt;80/tcp web46985aa1ab7b vipservice &quot;/bin/sh -c &apos;/run....&quot; 9 minutes ago Up 9 minutes 80/tcp, 15672/tcp, 0.0.0.0:3307-&gt;3306/tcp dbrenyimindeMacBook-Pro:testVip renyimin$ docker exec -it web /bin/shsh-4.2# cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 db 46985aa1ab7b172.17.0.3 297c83c40bc4sh-4.2# 可以在 web 容器中安装 ping 命令来测试跟db容器的连通, 用 ping 来测试db容器，它会解析成 172.17.0.2 123456sh-4.2# ping dbPING db (172.17.0.2) 56(84) bytes of data.64 bytes from db (172.17.0.2): icmp_seq=1 ttl=64 time=0.243 ms64 bytes from db (172.17.0.2): icmp_seq=2 ttl=64 time=0.330 ms64 bytes from db (172.17.0.2): icmp_seq=3 ttl=64 time=0.136 ms64 bytes from db (172.17.0.2): icmp_seq=4 ttl=64 time=0.129 ms 用户可以链接多个父容器到子容器，比如可以链接多个 web 到 db 容器上。 参考: https://yeasy.gitbooks.io/docker_practice/content/network/","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"09.Docker数据管理","slug":"docker/2017-10-17-09-docker","date":"2017-10-17T06:40:23.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/17/docker/2017-10-17-09-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/17/docker/2017-10-17-09-docker/","excerpt":"","text":"容器中管理数据主要有两种方式：数据卷(Volumes) 和 挂载主机目录(Bind mounts) 数据卷(Volumes) 数据卷 是一个可供一个或多个容器使用的特殊目录, 它绕过UFS, 可以提供很多有用的特性: 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意: 数据卷 的使用, 类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉, 能显示看的是挂载的数据卷。 数据卷操作(在主机里使用) 创建一个数据卷: docker volume create my-vol (其实还有一种方式就是在docker run的时候直接指定一个数据卷名, 就会自动帮你创建数据卷) 查看所有数据卷: docker volume ls 查看指定数据卷的信息: docker volume inspect my-vol 查看容器的数据卷挂载信息: docker inspect 容器名 删除数据卷 $ docker volume rm my-vol 数据卷 是被设计用来持久化数据的, 它的生命周期独立于容器, Docker不会在容器被删除后自动删除数据卷, 并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷。 如果需要在删除容器的同时移除数据卷, 可以在删除容器的时候使用 docker rm -v 这个命令, 这个命令貌似只是移除该容器和数据卷之间的关系, 除非这个数据卷没有任何容器引用了, 才可以使用下面介绍的命令来删除掉。 无主的数据卷可能会占据很多空间，要清理请使用命令 $ docker volume prune 可以看到清除时会提醒你 WARNING! This will remove all volumes not used by at least one container 清除的是没有被至少一个容器使用的数据卷! 创建并启动容器时,挂载数据卷 在使用 docker run 命令的时候, 还可以使用 --mount 参数来将数据卷挂载到容器里, 另外, 在一次 docker run 中可以挂载多个数据卷。 下面创建一个名为 my-first-vol 的数据卷 12345678910111213141516localhost:~ renyimin$ docker volume create my-first-volmy-first-vollocalhost:~ renyimin$ localhost:~ renyimin$ docker volume inspect my-first-vol[ &#123; &quot;CreatedAt&quot;: &quot;2017-04-26T13:43:16Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-first-vol/_data&quot;, &quot;Name&quot;: &quot;my-first-vol&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;]localhost:~ renyimin$ 创建并运行一个名为 web 的容器, 同时加载上面的数据卷到容器内的 /test-vol 目录 1234567localhost:~ renyimin$ docker run -d -p 5000:5000 --name myFirstRegistry --mount source=my-first1-vol,target=/test-vol registry45e2da7a8ee3c51ae682ff78d02c9779531a79fe2f7a58739023cd19ab48b09dlocalhost:~ renyimin$ docker exec -it myFirstRegistry /bin/sh/ # lsbin entrypoint.sh home linuxrc mnt root sbin sys tmp vardev etc lib media proc run srv test-vol usr/ # 容器中会自动创建虚拟机中的挂载目录; 另外, 之前我们创建的数据卷是 my-first-vol, 此次运行容器时, 加载的数据卷却是 my-first1-vol,后面通过查看容器在&quot;Mounts&quot;key下面的数据卷信息, 会发现在启动容器时如果指定的数据卷不存在, 则会自动创建;通过docker volume ls也可以看到现在有my-first-vol 和 my-first1-vol 这两个我们创建的数据卷; 可以在主机里使用以下命令查看 myFirstRegistry 容器的信息, 数据卷信息在 &quot;Mounts&quot; Key 下面 1234567891011121314151617181920212223$ docker inspect myFirstRegistry&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;my-first1-vol&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/my-first1-vol/_data&quot;, &quot;Destination&quot;: &quot;/test-vol&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;, &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;4fde6460059e4f0f07bc1c91ed852da7884b872dffc66bc4e237a260c7248250&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/4fde6460059e4f0f07bc1c91ed852da7884b872dffc66bc4e237a260c7248250/_data&quot;, &quot;Destination&quot;: &quot;/var/lib/registry&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;], 挂载主机目录 挂载一个主机目录作为数据卷: 使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去 12345678910$ docker run -d -p 8090:80 --name testVip --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha vipservice2dea428aa379aaf415e5eff38b76f43d64e77b6b34d25bb5e353354ba897cee6renyimindeMacBook-Pro:testVip renyimin$ docker exec -it 2dea428aa379aaf415e5eff38b76f43d64e77b6b34d25bb5e353354ba897cee6 /bin/shsh-4.2# cd /sh-4.2# lsanaconda-post.log bin data dev etc haha home lib lib64 lost+found media mnt opt proc root run run.sh sbin srv sys tmp usr varsh-4.2# cd hahash-4.2# lsmyfirstregistry registry.tarsh-4.2# 上面的命令会加载主机的 /Users/renyimin/Desktop/testVip 目录到容器的 /haha 目录, 这个功能在进行测试的时候十分方便, 比如, 你可以放置一些程序到本地目录中,来查看容器是否正常工作。本地目录的路径必须是绝对路径, 以前使用 -v 参数时, 如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 --mount 参数时如果本地目录不存在, Docker 会报错。 查看容器的挂载信息, 发现和数据卷相比, Type信息是bind而不是volume, 并且没有数据卷的name信息, docker volume ls 也不会看到有新的数据卷被创建, 所以…可以认为只是一次简单的目录绑定 1234567891011renyimindeMacBook-Pro:testVip renyimin$ docker inspect testVip&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/renyimin/Desktop/testVip&quot;, &quot;Destination&quot;: &quot;/haha&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;] 选择 -v 还是 --mount 参数? Docker 新用户应该选择 --mount 参数，经验丰富的 Docker 使用者对 -v 或者 --volume 已经很熟悉了，但是推荐使用 --mount 参数; 可以理解为, --mount 参数应该可以挂载数据卷, 也可以代替-v来进行目录关联。 Docker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读 加了 readonly 之后，就挂载为 只读 了。如果你在容器内 /haha 目录新建文件，会显示如下错误 1234567891011121314renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 8090:80 --name testVip --mount type=bind,source=/Users/renyimin/Desktop/testVip,target=/haha,readonly vipservice27863a3a8f70fa4bddb9c97fabfee2db7f35d5615d4b90ad0be13717dc23d092renyimindeMacBook-Pro:testVip renyimin$ docker exec -it 27863a3a8f70fa4bddb9c97fabfee2db7f35d5615d4b90ad0be13717dc23d092 /bin/shsh-4.2# sh-4.2# cd /sh-4.2# lsanaconda-post.log bin data dev etc haha home lib lib64 lost+found media mnt opt proc root run run.sh sbin srv sys tmp usr varsh-4.2# cd hahash-4.2# lsmyfirstregistry registry.tar// 可以看到报错了sh-4.2# touch a.txttouch: cannot touch &apos;a.txt&apos;: Read-only file systemsh-4.2# 查看数据卷的具体信息 $ docker inspect testVip 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/renyimin/Desktop/testVip&quot;, &quot;Destination&quot;: &quot;/haha&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: false, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 数据卷挂载的问题上面提到的数据卷, 对很多容器都非常有用, 比如 mysql容器中存储数据文件的 /var/lib/mysql 目录你就需要挂载数据卷; mysql, php-fpm, nginx等容器中, 关于服务配置的目录你也需要挂载到数据卷, 这些配置你可能需要进行改动; 但是挂载数据卷有个问题, 一旦挂载之后, 容器中的目录就是空的, 原本服务的配置文件就被清空了, 也就导致有些容器在挂载数据卷之后, 无法正常启动; docker cp 命令 可以将本地目录/文件拷贝到容器, 也可以将容器中的目录/文件拷贝到本地; 格式: docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- 所以为了避免挂载导致容器无法正常启动, 挂载的可以这样来: 先确定你需要挂载的容器中目录的位置(比如: nginx容器中的配置文件在/etc/nginx/conf.d/default.conf ) 使用 docker cp 命令, 将需要映射的目录从容器复制到本地; (比如: docker cp nginx_test:/etc/nginx/ ./conf/) 然后再将本地default.conf文件挂载到nginx容器的/etc/nginx/conf.d/default.conf 参考: https://yeasy.gitbooks.io/docker_practice/content/data_management/","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"08.容器--基本操作","slug":"docker/2017-10-17-08-docker","date":"2017-10-17T04:01:08.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/17/docker/2017-10-17-08-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/17/docker/2017-10-17-08-docker/","excerpt":"","text":"容器1.镜像(Image)和容器(Container)的关系, 就像是面向对象程序设计中的 类 和 实例 的关系一样, 镜像是静态的定义, 容器是镜像运行时的实体。容器可以被 创建、启动、停止、删除、暂停等。 2.容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。 因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID空间。 容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会把容器和虚拟机搞混。 3.前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器的删除而丢失。 4.按照 Docker 最佳实践的要求, 容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化 所有的文件写入操作，都应该使用数据卷(Volume)、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器可以随意删除、重新run，数据却不会丢失。 容器操作启动容器有两种方式：一种是基于镜像新建一个容器并启动, 另外一个是将在终止状态(stopped)的容器启动 创建并启动1.因为 Docker 的容器实在太轻量级了, 很多时候用户都是随时删除和新创建容器。 2.新建并启动一个容器, 所需要的命令主要为 docker run, 例如: $ docker run -d -p 5000:5000 --name myFirstRegistry registry, 是根据名为registry的镜像创建并运行一个名为myFirstRegistry容器; 3.当利用 docker run 来创建容器时, Docker 在后台运行的标准操作包括: 检查本地是否存在指定的镜像, 不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统, 并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 4.容器被启动后, 设置的挂载目录, 端口映射都会随着此容器, 容器stop后, 再次start, 这些设置都还在; 启动已终止容器可以利用 docker start [containerID or NAME] 命令, 直接将一个已经终止的容器启动运行。 守护态运行容器其实更多时候, 我们需要让容器在后台运行, 而不是直接运行容器并展示出结果, 此时只用在运行时加上 -d 参数即可; (在容器的第一种启动方式中已经介绍过了) 查看容器信息可以通过 docker ps 命令来查看正在运行的容器信息 可以通过 docker ps -a 命令来查看 正在运行的和终止的 容器信息 终止容器可以使用 docker stop [containerID or NAME] 来终止一个运行中的容器 重启容器可以使用 docker restart [containerID or NAME] 来重启一个运行中的容器 删除容器1.可以使用 docker rm 容器ID/容器NAME 来删除一个处于终止状态的容器。 2.如果要删除一个运行中的容器，可以添加 -f 参数。 进入容器可参考书中介绍 本人通常使用 docker exec -it [containerID or NAME] /bin/sh","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"07.Docker Registry 仓库","slug":"docker/2017-10-17-07-docker","date":"2017-10-17T03:30:08.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/17/docker/2017-10-17-07-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/17/docker/2017-10-17-07-docker/","excerpt":"","text":"公开 Docker RegistryDocker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务 一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的Registry公开服务是官方的 Docker Hub, 这也是默认的 Registry，并拥有大量的高质量的官方镜像。 由于某些原因，在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对 Docker Hub 的镜像服务(Registry Mirror)，这些镜像服务被称为加速器。 但是, 有时候使用 Docker Hub 或其他公共仓库可能不方便(比如, 有时候我们的服务器无法访问互联网 或者 你不希望将自己的镜像放到公网当中)，则可以创建一个 本地仓库供 私人使用。 私有 Docker Registry1.除了使用公开服务外, 用户还可以在本地搭建私有Docker Registry, docker-registry是官方提供的工具, 可以用于构建私有的镜像仓库。 2.安装运行 docker-registry 你可以通过获取官方registry镜像来在本地运行一个自己的私有镜像仓库 (如 $ docker run -d -p 5000:5000 --restart=always --name registry registry, 将使用官方的registry镜像来启动一个私有仓库) 默认情况下, 仓库中的镜像会被创建在容器的 /var/lib/registry 目录下, 你可以通过 -v 参数来将镜像文件存放到本地的指定路径中。 另外, 可以将私有仓库的配置文件指定到本地的路径下 (如 ~/Desktop/registry-config/ 下 ) 3.我们大可不必这么麻烦, 只是简单运行一个私有仓库服务 $ docker run -d -p 5000:5000 --restart=always --name registry registry 查看私有仓库中镜像1.用 curl 查看仓库中的镜像, 可以看到你的私有仓库暂时还是空的 123$ curl 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125;$ 2.还可以在浏览器中直接查看私有仓库中的镜像(并且内网其他机器也可以通过内网地址来访问你所搭建的私有仓库的镜像): 上传镜像到私有仓库中1.之前我们已经通过获取官方 registry镜像 来创建好了自己的私有仓库, 接下来就可以使用 docker tag 来标记一个镜像, 然后推送它到仓库。 2.先查看一下本地已有的镜像 docker image ls : 12345678$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEregistry latest d1fd7d86a825 4 weeks ago 33.3MBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 3.使用 docker tag 将 registry:lates 这个镜像标记为一个新的本地镜像 127.0.0.1:5000/registry:latest ; 格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG] 12345678910$ docker tag registry:latest 127.0.0.1:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 这样, 假设你的小组成员需要尝试在本地搭建自己的私有仓库的话，就不用去公共镜像仓库去下载了, 只用在内网就可以方便地下载registry镜像 4.使用 docker push 上传标记的镜像 12345678$ docker push 127.0.0.1:5000/registry:latestThe push refers to a repository [127.0.0.1:5000/registry]9113493eaae1: Pushed 621c2399d41a: Pushed 59e80739ed3f: Pushed febf19f93653: Pushed e53f74215d12: Pushed latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 5.然后查看仓库中的镜像，可以看到镜像已经被成功上传了 curl 查看 12curl 127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;registry&quot;]&#125; 浏览器查看 上传私有仓库问题1.如果上传的时候, 打包的镜像使用的是本机的内网地址, 最后在上传的时候, 你会发现如下报错信息: 1234$ docker push 192.168.1.3:5000/registry:latestThe push refers to a repository [192.168.1.3:5000/registry]Get https://192.168.1.3:5000/v2/: http: server gave HTTP response to HTTPS clientrenyimindembp:vipvip renyimin$ 2.此时, 你需要将内网地址配置到本机docker的 insecure registries 中, 如下: 3.之后, 无论本机还是在同一内网中的其他机器也都可以推送镜像到仓库中了(之前打包好的两个镜像, 都可以成功推送到私有仓库中): 1234567891011121314151617$ docker push 192.168.1.3:5000/registryThe push refers to a repository [192.168.1.3:5000/registry]9113493eaae1: Pushed 621c2399d41a: Pushed 59e80739ed3f: Pushed febf19f93653: Pushed e53f74215d12: Pushed latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 $ docker push 127.0.0.1:5000/registryThe push refers to a repository [127.0.0.1:5000/registry]9113493eaae1: Layer already exists 621c2399d41a: Layer already exists 59e80739ed3f: Layer already exists febf19f93653: Layer already exists e53f74215d12: Layer already exists latest: digest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c size: 1364 从私有仓库中下载镜像1.先删除已有镜像 1234567891011121314151617181920212223242526$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEregistry latest d1fd7d86a825 4 weeks ago 33.3MB127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MB192.168.1.3:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GB$ docker image rm 127.0.0.1:5000/registry:latest 192.168.1.3:5000/registry:latestUntagged: 127.0.0.1:5000/registry:latestUntagged: 127.0.0.1:5000/registry@sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cUntagged: 192.168.1.3:5000/registry:latestUntagged: 192.168.1.3:5000/registry@sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6c$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEregistry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBrenyimindembp:vipvip renyimin$ 2.再尝试从私有仓库中下载这个镜像 (两个地址都可以下载, 也是因为之前配置了 Insecure registries, 这里最后才可以使用内网地址来下载) 123456789101112131415161718192021222324252627282930$ docker pull 127.0.0.1:5000/registry:latestlatest: Pulling from registryDigest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cStatus: Downloaded newer image for 127.0.0.1:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GB$ docker pull 192.168.1.3:5000/registry:latestlatest: Pulling from registryDigest: sha256:feb40d14cd33e646b9985e2d6754ed66616fedb840226c4d917ef53d616dcd6cStatus: Downloaded newer image for 192.168.1.3:5000/registry:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE192.168.1.3:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBregistry latest d1fd7d86a825 4 weeks ago 33.3MB127.0.0.1:5000/registry latest d1fd7d86a825 4 weeks ago 33.3MBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.3 47c844c76c53 2 months ago 2.92GBvipservice latest 47c844c76c53 2 months ago 2.92GBdocker-registry.haodai.com:80/devhdjfapi.haodai.com 1.1.1 52bd20b1d39b 3 months ago 2.46GBdevhdjfapi.haodai.com_full latest 52bd20b1d39b 3 months ago 2.46GBoldvip.haodai.com latest 52bd20b1d39b 3 months ago 2.46GB 几个简单问题1.删除仓库镜像 自己的docker仓库中存放的镜像, 时间长了难免存在一些废弃的镜像在里面, 如果不删除就造成空间的浪费。2.容器启动之后, 如果忘记挂载某个目录, 能否再进行挂载?其实没有必要, 直接停止删除, 重开一个即可！ ~~未完待续~~","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"06.其他方法定制 -- (镜像/容器 的 导入导出)","slug":"docker/2017-10-14-06-docker","date":"2017-10-14T12:15:28.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/14/docker/2017-10-14-06-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/14/docker/2017-10-14-06-docker/","excerpt":"","text":"镜像 (save/load)1.docker save nginx:v1 &gt; alpine-latest.tar.gz：将指定镜像导出为 镜像存储文件; 2.再使用 docker load -i alpine-latest.tar.gz 命令将 镜像存储文件 导入到本地镜像库; 容器(export/import)1.docker export container_id &gt; my_container.tar：将一个容器导出为 容器快照文件;2.再使用 docker import my_container.tar imageName:tag 命令将 容器快照文件 导入为一个新的镜像到本地镜像库;3.测试, 将nginx_v1容器导出, 并在本地重新生成nginx:v3镜像, 然后运行1234567891011121314151617181920// 运行中的nginx_v1容器renyimindeMacBook-Pro:Desktop renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd5d17a25cc35 nginx:v1 &quot;nginx -g &apos;daemon ...&quot; 2 hours ago Up 11 minutes 0.0.0.0:8089-&gt;80/tcp nginx_v10e7070854958 registry &quot;/entrypoint.sh /e...&quot; 26 hours ago Up About an hour 0.0.0.0:5000-&gt;5000/tcp registry//导出容器为 容器快照文件renyimindeMacBook-Pro:Desktop renyimin$ docker export nginx_v1 &gt; nginx_v1.tar//将 nginx_v1的容器快照文件, 重新导入为一个新的本地镜像 nginx:v3renyimindeMacBook-Pro:Desktop renyimin$ docker import nginx_v1.tar nginx:v3sha256:985fba7fd176d79685322d184bc79b81ca4365619bc0a773672e5c87cfc701cdrenyimindeMacBook-Pro:Desktop renyimin$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx v3 985fba7fd176 10 seconds ago 107MBnginx v1 8b748aee8b23 2 hours ago 109MBnginx latest e548f1a579cf 8 days ago 109MB// 运行新镜像, 发现报错renyimindeMacBook-Pro:Desktop renyimin$ docker run -d --name nginx_v3 nginx:v3docker: Error response from daemon: No command specified.See &apos;docker run --help&apos;.renyimindeMacBook-Pro:Desktop renyimin$ 4.解决方案, 通过docker import 导入容器快照文件生成的镜像, 在初次创建并启动(run)时, 需要提供快照文件所对应的容器的COMMAND字段, 可以通过docker ps查看, 如果命令太长, 则需要使用 docker inspect 容器名 查看 cmd 字段12345678renyimindeMacBook-Pro:Desktop renyimin$ docker run -d --name nginx_v3 nginx:v3 nginx &apos;-g&apos; &apos;daemon off;&apos;0fb30bafe266248bbb58e31d9dcab7479f4a59494fcc636f7f7dfd2df498066drenyimindeMacBook-Pro:Desktop renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0fb30bafe266 nginx:v3 &quot;nginx -g &apos;daemon ...&quot; 2 seconds ago Up 1 second nginx_v3d5d17a25cc35 nginx:v1 &quot;nginx -g &apos;daemon ...&quot; 2 hours ago Up 19 minutes 0.0.0.0:8089-&gt;80/tcp nginx_v10e7070854958 registry &quot;/entrypoint.sh /e...&quot; 26 hours ago Up About an hour 0.0.0.0:5000-&gt;5000/tcp registryrenyimindeMacBook-Pro:Desktop renyimin$ 注意1.可以使用 docker load 来导入镜像存储文件到本地镜像库。2.也可以使用 docker import 来导入一个容器快照文件到本地镜像库。3.这两者的区别在于: 镜像存储文件将保存完整记录, 体积也要大 而容器快照文件将丢弃所有的历史记录和元数据信息(即仅保存容器当时的快照状态) 此外，从容器快照文件导入时可以重新指定标签等元数据信息 另外, 使用docker save保存的镜像存储文件, 不能使用 docker import 来导入成镜像, 这样导入的镜像无法运行起来 构建镜像在初次为公司内部准备镜像时, 可以启动一个基础镜像, 然后在容器中做好软件安装, 完成各种配置工作之后: 可以手动commit成功新的 镜像:tag, 然后将镜像push到私有仓库供给大家下载使用; 可以简单将容器直接export成容器快照文件, 然后import为新的 镜像:tag, 然后push到私有仓库供给大家下载使用; 或者通过Dockerfile定制脚本及docker build来构建新的 镜像:tag, 然后push到私有仓库供给大家下载使用; 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡, 这里的消亡是指容器被删除, 而不是stop容器, stop容器后, 容器中发生的改变不会被忽略, 除非容器被删除掉 小结 如果只是做集成环境的镜像, 比如lnmp全部在一个镜像中运行, 自己觉着可以: 使用一个基础镜像运行起容器, 然后自己安装各种环境, 最后可以直接 export 导出容器为快照文件, 然后分发给组员;(如果需要再次修改环境信息, 直接修改好, 然后导出新的容器快找文件给组员即可) 可以在容器中构建好各环境之后, commit 手动构建好镜像, 然后将镜像 push 到仓库中, 供组员使用 (变动频次较高的时候不方便, 因为这样会导致容器变得越来越臃肿); 可以使用Dockerfile对镜像进行定制(如果镜像比较基础, 可能编写的比较多), 然后将镜像 push 到仓库中, 供组员使用 如果做多容器部署, 需要使用到docker-compose….未完待续~~ 参考 https://yeasy.gitbooks.io/docker_practice/content/image/other.html https://yeasy.gitbooks.io/docker_practice/content/container/import_export.html","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"05.镜像 - Dockerfile常见指令详解","slug":"docker/2017-10-13-05-docker","date":"2017-10-13T02:25:07.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/13/docker/2017-10-13-05-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/13/docker/2017-10-13-05-docker/","excerpt":"","text":"FROM 指定基础镜像 所谓定制镜像, 是以一个镜像为基础, 在其上进行定制。FROM 就用来指定基础镜像, 因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 格式: FROM 镜像名[:标签] MAINTAINER 指定镜像的作者和联系方式信息(执行docker inspect image 输出中有相应的字段记录该信息) 格式:MAINTAINER author &quot;e-mail&quot; 指定作者名和E-mail RUN 执行命令 RUN指令是用来执行命令行命令的。(由于命令行的强大能力, RUN指令在定制镜像时是最常用的指令之一) 其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样, 如: RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#39; &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;] , 这更像是函数调用中的格式, 如: RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;]。 虽然RUN就像Shell脚本一样可以执行命但是我们也不能像Shell脚本一样把每个命令对应一个RUN, 因为每一个RUN的行为, 就像手工建立commit镜像的过程一样:新建立一层，在其上执行这些命令, 执行结束后, commit 这一层的修改, 构成新的镜像。 像下面的这种写法，创建了 7 层镜像, 这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。这是很多初学 Docker 的人常犯的一个错误。 123456789FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 上面的 Dockerfile 正确的写法应该是这样： 1234567891011121314FROM debian:jessieRUN buildDeps=&apos;gcc libc6-dev make&apos; \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot; \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层, 这只是一层的事情。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，因为镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 ENV 设置环境变量 这个指令很简单,就是设置环境变量而已,无论是后面的其它指令,如RUN,还是运行时的应用,都可以直接使用这里定义的环境变量。 ENV设置环境变量格式有两种: ENV &lt;key&gt; &lt;value&gt; ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 下面例子中演示了”设置环境变量”, “如何换行”, 以及对”含有空格的值用双引号括起来”的办法, 这和Shell下的行为是一致的 12ENV VERSION=1.0 DEBUG=on \\NAME=&quot;Happy Feet&quot; ARG 构建参数 构建参数和ENV的效果一样, 都是设置环境变量。所不同的是,ARG所设置的环境变量,在将来容器运行时是不会存在这些环境变量的;但是不要因此就使用ARG保存密码之类的信息, 因为 docker history 还是可以看到所有值的; Dockerfile 中的ARG指令是定义参数名称及其默认值, 该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖 格式: ARG &lt;参数名&gt;[=&lt;默认值&gt;] WORKDIR 切换工作目录 使用 WORKDIR 指令可以来指定接下来之后各层的工作目录, 如该目录不存在, WORKDIR 会帮你建立目录。 格式为 WORKDIR &lt;工作目录路径&gt; 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写, 这种错误的理解还可能会导致出现下面这样的错误: 如果将下面这个 Dockerfile 进行构建镜像运行后, 会发现找不到 /app/world.txt 文件，或者其内容不是 hello 12RUN cd /app #第一层 `RUN cd /app` 的执行仅仅是当前进程的工作目录变更, 一个内存上的变化而已, 其结果不会造成任何文件变更。RUN echo &quot;hello&quot; &gt; world.txt #而到第二层的时候, 启动的是一个全新的容器, 跟第一层的容器更完全没关系, 自然不可能继承前一层构建过程中的内存变化 原因其实很简单, 在Shell中, 上面连续的两行是同一个进程执行环境, 而在 Dockerfile 中, 这两行 RUN 命令的执行环境根本不同, 是两个完全不同的容器。(这就是对 Dokerfile 构建分层存储的概念不了解所导致的错误) 之前说过每一个RUN都是启动一个容器、执行命令、然后提交存储层文件变更 USER 指定当前用户 USER 指令和 WORKDIR 相似, 都是改变环境状态并影响以后的层 (WORKDIR是改变工作目录, 而USER则是改变之后层执行RUN,CMD以及ENTRYPOINT这类命令时的身份)。 格式: USER &lt;用户名&gt; 注意: USER只是帮助你切换到指定用户而已, 这个用户必须是事先建立好的, 否则无法切换。 如果以root执行的脚本, 在执行期间希望改变身份, 比如希望以某个已经建立好的用户来运行某个服务进程, 不要使用 su 或者 sudo, 这些都需要比较麻烦的配置, 而且在TTY缺失的环境下经常出错。 建议使用 gosu ，可以从其项目网站看到进一步的信息12345678# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64&quot; \\ &amp;&amp; chmod +x /usr/local/bin/gosu \\ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ] VOLUME 定义匿名卷 容器运行时应该尽量保持容器存储层不发生写操作, 对于像数据库这类需要保存动态数据的应用, 其数据库文件应该保存于卷(volume)中。 为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在Dockerfile中, 可以事先指定某些目录挂载为匿名卷, 这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 格式为: VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] VOLUME /data : 容器中的 “/data” 目录就会在运行时自动挂载为匿名卷, 任何向 /data 中写入的信息都不会记录进容器存储层。当然，运行时可以覆盖这个挂载设置。比如:docker run -d -v mydata:/data xxxx 在这行命令中, 就使用了 mydata 这个命名卷挂载到了 /data 这个位置, 替代了 Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE 声明端口 EXPOSE指令是声明运行时为容器提供的映射端口, 这只是一个声明, 并不会自动在宿主进行端口映射。 格式为:EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] 在 Dockerfile 中写入这样的声明有两个好处 一个是帮助镜像使用者理解这个镜像服务的守护端口, 以方便配置映射; 另一个用处则是在运行时使用随机端口映射时, 也就是 docker run -P 时, 会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和 在运行时使用-p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来 -p 是映射宿主端口和容器端口, 换句话说, 就是将容器的对应端口服务公开给外界访问; 而 EXPOSE 仅仅是声明, 容器打算使用什么端口而已, 并不会自动在宿主进行端口映射; CMD 容器启动命令 CMD 指令的格式和 RUN 相似，也是两种格式: shell 格式: CMD &lt;命令&gt; exec 格式: CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 但是如果指定了 ENTRYPOINT 指令，用 CMD 指定的就是具体的参数 CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。 Docker不是虚拟机, 容器就是进程, 那么在启动容器的时候, CMD 指令就是用于指定默认的容器主进程的启动命令的。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令比如ubuntu镜像默认的CMD是/bin/bash, 如果我们直接 docker run -it ubuntu 的话, 会直接进入 bash ;我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release, 这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了, 输出了系统版本信息。 在指令格式上, 一般推荐使用 exec 格式, 这类格式在解析时会被解析为JSON数组，因此一定要使用双引号, 而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为sh -c的参数的形式进行执行。比如 CMD echo $HOME 在实际执行中，会将其变更为 CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]这就是为什么我们可以使用环境变量的原因, 因为这些环境变量会被shell进行解析处理。 Docker不是虚拟机, 容器中的应用都应该以前台执行, 而不是像虚拟机、物理机里面那样, 用 upstart/systemd 去启动后台服务, 容器内没有后台服务的概念。 一些初学者将 CMD 写为 CMD service nginx start, 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念, 没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言, 其启动程序就是容器应用进程, 容器就是为了主进程而存在的，主进程退出, 容器就失去了存在的意义, 从而退出, 其它辅助进程不是它需要关心的东西。而使用 service nginx start 命令，则是希望upstart来以后台守护进程形式启动nginx服务, CMD service nginx start 会被理解为 CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;service nginx start&quot;], 因此主进程实际上是sh, 那么当 service nginx start 命令结束后, sh也就结束了, sh 作为主进程退出了, 自然就会令容器退出。 正确的做法是直接执行nginx可执行文件，并且要求以前台形式运行。比如: CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] ENTRYPOINT 入口点 ENTRYPOINT 的目的和 CMD 一样, 都是在指定容器启动程序及参数。 ENTRYPOINT 在运行时也可以被替代, 不过比CMD要略显繁琐, 需要通过 docker run 的参数 --entrypoint 来指定; 当指定了 ENTRYPOINT 后, CMD 的含义就发生了改变, 不再是直接的运行其命令, 而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令, 换句话说实际执行时, 将变为:&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 有了CMD后, 为什么还要有ENTRYPOINT? 这种 &lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 有什么好处? 参考书中 场景一：让镜像变成像命令一样使用 场景二：应用运行前的准备工作 COPY 复制文件 和RUN指令一样, 也有两种格式: COPY &lt;源路径&gt;... &lt;目标路径&gt; COPY [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] COPY 指令将从 构建上下文目录 中 &lt;源路径&gt;目录/ 复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。比如:COPY package.json /usr/src/app/ &lt;源路径&gt; 可以是多个, 甚至可以是通配符 &lt;目标路径&gt; 可以是容器内的绝对路径, 也可以是相对于工作目录的相对路径(工作目录可以用 WORKDIR 指令来指定)。 目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外, 还需要注意一点, 使用 COPY 指令, 源文件的各种元数据都会保留。比如读、写、执 行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 ADD 更高级的复制文件 ADD 指令 和 COPY 的格式和性质基本一致, 但是在 COPY 基础上增加了一些功能: 比如 &lt;源路径&gt; 可以是一个 URL , 这种情况下, Docker 引擎会试图去下载这个链接的文件放 到 &lt;目标路径&gt; 去, 下载后的文件权限自动设置为 600 , 如果这并不是想要的权限, 则需要增加额外的一层 RUN 进行权限调整另外, 如果下载的是个压缩包, 需要解压缩, 也一样, 还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令, 然后使用 wget 或 者 curl 工具下载, 处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并 不实用，而且不推荐使用。 如果 &lt;源路径&gt; 为一个tar压缩文件的话,压缩格式为 gzip , bzip2 以及 xz 的情况下, ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的最佳实践文档中要求，尽可能的使用 COPY ，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用ADD 的场合，就是所提及的需要自动解压缩的场合。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD 。 HEALTHCHECK 健康检查ONBUILD 为他人做嫁衣裳 格式：ONBUILD &lt;其它指令&gt; ONBUILD 是一个特殊的指令, 它后面跟的是其它指令, 比如 RUN, COPY 等, 而这些指令, 在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像, 去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的, 唯有 ONBUILD 是为了帮助别人定制自己而准备的。 假设我们要制作Node.js所写的应用的镜像, 我们都知道Node.js使用npm进行包管理, 所有依赖、配置、启动信息等会放到package.json文件里。 在拿到程序代码后, 需要先进行 npm install 才可以获得所有需要的依赖。 然后就可以通过 npm start 来启动应用, 因此, 一般来说会这样写 Dockerfile: 1234567FROM node:slimRUN mkdir /appWORKDIR /appCOPY ./package.json /appRUN [ &quot;npm&quot;, &quot;install&quot; ]COPY . /app/CMD [ &quot;npm&quot;, &quot;start&quot; ] 把这个 Dockerfile 放到 Node.js 项目的根目录, 构建好镜像后, 就可以直接拿来启动容器运行。 但是如果我们还有第二个 Node.js 项目也差不多呢？好吧, 那就再把这个Dockerfile复制到第二个项目里。那如果有第三个项目呢?再复制么?文件的副本越多,版本控制就越困难,让我们继续看这样的场景维护的问题。 如果第一个Node.js项目在开发过程中, 发现这个Dockerfile里存在问题,比如敲错字了、或者需要安装额外的包, 然后开发人员修复了这个 Dockerfile, 再次构建, 问题解决。 第一个项目没问题了,但是第二个项目呢?虽然最初 Dockerfile 是复制、粘贴自第一个项目的,但是并不会因为第一个项目修复了 Dockerfile, 而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像, 然后各个项目使用这个基础镜像呢? 这样基础镜像更新, 各个项目不用同步Dockerfile的变化, 重新构建后就继承了基础镜像的更新? 其实是可以的, 那么上面的这个 Dockerfile 就会变为： 1234FROM node:slimRUN mkdir /appWORKDIR /appCMD [ &quot;npm&quot;, &quot;start&quot; ] 这里我们把项目相关的构建指令拿出来, 放到子项目里去。假设这个基础镜像的名字为my-node的话, 各个项目内的自己的 Dockerfile 就变为： 1234FROM my-nodeCOPY ./package.json /appRUN [ &quot;npm&quot;, &quot;install&quot; ]COPY . /app/ 基础镜像变化后, 各个项目都用这个 Dockerfile 重新构建镜像, 会继承基础镜像的更新。 此时问题只解决了一半, 如果这个Dockerfile里面有些东西需要调整, 比如 npm install 都需要加一些参数, 那怎么办? 这一行 RUN 是不可能放入基础镜像的, 因为涉及到了当前项目的 ./package.json, 难道又要一个个修改么? 所以说, 这样制作基础镜像, 只解决了原来的 Dockerfile 的前4条指令的变化问题, 而后面三条指令的变化则完全没办法处理。 ONBUILD 可以解决这个问题, 用ONBUILD重新写一下基础镜像的 Dockerfile: 1234567FROM node:slimRUN mkdir /appWORKDIR /appONBUILD COPY ./package.json /appONBUILD RUN [ &quot;npm&quot;, &quot;install&quot; ]ONBUILD COPY . /app/CMD [ &quot;npm&quot;, &quot;start&quot; ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地：FROM my-node, 只有这么一行。当在各个项目目录中, 用这个只有一行的 Dockerfile 构建镜像时, 之前基础镜像的那三行 ONBUILD 就会开始执行, 成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install, 生成应用镜像。","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"04.定制镜像 - Dockerfile脚本","slug":"docker/2017-10-12-04-docker","date":"2017-10-12T15:05:07.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/12/docker/2017-10-12-04-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/12/docker/2017-10-12-04-docker/","excerpt":"","text":"使用Dockerfile定制镜像 通过之前docker commit的学习了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile是一个文本文件，其内包含了一条条的指令, 每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 在使用Dockerfile脚本定制镜像时, 有很多指令可以使用; Dockerfile指令详解参考下一篇博文05.Dockerfile指令详解 构建镜像1.还以之前定制 nginx 镜像为例，下面使用Dockerfile来定制 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile, 其内容为如下: 12FROM nginxRUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN 2.在 Dockerfile 文件所在目录执行 从命令的输出结果如下 1234567891011renyimindeMacBook-Pro:image_build renyimin$ docker build -t nginx:v5 .Sending build context to Docker daemon 2.048kBStep 1/2 : FROM nginx ---&gt; e548f1a579cfStep 2/2 : RUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html ---&gt; Running in 5dc1025063e7 ---&gt; 65c15ffa7c7cRemoving intermediate container 5dc1025063e7Successfully built 65c15ffa7c7cSuccessfully tagged nginx:v5renyimindeMacBook-Pro:image_build renyimin$ 可以看到镜像的构建过程: 在Step2中, RUN指令重新启动了一个容器 5dc1025063e7，执行了所要求的命令，并最后提交了新的层 65c15ffa7c7c, 随后删除了所用到的容器5dc1025063e7 3.这里我们使用了 docker build 命令进行镜像构建, 其格式为: docker build [选项] &lt;上下文路径/URL/-&gt; 在这里我们指定了最终镜像的名称 -t nginx:v5，构建成功后, docker images 就会看到这个新镜像 123renyimindeMacBook-Pro:image_build renyimin$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx v5 65c15ffa7c7c 5 minutes ago 109MB 4.启动这个新镜像: docker run -d -p 8098:80 --name nginx_v5 nginx:v5 执行过程 123456789renyimindeMacBook-Pro:image_build renyimin$ docker run -d -p 8098:80 --name nginx_v5 nginx:v5247dfedcf52c3cbfd652302a8f2226274661cea77200092b54044d37e0a2ae25renyimindeMacBook-Pro:image_build renyimin$ renyimindeMacBook-Pro:image_build renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES247dfedcf52c nginx:v5 &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8098-&gt;80/tcp nginx_v5d762abfc1ab6 nginx:latest &quot;nginx -g &apos;daemon ...&quot; 8 minutes ago Up 8 minutes 0.0.0.0:8090-&gt;80/tcp nginx_l0e7070854958 registry &quot;/entrypoint.sh /e...&quot; 2 days ago Up 39 hours 0.0.0.0:5000-&gt;5000/tcp registryrenyimindeMacBook-Pro:image_build renyimin$ 对比定制前后 镜像构建上下文(Context)上面在构建镜像时, 如果注意, 会看到 docker build 命令最后有一个 ., 表示当前目录, 而 Dockerfile 就在当前目录, 因此不少初学者以为这个路径是在指定Dockerfile所在的路径, 但这么理解其实是不准确的。这是在指定上下文路径, 那么什么是上下文呢? 首先我们要理解 docker build 的工作原理 Docker 在运行时分为Docker引擎(也就是服务端守护进程)和客户端工具; Docker 的引擎提供了一组REST API, 被称为Docker Remote API, 而如docker命令这样的客户端工具, 则是通过这组 API 与 Docker 引擎交互, 从而完成各种功能, 因此, 虽然表面上我们好像是在本机执行各种docker功能, 但实际上, 一切都是使用的远程调用形式在服务端(Docker 引擎)完成。也因为这种C/S设计, 让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候, 并非所有定制都会通过RUN指令完成, 经常会需要将一些主机本地文件复制进镜像, 比如通过COPY、ADD 指令等。而 docker build 命令构建镜像, 其实并非在本地构建, 而是在服务端, 也就是Docker引擎中构建的。那么在这种客户端/服务端的架构中, 如何才能让服务端获得本地文件呢? 这就引入了上下文的概念, 当构建的时候, 用户会指定构建镜像上下文的路径, docker build 命令得知这个路径后, 会将路径下的所有内容打包, 然后上传给Docker引擎。这样 Docker 引擎收到这个上下文包后, 展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中 COPY ./package.json 这么写: 这并不是要复制你执行 docker build 命令时, 所在的目录下的 package.json, 也不是复制 Dockerfile 所在目录下的 package.json, 而是复制上下文(context)目录下的 package.json COPY 这类指令中的源文件的路径都是相对上下文路径的 理解构建上下文对于镜像构建是很重要的, 避免犯一些不应该的错误, 比如有些初学者在发现 COPY /opt/xxxx /app 不工作后, 于是干脆将 Dockerfile 放到了硬盘根目录去构建, 结果发现 docker build 执行后, 在发送一个几十GB的东西, 极为缓慢而且很容易构建失败(因为这种做法是在让 docker build 打包整个硬盘, 这显然是使用错误)。 一般来说, 应该将 Dockerfile 置于一个空目录下, 或者项目根目录下。 如果该目录下没有所需文件, 那么应该把所需文件复制一份过来。 如果目录下有些东西确实不希望构建时传给 Docker 引擎, 那么可以用 .gitignore 一样的语法写一个 .dockerignore, 该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢? 这是因为在默认情况下, 如果不额外指定 Dockerfile 的话, 会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。 这只是默认行为, 实际上Dockerfile的文件名并不要求必须为Dockerfile, 而且并不要求 必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为Dockerfile。 参考 : https://yeasy.gitbooks.io/docker_practice/content/image/commit.html","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"03.定制镜像 - docker commit手动定制","slug":"docker/2017-10-12-03-docker","date":"2017-10-12T13:50:07.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/12/docker/2017-10-12-03-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/12/docker/2017-10-12-03-docker/","excerpt":"","text":"前言镜像是容器的基础, 每次执行 docker run 的时候都会指定哪个镜像作为容器运行的基础。在之前的例子中, 我们所使用的都是来自于 Docker Hub 的镜像, 直接使用这些镜像是可以满足一定的需求, 而当这些镜像无法直接满足需求时, 我们就需要定制这些镜像。 docker commit 手动定制镜像1.当运行一个容器的后(如果不使用数据卷的话), 你所做的任何文件修改都会被记录于容器存储层里 如果改动了容器的存储层, 我们可以通过 docker diff 命令看到具体的改动 但是如果改动的是数据卷挂载到容器对应目录下的内容, docker diff 看不到具体的改动 2.注意: 容器存储层的生存周期和容器一样, 容器被删除后, 存储层中的内容也就会被删除掉, 而不会保留到镜像中。 3.Docker提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像 换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像 以后我们运行这个新镜像的时候, 就会拥有原有容器最后的文件变化 4.docker commit 的语法格式为: docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]], 如下: 其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空 1$ docker commit --author &quot;Tao Wang &lt;twang2218@gmail.com&gt;&quot; --message &quot;修改了默认网页&quot; webserver nginx:v2 手动定制镜像~~挂载数据卷问题1.之前已经配置了docker中国加速镜像, 现在通过git pull nginx获取一个nginx基础镜像; 2.直接运行这个nginx基础镜像为一个容器(由于该镜像非常基础,甚至没有像vi的工具,因此在启动时可以将nginx的项目根目录/usr/share/nginx/html映射出来, 以便于测试) 123456renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 8088:80 --mount type=bind,source=/Users/renyimin/Desktop/nginx_test,target=/usr/share/nginx/html --name nginx_test nginx1c39a78311e579482a2334347f80ba479555097d869a1254a27c9ac581c13eedrenyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1c39a78311e5 nginx &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8088-&gt;80/tcp nginx_testrenyimindeMacBook-Pro:testVip renyimin$ 3.通过 localhost:8088 会看到nginx的欢迎界面 4.现在，假设需要更改欢迎界面文字(本应该使用 docker exec -it nginx_test /bin/sh 命令进入容器修改其内容, 但后来发现该进行非常基础,很多基础工具如vi都没有, 所以最终采用挂载主机目录方式启动, 然后在主机新增一个index.html文件) 5.之后直接刷新浏览器的话，会发现内容被改变了 6.现在修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动, 你会发现自己的改动并没有体现出来, 其实这主要是因为你将目录挂载出来了, 如果不是挂载出来,而是直接在容器中修改的话, 则会体现出来 12345678910renyimindeMacBook-Pro:testVip renyimin$ docker diff nginx_testC /runA /run/nginx.pidC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temprenyimindeMacBook-Pro:testVip renyimin$ 7.当运行一个容器的时候(如果不使用卷的话)，我们做的任何文件修改都会被记录于容器存储层里, 可以使用 docker diff 命令看到容器存储层中的变动, 但是如果使用了数据卷, 则挂载目录中的改动无法体现出来; 手动定制镜像~~实验1.既然数据卷中的内容无法使用docker diff来查看差异, 我们可以将index.html文件复制到容器的/home下, 然后使用 docker diff 命令就可以看到容器存储层中的改动 123456789101112renyimindeMacBook-Pro:testVip renyimin$ docker diff nginx_testC /homeA /home/index.htmlC /runA /run/nginx.pidC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temprenyimindeMacBook-Pro:testVip renyimin$ 2.docker commit --author &#39;renyimin&#39; --message &quot;在/home下添加了一个index.html文件&quot; nginx_test nginx:v1 3.docker images 可以看到这个新定制的镜像 4.我们还可以用 docker history 具体查看镜像内的历史记录，如果比较 nginx:v1 的历史记录，我们会发现新增了我们刚刚提交的这一层 123456789renyimindeMacBook-Pro:Desktop renyimin$ docker history nginx:v1IMAGE CREATED CREATED BY SIZE COMMENTfe3e3a4c47b8 About a minute ago nginx -g daemon off; 1.4MB 在/home下添加了一个index.html文件e548f1a579cf 8 days ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daem... 0B &lt;missing&gt; 8 days ago /bin/sh -c #(nop) STOPSIGNAL [SIGTERM] 0B &lt;missing&gt; 8 days ago /bin/sh -c #(nop) EXPOSE 80/tcp 0B ..........renyimindeMacBook-Pro:Desktop renyimin$ 5.新的镜像定制好后，就可以来运行这个镜像 123456789101112renyimindeMacBook-Pro:testVip renyimin$ docker run -d -p 8089:80 --name nginx_v1 nginx:v1d5d17a25cc35837c64f446a6623afa0a23b6c543593276979ebb7c5ced1e7e1drenyimindeMacBook-Pro:testVip renyimin$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd5d17a25cc35 nginx:v1 &quot;nginx -g &apos;daemon ...&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8089-&gt;80/tcp nginx_v1785d252f5c94 nginx &quot;nginx -g &apos;daemon ...&quot; 4 minutes ago Up 3 minutes 0.0.0.0:8088-&gt;80/tcp nginx_test// 会发现使用新的镜像启动容器后, 容器中的/home目录下包含我们提交的index.html个文件, 当然, localhost:8089和localhost:8088不同, 8089访问的还是默认欢迎页renyimindeMacBook-Pro:testVip renyimin$ docker exec -it nginx_v1 /bin/sh# cd /home# lsindex.html# 至此，我们第一次完成了定制镜像，使用的是 docker commit 命令，手动操作给旧的镜像添加了新的一层，形成新的镜像，对镜像多层存储应该有了更直观的感觉。 慎用 docker commit使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念, 但是实际环境中并不会这样使用。1.首先，如果仔细观察之前的 docker diff nginx_test 的结果，你会发现除了真正想要的 /home/index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。 2.此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。 3.而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。 4.可以结合07.镜像/容器 – 导入导出","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"02.镜像 - 基础","slug":"docker/2017-10-12-02-docker","date":"2017-10-12T11:40:07.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/12/docker/2017-10-12-02-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/12/docker/2017-10-12-02-docker/","excerpt":"","text":"前言镜像(Image)和容器(Container)的关系，就像是面向对象程序设计中的类和实例一样, 镜像是静态的定义, 容器是镜像运行时的实体。所以 Docker 运行容器前首先需要本地存在对应的镜像, 如果本地不存在该镜像, Docker 会从镜像仓库下载该镜像。 镜像的获取Docker Hub上有大量的高质量的镜像可以用, 如何获取这些镜像呢? 1.从Docker镜像仓库获取镜像的命令是 docker pull, 其命令格式为：docker pull [选项] [Docker Registry地址[:端口号]/]仓库名[:标签] docker pull命令的具体选项可以通过 docker pull --help 命令看到 镜像名称的格式:Docker镜像仓库地址: 地址的格式一般是 &lt;域名/IP&gt;[:端口号] (默认地址是 Docker Hub 仓库地址)仓库名: 仓库名是两段式名称, 即 &lt;用户名&gt;/&lt;软件名&gt; (对于 Docker Hub, 如果不给出用户名, 则默认为 library, 也就是官方镜像。) 比如 $ docker pull ubuntu:16.04: 由于没有给出Docker镜像仓库地址, 因此将会从Docker Hub获取镜像, 而镜像名称是 ubuntu:16.04(没有用户名), 因此将会获取官方镜像 library/ubuntu 仓库中标签为 16.04 的镜像; 2.另外, 如果从 Docker Hub 下载镜像非常缓慢，可以 配置镜像加速器。 配置镜像加速器1.国内从 Docker Hub 拉取镜像有时会遇到困难, 此时可以配置镜像加速器, Docker 官方和国内很多云服务商都提供了国内加速器服务, 例如: Docker 官方提供的中国 registry mirror 阿里云加速器 DaoCloud 加速器 2.此处以 Docker 官方加速器为例进行介绍(由于本人使用macOS系统,下面只列出macOS上如何配置镜像加速器, 其他系统请参考) 在任务栏点击Docker for mac 应用图标 -&gt; Perferences… -&gt; Daemon -&gt; Registry mirrors 在列表中填写加速器地址即可, 修改完成之后，点击 Apply &amp; Restart 按钮，Docker 就会重启并应用配置的镜像地址了 3.如果在添加加速器地址后出现 registry-mirrors no certs for egistry.docker-.... 4.网上查找资料后, 有人说是证书问题, 尝试修改https为http后正常 5.检查加速器是否生效 配置加速器之后,如果拉取镜像仍然十分缓慢,请手动检查加速器配置是否生效,在命令行执行 docker info 由于我配置的是docker hub提供的中国镜像站点, 所以如果从结果中看到了如下内容，说明配置成功(你看到的可能和我的不一样)123Registry Mirrors:http://registry.docker-cn.com///如果添加了多个加速站点, 此处也会有多个 镜像相关基础操作列出已存在镜像1.docker images：列表包含了 仓库名、标签、镜像ID、创建时间 以及 所占用的空间; 2.虽然 镜像ID 是镜像的唯一标识, 但是一个镜像可以打包出多个不同标签的镜像 (所以有些镜像的ID一样, 但是tag会不一样); 12345renyimindeMacBook-Pro:testVip renyimin$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE127.0.0.1:5000/registry latest d1fd7d86a825 7 weeks ago 33.3MBregistry latest d1fd7d86a825 7 weeks ago 33.3MBrenyimindeMacBook-Pro:testVip renyimin$ 删除镜像如果要删除本地的镜像, 可以使用 $ docker image rm [选项] &lt;镜像名1&gt; [&lt;镜像名2&gt; ...] 命令; (因为镜像ID可能会一样, 所以删除镜像用的是镜像名) 镜像更名镜像更改名称也很简单, 直接 $ docker tag 镜像名 新镜像名:标签 在 Docker 1.13+ 版本中推荐使用 docker image 来管理镜像。 (比如 docker image ls 会列出所有镜像); 理解分层存储1.严格来说，镜像并非是像一个ISO那样的打包文件, 镜像只是一个虚拟的概念, 其实际体现并非由一个文件组成, 而是由一组文件系统组成, 或者说, 由多层文件系统联合组成。 2.镜像是多层存储，每一层是在前一层的基础上进行的修改; 而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。 3.镜像构建时, 会一层层构建, 前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 4.容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡, 这里的消亡是指容器被删除, 而不是stop容器, stop容器后, 容器中发生的改变不会被忽略, 除非容器被删除掉。","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"01.认识Docker","slug":"docker/2017-10-12-01-docker","date":"2017-10-12T11:30:41.000Z","updated":"2018-03-05T03:16:48.000Z","comments":true,"path":"2017/10/12/docker/2017-10-12-01-docker/","link":"","permalink":"http://blog.renyimin.com/2017/10/12/docker/2017-10-12-01-docker/","excerpt":"","text":"简介 Docker使用Google公司推出的Go语言实现; 属于操作系统层面的虚拟化技术; 也称其为容器; docker 和 传统虚拟机技术 对比 传统虚拟机技术是: 虚拟出一套硬件后; 在其上运行一个完整操作系统; 最后在该系统上再运行所需应用进程; 而容器内的应用进程直接运行于宿主的内核, 容器内没有自己的内核, 而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便; 如下图, 可以看到有应用A和应用B两个应用, 相比于传统虚拟技术, docker少了Hypervisor(所有虚拟化技术的核心)和Guest OS这两层 为什么使用docker?作为一种新兴的虚拟化方式，Docker 跟传统的虚拟化方式相比具有众多的优势 更高效的利用系统资源由于容器不需要进行 硬件虚拟 以及 运行完整操作系统 等额外开销, 所以其实Docker对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 一致的运行环境开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。 更多好处请参考(https://yeasy.gitbooks.io/docker_practice/content/introduction/why.html) 对比传统虚拟机总结 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于原生 系统支持量 单机支持上千个容器 一般几十个 Docker三个基本概念理解了这三个概念，就理解了 Docker 的整个生命周期 镜像 (Image)容器 (Container)仓库 (Repository) 《Docker从入门到实践》","categories":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/categories/Docker学习/"}],"tags":[{"name":"Docker学习","slug":"Docker学习","permalink":"http://blog.renyimin.com/tags/Docker学习/"}]},{"title":"70. 查询性能优化","slug":"mysql/2017-09-27-mysql-70","date":"2017-09-27T12:50:37.000Z","updated":"2018-03-08T06:03:46.000Z","comments":true,"path":"2017/09/27/mysql/2017-09-27-mysql-70/","link":"","permalink":"http://blog.renyimin.com/2017/09/27/mysql/2017-09-27-mysql-70/","excerpt":"","text":"前言之前已经了解了索引优化的相关内容, 它对于高性能是必不可少的, 但还不够, 还需要合理设计查询; 如果查询写的很糟糕, 即使库表结构再合理, 索引再合适, 也无法实现高性能; 查询优化, 库表结构优化, 索引优化需要齐头并进, 一个不落; 慢查询基础优化数据访问 确认应用程序是否在检索大量超过需要的数据, 你可能访问了太多的行, 也可能是太多的列;比如: 总是返回全部的列; 只展示5条数据,你却查出100条; 确认MySQL服务器层是否在分析大量超过需要的数据行; (注意: 索引是在存储引擎层, 一旦服务器层分析的数据过多, 可能你的索引不太合适, 没有在存储引擎层过滤掉数据) 未完待续~~","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"查询性能优化","slug":"查询性能优化","permalink":"http://blog.renyimin.com/tags/查询性能优化/"}]},{"title":"50. EXPLAIN 分析","slug":"mysql/2017-09-25-mysql-50","date":"2017-09-25T13:23:08.000Z","updated":"2018-03-08T05:41:47.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-50/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-50/","excerpt":"","text":"准备环境1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB, DEFAULT CHARSET = utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`))ENGINE = InnoDB,DEFAULT CHARSET = utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) select_type select_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 (最常见的查询类别就是 SIMPLE 了) PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 如果使用了UNION查询, 那么EXPLAIN 输出结果类似如下: 123456789101112mysql&gt; EXPLAIN ( SELECT * FROM user_info WHERE id IN ( 1, 2, 3 ) ) UNION( SELECT * FROM user_info WHERE id IN ( 3, 4, 5 ) );+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+-------+---------------+---------+---------+------+------+-----------------+3 rows in set (0.00 sec) mysql&gt; type type 字段比较重要, 它提供了判断查询是否高效的重要依据依据; 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等; type 常用的取值有: system: 表中只有一条数据, 这个类型是特殊的 const 类型; ?? const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据(const 查询速度非常快, 因为它仅仅读取一次即可) eq_ref: 此类型通常出现在多表的join查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果, 并且查询的比较操作通常是 =, 查询效率较高, 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using where; Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | NULL |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+--------------------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询, 例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5;+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | NULL || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+-------------+2 rows in set (0.00 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录; 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL (没用到索引), 并且 key_len 字段是此次查询中使用到的索引的最长的那个 1234567mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+1 row in set (0.00 sec) 下面对比, 都使用了范围查询, 但是一个可以使用索引范围查询, 另一个不能使用索引 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 另外, 可参考 P185: in语句虽然有时候 type结果也是range (不过, 对于真正的范围查询, 确实是无法使用范围列后面的其他索引了, 但是对于”多个等值条件查询”则没有这个限制) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过ALL类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据, 即 做的是覆盖索引, 当是这种情况时, Extra 字段会显示 Using index 下面的例子中, 查询的 name 字段恰好是一个索引(做到了覆盖索引), 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据;因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index; 1234567mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 下面不但使用了全索引扫描, 而且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;nihao&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) 但是, 如果不使用索引的话, 下面type就是ALL, 表示使用了全表扫描, 并且使用了where条件 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age=10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where age&gt;10;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+1 row in set (0.00 sec) 下面 1234567mysql&gt; EXPLAIN SELECT name FROM user_info where name&gt;&apos;nihao&apos;;+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+| 1 | SIMPLE | user_info | range | name_index | name_index | 152 | NULL | 1 | Using where; Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+--------------------------+1 row in set (0.00 sec) ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一, 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. type小结type 类型的性能比较 : 通常来说, 不同的 type 类型的性能关系如: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的; 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快; 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了; possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引;注意: 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到;(MySQL 在查询时具体使用了哪些索引, 由 key 字段决定) key此字段是 MySQL 在当前查询时所真正使用到的索引 rowsrows 也是一个重要的字段, MySQL 查询优化器根据统计信息, 估算SQL要查找到结果集需要到表中扫描读取的数据行数(上面的例子可以看到, 基本上使用到了索引的话, 真正扫描的行数都很少); 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好 ExtraExplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 比如下面, 使用索引扫描做排序 和 不使用索引扫描做排序 的效果: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY name;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name FROM user_info ORDER BY age;+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using filesort |+----+-------------+-----------+------+---------------+------+---------+------+------+----------------+1 row in set (0.00 sec) Using index “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 比如下面, 第一个做到了覆盖索引扫描, 后面两个都没做到1234567891011121314151617181920212223mysql&gt; EXPLAIN SELECT name FROM user_info where name=&apos;haha&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+------------+---------+-------+------+--------------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT name,age FROM user_info where name=&apos;haha&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition |+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+1 row in set (0.00 sec) mysql&gt; EXPLAIN SELECT * FROM user_info where name=&apos;haha&apos;;+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+| 1 | SIMPLE | user_info | ref | name_index | name_index | 152 | const | 1 | Using index condition |+----+-------------+-----------+------+---------------+------------+---------+-------+------+-----------------------+1 row in set (0.00 sec) Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. 参考","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"49. 索引和锁","slug":"mysql/2017-09-25-mysql-49","date":"2017-09-25T13:10:40.000Z","updated":"2018-03-08T02:57:03.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-49/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-49/","excerpt":"","text":"索引可以让查询锁定更少的行 因为InnoDB只有在访问行的时候才会对其加锁, 而索引能够减少InnoDB访问的行数, 从而减少锁的数量;但这只有当InnoDB在存储引擎层就能过滤掉所有不需要的行时才行, 如果索引(处在存储引擎层)无法过滤掉无效的行, 那么在InnoDB检索到数据并发送给服务器层以后, 服务器层才能应用where子句, 这时已经无法避免锁定行了;虽然InnoDB的行锁效率很高, 内存使用也很少, 但是锁定行的时候仍然会带来额外开销;锁定超过需要的行会增加锁争用并减少并发性;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"48. 冗余和重复索引","slug":"mysql/2017-09-25-mysql-48","date":"2017-09-25T10:27:40.000Z","updated":"2018-03-08T02:39:49.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-48/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-48/","excerpt":"","text":"重复索引 是指在相同的列上按照相同的顺序创建的相同类型的索引; 应该避免这样的重复索引, 发现后也应该立即删除; MySQL允许在相同的列上创建多个索引, 但是MySQL需要单独维护重复的索引, 并且优化器在优化查询的时候也需要逐个地进行考虑, 这会影响性能; 冗余索引 和 重复索引 不同, 如果创建了索引(A,B), 在创建索引(A)就是冗余索引, 因为这只是(A,B)索引的前缀索引; 大多数情况下都不需要冗余索引; 因此索引(A,B)也可以当做索引(A)来使用 但是如果再创建索引(B,A), (B) 则都不是冗余索引 有时候为了让两个查询都变快, 也会需要冗余索引 (P179) 应该尽量扩展已有的索引而不是创建新的索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"47. 使用索引扫描来做排序","slug":"mysql/2017-09-25-mysql-47","date":"2017-09-25T10:25:11.000Z","updated":"2018-03-08T05:38:13.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-47/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-47/","excerpt":"","text":"简介 只有当索引的列顺序 和 ORDER BY 子句的顺序完全一致, 并且所有列的排序方向都一样时(要么都是正序, 要么都是倒序), MySQL才能够使用索引来对结果做排序; 如果查询需要关联多张表, 则只有当 ORDER BY 子句引用的字段全部为第一个表时, 才能使用索引做排序; ORDER BY 子句 和 查找型查询的限制是一样的, 需要满足索引的最左前缀的要求, 否则, MySQL都需要亲自去执行排序操作, 而无法利用索引排序; 有一种情况下, ORDER BY 子句可以不用满足最左前缀的要求, 那就是前导列为常量的时候; 比如一张表的索引是 key(a,b,c) , 而 查询语句是 ... where a=100 order by b,c, 即使 order by 不满足最左前缀的要求, 也可以使用索引做排序; P177 列出了很多不可以使用索引做排序的查询; 当查询同时有 ORDER BY 和 LIMIT 子句的时候 像select &lt;col...&gt; from profiles where sex=&#39;m&#39; order by rating limit 10;这种查询语句, 同时使用了order by和limit, 如果没有索引就会很慢; 即使有索引, 如果用户界面有翻页, 翻页比较靠后时, 也会非常慢, 因为随着偏移量的增加, MySQL需要花费大量的时间来扫描需要丢弃的数据; 但是sex的选择性又很低, 如何优化呢? 对于选择性非常低的列, 如果要做排序的话, 可以增加一些特殊的索引来做排序, 例如, 可以创建 (sex, rating)索引 然后采用 延迟关联 , 通过覆盖索引先查询返回需要的主键, 在根据这些主键关联原表获得需要的行;select &lt;col...&gt; from profiles INNER JOIN (select &lt;primart key&gt; from profiles where x.sex=&#39;m&#39; order by rating limit 100000, 10) as x using(primary key)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"46. 覆盖索引","slug":"mysql/2017-09-25-mysql-46","date":"2017-09-25T09:30:26.000Z","updated":"2018-03-08T02:29:53.000Z","comments":true,"path":"2017/09/25/mysql/2017-09-25-mysql-46/","link":"","permalink":"http://blog.renyimin.com/2017/09/25/mysql/2017-09-25-mysql-46/","excerpt":"","text":"介绍 通常, 大家都会根据查询的WHERE条件来创建合适的索引, 不过这只是索引优化的一个方面, 设计优秀的索引应该考虑到整个查询, 而不单单是where条件部分。 覆盖索引 索引确实是一种查找数据的高效方式, 但是MySQL也可以使用索引就能直接获取列的数据, 这样就不需要再去读取数据行; 如果叶子节点中已经包含了要查询的数据, 那么就没有必要再回表查询了; 即, 如果一个索引包含(或者说覆盖)所有需要查询的字段值, 我们就称之为 覆盖索引; 覆盖索引是非常有用的工具, 能够极大地提高性能; 拿InnoDB来说, 覆盖索引就非常有用, 如果你的查询能够做到覆盖你的二级索引列, 那么只需要遍历一次B-Tree(可以直接在二级索引中找到数据), 可以避免对聚簇索引的二次查询; 其他更多参考 P171 注意: 覆盖索引必须要保存索引列的值, 而 哈希索引, 空间索引 和 全文索引 等都不存储索引列的值; 所以MySQL只能使用B-Tree索引做覆盖索引; 如果索引不能覆盖查询所需的全部列, 那就不得不每扫描一次索引记录, 就回表查询一次对应的行, 优化小案例 select * .... : 因为查询从表中选择了所有的列, 而一般你不会创建覆盖了所有列的二级索引, 所以这种局域肯定不会用到覆盖索引; .... where title LIKE &#39;%ren&#39;: Mysql只能在where条件中做索引的 最左前缀匹配的LIKE比较, 而这里的where条件是以通配符开头的LIKE查询; 查看 (P171) 的优化案例 (做表的自关联, 子句使用覆盖索引, 外部不用) 这样虽然无法使用索引覆盖整个查询, 但总算比完全无法利用覆盖索引要好","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"45. 聚簇索引","slug":"mysql/2017-09-24-mysql-45","date":"2017-09-24T12:10:31.000Z","updated":"2018-03-07T13:32:55.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-45/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-45/","excerpt":"","text":"简介 聚簇索引并不是一种单独的索引类型, 而是一种 数据存储方式; 因为是存储引擎负责实现索引, 所以不是所有的存储引擎都支持聚簇索引, 这里主要讨论的是 InnoDB引擎的聚簇索引; InnoDB的 聚簇索引 实际上在同一个结构中保存了 B-Tree索引 和 数据行 当表有聚簇索引时, 它的数据行实际上存放在索引的叶子页中(叶子页包含了数据行的 全部列数据) 因为无法同时把数据行存放在两个不同的地方, 所以一个表只能有一个聚簇索引 不过覆盖索引, 可以模拟多个聚簇索引的情况 InnoDB默认会创建聚簇索引: InnoDB通过主键来作为聚簇索引, 如果没有定义主键, 则会选择一个唯一的非空索引代替, 如果连非空索引都没有, InnoDB会隐式定义一个主键来作为聚簇索引; 将主键组织到一棵B+树中, 而行数据就储存在叶子节点上，若使用where id = 14这样的条件查找数据; 则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据; InnoDB只聚集 在同一个磁盘页面中的记录, 因此, 如果数据在物理上是相邻的, 那么在索引上就也是相邻的; 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的; 但是包含相邻键值的页面可能物理上会相距甚远; 聚簇索引的优点 访问速度更快: 聚簇索引将索引和数据保存在同一个B-Tree中, 因此从聚簇索引中获取数据通常比在非聚簇索引中查询要快; 使用覆盖索引的查询, 可以直接使用页节点中的主键值; 聚簇索引缺点 聚簇索引最大限度地提高了I/O密集型应用的性能, 但如果数据全部都放在内存中, 则访问顺序就没那么重要了, 聚簇索引也就没什么优势了; 插入速度严重依赖于插入顺序, 按照主键的顺序插入是速度最快的方式, 但如果不是按照主键顺序, 在完成操作后最好执行 OPTIMIZE TABLE 命令重新组织一下表; 更新聚簇索引的代价很高, 因为会强制InnoDB将每个被更新的行移动到新的位置; 基于聚簇索引的表在插入新行, 或者主键被更新导致需要移动行的时候, 可能面临 “页分裂” 问题; 当前主键值要求必须将这一行插入到某个已满的页中时, 存储引擎会将该页分裂成两个页面来容纳该行, 这就是一次页分裂操作。 页分裂操作会导致表占用更多的磁盘空间 聚簇索引会导致全表扫描变慢, 尤其是行比较稀疏, 或者由于页分裂导致数据存储不连续的时候; 二级索引(非聚簇索引)可能比想象的要更大, 因为在二级索引的叶子节点包含了引用行的主键列; 二级索引访问需要两次索引查找, 而不是一次 二级索引叶子几点保存的 “行指针” 是行的主键; 这意味着通过二级索引查找行, 存储引擎需要找到二级索引叶子节点获得对应的主键值; 然后根据这个主键值去聚簇索引中查找对应的行数据; 这里做了重复工作, 两次 B-Tree 查找, 而不是一次。 InnoDB 和 MyISAM 索引对比 InnoDB支持聚簇索引, 而MyISAM不支持; MyISAM中主键索引和其他索引在索引结构上没有区别; 而InnoDB中 (主键)聚簇索引 和 二级索引(普通索引) 是有区别的;(P167) 上图总结","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"44. 高性能索引策略 -- 多列索引","slug":"mysql/2017-09-24-mysql-44","date":"2017-09-24T09:25:31.000Z","updated":"2018-03-07T12:23:09.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-44/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-44/","excerpt":"","text":"前言 在多个列上建立独立的单列索引, 在大部分情况下并不能提高MySQL的查询性能; 例如: film_actor 表在 film_id 和 actor_id 上各有一个单列索引, 但是对于下面这个查询WHERE条件, 这两个单列索引都不是好的选择 select film_id, actor_id from actor where actor_id=1 OR film_id=1; 对于上面的查询 老版本的MySQL会使用全表扫描; 而新版本会使用 索引合并策略(参考P158) 来进行优化, 但这更说明了表上的索引建的很糟糕 接下来除了 之前已经在博文: B-Tree索引中介绍过的多列索引的 左前缀策略; 你还需要关注的是创建索引时, 索引列的顺序 选择合适的索引列顺序 在一个多列索引中, 索引列的顺序首先决定了最左前缀策略在查询时是如何进行的; 其次还意味着索引首先按照最左列进行排序, 其次是第二列, 等等; 所以多列索引的列顺序至关重要; 在不需要考虑排序和分组的时候, 将选择性最高的列放在前面通常是很好的, 这时候索引的作用只是用于优化where条件的查找。 然而, 性能不知是依赖于所有索引列的选择性(整体基数), 也和查询条件的具体值有关, 也就是和值的分布有关; ~~未完待续","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"43. 高性能索引策略 - 独立的列, 前缀索引","slug":"mysql/2017-09-24-mysql-43","date":"2017-09-24T09:20:31.000Z","updated":"2018-03-08T05:22:03.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-43/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-43/","excerpt":"","text":"前言前面已经对常见的索引类型及其对应的优缺点有了一定的了解, 接下来要考虑的就是如何 高效正确地选择并使用索引 独立的列独立的列是指: 在查询条件中, 索引列 不能是表达式的一部分, 也不能是函数的参数; (如: select actor_id from actor where actor_id+1=5; 就无法使用索引, 应该始终将索引列单独放在比较符号的一侧, select actor_id from actor where actor_id=4;); 前缀索引 因为B-Tree索引中存储了实际的列值, 所以如果你需要索引的列的内容很长, 就会导致 索引变得大且慢; 对于BLOB, TEXT 或者很长的 VARCHAR 类型的列, 必须使用前缀索引, 因为MySQL不允许索引这些列的完整长度; 正常情况下, 不论是创建 单列索引 还是 多列索引, 创建的索引都是以 某个列完整的值 来创建索引, 而 前缀索引则是以 列开始的部分字符 来创建索引, 从而大大节约索引空间, 提高索引效率, 但是这样会降低索引的选择性; 索引选择性 索引选择性: 是指不重复的索引值(基数) 和 数据表的记录总数的比值(当然, 唯一索引的所有索引值都不同, 选择性是1, 这是最好的索引选择性, 性能也是最好的); 假设一张订单表, 按照 city(城市全名) 来分 和 按照 city(第一个字)来分组 , 那肯定前一种情况分出来的组比较多, 也就是不重复的索引值多; 如果按照 city 字段的前3个字符来分组的话, 效果如下 如果按照 city 字段的前7个字符来分组的话, 可以想到, 自然可能会是 分组会更多, 每组的数据会更少 分的组越多, 也就是如果以此长度的前缀创建索引的话, 不重复的索引值也就越多, 那么选择性就越高; 选择性越高, 则查询效率越高, 因为MySQL在查找时能够通过索引就过滤掉更多的行, 否则一个索引还是对应了很多的数据行, 那效率还是很低; 而我们要做的其实就是让我们的 前缀选择性 接近 完整列的选择性 简单点说, 让 city(n) 接近 city 的选择性; 计算完整列的选择性 下面给出了同一个查询中计算不同前缀长度的选择性 前缀索引是一种能使索引更小, 更快的有效办法, 但也有其缺点: MySQL无法使用前缀索引做 ORDER BY 和 GROUP BY 有无法使用前缀索引做覆盖扫描 创建前缀索引: alter table city add key(city(7)), 表示以 city 字段的前7个字符 来创建索引; 参考p189: 根据传统经验, 不应该在选择性低的裂伤创建索引, 但是如果很过查询都用到该列, 比如一个表中的 gender 列, 考虑到使用的频率, 还是建议在创建不同组合的索引时, 将 (sex) 作为前缀!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"},{"name":"高性能索引策略","slug":"高性能索引策略","permalink":"http://blog.renyimin.com/tags/高性能索引策略/"}]},{"title":"42. 哈希索引","slug":"mysql/2017-09-24-mysql-42","date":"2017-09-24T07:01:31.000Z","updated":"2018-03-07T10:04:36.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-42/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-42/","excerpt":"","text":"简介 哈希索引(hash index)基于哈希表实现, 只有精确匹配索引中所有列的查询才有效; 对于每一行数据, 存储引擎都会对所有的索引列计算一个哈希码(hash code): 哈希索引将所有的哈希码存储在索引中 同时在哈希表中保存指向每个数据行的指针 在MySQL中, 只有Memory引擎显示支持哈希索引, 这也是Memory引擎表的默认索引类型, Memory引擎也支持B-Tree索引。 Memory引擎支持 非唯一哈希索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码, 如果出现这种情况, 索引会以链表的方式存放多个行指针到同一个哈希条目中 查找时, 会1. 先在索引中按照哈希码来找到指向数据行的指针, 2. 然后比较数据行的值是否是你查找的行 哈希索引的限制因为索引自身只要存储对应的哈希值和行指针, 所以索引的结构十分紧凑, 这也让哈希索引的查找速度非常快, 然而哈希索引也有它的限制: 哈希索引只包含哈希值和行指针, 而不存储字段值, 所以不能使用索引中的值来避免读取行, 不过, 访问内存中的行的速度很快; 哈希索引数据并不是按照索引值顺序排序的, 所以无法用于排序; 不支持 部分索引列匹配查找 , 因为哈希索引始终是使用索引列的全部内容来计算哈希值的; 不支持范围查询 (只支持如 =, &lt;&gt;, in 等一些 等值比较) 哈希索引数据非常快, 除非有很多哈希冲突 (因为memory引擎支持非唯一索引, 也就是同样的索引, 不同的数据可能产生相同的哈希码)当出现哈希冲突时, 存储引擎必须遍历 冲突的哈希值 所对应的 链表 中所有的行指针, 逐行到表中进行比较, 直到找到所有符合条件的行; 哈希冲突如果哈希冲突很多的话, 一些索引维护操作的代价也为很高, 如果表中删除一行数据, 存储引擎需要遍历对应哈希值的链表中的每一行, 找到并删除对应行的指针, 冲突越多代价越大;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"41. B-Tree索引","slug":"mysql/2017-09-24-mysql-41","date":"2017-09-24T06:50:19.000Z","updated":"2018-03-07T11:41:40.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-41/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-41/","excerpt":"","text":"简介 当人们谈论索引时, 如果没有特别指明索引类型, 多半说的就是 B-Tree 索引, 它使用 B-Tree 数据结构来存储数据; MySQL的大多数存储引擎都支持这种索引 (Archive引擎不支持) 存储引擎以不同的方式使用 B-Tree 索引, 性能也各有优劣: MyISAM使用 前缀压缩技术 使得索引更小 InnoDB则按照原数据格式进行存储 MyISAM的索引是通过 数据的物理位置 引用被索引的行叶子页中的值指向被索引的行的物理地址 InnoDB的索引则是通过 主键 引用被索引的行叶子页中的值指向被索引的行的主键 建立在B-Tree结构(从技术上来说是B+Tree)上的索引 注意: B-Tree索引中存储了 被索引的列实际的列值, 指向数据行的指针 (上面的图可能没体现出来, 结合下面多列索引的图找找感觉~~) B-Tree 通常意味着: 所有的值都是 按顺序 存储的;B-Tree对索引列是顺序组织存储的, 所以很适合查找范围数据; 每一个叶子页到根的距离都是相同的； 叶子页比较特别, 他们的指针指向的是被索引的数据, 而不是其他的节点页(不同引擎的”指针”类型不同) 下图显示了多列索引是如何组织数据存储的 对于下表中的每一行数据, 索引中包含了 last_name, first_name, dob(date of birth, 即出生日期) 列的值 1234567CREATE TABLE People last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum(&apos;m&apos;, &apos;f&apos;) not null, key(last_name, first_name, dob)); 注意: 索引对多个列的值进行排序的依据是定义索引时列的顺序 (如上图中, 最后两个条目, 两个人的姓和名都一样, 则根据他们的出生日期来进行排列顺序) 最左前缀效应之前提到过 “索引可以包含一个或多个列的值。如果包含多个列, 那么列的顺序也十分重要, 因为MySQL只能高效地使用索引的最左前缀列“ 拿之前的 People 表来做参考(其创建的索引是 key(last_name, first_name, dob)) 全值匹配 : 指的是如果查询条件和某个索引中的所有列值进行匹配, 这样就可以利用到上面创建的索引; 比如, 查找 ‘姓为Allen,名为Cuba,出生日期为1960-01-01’ 的人时 并且指的注意的是, 如果你的查询条件做到了全值匹配, 那么即使你查询条件的顺序不是依照左前缀原则, MySQL也会做优化; 匹配最左前缀: 比如, 查找 ‘姓为Allen’ 的人, 即使用索引的第一列, 这样就可以利用到上面创建的索引; 匹配最左前缀列的前缀: 可以用来匹配索引中第一列的值的开头部分, 比如, 查找 姓以’J’开头 的人, 这样就可以利用到上面创建的索引; 匹配最左前缀范围值: 查找 ‘姓在Allen和Barrymore’ 之间的人, 这样就可以利用到上面创建的索引 精确匹配第一列 并 范围匹配第二列: 查找 ‘姓为Allen并且名字以K开头’ 的人, 这样就可以利用到上面创建的索引 只用访问索引的查询 : 这种查询只需要访问索引, 而无需访问数据行; 后面会单独讨论这种 覆盖索引 的优化。 B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了 B-tree索引的限制 如果查询时, 查询条件不是按照索引的最左列开始写, 则无法使用索引; 上面例子中, 索引就无法用于 ‘查找名字为Bill的人’, 也无法查找 ‘生日为1960-01-01的人’, 因为这两列都不是最左数据列。 也无法用于查找 ‘姓以某个字母结尾的人’ (你建索引时指定的列顺序, 列的值内容 都要符合最左前缀才能利用到索引) 查询条件不能跳过索引中的列 比如, 查询 ‘姓为Smith 并且 生日为1960-01-01’ 就无法使用到索引 如果查询条件中有某个列是范围查询, 则其右边的所有列都无法使用索引来优化查找 比如, where last_name=&#39;Smith&#39; and first_name LIKE &#39;J%&#39; AND dob=&#39;1976-12-23&#39; 这个查询只能使用索引的前两列 如果范围查询的列的值结果有限, 比如数据表中只有2个人是 ‘名字以J开头’ 的, 那你也别用范围查找了, 直接用多个等于条件来代替就行比如: where last_name=&#39;Smith&#39; and (first_name=&#39;Jack&#39; or first_name=&#39;Jieke&#39; ) AND dob=&#39;1976-12-23&#39; 到这里可以看到: 索引列的顺序是非常重要的! 在优化性能的时候, 可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"40. 索引基础","slug":"mysql/2017-09-24-mysql-40","date":"2017-09-24T06:30:25.000Z","updated":"2018-03-07T11:37:16.000Z","comments":true,"path":"2017/09/24/mysql/2017-09-24-mysql-40/","link":"","permalink":"http://blog.renyimin.com/2017/09/24/mysql/2017-09-24-mysql-40/","excerpt":"","text":"索引基础 索引是存储引擎用于快速找到记录的一种数据结构; 索引优化是对查询性能优化最有效的手段了; 索引可以包含 一个 或 多个列 的值; 如果如果索引包含多个列, 索引中列的顺序也非常重要, 因为MySQL只能高效地使用索引的最左前缀列; 创建一个包含两个列的索引 和 创建两个只包含一个列的索引 是大不相同的; 在MySQL中, 索引是在 存储引擎层实现, 而不是在服务器层实现: 索引有很多种类型, 可以为不同场景提供更好的性能; 但并不是每个存储引擎都支持所有的索引类型; 不同存储引擎的索引, 其工作方式和底层实现也可能是不一样的; MySQL支持的索引类型B-Tree索引哈希索引数据空间索引全文索引其他索引类别索引优点索引可以让服务器快速地定位到表的指定位置; 但这并不是索引的唯一作用, 到现在可以看到, 根据创建索引的数据结构不同, 索引也有一些其他的附加组作用 最常见的B-Tree索引, 按照顺序存储数据, 所以MySQL可以用来做 ORDER BY 和 GROUP BY 操作; 因为数据是有序的, 所以B-Tree也就会将相关的列值都存储在一起 因为B-Tree索引中存储了实际的列值, 所以某些查询(说的是覆盖索引)可能只使用索引就能够完成查询工作了","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://blog.renyimin.com/tags/索引/"}]},{"title":"08.","slug":"composer/2017-09-22-composer-08","date":"2017-09-22T13:20:17.000Z","updated":"2018-02-03T07:34:43.000Z","comments":true,"path":"2017/09/22/composer/2017-09-22-composer-08/","link":"","permalink":"http://blog.renyimin.com/2017/09/22/composer/2017-09-22-composer-08/","excerpt":"","text":"","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"07. autoload之psr-4, psr-0, classmap, files","slug":"composer/2017-09-22-composer-07","date":"2017-09-22T05:10:30.000Z","updated":"2018-02-28T08:31:04.000Z","comments":true,"path":"2017/09/22/composer/2017-09-22-composer-07/","link":"","permalink":"http://blog.renyimin.com/2017/09/22/composer/2017-09-22-composer-07/","excerpt":"","text":"自动加载 vendor/ 库 对于composer安装到vendor/目录下的库 如何在项目中自动加载到? composer安装完库之后, 会生成一个 vendor/autoload.php 文件, 你可以在项目中简单的引入这个文件, 这样就可以自动加载composer管理的vendor/下的这些库; require ‘vendor/autoload.php’; 当然, 你自己发布的packagist包一定要注意其中也需要设置自动加载规则(命名空间和目录对应关系) composer 自动加载类型psr-4 如果你的项目中还没有准备好自动加载功能(来实现对你项目各个目录中类的自动加载),现在你已经不需要自己去准备了, 因为一旦你引入了composer, 它就已经为你准备好了这一功能!!(当然, 引入composer很简单, 无论你是自己 composer init初始化composer.json文件还是通过composer require安装一个包来生成 composer.json文件都可以) 它不仅能像上面说的那样帮你实现自动加载composer帮你管理的包, 也可以帮你在项目中创建自己的自动加载!! 你可以在 composer.json 的 autoload 字段中增加自己的 autoloader 12345&#123; &quot;autoload&quot;: &#123; &quot;psr-4&quot;: &#123;&quot;Acme\\\\&quot;: &quot;src/&quot;&#125; &#125;&#125; 像上面那样, 你就定义一个从 命名空间 到 目录 的映射关系, 此时 src 应该和vendor目录同级, 都在你项目的根目录下; 最后, 你在src目录下写的类就应该是Acme命名空间; 如果指定的 Acme 是个顶级命名空间, 那src下不管目录多深, 都可以从这个顶级开始找到, 不用再一一配置子命名空间与目录的对应关系了; 如果 Acme 不是个顶级命名空间, 那么和src同级的目录也得配置其命名空间和对应关系 像上面那样配置好之后, composer 将注册一个 PSR-4 autoloader 到 Acme 命名空间 添加完 autoload 字段后，你应该再次运行 install 命令来生成 vendor/autoload.php 文件 注意: 此时虽然修改了 composer.json 文件, 但是由于并没有涉及到包信息(比如版本信息)的修改, 所以install和update都一样; 引用这个文件也将返回 autoloader 的实例，你可以将包含调用的返回值存储在变量中，并添加更多的命名空间。这对于在一个测试套件中自动加载类文件是非常有用的。 可参考: http://docs.phpcomposer.com/01-basic-usage.html#Autoloading psr-0不推荐…. 这里就不扯了 classmap classmap 需要写在autoload内; classmap 所配置的目录下的所有 .php 和 .inc 文件里的类, 都会在 install/update 过程中存储到 vendor/composer/autoload_classmap.php 文件中的map数组中; vendor/composer/autoload_classmap.php文件中的映射关系, key是扫描到的类的namespace\\类名 或者 类名(没有命名空间的), value是类文件的路径 格式 12345&#123; &quot;autoload&quot;: &#123; &quot;classmap&quot;: [&quot;src/&quot;, &quot;lib/&quot;, &quot;test/Something.php&quot;] // 可以看到不仅可以指定目录, 也可以指定文件 &#125;&#125; files 也需要写在autoload内; Files方式就是手动指定供直接加载的文件, 比如说我们有一系列全局的helper functions，可以放到一个helper文件里然后直接进行加载;&quot;autoload&quot;: { &quot;psr-4&quot;: { //一些自己写的类库 &quot;Test\\\\&quot;: &quot;src/&quot;, &quot;Test1\\\\&quot;: &quot;src1/&quot;, &quot;Test2\\\\&quot;: &quot;src2/&quot; }, &quot;classmap&quot;: [&quot;src/&quot;, &quot;src1/hehe.php&quot;], // 一些类 &quot;files&quot;: [&quot;common/functions.php&quot;] // 一些公共函数 }","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"06. 发布自己的包","slug":"composer/2017-09-20-composer-06","date":"2017-09-20T10:40:17.000Z","updated":"2018-03-01T02:20:43.000Z","comments":true,"path":"2017/09/20/composer/2017-09-20-composer-06/","link":"","permalink":"http://blog.renyimin.com/2017/09/20/composer/2017-09-20-composer-06/","excerpt":"","text":"前言 GitHub官方提供了和Packagist相关的钩子服务; Packagist主要提供Composer包发布和索引, 默认Composer从Packagist获取资源。(可以使用你的GitHub帐号登录Packagist) 也可以理解为你真正的项目代码是在Gihub仓库中, 而相关介绍及索引信息是在Packagist平台 发布步骤github仓库部分 github创建仓库 将本地目录与github中仓库关联 在本地项目中初始化 composer.json 文件 composer init(使用composer自带的初始化命令，创建一个composer.json描述文件)。 如果想手动编辑，可以去composer官网阅读相关文档获得帮助。 在本地开发一个功能包, 并上传到github仓库中 开发功能包的注意事项: 需要在composer.json文件中配置好包内部的自动加载规则, 比如: 12345\"autoload\": &#123; \"psr-4\": &#123; \"Lant\\\\\": \"./\" &#125; &#125; 否则, 即使你下载下来你的包, 并且你引入了 vendor/autoload.php 文件, 也无法正常自动加载到你的包代码 如果你的包composer.json文件中指定了 顶级命名空间名 与 目录 的关系, 子命名空间和目录就不用设置了 只用在子目录的类文件中声明 namespace 顶级命名空间/本命名空间 即可! 如果你包中的是几个同级目录, 那你可能就需要为每个目录设置 命名空间 和 目录的对应关系! Packagist部分 访问Packagist主页，确认自己已经登录，然后点击右上角大大的Submit Package，然后填入我们创建的仓库的地址，点击Check，然后没问题，再点击Submit。 为了让Packagist平台可以自动更新github仓库中的信息, 需要我们配置Github仓库的钩子服务 进入仓库, 点击 “setting” 点击左侧 “Integrations &amp; services” 然后 “Add service” - 选择 “Packagist” 然后填写表单, username为你的packagist账户名(如果使用github账户登录, 则为github账户名), Token是packagist中的Token; 注意有时候你的包修改完之后, 发现github仓库中有了, packagist中的同步时间也有了, 那你得看一下中国全量镜像站点的同步时间是否正确 (说好的一分钟同步一次, 但..不尽然, 被坑过); 参考http://note.youdao.com/noteshare?id=8265aa9789dba4451ada428a95048f33","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"05. require 和 create-project","slug":"composer/2017-09-20-composer-05","date":"2017-09-20T10:21:46.000Z","updated":"2018-02-03T07:27:07.000Z","comments":true,"path":"2017/09/20/composer/2017-09-20-composer-05/","link":"","permalink":"http://blog.renyimin.com/2017/09/20/composer/2017-09-20-composer-05/","excerpt":"","text":"require是在当前项目目录下进行包安装, 一般安装到vendor/下; create-project 是指把包当成一个项目来安装, 也就是如果你创建的这个包是一个完整的项目, 你就可以来直接把这个包当成项目来创建 composer create-project lant/packagist_test 项目在本地的目录名 项目版本(如:dev-master)","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"04.更新包的方式(require, install, update)对比","slug":"composer/2017-09-18-composer-04","date":"2017-09-18T14:23:17.000Z","updated":"2018-03-01T09:52:19.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-04/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-04/","excerpt":"","text":"require虽然是安装, 但也可以用来更新包到指定版本, 并同时更新composer.json和composer.lock文件, 如果没有则会创建! composer require 包名 新版本 当你手动修改了composer.json文件中包的版本之后, 可以执行 composer update 来重新安装该包, 并更新composer.lock文件, 如果没有则会创建! 当然, 你可以指定你需要更新的包 composer update 包名, 包名... 另外, 还应该注意一下 install 和 update 的一个小细节:当你修改了composer.json文件, 如果不是对包做增删改(比如增加一个包, 删除一个包, 或者修改包的版本信息), 而是增删改其他信息(比如配置自动加载之类的信息), 那么你使用 install 和 update 是没有区别的! 小结 install, require, update都可以做包安装 (如果在composer.json中做了像自动加载的信息的配置, install和update会重新进行类关系映射) require 和 update 都可以做包更新 (如果在composer.json文件中做了包版本的增删改查, install无法进行更新) 另外: install: 主要是在部署阶段使用，以便在生产环境和开发环境使用的都是composer.lock文件中相同的依赖项，保证线上部署环境与本地开发环境的一致性。 update: 主要是在开发阶段使用，根据我们在composer.json文件中指定的内容升级项目的依赖包。 此更新非彼更新比如packagist中的包现在是v2.0, 你本地的是v1.0, 你直接执行 composer update, 不要指望composer会自动帮你更新到v2.0, 要更新你得在composer.json中指明版本号, 然后composer update才会根据composer.json文件去更新!这样, 至于你的更新是升级还是降级就看你在composer.json文件中如何指定的了!","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"03.安装包的方式(require, install, update)对比","slug":"composer/2017-09-18-composer-03","date":"2017-09-18T13:20:31.000Z","updated":"2018-03-01T09:50:48.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-03/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-03/","excerpt":"","text":"初次做包安装 如果你的项目刚引入composer (composer init), 现在是第一次进行包的安装, 也就是只有composer.json文件, 并没有composer.lock文件, 可以, 直接执行 composer require 包名 包版本 该命令会帮你安装所需的包到 vendor/ 目录下 生成 composer.json 文件, 并将依赖信息写入文件中 生成 composer.lock 文件 composer将会通过composer.json来读取需要的包和相对的版本, 然后创建composer.lock文件 所以，对于除了包版本之外的其他配置, 如自动加载…等, composer.lock 文件不会包含! (所以当改变这些信息之后, install 和 update的效果一样) 非初次安装 如果你的项目已经安装过一些包了, 即已经有 composer.json, composer.lock 文件; 此时, 你有两种方式: 和初次安装一样, 使用 composer require 包名 包版本 进行安装 手动在composer.json文件中进行配置, 然后运行 composer update注意, 此时不能使用 composer install, 因为该命令是依据 composer.lock 文件来进行安装的, 而composer.lock文件又是依据composer.json文件中的包及包版本信息生成的,而此时你改变了 composer.json 文件, 并且是新增了一个包(及版本对应关系), 所以你需要更新 composer.lock 文件! 如果你执行的是 composer install, 则会警告: Warning: The lock file is not up to date with the latest changes in composer.json. You may be getting outdated dependencies. Run update to update them. 当然, 你也可以删除 composer.lock 文件, 然后手动在composer.json文件中进行配置, 最后执行 composer install install, require, update都可以做包安装","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"02. 设置中国全量镜像","slug":"composer/2017-09-18-composer-02","date":"2017-09-18T09:40:11.000Z","updated":"2018-03-01T09:54:16.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-02/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-02/","excerpt":"","text":"前言 一般情况下,安装包的数据(主要是 zip 文件) 一般是从 github.com 上下载的, 安装包的元数据是从 packagist.org 上下载的。然而，由于众所周知的原因，国外的网站连接速度很慢，并且随时可能被“墙”甚至“不存在”。“Packagist 中国全量镜像”所做的就是缓存所有安装包和元数据到国内的机房并通过国内的 CDN 进行加速，这样就不必再去向国外的网站发起请求, 从而达到加速 composer install以及 composer update 的过程, 并且更加快速、稳定。因此, 即使 packagist.org、github.com 发生故障(主要是连接速度太慢和被墙), 你仍然可以下载、更新安装包。 原理图下面是一张从网上找的图 配置方法1.修改 composer 的全局配置文件打开命令行窗口（windows用户）或控制台（Linux、Mac 用户）并执行命令: composer config -g repo.packagist composer https://packagist.phpcomposer.com 2.修改当前项目的 composer.json 配置文件 打开命令行窗口（windows用户）或控制台（Linux、Mac 用户）, 进入你的项目的根目录（也就是 composer.json 文件所在目录），执行命令: composer config repo.packagist composer https://packagist.phpcomposer.com 上述操作将会在 当前项目的 composer.json 文件的末尾自动添加镜像的配置信息（你也可以自己手工添加）： 123456\"repositories\": &#123; \"packagist\": &#123; \"type\": \"composer\", \"url\": \"https://packagist.phpcomposer.com\" &#125; &#125; 参考: https://pkg.phpcomposer.com/","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"01.安装","slug":"composer/2017-09-18-composer-01","date":"2017-09-18T09:10:11.000Z","updated":"2018-02-28T08:16:31.000Z","comments":true,"path":"2017/09/18/composer/2017-09-18-composer-01/","link":"","permalink":"http://blog.renyimin.com/2017/09/18/composer/2017-09-18-composer-01/","excerpt":"","text":"composer.phar文件下载先下载composer.phar文件, 3种方式: curl -sS https://getcomposer.org/installer | php php -r &quot;readfile(&#39;https://getcomposer.org/installer&#39;);&quot; | php 手动下载Composer, 地址 Linux/Unix/OS 全局安装 : mv composer.phar /usr/local/bin/composer 局部安装 : mv composer.phar /局部目录/composer windows 全局安装 : 配置composer.phar文件路径(D:\\WWW\\composer)到环境变量中 ; 接下来需要在composer.phar同级目录下新建文件composer.bat : echo @php &quot;%~dp0composer.phar&quot; %*&gt;composer.bat 这就安装好了 局部安装 : 直接把下载的composer.phar文件放到项目根目录下; 运行命令 : php composer.phar install 然后就安装成功了(每个局部目录都需要这么安装, 比较麻烦) 推荐全局安装即可. composer自我更新 self-update 将 Composer 自身升级到最新版本, 只需要运行 composer self-update 命令。它将替换你的 composer.phar 文件到最新版本。 如果你想要升级到一个特定的版本，可以 composer self-update 1.0.0-alpha7 自我更新还可以跟参数 --rollback (-r): 回滚到你已经安装的最后一个版本。 --clean-backups: 在更新过程中删除旧的备份，这使得更新过后的当前版本是唯一可用的备份。","categories":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/categories/Composer/"}],"tags":[{"name":"Composer","slug":"Composer","permalink":"http://blog.renyimin.com/tags/Composer/"}]},{"title":"27. REPEATABLE READ 可重复读","slug":"mysql/2017-09-17-mysql-27","date":"2017-09-17T14:10:52.000Z","updated":"2018-03-07T09:19:48.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-27/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-27/","excerpt":"","text":"前言 该隔离级别可以解决不可重复读问题, 脏读问题; 也就是它既可以让事务只能读其他事务已提交的的记录, 又能在同一事务中保证多次读取的数据即使被其他事务修改,读到的数据也是一致的。 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免 脏读, 不可重复读 这两个问题的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"26. MVCC 多版本并发控制","slug":"mysql/2017-09-17-mysql-26","date":"2017-09-17T10:10:21.000Z","updated":"2018-03-07T09:19:44.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-26/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-26/","excerpt":"","text":"简介Multiversion Concurrency Control 阿里数据库内核’2017/12’月报中对MVCC的解释是: 多版本控制: 指的是一种 提高并发 的技术。最早的数据库系统, 只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。 &lt;高性能MySQL&gt;中对MVCC的部分介绍 MySQL的大多数事务型存储引擎实现的其实都不是简单的行级锁。基于提升并发性能的考虑, 它们一般都同时实现了多版本并发控制(MVCC)。不仅是MySQL, 包括Oracle,PostgreSQL等其他数据库系统也都实现了MVCC, 但各自的实现机制不尽相同, 因为MVCC没有一个统一的实现标准。可以认为MVCC是行级锁的一个变种, 但是它在很多情况下避免了加锁操作, 因此开销更低。虽然实现机制有所不同, 但大都实现了非阻塞的读操作，写操作也只锁定必要的行。MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。 相关概念 read view 主要是用来做可见性判断的, 比较普遍的解释便是”本事务不可见的当前其他活跃事务”, 但正是该解释, 可能会造成一节理解上的误区, 所以此处提供两个参考, 用来避开理解误区:read view中的高水位low_limit_id可以参考 对于read view快照的生成时机, 也非常关键, 也正是因为生成时机的不同, 造成了RC, RR两种隔离级别的不同可见性 可以参考; 在innodb(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来; 在innodb(默认repeatable committed级别), 事务中每条select语句都会创建一个快照(read view); 参考1234With REPEATABLE READ isolation level, the snapshot is based on the time when the first read operation is performed.使用REPEATABLE READ隔离级别，快照是基于执行第一个读操作的时间。With READ COMMITTED isolation level, the snapshot is reset to the time of each consistent read operation.使用READ COMMITTED隔离级别，快照被重置为每个一致的读取操作的时间。 undo-log 可以参考数据库内核月报2015/04/01 前言Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中, 但从5.6开始，也可以使用独立的Undo表空间。Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作。大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除, 而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo; 在回滚段中的undo logs分为: insert undo log 和 update undo loginsert undo log : 事务对insert新记录时产生的undolog, 只在本事务回滚时需要, 并且在事务提交后就可以立即丢弃;update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除。 InnoDB存储引擎在数据库每行数据的后面添加了三个字段 6字节的事务ID(DB_TRX_ID)字段: 用来标识最近一次对本行记录做修改(insert|update)的事务的标识符, 即最后一次修改(insert|update)本行记录的事务id。至于delete操作，在innodb看来也不过是一次update操作，更新行中的一个特殊位将行表示为deleted, 并非真正删除。 7字节的回滚指针(DB_ROLL_PTR)字段: 指写入回滚段(rollback segment)的 undo log record (撤销日志记录)。如果一行记录被更新, 则 undo log record 包含 ‘重建该行记录被更新之前内容’ 所必须的信息。 6字节的DB_ROW_ID字段: 包含一个随着新行插入而单调递增的行ID, 当由innodb自动产生聚簇索引时, 聚簇索引会包括这个行ID的值, 否则这个行ID不会出现在任何索引中。结合聚簇索引的相关知识点, 大概可以理解为: 如果我们的表中没有主键或合适的唯一索引, 也就是无法生成聚簇索引的时候, InnoDB会帮我们自动生成聚集索引, 但聚簇索引会使用DB_ROW_ID的值来作为主键; 如果我们有自己的主键或者合适的唯一索引, 那么聚簇索引中也就不会包含 DB_ROW_ID 了 , 如果有误, 希望指正, 谢谢。 可见性比较算法 假设要读取的数据行, 最后完成事务提交的事务id(即, 让当前数据行最后落地的事务id)为 trx_id_current ; 当前新开事务id为 new_id 当前新开事务创建的快照 read view 中最早的事务id为 up_limit_id, 最迟的事务id为 low_limit_id(注意 low_limit_id=未开启的事务id=当前最大事务id+1) 比较: 1.trx_id_current &lt; up_limit_id, 这种情况比较好理解, 表示, 新事务在读取该行记录时, 该行记录的最后稳定事务ID小于系统当前所有活跃的事务ID, 所以当前行稳定数据对新事务可见, 跳到步骤5; 2.trx_id_current &gt;= trx_id_last, 这种情况也比较好理解, 表示, 该行记录的 最后稳定事务ID 是在 本次新事务 创建之后才开启的,但是却在本次新事务执行第二个select前就commit了, 所以该行记录的当前值对本次新事务不可见(RR级别), 跳到步骤4; 3.trx_id_current &lt;= trx_id_current &lt;= trx_id_last, 表示: 该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见, 调到步骤4,否则表示可见。 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 trx_id_current，然后跳到步骤1重新开始判断; 5.将该可见行的值返回。 案例分析1.下面是一个非常简版的演示事务对某行记录的更新过程, 当然, InnoDB引擎在内部要做的工作非常多: 2.下面是一套比较算法的应用过程也可参考https://github.com/zhangyachen/zhangyachen.github.io/issues/68中的案例 当前读和快照读 MySQL的InnoDB存储引擎默认事务隔离级别是RR(可重复读), 是通过 “行排他锁+MVCC” 一起实现的, 不仅可以保证可重复读, 还可以部分防止幻读; 为什么是部分防止幻读, 而不是完全防止? 效果: 在如果事务B在事务A执行中, insert了一条数据并提交, 事务A再次查询, 虽然读取的是undo中的旧版本数据(防止了部分幻读), 但是事务A中执行update或者delete都是可以成功的!! 因为在innodb中的操作可以分为当前读(current read)和快照读(snapshot read): 快照读 和 当前读 可参考之前的博文幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read) 小结 InnoDB所谓的MVCC 一般我们认为MVCC有下面几个特点： 每行数据都存在一个版本，每次数据更新时都更新该版本 修改时Copy出当前版本, 然后随意修改，各个事务之间无干扰 保存时比较版本号，如果成功(commit)，则覆盖原记录, 失败则放弃copy(rollback) 就是每行都有版本号，保存时根据版本号决定是否成功，听起来含有乐观锁的味道, 因为这看起来正是，在提交的时候才能知道到底能否提交成功 而InnoDB实现MVCC的方式是: 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） 二者最本质的区别是: 当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ Innodb的实现真算不上MVCC, 因为并没有实现核心的多版本共存, undo log 中的内容只是串行化的结果, 记录了多个事务的过程, 不属于多版本共存。但理想的MVCC是难以实现的, 当事务仅修改一行记录使用理想的MVCC模式是没有问题的, 可以通过比较版本号进行回滚, 但当事务影响到多行数据时, 理想的MVCC就无能为力了。 比如, 如果事务A执行理想的MVCC, 修改Row1成功, 而修改Row2失败, 此时需要回滚Row1, 但因为Row1没有被锁定, 其数据可能又被事务B所修改, 如果此时回滚Row1的内容，则会破坏事务B的修改结果，导致事务B违反ACID。 这也正是所谓的 第一类更新丢失 的情况。 也正是因为InnoDB使用的MVCC中结合了排他锁, 不是纯的MVCC, 所以第一类更新丢失是不会出现了, 一般说更新丢失都是指第二类丢失更新。 MVCC 高并发特点写不影响读，读不影响写，写只影响写只有写写会阻塞!!! 参考 关于read view创建时机: http://www.sohu.com/a/194511597_610509 https://www.cnblogs.com/digdeep/p/4947694.html https://www.zhihu.com/question/265280455/answer/292022808 关于比较算法 low_limit_id 高水位事务: https://github.com/zhangyachen/zhangyachen.github.io/issues/68 https://www.zhihu.com/question/66320138 https://www.zhihu.com/question/265280455/answer/292022808 大咖问答: https://www.zhihu.com/inbox/4577674200 更多可以参考数据库内核月报: https://yq.aliyun.com/articles/303200?spm=5176.100240.searchblog.9.271fd153pQ9FgV 官方文档","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"MVCC","slug":"MVCC","permalink":"http://blog.renyimin.com/tags/MVCC/"}]},{"title":"25. READ COMMITTED","slug":"mysql/2017-09-17-mysql-25","date":"2017-09-17T06:50:52.000Z","updated":"2018-03-07T09:19:39.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-25/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-25/","excerpt":"","text":"前言 READ COMMITTED 隔离级别可以解决高并发场景下, 事务 脏读 的问题; 也就是可以让事务只能读其他事务已完成(提交/回滚)的落地数据; 如果让你用 锁 来设计该隔离级别 ? 假设, 在事务A中读取数据前, 事务B对同一数据做了修改并且还没有完成(commit/rollback), 那如何让事务A无法读取事务B中 尚未落地的脏数据 呢? 当事务B在对数据做写操作的时候, 给数据加上行级的排他锁(X lock)(读取的时候也别忘了加上共享锁(S lock)), 注意两种锁都要使用; 那事务A由于加不上共享锁/排他锁, 自然只能阻塞等事务A完成后才能读取/修改数据了 这样做的话确实实现了效果, 也就避免了脏读, 事实上, 也解决可了 不可重复读(因为一旦加了共享锁, 其他事务也无法加排他锁进行修改), 但问题是这是一种很低效的传统思路, 因为对于大部分应用来说, 读操作是多于写操作的, 当写操作加锁时, 那么读操作全部被阻塞, 这样在大用户量高并发的情况下, 会直接降低数据库的读效率。 所以, 为了提高并发性, MySQL是自然不会简单地使用传统思路(直接加锁)来解决的问题 注意: 如果只是 写操作加排他锁的话 是无法避免脏读的; 事实上, MySQL是使用 写操作加排他锁(读操作不加锁), 结合MVCC 多版本并发控制 来实现该隔离级别; 方案MySQL 在事务隔离级别Read committed 、Repeatable Read下，InnoDB 存储引擎采用 非锁定 的 一致性读－－即读取数据不用加锁，而是采用的是MVCC中一致性非锁定读模式; 从而做到: 写不影响读，读不影响写，写只影响写, 只有写写会阻塞!!! 读不影响写: 当事务A中正在执行读操作时，事务B的写操作不会因此去等待当前事务A上S锁的释放(因为事务A读取压根就没加锁)，而是直接可以对数据加X锁进行操作。 写不影响读: 当事务A中正在执行写操作时, 虽然对数据加了X锁, 但是事务B的读操作不会因此去等待当前事务行上X锁的释放，而是会去读取快照数据 (RC和RR因快照产生时机不同, 导致了隔离级别不一样, 读取的落地数据也不相同)。 所以总结来看, READ UNCOMMITTED 和 REPEATABLE READ 这两个隔离级别都是使用 写用排他锁 + 读用MVCC, 区别可以参考 MVCC 多版本并发控制","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"24. READ UNCOMMITTED 未提交读","slug":"mysql/2017-09-17-mysql-24","date":"2017-09-17T06:40:04.000Z","updated":"2018-03-07T09:19:35.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-24/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-24/","excerpt":"","text":"简介 READ UNCOMMITTED 隔离级别会造成 脏读(Dirty Read) 现象的出现:也就是说, 事务 可以读取 其他事务 未提交的数据(事务A中对数据所做的修改, 即使还没有被提交, 对其他事务也都是可见的); 这个级别会导致很多问题, 而且从性能上来说, READ COMMITTED 并不会比其他的级别好太多, 却缺乏其他级别的很多好处, 在实际应用中一般很少使用。 虽然该隔离级别很少使用, 但还是有必要了解一下, 它这个隔离级别究竟是如何进行隔离的, 竟还能容许很多问题的存在? 测试准备环境 先准备一张测试表test_transaction: 1234567891011121314DROP TABLE IF EXISTS `test_transaction`;CREATE TABLE `test_transaction` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `user_name` char(20) NOT NULL COMMENT &apos;姓名&apos;, `age` tinyint(3) NOT NULL COMMENT &apos;年龄&apos;, `gender` tinyint(1) NOT NULL COMMENT &apos;1:男, 2:女&apos;, `desctiption` text NOT NULL COMMENT &apos;简介&apos;, PRIMARY KEY (`id`), KEY `name_age_gender_index` (`user_name`,`age`,`gender`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;INSERT INTO `test_transaction` VALUES (1, &apos;金刚狼&apos;, 127, 1, &apos;我有一双铁爪&apos;);INSERT INTO `test_transaction` VALUES (2, &apos;钢铁侠&apos;, 120, 1, &apos;我有一身铁甲&apos;);INSERT INTO `test_transaction` VALUES (3, &apos;绿巨人&apos;, 0, 2, &apos;我有一身肉&apos;); 如下: 123456789mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 2 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) 脏读效果 先查看 客户端1 事务的隔离级别: SELECT @@SESSION.tx_isolation;; 可以看到, InnoDB默认事务隔离级别为 REPEATABLE READ 123456789mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; 重新设置 会话端1 的事务隔离级别为 read uncommitted: SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; 注意, 此时只是当前会话端的隔离级别被改, 其余 会话端 还是默认的 REPEATABLE READ 隔离级别 接下来将 会话端2 的事务隔离级别也设置为read uncommitted; 客户端1 开启事务, 并执行一个查询 ‘读取数据’, 注意, 客户端1 的事务并未提交 1234567891011121314151617181920mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction where id=2;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 |+----+-----------+-----+--------+--------------------+1 row in set (0.00 sec) mysql&gt; 客户端2 开启事务, 并修改客户端1查询的数据 123456789101112131415mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-托尼&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 此时发现, 客户端2 可以对 客户端1 正在读取的记录进行修改 注意, 客户端2此时的事务也并未提交 回到 客户端1, 再次查询数据, 发现数据已经变成了’钢铁侠-托尼’; 然后客户端2 rollback 事务, 再到客户端1中查询, 发现user_name又变成了’钢铁侠’, 那之前读到 ‘钢铁侠-托尼’ 就是脏数据了, 这就是一次 脏读 小结: 可以用一张图来演示整个实验过程 分析该隔离级别如何加锁重新构造测试条件 客户端1开启事务, 然后对数据做修改 1234567mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rymuscle&apos; where id=2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; 注意, 客户端1此时的事务并未提交 客户端2开启事务, 对相同的数据行做修改 12345mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; update test_transaction set user_name=&apos;钢铁侠-rym&apos; where id=2;....阻塞等待了 最终会如下: 注意: 在上面的过程, 在客户端2阻塞阶段, 你可以通过一个新的客户端来分析, 客户端2 在锁等待的情况下的 加锁情况 和 事务状态: 查看表的加锁情况: select * from information_schema.INNODB_LOCKS; 事务状态 select * from information_schema.INNODB_TRX; 小结 所以: READ UNCOMMITTED 隔离级别下, 写操作是会加锁的, 而且是X排他锁, 直到客户端1事务完成, 锁才释放, 客户端2才能进行写操作 接下来你肯定会纳闷 “既然该隔离级别下事务在修改数据的时候加的是x锁, 并且是事务完成后才释放, 那一次场测试中, 客户端2 在事务中修改完数据之后, 为什么还没提交事务, 也就是x锁还在, 结果客户端1却能读取到客户端2修改的数据”？ 这完全不符合排他锁的特性啊(排他锁会阻塞除当前事务之外的其他事务的读,写操作) 其实网上已经有人在sqlserver的官网上找到了相关资料: 12345ansactions running at the READ UNCOMMITTED level do not issue shared locks to prevent other transactions from modifying data read by the current transaction. READ UNCOMMITTED transactions are also not blocked by exclusive locks that would prevent the current transaction from reading rows that have been modified but not committed by other transactions. When this option is set, it is possible to read uncommitted modifications, which are called dirty reads. Values in the data can be changed and rows can appear or disappear in the data set before the end of the transaction. This option has the same effect as setting NOLOCK on all tables in all SELECT statements in a transaction. This is the least restrictive of the isolation levels. 翻译翻译, 在思考思考, 大概说的是在 READ UNCOMMITTED 级别运行的事务不会发出共享锁: 也就是事务A在读取数据时, 什么锁都不加; 这样的话, 事务B就可以对同样的数据进行修改(同时会加上排他锁);而事务A要读取事务B未提交的修改, 也不会被事务B所加的排他锁阻止, 因为排他锁会阻止其他事务再对其锁定的数据加读写锁, 但是可笑的是, 事务在该隔离级别下去读数据的话根本什么锁都不加, 这就让排他锁无法排它了; 所以可以得出: READ UNCOMMITTED隔离级别下, 读不会加任何锁。而写会加排他锁，并到事务结束之后释放。(读写不阻塞, 写写阻塞 (但会读到脏数据))","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"23. 事务隔离级别(READ UNCOMMITTED) 与 锁","slug":"mysql/2017-09-17-mysql-23","date":"2017-09-17T06:20:52.000Z","updated":"2018-03-07T09:19:29.000Z","comments":true,"path":"2017/09/17/mysql/2017-09-17-mysql-23/","link":"","permalink":"http://blog.renyimin.com/2017/09/17/mysql/2017-09-17-mysql-23/","excerpt":"","text":"前言 之前几篇博文已经介绍了 Mysql事务, 高并发下事务将会面对的问题 及 解决方案;从之前的博文中可以了解到: MySQL在高并发场景下, 主要采用 事务隔离性中的4种隔离级别 及 MVCC机制 来解决事务可能会面临的一些问题; 接下来主要探讨一下, 在MySQL中, 事务的各隔离级别是如何实现的, 如何解决问题的, 隔离级别和锁之间是什么关系? 隔离级别 和 锁的关系 对于事务中各隔离级别与锁的关系, 先看下一参考美团技术博客中对事务隔离级别的介绍: 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 从上面的博文大概可以了解到: 事务隔离级别的核心就是锁, 各隔离级别使用了不同的加锁策略。 接下来看一下各隔离级别是如何实现及如何解决高并发事务问题的。 READ UNCOMMITTED 未提交读READ UNCOMMITTED 提交读MVCC 多版本并发控制REPEATABLE READ 可重复读参考资料:-《高性能MySQL》 MySQL官方文档 慕课mark_rock同学手记 美团技术博客","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"22. 幻读的延伸 - 快照读 (snapshot read) 与 当前读 (current read)","slug":"mysql/2017-09-16-mysql-22","date":"2017-09-16T06:10:07.000Z","updated":"2018-03-07T09:19:20.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-22/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-22/","excerpt":"","text":"前言RR + MVCC 虽然解决了 幻读 问题, 但严格来说, 只是部分解决幻读问题 演示 打开 客户端1 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 打开 客户端2 查看隔离级别及初始数据 12345678910111213141516171819mysql&gt; SELECT @@SESSION.tx_isolation;+------------------------+| @@SESSION.tx_isolation |+------------------------+| REPEATABLE-READ |+------------------------+1 row in set (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端2中 开启事务, 然后查询数据 1234567891011121314mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec) mysql&gt; 在客户端1中插入一条id为4的新数据 (未开启事务, 所以会自动提交) 1234567891011121314mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 || 4 | 死侍 | 18 | 0 | A bad boy |+----+-----------+-----+--------+--------------------+4 rows in set (0.00 sec) mysql&gt; 在 客户端2 之前开启的事务中再次查询数据, 发现数据没有变化(表示可以重复读, 并且克服了select幻读)!! 12345678910111213141516171819202122232425262728mysql&gt; begin;Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; select * from test_transaction;+----+-----------+-----+--------+--------------------+| id | user_name | age | gender | desctiption |+----+-----------+-----+--------+--------------------+| 1 | 金刚狼 | 127 | 1 | 我有一双铁爪 || 2 | 钢铁侠 | 120 | 1 | 我有一身铁甲 || 3 | 绿巨人 | 0 | 2 | 我有一身肉 |+----+-----------+-----+--------+--------------------+3 rows in set (0.00 sec)mysql&gt; insert into test_transaction (`id`,`user_name`,`age`,`gender`,`desctiption`) values (4, &apos;死侍&apos;, 18, 0, &apos;A bad boy&apos;);1062 - Duplicate entry &apos;4&apos; for key &apos;PRIMARY&apos; //( 后面会看到: 其实是因为insert是当前读)mysql&gt; //并且, 此时`update/delete`也是可以操作这条在事务中看不到的记录的! //( 后面会看到: update，delete也都是当前读) 问题的出现 虽然多次select读取,发现已经克服了幻读问题 但当 在客户端2事务中 insert插入一条id为4的新数据, 发现提示数据已经存在, 那么这是什么问题呢? 可以参考MySQL官方文档 – 一致性非阻塞读 The snapshot of the database state applies to SELECT statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a DELETE or UPDATE statement issued from another concurrent REPEATABLE READ transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.个人认为应该翻译为: 数据库状态的快照适用于事务中的SELECT语句, 而不一定适用于所有DML语句。 如果您插入或修改某些行, 然后提交该事务, 则从另一个并发REPEATABLE READ事务发出的DELETE或UPDATE语句就可能会影响那些刚刚提交的行, 即使该事务无法查询到它们。 如果事务更新或删除由不同事务提交的行, 则那些更改对当前事务就变得可见。 但是如果事务select由不同事务提交的行, 则那些更改对当前事务就不可见(此时算是rr的可重复读); 小结 也就是RR隔离级别, 在同一事务中多次读取的话, 只是对 select 克服了幻读; 但是对其他DML并没有做到(其他DML能察觉到数据被别的事务提交过了)! 这就引出了新的两个概念 当前读 和 快照读通常在RC,RR隔离级别下, 不做特殊处理, 使用的select都是快照读, 其他dml就算是当前读; (MVCC写阻塞写) 其实, MVCC并发控制中的读操作分为两类: 快照读 (snapshot read) 与 当前读 (current read); 参考 在RR级别下 快照读 是通过MVVC(多版本控制)和undo log来实现的 而当前读 是需要加 record lock(记录锁) 和 gap lock(间隙锁) 来实现的, 如果需要实时显示数据，还是需要通过加锁来实现, 这个时候会使用next-key技术来实现。 快照读, 读取专门的快照(对于RC，快照(ReadView)会在每个语句中创建, 对于RR, 快照是在事务启动时创建的), 快照读的操作如下: 1简单的select操作 (不包括: select ... lock in share mode, select ... for update) 当前读, 读取最新版本的记录, 没有快照, 在InnoDB中, 当前读取根本不会创建任何快照。当前读的操作如下, 可以看到 insert, update, delete都是快照读, 所以这几个操作会察觉到其他事务对数据做的更改, 而select察觉不到: 12345select ... lock in share modeselect ... for updateinsertupdatedelete 最后, 使用隔离性的最高隔离级别SERIALIZABLE也可以解决幻读, 但该隔离级别在实际中很少使用!","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"21. MySQL高并发事务问题 及 解决方案","slug":"mysql/2017-09-16-mysql-21","date":"2017-09-16T05:40:53.000Z","updated":"2018-03-09T00:47:59.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-21/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-21/","excerpt":"","text":"前言上一篇MySQL事务简介简单介绍了MySQL事务的相关概念及特性; 但在实际场景中, 可能经常会面对一些高并发应用, 此时, 简单了解事务概念和特性就不足以应对问题了; 高并发的事务问题在并发量比较大的时候, 很容易出现 多个事务同时进行 的情况。 假设有两个事务正在同时进行, 值得注意的是: 它们两者之间是互相不知道对方的存在的, 各自都对自身所处的环境过分乐观, 从而并没有对自己所操作的数据做一定的保护处理, 所以最终导致了一些问题的出现; 脏读 Mysql中, 如果 事务A 读取了另一个并行 事务B 未最终提交的写数据, 那事务A的这次读取操作就叫 脏读。 因为 ‘事务A’ 此时读取到的是 尚未被持久化 的数据 (事务B中修改的数据还不具备事务的持久性特性) 事务A此时读取的数据也叫 脏数据事务B 最终可能会因为内部其他后续操作的失败或者系统后续突然崩溃等原因, 导致事务B最终整体提交失败, 从而导致事务A读取的数据被回滚;那么最终 事务A 拿到的自然就是脏的数据了, 因为 事务A 拿到的数据已经和数据表中持久化的真实数据不一致了。 图示: 解决方案 : RC+ 在MySQL中, 事务已经用自身特性 隔离性 的 – READ COMMITED或以上隔离级别 解决了这个问题; READ COMMITED 级别保证了: 在事务中, 某条语句执行前, 已经被其他事务提交的数据, 对该语句都是可见的。 不可重复读 现在, 上面的 脏读问题 已经被解决了, 那就意味着事务中每次读取到的数据都是 持久性 的数据(被别的事务最终 提交/回滚 的落地数据)。 但脏读问题的解决, 也仅仅只能保证你在事务中每次读到的数据都是持久性的数据而已。 试想, 如果在一个事务中多次读取同一个数据, 正好在两次读取之间, 另外一个事务已经完成了对该数据的修改并提交, 那问题就来了: 两次读取的结果不一样了; 那么, 多次读取数据不一致, 有什么危害呢? 首先, 一个事务中为什么要多次读取同一数据, 什么场景下需要这么做? 1234查询余额: 100给别人汇款: 20记录日志: ....展示余额(你可以直接计算然后返回80, 但如果你是查询的话, 就应该保证你查到的是80, 而不受其他事务干扰) 解决方案 : RR+ 在MySQL中, 事务已经用自身特性 隔离性 的 – REPEATABLE READ或以上隔离级别 解决了这个问题; REPEATABLE READ 级别保证了:1.在事务中, 某条语句执行前, 已经被其他事务 提交/回滚 的落地数据, 对该语句都是可见的;2.在事务中, 多次读取同一个数据 (在两次读取操作之间, 无论数据被 提交/回滚 多少次(即无论落地过多少遍), 每次读取的结果都应该是一样的; 幻读 (区分不可重复读取)1.之前经常搞混 不可重复读 和 幻读 这两个概念（这两者确实非常相似） 但 不可重复读 主要是说多次读取同一条记录, 发现该记录中某些列值被其他事务修改过; 而 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(比如发现增加/减少了一条记录)。(可以参考 MySQL官方文档对 Phantom Rows 的介绍 ) 2.解决方案: RR + MVCC 其实对于 幻读, 在Mysql的InnoDB存储引擎中, 事务默认的 RR 级别已经通过 MVCC机制 帮我们解决了(并非完全解决), 所以该级别下, 你也模拟不出幻读的场景; 退回到 RC 隔离级别的话, 你又容易把 幻读 和 不可重复读 搞混淆, 所以这可能就是比较头痛的点吧! 另外可以参考《高性能MySQL》对 RR 隔离级别的描述 理论上, RR级别是无法解决幻读的问题, 但是由于InnoDB引擎的RR级别还使用了MVCC, 所以也就避免了幻读的出现! 3.想了解更多, 可以参考下一篇幻读的延伸 更新丢失 最后聊一下 高并发事务 的另一个问题: 丢失更新问题, 该问题和之前几个问题需要区分开, 因为解决方案不是一类! 第一类丢失更新: A事务撤销时, 把已经提交的B事务的更新数据覆盖了不过这种情况应该只在Mysql事务的最低隔离级别(READ UNCOMMITED)才会出现, RC 及 以上隔离级别 已经保证了不会出现脏读, 也就不会出现该情况! 第二类丢失更新: A事务 覆盖 B事务 已经提交的数据，造成B事务所做操作丢失 下图可以看到, 搞笑的是: 也正是因为RR隔离级别所保证的可重复读, 给此类问题的出现提供了基础条件; 这种丢失更新, 无法依靠前三种隔离级别来解决, 只能手动使用 乐观锁(在修改时, where判断数据是否为你读取时的数据; 或者提供数据版本字段来控制), 悲观锁 来解决。 注意注意: 最高隔离级别Serializable在实际应用场景中并不被采用 (并且, 它也无法解决更新丢失的问题) 因为虽然serialize将事务串行化了, 但对于事务, 只要开启事务的条件都满足, 事务A,B,C假设并行到来, 他们都会顺利依次开启事务(只不过以前是并行开启)); 但是开启之后, 虽然读写都会互相阻塞(经过测试), 但是这只是代表事务之间需要等待, 但最终都会执行, 所以还会造成超卖!; 所以还是会出现 更新丢失 的高并发事务问题: 可以参考: https://segmentfault.com/q/1010000010353164/a-1020000010353684 参考资料: 淘宝数据库内核6月报 美团技术博客 MySQL官方文档 《高性能MySQL》","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"20. MySQL事务简介","slug":"mysql/2017-09-16-mysql-20","date":"2017-09-16T03:01:07.000Z","updated":"2018-03-08T13:36:24.000Z","comments":true,"path":"2017/09/16/mysql/2017-09-16-mysql-20/","link":"","permalink":"http://blog.renyimin.com/2017/09/16/mysql/2017-09-16-mysql-20/","excerpt":"","text":"事务的概念 事务：可以理解为一个 独立的 工作单元, 在这个 独立的 工作单元中, 可以有 一组 操作; 放在这个独立工作单元中的一组操作, 要么全部执行成功, 要么全部执行失败。 仍然通过最经典的银行转账应用来解释一下: 假设有两个角色 ‘Iron Man’(余额500), ‘Wolverine’(余额15), 现在 ‘Iron Man’ 通过该银行应用给 ‘Wolverine’ 转账100元, 那么本次转账操作至少需要三个步骤 123检查`Iron Man`余额`&gt;=100`元从`Iron Man`余额中`-100`元给`Wolverine`余额`+100`元 注意: 上面的三个步操作，其实就需要打包在一个事务中, 这样就可以保证一组操作可以作为一个 独立的工作单元 来执行。并且在 独立工作单元(即事务) 中的这三个操作, 只要有任何一个操作失败, 则事务整体就应该是失败的, 那就必须回滚所有已经执行了的步骤。 假设第二步操作成功, 但是第三步操作失败, 那么整个事务就应该是失败的, 就必须将第二步的操作回滚。(其实这里也体现了事务最基本的一个特性: 保证数据的一致性) 事务的ACID特性一个运行良好的事务处理系统必须具备下面这些标准特性(高并发场景离不开事务的这几个标准特性) Atomicity 原子性一个事务必须被视为一个不可分割的最小工作单元, 整个事务中的所有操作要么全部提交成功, 要么全部失败回滚。对于一个事务来说, 不能只成功执行其中的一部分操作, 这就是事务的原子性。 Consistency 一致性你大概可以这样来理解: 虽然数据表中的数据可能一直在变化, 但是事务的一致性特性总是能够保证 数据库总是从一个数据一致性的状态 转换到 另一个数据一致性的状态; 比如之前转账的例子:转账前的数据一致性状态是: ‘Iron Man’(余额500), ‘Wolverine’(余额15)转账成功后的数据一致性状态是: ‘Iron Man’(余额400), ‘Wolverine’(余额115)转账如果失败的话, 数据的一致性的状态应该回滚到转账前的状态: ‘Iron Man’(余额500), ‘Wolverine’(余额15) Isolation 隔离性 通常来说, 一个事务所做的修改在最终提交以前, 对其他事务是不可见的;比如在之前的转账例子中, 在执行完成最后一步(第三步), 事务还没来得及最终提交之前, 此时有另一个程序去读取A账户的余额, 那么这个程序读到的应该是没有被减100的余额才对 上面为什么说 通常来说, 难道还有其他情况 ?后面会详细讨论事务隔离性 的四个 隔离级别, 到时候就知道这里为什么说通常来说对其他事务是不可见的; (但确实也还有特例, 比如最低隔离级别 READ UNCOMMITTED, 对其他事务的可见就造成了 脏读问题 的出现) 事务有四种隔离级别(从低到高) READ UNCOMMITTED (未提交读) READ COMMITTED (提交读) REPEATABLE READ (可重复读) SERIALIZABLE (可串行化) 注意: 它仍然无法解决更新丢失的问题(可以参考) Durability 持久性一旦事务被最终提交后, 在这个独立单元中的所有操作所做的修改将会 永久 保存到数据库中; (所谓永久可以理解为被事务修改的数据是真正存放到了表中, 而不是存放在了诸如临时表之类的地方);","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"http://blog.renyimin.com/tags/事务/"}]},{"title":"Git","slug":"2017-08-21-Git","date":"2017-08-21T02:07:12.000Z","updated":"2018-02-04T04:15:14.000Z","comments":true,"path":"2017/08/21/2017-08-21-Git/","link":"","permalink":"http://blog.renyimin.com/2017/08/21/2017-08-21-Git/","excerpt":"","text":"暂时没迁移云笔记, 可以点击此处进行阅读, 经常会更新重建笔记","categories":[{"name":"Git","slug":"Git","permalink":"http://blog.renyimin.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://blog.renyimin.com/tags/Git/"}]},{"title":"23. redis 主从, 哨兵","slug":"Redis/2017-06-24-redis-23","date":"2017-06-24T12:50:07.000Z","updated":"2018-03-09T07:50:19.000Z","comments":true,"path":"2017/06/24/Redis/2017-06-24-redis-23/","link":"","permalink":"http://blog.renyimin.com/2017/06/24/Redis/2017-06-24-redis-23/","excerpt":"","text":"前言 和MySQL主从复制的原因一样, Redis虽然读取写入的速度都特别快, 但是也会产生读压力特别大的情况, 为了分担读压力, redis支持主从复制; Redis的主从结构可以采用 一主多从 或者 级联结构 , 下图为级联结构: 主从复制(Replication)实现读写分离, 提高服务器负载能力 Redis的主从复制架构中, 一类是主数据库(master), 一类是从数据库(slave)master可以进行读写操作, 当发生写操作的时候自动将数据同步到从数据库;而从数据库一般是只读的, 并接收主数据库同步过来的数据; 一个主数据库可以有多个从数据库, 而一个从数据库只能有一个主数据库; Redis主从复制可以根据是否全量分为 全量同步 和 增量同步 Redis全量复制 一般发生在Slave初始化阶段, 这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器, 并发送SYNC命令 ; 主服务器接收到SYNC命令后, 开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令; 主服务器BGSAVE执行完后, 向所有从服务器发送快照文件, 并在发送期间继续记录被执行的写命令; 从服务器收到快照文件后丢弃所有旧数据,载入收到的快照; 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令; 从服务器完成对快照的载入, 开始接收命令请求, 并执行来自主服务器缓冲区的写命令 ; 注意：redis2.8之前的版本, 当主从数据库同步的时候从数据库因为网络原因断开重连后会重新执行上述操作, 不支持断点续传, redis2.8之后支持断点续传。 Redis增量复制 Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令, 从服务器接收并执行收到的写命令。 小结 主从刚刚连接的时候, 进行全量同步; 全同步结束后, 进行增量同步; 当然, 如果有需要, slave 在任何时候都可以发起全量同步; redis 策略是, 无论如何, 首先会尝试进行增量同步, 如不成功, 则要求从机进行全量同步; Redis主从配置方法动态配置 在启动新的redis实例的时候, 设置这个新的实例为某个redis主服务器的Slave: redis-server --port 6379 --slaveof 127.0.0.1 6379 尝试向从服务器发送写命令, 发现不会成功; 在master上添加一个键值对, 发现从服务器上立马就会同步到这条数据; 动态配置对一个已经启动的redis服务发送命令: slaveof ip port, 则会将当前服务器状态从Master修改为别的服务器的Slave 配置文件中配置为每个准备启动的redis从服务, 创建各自的配置文件, 然后在启动redis新实例的时候, 为新实例指定各自的配置文件即可 比如一个6380的从实例, 其配置文件为: redis-server /usr/local/redis/etc/redis-6380.conf 配置文件内容如下: 12slaveof 127.0.0.1 6379port 6380 上面三种方式都可以很轻松地配出主从效果来! sentinel哨兵引出sentinel 之前的主从配置可以为我们解决一个redis实例读压力大的问题, 可以在一台服务器上进行主从多实例的配置, 也可以在不同机器之间进行主从配置; 如果slave服务器挂掉, 只是读性能会下降, 现在的问题是, 如果一旦主redis服务器(也就是master实例)挂了, 你目前貌似只能手动去把这台master实例启动起来; 如果确实这台机器/实例就是启动不起来的话, 那你可能就需要手动去将当前的某一个slave实例切换为master:slaveof no one //先把这个slave实例变成一个master实例slaveof ip port //在之前的从实例中执行, 将他们的master重新切换到这个新的master上 这种手动操作明显不可能被接收, 我们需要当master主挂了之后, 自动有一个slave能够勇于承担地顶上来(因为slave相对于之前的master除了分担读压力外, 还是之前master的备份), 这就引出了redis的sentinel哨兵; 简介 sentinel哨兵 是redis官方提供的高可用解决方案, 可以用它来管理多个redis服务的实例; 之前在编译安装好redis之后, 就可以在 /usr/local/redis/src/ 目录下看到 redis-sentinel 等很多命令; sentinel的监控: 它会不断地检查master和slaves是否正常; 一个sentinel可以监控任意多个master及其下的slaves; 当然, sentinel也可能挂掉, 也有单点问题 (还好Sentinel是一个分布式系统, 可以在一个架构中运行多个Sentinel进程, 他们可以组成一个sentinel网络, 之间是可以互相通信的, 可以通过”投票”来决定master是否挂了) 当一个sentinel认为被监控的服务已经下线时, 它会向网络中的其他sentinel进行确认, 判断该服务器是否真的下线; 如果下线的是一个主服务器, 那么sentinel将会对下线的主服务器进行 自动故障转移通过将下线主服务器的某个从服务器提升为新的主服务器;并将下线主服务器下的从服务器重新指向新的主服务器;来让系统从新回到正常; 之前的master下线后, 如果重新上线了, sentinel会让它作为一个salve, 去新的master中同步数据 实战 启动sentinel: 将 /usr/local/src/ 目录下的redis-sentinel程序文件复制到 /usr/local/redis/bin 目录下; 启动一个运行在sentinel模式下的redis服务实例(两种方式): redis-sentinel 或者 redis-server /usr/local/redis/sentinel.conf --sentinel sentinel配置 Sentinel之间的自动发现机制虽然sentinel集群中各个sentinel都互相连接彼此来检查对方的可用性以及互相发送消息, 但其实你是不用在任何一个sentinel中配置任何其它的sentinel的节点的, 因为sentinel利用了master的发布/订阅机制去自动发现其它也监控了同一master的sentinel节点。 同样，你也不需要在sentinel中配置某个master的所有slave的地址，sentinel会通过询问master来得到这些slave的地址的。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"21. 了解redis持久化","slug":"Redis/2017-06-24-redis-21","date":"2017-06-24T05:40:54.000Z","updated":"2018-03-09T07:49:23.000Z","comments":true,"path":"2017/06/24/Redis/2017-06-24-redis-21/","link":"","permalink":"http://blog.renyimin.com/2017/06/24/Redis/2017-06-24-redis-21/","excerpt":"","text":"Redis的两种持久化模式介绍 RDB(Redis DB)简介 RDB持久化功能可以将redis服务器包含的所有数据库数据以二进制文件(.rdb文件)的形式保存到硬盘; RDB 默认是开启的, 关闭方式有: 注释掉配置文件中save策略行; 在所有save策略下添加save &quot;&quot;; 创建RDB文件, 常见的三种方式 给redis服务器发送SAVE命令; 给redis服务器发送BGSAVE命令; 在redis配置文件中配置rdb持久化的save策略, 如果配置选项被满足, 则服务器会自动执行BGSAVE; 这三种创建RDB的方式, 前两种需要手动去执行;而第三种是服务器自动执行的; rdb文件每次都是覆盖创建, 所以为了安全起见, 需要不时地去备份生成的rdb文件; 三种方式的区别 通过使用客户端向redis服务器发送SAVE命令, 可以命令服务器去创建一个新的RDB文件; 注意: 在redis服务器执行SAVE命令的过程中(也即创建RDB文件的过程中), redis服务器将被阻塞, 服务器端无法再去处理客户端发送的命令请求, 只有在SAVE命令执行完毕之后(也即RDB文件创建完毕之后), 服务器才会重新开始处理客户端发送的命令请求; 注意: 如果rdb文件已经存在, 服务器会自动使用新的rdb文件去代替旧的rdb文件;所以: 之前的rdb文件你可以用定时脚本定时地拷走, 可以防止外一执行了flush db并且服务器进行了save, 此时旧的rdb会被新的rdb文件覆盖掉, 那就完蛋了!!! 通过使用客户端向redis服务器发送BGSAVE命令,也可以命令服务器去创建一个新的RDB文件; 注意: BGSAVE命令不会造成服务器阻塞, 也就是说redis服务器在执行BGSAVE命令的过程中, redis服务器仍然可以正常的处理其他客户端发送的命令请求;BGSAVE命令不会造成服务器阻塞的原因在于: 12当redis服务器接收到BGSAVE命令后,它不会亲自去创建rdb持久化文件,而是通过fork一个子进程,然后由子进程去负责这个rdb文件的生成, 自己则继续处理客户端的命令请求;当子进程创建好rdb文件并退出后, 它会向父进程(也就是负责处理命令请求的redis服务器)发送一个信号,告知redis服务器rdb文件已经创建完毕; 注意: 由于创建子进程会消耗额外的内存, 所以save创建RDB文件的速度其实会比bgsave快, 因为它可以集中资源来创建rdb文件; save 和 bgsave 没有孰好孰坏, 你要考虑哪个更适合你如果你的数据库正在上线当中, 自然使用 bgsave (让服务器以非阻塞方式进行最好);相反,如果你需要在凌晨3点钟维护你的redis, 比如维护需要停机一小时, 这时系统被阻塞了也是没关系的, 这时候你使用save命令就会好一点; 自动创建rdb文件 通过在redis配置文件中配置redis服务器创建rdb文件的条件, 就可以让服务器在客户端操作满足save策略所指定的条件时, 自动去创建rdb文件; 比如, redis默认情况下就是开启rdb持久化的, 并且制定了默认的save条件:123456save 900 1 #900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化） save 300 10 #300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化） save 60 10000 #60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）这些默认的save选项的意思是:只要三个条件中的任意一个条件被满足时,服务器就会自动执行 BGSAVE 命令来创建新的rdb文件; 每次创建完rdb文件之后,服务器为实现自动持久化会将&apos;为实现持久化而设置的时间计数器和次数计数器清零并重新开始计数&apos;, 所以多个保存条件的效果是不会叠加的; RDB持久化问题 redis关机持久化: 当你正常关闭redis的时候, redis服务器不会参考配置中的save策略; 而是会直接先调用save命令, 将redis所有数据持久化到磁盘之后才会真正进行退出; redis - crash不持久化问题: 而当redis服务器意外断电宕机的时候, 你会发现从上一次快照之后的数据将全部丢失; rdb持久化的优点 重建快: 在redis里, 默认使用RDB模式, 因为RDB模式重建数据库比较快, 这里的重建数据库是指将数据从硬盘移到内存,并建立起数据库的过程; 因为对于RDB模式来说, 重建就是把 dump.rdb 文件加载到内存, 并解压字符串,就建立起了数据库; 而对于AOF模式来说, 则是在启动Redis服务器的时候, 运行appendonly.aof日志文件, 在内存中重新建立数据库; 所以从这里的描述就可以看出,AOF的重建过程是要比RDB慢的; 使用RDB模式的话， 系统会将内存中数据库的快照每隔一段时间间隔更新到硬盘中(dump.rdb 文件里), 这个更新的频率是可以指定的。 在redis.conf中有三个配置用来指定内存数据更新到硬盘的频率： 1234//格式是：save &lt;seconds&gt; &lt;changes&gt;save 900 1 //如果仅有1-9次更改操作，那么要900s才写入硬盘一次save 300 10 //如果仅有10-9999次更改操作，那么要300s才写入硬盘一次save 60 10000 //如果超过10000次更改操作，那么60s才会写入硬盘一次 900s(也就是15分钟), 300s(就是5分钟): 这个时间挺长的, 这正是RDB模式的缺点所在如果服务器宕机的话, 可能会造成最后几分钟, 保存在内存中还来不及刷入硬盘的数据丢失, 如果数据很重要那就惨了; 但是如果数据不是那么重要, 丢失几分钟数据也没什么关系, 那么RDB模式是最好的选择。 rdb持久化的缺点 由于创建RDB文件需要将服务器所有的数据库的数据都保存起来, 这是一个非常消耗资源和时间的操作, 所以服务器需要隔一段事件再来创建一个新的rdb文件, 也就是说rdb文件的创建操作不能执行的过于频繁, 否则将会严重影响服务器的性能; 由于只能隔一段时间再去创建一下rdb文件, 所以如果在间隔的这段时间中服务器宕机, 那这段间隔中的数据就丢失了, 比如redis服务在创建rdb文件时, 如果使用了默认的save选项, 可能就会丢失60秒的数据, 这就有点严重了!!! 所以为了解决这个问题, rdb可以结合aof来一起进行持久化 aof就解决了服务器不能频繁执行rdb持久化的问题! AOF (Append Only File)简介 为了解决rdb持久化在服务器意外宕机的情况下造成的数据严重丢失的问题, redis还提供了另外一种持久化方案, 那就是 aof持久化! AOF模式是将操作日志记在appendonly.aof文件里, 每次启动服务器就会运行appendonly.aof里的命令重新建立数据库; 因为要重新运行命令, 所以appendonly.aof是比较慢的, 默认AOF模式也是关闭的; 你可以在redis配置文件(redis.conf)中, 配置 appendonly yes 打开AOF模式; aof需要注意的问题 在aof持久化的模式下, 虽然redis服务器在执行修改数据的命令后, 会把执行的命令写入到aof文件中, 但这并不意味着aof文件持久化不会丢失任何数据; 在常见的操作系统中, 执行系统调用write函数, 将一些内容写到某个文件中时, 为了提高效率, 系统通常不会直接将内容写入到磁盘里面, 而是先将内容放入一个内存缓冲区(buffer)里面, 等到缓冲区被填满, 或者用户执行fsync调用和fdatasync调用时, 系统才会将存储在缓冲区里面的内容真正写入到硬盘里; 所以对AOF持久化来说, 当一条命令真正的被写入到硬盘里面时, 这条命令才不会因停机而意外丢失; 因此,aof持久化在遭遇停机时丢失命令的数量, 取决于命令被写入到硬盘的时间;越早将命令写入到硬盘,发生意外停机时丢失的数据就越少;而越迟将命令写入硬盘,发生意外停机时丢失的数据就越多; 所以aof持久化模式还有三种执行策略供你选择; AOF模式三种追加策略这三种追加策略主要就是用来指定什么时机可以将操作日志真正追加到appendonly.aof文件里; always : 服务器每写入一个命令, 就调用一次fdatasync, 将缓冲区里面的命令写入到磁盘文件中, 在这种模式下, 服务器即使遭遇意外停机, 也不会丢失任何自己已经成功执行的命令数据; 比较安全, 但比较慢; (类似mysql了) everysec: 服务器每一秒重新调用一次fdatasync, 将缓冲区里面的操作日志写入到磁盘文件中, 这是系统默认的方式, 是一种权衡折衷; 通常这种方式会比较好, 在这种模式下, 服务器即使遭遇意外停机时, 最多只丢失一秒钟的内执行的命令; no: 服务器不主动调用fdatasync, 由操作系统去决定什么时候将缓冲区里面的命令写入到硬盘里面; 在这种模式下,服务器遭遇意外停机时, 丢失的命令数量是不确定的; 可以在redis.conf中配置这三种策略 可以看到默认使用的是everysec这种折中策略;123# appendfsync alwaysappendfsync everysec# appendfsync no AOF文件中的冗余命令(aof文件重写) 随着服务器的不断运行,为了记录数据库发生的变化, 服务器会将越来越多的命令写入到aof文件中, 使得aof文件的体积不断增大; 为了让aof文件的大小控制在合理的范围, 避免它疯狂增长, redis提供了AOF重写功能, 通过这个功能, 服务器可以产生一个新的aof文件: 新的aof文件记录的内容和原有的aof文件记录的内容在redis服务器重建数据后, 数据完全一样; 新的aof文件会使用尽可能少的命令来记录数据库数据, 因此新的aof文件的体积通常会比原有aof文件的体积要小得多; aof重写期间, 服务器不会被阻塞, 可以正常处理客户端发送的命令请求; 有两种方法可以触发aof文件重写 客户端向服务器发送 BGREWRITEAOF 命令; 通过设置配置文件选项来让服务器自动执行BGWRITEAOF命令; 1234567auto-aof-rewrite-min-size &lt;size&gt;触发aof重写所需的最小体积:只要aof文件的体积大于等于size时,服务器才会考虑是否需要进行aof重写,这个选项用于避免对体积过小的aof文件进行重写auto-aof-rewrite-percentage 100指定触发重写所需的aof文件体积百分比, 当aof文件的体积大于auto-aof-rewrite-min-size指定的体积,并且超过上一次重写之后的aof文件体积的percent%时, 就会触发aof重写,(如果服务器启动刚刚不就,还没有进行过aof重写,那么使用服务器启动时载入的aof文件体积来作为基准值)将这个值设置为0表示关闭自动aof重写; 例子 1234//只有当aof文件的增量大于100%的时候才进行重写auto-aof-rewrite-percentage 100//当aof文件大于64mb之后才考虑进行aof重写,还需要看上一条的百分比增量够不够auto-aof-rewrite-min-size 64mb 如果AOF文件出错了 服务器可能在程序正在对AOF文件进行写入时停机, 如果停机造成了AOF文件出错(corrupt), 那么 Redis 在重启时会拒绝载入这个 AOF 文件, 从而确保数据的一致性不会被破坏; 当发生这种情况时,可以用以下方法来修复出错的 AOF 文件： 首先先为现有的 AOF 文件创建一个备份; 然后使用 Redis 附带的 redis-check-aof 程序, 对原来的 AOF 文件进行修复: $ redis-check-aof --fix (可选)使用 diff -u 对比修复后的 AOF 文件和原始 AOF文件的备份, 查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"11. Redis -- task queue","slug":"Redis/2017-06-17-redis-11","date":"2017-06-17T03:30:03.000Z","updated":"2018-03-09T09:33:06.000Z","comments":true,"path":"2017/06/17/Redis/2017-06-17-redis-11/","link":"","permalink":"http://blog.renyimin.com/2017/06/17/Redis/2017-06-17-redis-11/","excerpt":"","text":"前言现在有很多专门的队列软件(如ActiveMQ, RabbitMQ等)，在缺少专门的任务队列可用的情况下, 也可以使用Redis的队列机制; FIFO 先进先出队列 Redis的列表结构, 允许通过 RPUSH和LPUSH以及RPOP和LPOP, 从列表的两端推入和弹出元素。 假设一个操作中, 有向用户发送电子邮件这一功能, 由于这一功能可能会有非常高的延迟,甚至可能会出现发送失败, 所以这里就不能采用平时常见的代码流方式来执行这个邮件发送操作; 为此, 可以使用 任务队列 来记录 “邮件的收信人,邮件内容,发送邮件的原因”, 以 先到先服务 的方式发送邮件(此处采用的是 从右推入队列, 从左弹出元素) 队列: redis服务中的列表类型就充当了队列服务 消费者: 阻塞等待 (在Laravel将其做成命令进行启动, 该消费者就会阻塞等待) 12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email&apos;], 0); return $res;&#125; 生产者: 直接访问如下代码推送多条消息 1234567891011121314151617181920public function rPush()&#123; $data1 = json_encode([ &apos;user_id&apos; =&gt; 11, &apos;content&apos; =&gt; &apos;最近推出新款汽车--宝马, 售价:$12W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data2 = json_encode([ &apos;user_id&apos; =&gt; 12, &apos;content&apos; =&gt; &apos;最近推出新款汽车--奔驰, 售价:$15.8W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); $data3 = json_encode([ &apos;user_id&apos; =&gt; 13, &apos;content&apos; =&gt; &apos;最近推出新款汽车--大众, 售价:$20W&apos;, &apos;time&apos; =&gt; time(), ], JSON_UNESCAPED_UNICODE); //一旦运行就会阻塞起来 Redis::rpush(&apos;queue:email&apos;, [$data1, $data2, $data3]);&#125; 最终消费者正常拿到消息, 并且顺序也是正确的; 多种任务的队列 一般情况下, 我们为每种任务单独使用一个队列的; 但如果有一个队列处理多种任务的场景, 实现起来也很方便; 只用在消息中指明消息所需要调用的回调函数即可: priority 优先级队列优先级队列其实在redis中可以依靠 BRPOP和BLPOP的特性; 当 BLPOP 被调用时, 如果给定key(队列)列表中, 至少有一个非空列表, 那么弹出遇到的第一个非空列表的头元素, 并弹出元素所属的列表名字一起, 组成结果返回给调用者; 也就是说, BLPOP 给定的队列列表中, 靠前的就是优先级高的;假设你有 ‘重置密码的邮件’, ‘提醒邮件’, ‘发广告的邮件’, 三种队列, 如果你期望他们按照优先级依次排列, 那么只用设置为:12345public function redisListBrpop()&#123; $res = Redis::blpop([&apos;queue:email:resetpwd&apos;, &apos;queue:email:warning&apos;, &apos;queue:email:advertisement&apos;], 0); return $res;&#125; 想优先推送的, 你就放入第一个队列’queue:email:resetpwd’中(BRPOP)也类似 延迟队列未完待续~~","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"08. redis锁 -- 不太安全?","slug":"Redis/2017-06-12-redis-08","date":"2017-06-12T08:35:08.000Z","updated":"2018-03-09T07:48:02.000Z","comments":true,"path":"2017/06/12/Redis/2017-06-12-redis-08/","link":"","permalink":"http://blog.renyimin.com/2017/06/12/Redis/2017-06-12-redis-08/","excerpt":"","text":"前言一般来说, 在对数据进行”加锁”时, 程序首先需要获取锁来得到对数据进行排他性访问的能力, 然后才能对数据执行一系列操作, 最后还要将锁释放给其他程序;之前已经了解过, Redis使用WATCH命令来代替对数据进行加锁, 因为WATCH只会在数据被其他客户端抢先修改了的情况下通知执行了这个命令的客户端, 所以这个命令被称为乐观锁; SETEX 实现锁介绍为了对数据进行排他性访问, 程序首先要做的就是获取锁; 而 Redis 的 SETEX 命令天生就适合用来实现锁的获取功能; 这个命令只会在键不存在的情况下为键设置值, 而其他进程一旦发现键存在, 那就只能等待之前锁的释放; 准备环境 数据表准备 123456789DROP TABLE IF EXISTS `goods`;CREATE TABLE `goods` ( `id` int(10) NOT NULL AUTO_INCREMENT, `goods_name` varchar(100) NOT NULL, `num` int(100) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `goods` VALUES (1, &apos;iphone 6 plus&apos;, 10); 超卖代码: 12345678public function mysqlOverSell()&#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(500000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125;&#125; Jmeter压测配置: 结果发现超卖: (后来设置压测为每秒3个线程, 也超卖了1件) 使用redis的setex加锁 redis 123127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; 代码 123456789101112131415public function setnx()&#123; $lock = Redis::setnx(&apos;tag&apos;, 1); // 如果加锁成功, 则可以进行如下操作(其他客户端只能等待锁释放) if ($lock) &#123; $good = Goods::select(&apos;num&apos;)-&gt;find(1); if ($good[&apos;num&apos;] &gt; 0) &#123; usleep(200000); Goods::where([&apos;id&apos; =&gt; 1])-&gt;decrement(&apos;num&apos;, 1); &#125; // 注意: 执行完成之后, 必须释放锁 Redis::del(&apos;tag&apos;); &#125;&#125; 修改jmeter访问路由, 重新测试, 发现确实不会出现超卖现象了 SETEX锁 - 问题死锁(客户端的突然下线导致锁不被释放)因为客户端即使在使用锁的过程中也可能会因为这样或那样的原因而下线, 所以为了防止客户端在取得锁之后崩溃, 并导致锁一直处于 已获取 状态;所以还应该为锁的实现加上 超时限制 的特性: 如果获得锁的进程未能在指定时限内完成操作, 那么可能会认为客户端已经crash掉线, 所以锁将需要被自动释放; 为锁加超时限制改进(setnx–&gt;set) 为锁加超时限制的普通方法如下 1234567$lock = Redis::setnx(&quot;tag&quot;, 1)if ($lock) &#123; Redis::expire(&quot;my:lock&quot;, 10); // ... do something Redis::del(&quot;my:lock&quot;)&#125; 如果客户端是在 Redis::expire(&quot;my:lock&quot;, 10); 之前就崩溃, 锁不被释放的问题还是存在; 所以, 从redis2.6.12开始(set新增了可选选项), 官方建议使用set命令替代setnx来实现锁, 如下 123456789101112131415161718 if (Redis::set(&quot;tag&quot;, 1, &quot;nx&quot;, &quot;ex&quot;, 10)) &#123; ... do something Redis::del(&quot;tag&quot;) &#125; ``` ### **释放了其他进程的锁**持有锁的进程如果因为操作时间过长而导致锁被自动释放, 但进程本身不知道这一点, 并且其他进程可能已经获取了锁;这样, 进程在稍后做释放锁操作时, 如果只是简单的 `Redis::del(&quot;tag&quot;&quot;)`, 这可能会释放掉其他进程持有的锁; 因此, 在获取锁的时候, 需要设置一个`token`, 放入自己的锁中, 在释放锁的时候, 用来保证释放的是自己的锁; **锁释放注意**1. 如果客户端A是因为执行超时, 而导致锁被自动释放, 那么当客户端A最后在释放锁时, 可能此时客户端B已经加上了自己的锁, 所以在锁释放时需要做两个操作- 检查`token`是否一致- 释放锁2. 注意: 在上面两步之间, 也可能发生超时而自动释放锁, 导致锁token被改动, 所以`释放锁`这一步应该放在事务中, 并提前用watch监控代表锁的那个key, 伪代码如下: Redis::watch(‘tag’); if (Redis::get(‘tag’) == $token) { Redis::multi(); Redis::del(&apos;tag&apos;); Redis::exec(); } ``` 多个客户端同时获取锁 假设客户端A超时后, 锁被自动释放, 此时客户端B拿到了锁, 如果客户端B也因为超时导致锁被释放(此时客户端A还没执行完, B也没执行完), 那么客户端C也能拿到锁; 这样就同时存在3个客户端拿到了锁 这个问题貌似就是一直没解决的问题!!参考: 基于Redis的分布式锁到底安全吗(上)~~未完待续","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"07. redis事务与WATCH乐观锁","slug":"Redis/2017-06-11-redis-07","date":"2017-06-11T13:14:46.000Z","updated":"2018-03-08T14:37:55.000Z","comments":true,"path":"2017/06/11/Redis/2017-06-11-redis-07/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/Redis/2017-06-11-redis-07/","excerpt":"","text":"Redis事务与单线程 因为Redis是单线程的, 所以即使多个客户端同时来对同一数据发来很多命令, 也会被串行挨个执行; 和mysql的srialize隔离级别一样, 即使是串行执行命令, redis也逃不过高并发事务时的 更新丢失 问题; mysql是使用乐观锁, 悲观锁来解决的 参考MySQL高并发事务问题 及 解决方案 而redis的事务也是通过与WATCH(乐观锁)的结合才得以解决这个问题 Redis事务与WATCH 演示Redis事务在客户端高并发时出现的 丢失更新 问题 redis 为了解决高并发事务时这种 丢失更新 的问题, 提供了 WATCH WATCH介绍 redis 并没有实现典型的加锁功能(比如MySQL中, 在访问以写入为目的数据时 SELECT FOR UPDATE), 因为加这种悲观锁可能会造成长时间的等待; 所以redis为了尽可能地减少客户端的等待时间, 采用了WATCH监控机制, 如果某个客户端A在事务开始之前 WATCH 了一个key, 那么其实就相当于对该key加了乐观锁 事务中正常执行你要执行的操作 (乐观地认为不会有其他客户端抢在你前面去改动那个key) 直到当客户端A真正exec的时候, 才会验证客户端A之前WATCH(监控)的key是否被变动过, 如果变动过, 则客户端A可以进行重试; 测试:","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"06. redis 事务","slug":"Redis/2017-06-11-redis-06","date":"2017-06-11T11:10:03.000Z","updated":"2018-03-08T14:25:35.000Z","comments":true,"path":"2017/06/11/Redis/2017-06-11-redis-06/","link":"","permalink":"http://blog.renyimin.com/2017/06/11/Redis/2017-06-11-redis-06/","excerpt":"","text":"Redis事务介绍 redis事务是使用队列以先进先出(FIFO)的方法保存命令, 较先入队的命令会被放到数组的前面, 而较后入队的命令则会被放到数组的后面; redis事务从开始到结束通常会通过三个阶段: 事务开始 命令入队 事务执行 Redis事务通常会使用 MULTI, EXEC, WATCH等命令来完成; redis事务的ACID特性在redis中, 事务具有 原子性(Atomicity) , 一致性(Consistency) 和 隔离性(Isolation);并且当redis运行在某种特定的持久化模式下,事务也具有耐久性(Durability); 原子性 对于redis的事务来说, 事务队列中的命令也是要么就全部执行, 要么就一个都不执行, 因此redis的事务是具有原子性的; 不过需要注意的是: redis事务的原子性有两种情况需要区分 一种是 语法错误导致redis事务执行出错, 比如, 你事务中的某条命令语法错误, 那么你在exec的时候, 所有的命令都不会执行: 1234567891011127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age(error) ERR wrong number of arguments for &apos;set&apos; command127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; 另一种是 无语法错误,可以运行成功, 但是运行完之后会返回运行错误, 这种情况下, 错误之前的命令不会回滚; 123456789101112131415161718192021222324//比如命令或命令的参数格式错误,那么事务就会出现有可能部分命令成功,而部分命令失败的情况127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set name renyiminQUEUED127.0.0.1:6379&gt; set age 10QUEUED//这里就会出现运行错误127.0.0.1:6379&gt; incr nameQUEUED127.0.0.1:6379&gt; set addr yunchengQUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) (error) ERR value is not an integer or out of range4) OK127.0.0.1:6379&gt; keys *1) &quot;addr&quot;2) &quot;age&quot;3) &quot;name&quot;127.0.0.1:6379&gt; 需要说明的是: 上述的第二种情况, 从表面上看, 不符合”原子性”, 因为你所执行的命令中有一条出错后, redis并没有进行回滚; 其实是由于你的此种错误命令不属于语法错误, 对redis来说, 不会导致执行出错,所以你的这条错误命令是可以执行成功的, 不过成功后会返回错误提示, 所以redis并不会进行回滚; redis的作者在事务相关的文档中解释说:1234不支持事务回滚是因为这种复杂的功能和redis追求的简单高效的设计主旨不符合,并且他认为, redis事务的执行时错误通常都是编程错误造成的,这种错误通常只会出现在开发环境中, 而很少会在实际的生产环境中出现,所以他认为没有必要为redis开发事务回滚功能。 一致性redis通过谨慎的错误检测和简单的设计来保证事务一致性。 隔离性 事务的隔离性指的是, 即使数据库中有多个事务并发在执行, 各个事务之间也不会互相影响, 并且在并发状态下执行的事务和串行执行的事务产生的结果完全相同; 因为redis使用单线程的方式来执行事务(以及事务队列中的命令), 并且服务器保证, 在执行事务期间不会对事务进行中断, 因此, redis的事务总是以串行的方式运行的, 并且事务也总是具有隔离性的; Redis为单进程单线程模式, 采用队列模式将并发访问变为串行访问(Redis本身没有锁的概念, Redis对于多个客户端连接并不存在竞争) 持久性事务的耐久性指的是,当一个事务执行完毕时, 执行这个事务所得的结果已经被保持到永久存储介质里面; 因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能, 所以redis事务的耐久性由redis使用的持久化模式(rdb/aof)来决定: 关于持久化, 可以参考博文:了解redis持久化","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.renyimin.com/tags/Redis/"}]},{"title":"05. 主从复制","slug":"mysql/2017-02-18-mysql-05","date":"2017-02-18T10:51:28.000Z","updated":"2018-03-08T06:25:24.000Z","comments":true,"path":"2017/02/18/mysql/2017-02-18-mysql-05/","link":"","permalink":"http://blog.renyimin.com/2017/02/18/mysql/2017-02-18-mysql-05/","excerpt":"","text":"主从复制原理 master服务器上进行sql写操作的时候, 是会引起磁盘变化的; 所以slave服务器要想和master上的数据保持一致, 可以有两种办法: slave按照master服务器上每次的sql写语句来执行一遍; slave按照master服务器磁盘上的变化来做一次变化 ; 主服务器master上的写操作都会被记录到binlog二进制日志中, 从服务器slave去读主服务器的binlog二进制日志, 形成自己的relay中继日志, 然后执行一遍 ; 所以主从配置需要做到 主服务器要配置binlog二进制 从服务器要配置relaylog(中继日志) master要授予slave账号: 从服务器如何有权读取主服务器的 binlog (binlog非常敏感, 不可能让谁去随便读) 从服务器用账号连接master 从服务器一声令下开启同步功能 start slave 注意: 一般会在集群中的每个sql服务器中加一个server-id来做唯一标识 ; 配置启动主从 主服务器配置 1234#主从复制配置server-id=4 #服务器起一个唯一的id作为标识log-bin=mysql-bin #声明二进制日志文件名binlog-format= #二进制日志格式 mixed,row,statement 主服务器的 binlog二进制日志 有三种记录方式 mixed, row, statement ; statement: 二进制记录执行语句, 如 update….. row: 记录的是磁盘的变化 如何选择 binlog二进制日志 记录方式? update salary=salary+100; // 语句短, 但影响上万行, 磁盘变化大, 宜用statement update age=age+1 where id=3; // 语句长而磁盘变化小, 宜用row 你要是拿不准用哪个? 那就设置为mixed, 由系统根据语句来决定; 从服务器配置 首先从服务器肯定要开启relaylog日志功能 ; 从服务器一般也会开启binlog, 一方面为了备份, 一方面可能还有别的服务器作为这台从服务器的slave ; 主从之间建立关系 : 主服务器上建立一个用户: grant replication client,replication slave on *.* to repl@&#39;192.168.56.%&#39; identified by &#39;repl&#39; 告诉从服务器要连接哪个主服务器: 在从服务器上进入mysql执行如下语句 1234567reset slave #可以把之前的从服务器同步机制重置一下change master to master_host=&apos;192.168.56.4&apos;,master_user=&apos;repl&apos;,master_password=&apos;repl&apos;,master_log_file=&apos;mysql-bin.000001&apos;,#当前主服务器产生的binlog走到哪儿了(需要在主服务器上`show maste status`来查看file名和position指针位置)master_log_pos=349 然后查看从服务器的slave状态 show slave status 发现已经连上主服务器了 从服务器中启动slave: start slave 注意: 可以一主多从, 但一个从有多个主就会傻逼了 ; 此时我们做的只是mysql主从复制, 但不是读写分离, 距离读写分离还差一小步; 因为读写分离还需要对sql语句进行判断(可以在php层面判断sql语句进行路由, 决定哪种sql去哪个服务器) 可以参考本人有道笔记上的记录","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://blog.renyimin.com/tags/MySQL/"}]},{"title":"05.WebSocket - 实战","slug":"WebSocket/2017-10-29-websocket-05","date":"2016-10-19T14:09:10.000Z","updated":"2018-03-06T10:02:13.000Z","comments":true,"path":"2016/10/19/WebSocket/2017-10-29-websocket-05/","link":"","permalink":"http://blog.renyimin.com/2016/10/19/WebSocket/2017-10-29-websocket-05/","excerpt":"","text":"ThinkPHP + WorkerMan + WebSocket 实现节本聊天室功能:","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"04.WebSocket - 客户端API","slug":"WebSocket/2017-10-28-websocket-04","date":"2016-10-19T12:50:52.000Z","updated":"2018-03-05T12:38:15.000Z","comments":true,"path":"2016/10/19/WebSocket/2017-10-28-websocket-04/","link":"","permalink":"http://blog.renyimin.com/2016/10/19/WebSocket/2017-10-28-websocket-04/","excerpt":"","text":"WebSocket客户端API如下WebSocket 构造函数 WebSocket对象作为一个构造函数, 用于新建 WebSocket 实例, 执行 var ws = new WebSocket(&#39;ws://localhost:8080&#39;); 之后, 客户端就会与服务器进行连接。 ws.readyState: readyState属性返回实例对象的当前状态,共有四种: CONNECTING: 值为0，表示正在连接。 OPEN: 值为1，表示连接成功，可以通信了。 CLOSING: 值为2，表示连接正在关闭。 CLOSED: 值为3，表示连接已经关闭，或者打开连接失败。 下面是一个示例:1234567891011121314151617switch (ws.readyState) &#123; case WebSocket.CONNECTING: // do something break; case WebSocket.OPEN: // do something break; case WebSocket.CLOSING: // do something break; case WebSocket.CLOSED: // do something break; default: // this never happens break;&#125; ws.onopen: 实例对象的onopen属性 用于指定连接成功后的回调函数: 123ws.onopen = function () &#123; ws.send(&apos;Hello Server!&apos;);&#125; - 如果要指定多个回调函数，可以使用addEventListener方法。 123ws.addEventListener(&apos;open&apos;, function (event) &#123; ws.send(&apos;Hello Server!&apos;);&#125;); ws.onclose 实例对象的onclose属性，用于指定连接关闭后的回调函数12345678910111213ws.onclose = function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;;ws.addEventListener(&quot;close&quot;, function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;); ws.onmessage 实例对象的onmessage属性，用于指定收到服务器数据后的回调函数 123456789ws.onmessage = function(event) &#123; var data = event.data; // 处理数据&#125;;ws.addEventListener(&quot;message&quot;, function(event) &#123; var data = event.data; // 处理数据&#125;); 注意, 服务器数据可能是文本, 也可能是二进制数据(blob对象或Arraybuffer对象) 12345678910ws.onmessage = function(event)&#123; if(typeof event.data === String) &#123; console.log(&quot;Received data string&quot;); &#125; if(event.data instanceof ArrayBuffer)&#123; var buffer = event.data; console.log(&quot;Received arraybuffer&quot;); &#125;&#125; 除了动态判断收到的数据类型, 也可以使用binaryType属性, 显式指定收到的二进制数据类型 1234567891011// 收到的是 blob 数据ws.binaryType = &quot;blob&quot;;ws.onmessage = function(e) &#123; console.log(e.data.size);&#125;;// 收到的是 ArrayBuffer 数据ws.binaryType = &quot;arraybuffer&quot;;ws.onmessage = function(e) &#123; console.log(e.data.byteLength);&#125;; webSocket.send(): 实例对象的send()方法用于向服务器发送数据。 发送文本的 ws.send(&#39;your message&#39;); 发送 Blob 对象的例子 1234var file = document .querySelector(&apos;input[type=&quot;file&quot;]&apos;) .files[0];ws.send(file); 发送 ArrayBuffer 对象的例子 1234567// Sending canvas ImageData as ArrayBuffervar img = canvas_context.getImageData(0, 0, 400, 320);var binary = new Uint8Array(img.data.length);for (var i = 0; i &lt; img.data.length; i++) &#123; binary[i] = img.data[i];&#125;ws.send(binary.buffer); webSocket.bufferedAmount 实例对象的 bufferedAmount属性, 表示还有多少字节的二进制数据没有发送出去, 它可以用来判断发送是否结束12345678var data = new ArrayBuffer(10000000);ws.send(data);if (ws.bufferedAmount === 0) &#123; // 发送完毕&#125; else &#123; // 发送还没结束&#125; webSocket.onerror 实例对象的onerror属性, 用于指定报错时的回调函数1234567socket.onerror = function(event) &#123; // handle error event&#125;;socket.addEventListener(&quot;error&quot;, function(event) &#123; // handle error event&#125;); 关闭连接: ws.close() 摘自: 阮一峰","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"03.跨站点WebSocket劫持漏洞","slug":"WebSocket/2017-10-28-websocket-03","date":"2016-10-17T09:30:42.000Z","updated":"2018-03-05T03:39:56.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-03/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-03/","excerpt":"","text":"跨站点WebSocket劫持漏洞原理是劫持而非简单的伪造 为了创建全双工通信, 客户端需要基于HTTP进行握手切换到WebSocket协议, 正是这个升级协议的握手过程存在着潜在的问题。 示例: 客户端伪造 与服务端同顶级域名 的跨子域cookie 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; document.cookie = &quot;name=renyimin; domain=websocket.org&quot;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 客户端与websocket.org网站的Echo测试服务建立连接 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; var ws = new WebSocket(&quot;ws://echo.websocket.org&quot;); ws.onopen = function(evt) &#123; console.log(&quot;Connection open ...&quot;); ws.send(&quot;Hello WebSockets!&quot;); &#125;; ws.onmessage = function(evt) &#123; console.log( &quot;Received Message: &quot; + evt.data); ws.close(); &#125;; ws.onclose = function(evt) &#123; console.log(&quot;Connection closed.&quot;); &#125;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 此时, 握手(升级协议)阶段的请求头中就携带了伪造的cookie 上面握手阶段Get请求头中可以看到, Cookie头部把同顶级域名下的Cookie信息发送到服务器端。 WebSocket协议没有规定服务器在握手阶段应该如何认证客户端身份。服务器可以采用任何HTTP服务器的客户端身份认证机制, 譬如cookie, HTTP基础认证, TLS身份认证等。 而对于绝大多数Web应用来说, 客户端身份认证应该都是 SessionID 等 Cookie 或者 HTTP Auth 头部参数等。 所有的浏览器都会发送 Origin 请求头, 如果服务器端没有针对Origin头部进行验证可能会导致跨站点 WebSocket 劫持攻击。 假设某个用户已经在浏览器登录过一个目标站点, 接下来假设他被诱骗访问某个恶意网站, 一旦点击某个诱导性的按钮, 恶意网页在这个操作中植入一个WebSocket握手请求申请跟目标应用建立WebSocket连接 此时, 这个websocket握手请求将会携带浏览器cookie罐中之前存储的目标站点cookie; 请注意, Origin 和 Sec-WebSocket-Key 都是由浏览器自动生成, Cookie等身份认证参数也都是由浏览器自动上传到目标应用服务器端。如果服务器端疏于检查Origin, 该请求伪造的cookie可能就会成功握手切换到 WebSocket 协议，恶意网页就可以成功绕过身份认证连接到 WebSocket 服务器，进而窃取到服务器端发来的信息，抑或发送伪造信息到服务器端篡改服务器端数据。 此时, 熟悉cors跨域的你, 可能会认为服务器端没有指定Access-Control-Allow-Origin的话， 浏览器端的脚本是无法访问跨域资源的, 这确实也是 HTML5 带来的新特性之一。但是很不幸，CORS不适应于 WebSocket, WebSocket 没有明确规定跨域处理的方法。 CSRF主要是通过恶意网页悄悄发起数据修改请求, 而跨站点WebSocket伪造攻击不仅可以修改服务器数据, 还可以控制整个读取/修改双向沟通通道。正是因为这个原因, Christian 将这个漏洞命名为劫持(Hijacking), 而不是请求伪造(Request Forgery)。 检测跨站点 WebSocket 劫持漏洞 明白跨站点 WebSocket 劫持漏洞原理后, 检测该漏洞也就容易了, 先撇开cookie伪造, 首先检测服务端是否能拒绝不同源origin 简单来说就是针对WebSocket握手请求地址, 修改请求中的 Origin 为其他来源(本地可以随意配置虚拟域名的), 然后发送这个请求, 看看服务器是否能够成功返回101响应。 如果连接失败, 那么说明这个 WebSocket 是安全的, 因为它可以正确拒绝来自不同源(Origin)的连接请求。 如果连接成功, 就说明服务器端没有执行源检查, 为了严谨起见, 最好进一步测试是否可以发送 WebSocket 消息, 如果这个 WebSocket 连接能够发送/接受消息的话, 则完全证明跨站点 WebSocket 劫持漏洞的存在。 防范跨站点WebSocket劫持攻击服务端Origin检查 前文介绍了跨站点 WebSocket 劫持漏洞原理和检测, 如何防范这个漏洞? 这个漏洞的原理听起来略微复杂, 但幸运的是测试起来相对比较简单, 那么修复会不会也很简单? 很多读者会想到, 不就是在服务器代码中检查 Origin 参数嘛, 是的, 检查Origin很有必要，但不充分。 推荐在服务器端的代码中增加Origin检查, 如果客户端发来的 Origin 信息来自不同域, 建议服务器端拒绝这个请求, 发回 403 错误响应拒绝连接。 WebSocket令牌机制 仅仅检查 Origin 远远不够, 别忘记了, 如果 WebSocket 的客户端不是浏览器, 非浏览器的客户端发来的请求不会项浏览器那样自动设置Origin, 恶意客户端也就可能伪造 Origin 头信息。 所以更彻底的解决方案还是要借鉴CSRF的解决方案 – 令牌机制 服务器端为每个 WebSocket 客户端生成唯一的一次性 Token; 客户端将Token作为 WebSocket 连接 URL 的参数(譬如 ws://echo.websocket.org/?token＝randomOneTimeToken), 发送到服务器端进行 WebSocket 握手连接; 服务器端验证 Token 是否正确, 一旦正确则将这个 Token 标示为废弃不再重用, 同时确认 WebSocket 握手连接成功; 如果 Token 验证失败或者身份认证失败, 则返回 403 错误; 这个方案里的 Token 设计是关键 推荐的方案是为登录用户生成一个 Secure Random 存储在 Session 中，然后利用对称加密(譬如 AES GCM)加密这个Secure Random值作为令牌, 将加密后的令牌发送给客户端用来进行连接。 这样每个 Session 有一个唯一的随机数, 每个随机数可以通过对称加密生成若干份一次性令牌。用户即便通过不同终端通过 WebSocket 连接到服务器, 服务器可以在保障令牌唯一且一次性使用的前提下, 依然能将不同通道中的信息关联到同一用户中。 参考","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"02.WebSocket简单示例分析","slug":"WebSocket/2017-10-28-websocket-02","date":"2016-10-17T05:57:08.000Z","updated":"2018-03-05T03:40:36.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-02/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-02/","excerpt":"","text":"客户端简单示例1.下面通过 客户端与websocket.org网站的Echo测试服务建立连接 来进行学习 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ws&lt;/title&gt; &lt;script&gt; var ws = new WebSocket(&quot;wss://echo.websocket.org&quot;); ws.onopen = function(evt) &#123; console.log(&quot;Connection open ...&quot;); ws.send(&quot;Hello WebSockets!&quot;); &#125;; ws.onmessage = function(evt) &#123; console.log( &quot;Received Message: &quot; + evt.data); ws.close(); &#125;; ws.onclose = function(evt) &#123; console.log(&quot;Connection closed.&quot;); &#125;; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 2.chrome抓包发现, 实际发送了两次请求, 因为WebSocket协议由两阶段组成: 握手阶段 和 数据传输阶段 握手(handshake)阶段 请求头信息 12345678910111213GET wss://echo.websocket.org/ HTTP/1.1Host: echo.websocket.orgConnection: UpgradePragma: no-cacheCache-Control: no-cacheUpgrade: websocketOrigin: http://www.js.comSec-WebSocket-Version: 13User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36Accept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9Sec-WebSocket-Key: VxdjZ3Z53lPcCrB3OiVwJQ==Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits 为了建立一个 WebSocket 连接, 在握手阶段, 客户端浏览器首先要向服务器发起一个HTTP请求, 这个请求和通常的HTTP请求不同, 包含了一些附加头信息, 其中附加头信息 Upgrade: WebSocket 表明这是一个申请协议升级的 HTTP 请求, 服务器端解析这些附加的头信息然后产生应答信息返回给客户端, 客户端和服务器端的WebSocket连接就建立起来了, 双方就可以通过这个连接通道自由的传递信息, 并且这个连接会持续存在直到客户端或者服务器端的某一方主动的关闭连接。 首部字段的核心是 Connection:Upgrade 和 Upgrade:websocket这两个首部字段, 相当于告诉服务器端:我要申请切换到 WebSocket 协议。 响应头信息 12345678910111213HTTP/1.1 101 Web Socket Protocol HandshakeAccess-Control-Allow-Credentials: trueAccess-Control-Allow-Headers: content-typeAccess-Control-Allow-Headers: authorizationAccess-Control-Allow-Headers: x-websocket-extensionsAccess-Control-Allow-Headers: x-websocket-versionAccess-Control-Allow-Headers: x-websocket-protocolAccess-Control-Allow-Origin: http://www.js.comConnection: UpgradeDate: Fri, 02 Mar 2018 03:01:59 GMTSec-WebSocket-Accept: GAz+a+FeezkKAeHM/HK/fgFI9Wo=Server: Kaazing GatewayUpgrade: websocket 一旦服务器端返回101响应, 即可完成WebSocket协议切换。服务器端即可以基于相同端口, 将通信协议从 http:// 或 https:// 切换到 ws://或 wss://。 协议切换完成后, 浏览器和服务器端即可以使用 WebSocket API 互相发送和收取文本和二进制消息。 这里要解释一些涉及 WebSocket 安全相关的重要头部参数: Sec-WebSocket-Key , Sec-WebSocket-Accept: 客户端负责生成一个Base64编码过的随机数字作为Sec-WebSocket-Key, 服务器则会将一个GUID和这个客户端的随机数一起生成一个散列Key作为 Sec-WebSocket-Accept 返回给客户端。这个工作机制可以用来避免缓存代理(caching proxy), 也可以用来避免请求重播(request replay)。 其他Sec－开头的WebSocket相关Header其实也是WebSocket设计者为了安全的特意设计, 以Sec-开头的Header可以避免被浏览器脚本读取到,这样攻击者就不能利用 XMLHttpRequest 伪造 WebSocket 请求来执行跨协议攻击，因为 XMLHttpRequest 接口不允许设置 Sec- 开头的 Header。 Origin: 作安全使用, 防止跨站攻, 浏览器一般会使用这个来标识原始域 小结 浏览器通过JavaScript向服务器发出建立WebSocket连接的请求, 连接建立以后, 客户端和服务器端就可以通过 TCP 连接直接交换数据, 此后服务端与客户端通过此TCP连接进行实时通信, 不再需要原HTTP协议的参与了; 因为 WebSocket 连接本质上就是一个TCP连接, 所以在数据传输的稳定性和数据传输量的大小方面, 和轮询以及 Comet 技术比较, 具有很大的性能优势; Websocket.org 网站对传统的轮询方式和 WebSocket 调用方式作了一个详细的测试和比较，将一个简单的 Web 应用分别用轮询方式和 WebSocket 方式来实现，在这里引用一下他们的测试结果图： 参考参考参考参考","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"01.引出WebSocket","slug":"WebSocket/2017-10-28-websocket-01","date":"2016-10-17T02:40:16.000Z","updated":"2018-03-05T03:36:34.000Z","comments":true,"path":"2016/10/17/WebSocket/2017-10-28-websocket-01/","link":"","permalink":"http://blog.renyimin.com/2016/10/17/WebSocket/2017-10-28-websocket-01/","excerpt":"","text":"前言 传统模式的 Web 系统使用http协议在服务器和客户端之间进行通信, 是以客户端发出请求,服务器端响应的方式进行工作的, 服务器是无法主动的推送数据给客户端的, 这也是为了安全起见; 既然已经有了HTTP协议，为什么还需要另一个协议? 答案很简单, 因为HTTP协议有一个缺陷: 通信只能由客户端发起。 由于HTTP协议做不到服务器主动向客户端推送信息, 这种单向请求的特点就注定了, 如果服务器有连续频繁的状态变化, 客户端要获知就非常麻烦。(我们只能使用”轮询”：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室) 而现代web开发中, 经常会面对一些对实时性要求比较高的实时Web应用, 比如: 即时通信系统, 即时报价系统, 在线游戏, 在线证券, 设备监控等, 保持客户端和服务器端的信息同步是 实时Web应用 的关键要素。 传统窘境简介在 WebSocket 规范出来之前, 开发人员想实现这些实时的Web应用, 不得不采用一些折衷的方案, 其中最常用的就是 轮询 (Polling), 基于客户端套接口的&quot;服务器推&quot;技术, Comet 技术, 而Comet技术实际上是轮询技术的改进, 又可细分为两种实现方式: 一种是长轮询机制, 一种称为流技术。 下面简单介绍一下 ajax短轮询(Polling) 和 Comet长轮询, 其他请自行搜索资料~~ 传统Ajax短轮训(Polling) 这是最早的一种实现实时 Web 应用的方案, 主要就是客户端(浏览器), 以一定的时间间隔向服务端发出请求http请求, 以频繁请求的方式来保持客户端和服务器端的同步 ; 这种方案最大的问题就是: 你无法给出客户端发送请求的适当时间间隔 如果客户端(浏览器)发送请求的间隔时间过小, 服务器端的数据可能并没有更新, 这样会带来很多无谓的网络传输, 这样既浪费了网络带宽, 又浪费了CPU的利用率 ; 如果客户端(浏览器)发送请求的间隔时间过大, 在服务器端的数据更新很快时, 又不能保证客户端获取数据的实时性 ; Comet最近几年,因为 AJAX 技术的普及,以及把 IFrame 嵌在”htmlfile”的 ActiveX 组件中可以解决 IE 的加载显示问题, 一些受欢迎的应用如 meebo，gmail+gtalk 在实现中使用了这些新技术, 同时”服务器推”在现实应用中确实存在很多需求。因为这些原因, 基于纯浏览器的”服务器推”技术开始受到较多关注, Alex Russell(Dojo Toolkit 的项目 Lead)称这种基于 HTTP 长连接、无须在浏览器端安装插件的”服务器推”技术为Comet。 基于 AJAX 的长轮询long-polling 基于客户端套接口的”服务器推”技术, 需要在浏览器端安装插件, 基于套接口传送信息, 或是使用 RMI、CORBA 进行远程调用 ; 使用 AJAX 实现”服务器推” 与 传统的 AJAX 应用不同之处在于: 服务器端会阻塞请求直到有数据传递或超时才返回; 客户端 JavaScript 响应处理函数会在处理完服务器返回的信息后,再次发出请求,重新建立连接; 当客户端处理接收的数据、重新建立连接时, 服务器端可能有新的数据到达, 这些信息会被服务器端保存直到客户端重新建立连接, 客户端会一次把当前服务器端所有的信息取回; 基于长轮询的服务器推模型如下: 相对于”短轮询”(poll), 这种长轮询方式也可以称为”拉”(pull), 这种方案基于 AJAX，具有以下一些优点: 请求异步发出; 无须安装插件; IE、Mozilla FireFox 都支持 AJAX; 综合这几种方案, 可以发现这些所谓的实时技术并不是真正的实时技术, 它们只是在用 Ajax 方式来模拟实时的效果, 在每次客户端和服务器端交互的时候都是一次HTTP的请求和应答的过程, 而每一次的 HTTP 请求和应答都带有完整的 HTTP 头信息, 这就增加了每次传输的数据量, 而且这些方案中客户端和服务器端的编程实现都比较复杂, 在实际的应用中, 为了模拟比较真实的实时效果, 开发人员往往需要构造两个 HTTP 连接来模拟客户端和服务器之间的双向通讯, 一个连接用来处理客户端到服务器端的数据传输, 一个连接用来处理服务器端到客户端的数据传输, 这不可避免地增加了编程实现的复杂度, 也增加了服务器端的负载, 制约了应用系统的扩展性。 WebSocket简介 WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，就像Socket一样。 WebSocket 是HTML5中定义的新协议, 它实现了真正的长连接, 实现了浏览器与服务器的全双工通信; HTML5 WebSocket设计出来的目的就是要取代 轮询 和 Comet 技术, 使客户端浏览器具备像 C/S 架构下桌面系统的实时通讯能力。 特点: 建立在 TCP 协议之上，服务器端的实现比较容易。 与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用HTTP协议，因此握手时不容易屏蔽，能通过各种HTTP代理服务器 数据格式比较轻量，性能开销小，通信高效 可以发送文本，也可以发送二进制数据 没有同源限制，客户端可以与任意服务器通信 协议标识符是ws(如果加密，则为wss),服务器网址就是 URL 协议由两阶段组成: 握手阶段(handshake) 和 数据传输阶段 参考参考参考 阮一峰","categories":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/categories/WebSocket/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"http://blog.renyimin.com/tags/WebSocket/"}]},{"title":"07.\"Jsonp\" 对比 \"CORS简单/非简单请求\"","slug":"2016-09-18-sameoriginpolicy-07","date":"2016-09-18T13:20:16.000Z","updated":"2018-03-02T06:52:27.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-07/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-07/","excerpt":"","text":"Jsonp 对比 CORS简单/非简单请求都可以方便实现跨域; Jsonp 简单适用, 老式浏览器全部支持, 服务器端改动很小; 但是JSONP只能发GET请求; JSONP跨域发送Cookie的话, 只用设置 cookie的domain属性 为相同的顶级域名即可; CORS简单请求 服务端需要设置一些允许选项; 请求方法比较简单, 如 GET, POST, HEAD ; 跨子域共享cookie的话, 服务端和客户端都要对Credentials header属性进行设置; 另外, 跨子域的话, 如果想同时共享cookie的话, 服务端 Access-Control-Allow-Origin 不能设置为 *, 否则会提示 :123Failed to load http://test.test.com/index.php?sex=renyimin&amp;age=100: The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' when the request's credentials mode is 'include'. Origin 'http://www.test.com' is therefore not allowed access. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute. CORS非简单请求 服务端需要设置一些允许选项; 其他请求方法 PUT, DELETE…. 可以设置自定义header头 跨子域共享cookie方面 和 CORS简单请求一样","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"06.CORS方案 not-so-simple request","slug":"2016-09-18-sameoriginpolicy-06","date":"2016-09-18T12:10:16.000Z","updated":"2018-03-01T02:14:12.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-06/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-06/","excerpt":"","text":"预检请求 preflight 说明非简单请求是那种对服务器有特殊要求的请求, 比如请求方法是 PUT 或 DELETE, 或者 Content-Type 字段的类型是 application/json ; 非简单请求的CORS请求, 会在正式通信之前, 增加一次HTTP查询请求, 称为 &quot;预检&quot;请求(preflight) 浏览器先询问服务器, 当前网页所在的域名是否在服务器的许可名单之中, 以及可以使用哪些HTTP动词和头信息字段; 只有得到肯定答复, 浏览器才会发出正式的XMLHttpRequest请求, 否则就报错 ; 非简单请求会导致原先的一次请求变成两次, 第一次请求是 预检请求;“预检”请求用的请求方法是 OPTIONS，表示这个请求是用来询问的，头信息里面关键字段是Origin，表示请求来自哪个源 ; 非简单请求的例子1.服务器前端代码: www.test.com/index.html 本例子使用 PUT 来进行ajax请求, 满足 非简单请求 的条件 ; 另外, 本例还自定义了请求时的 header 首部字段, 也满足 非简单请求 的条件 ; 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; //设置cookie document.cookie = &quot;name=renyimin; domain=test.com&quot;; document.cookie = &quot;age=100; domain=test.com&quot;; document.cookie = &quot;gender=boy; domain=test.com&quot;; //跨子域带cookie和简单请求的设置方法一样 $.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); $(&quot;#btn&quot;).click(function() &#123; //序列化name/value var data = $(&quot;form&quot;).serializeArray(); $.ajax(&#123; //这里用PUT, 则为 `非简单` 请求 type: &apos;PUT&apos;, url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, data: data, //或者如果你自定义了一些请求时的 header 首部字段, 那么请求就也是 复杂请求 headers: &#123;&quot;custom-header-field&quot; : &quot;test&quot;&#125;, success: function (result) &#123; console.log(result); &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: test.test.com/index.php 123456789101112&lt;?php//服务器允许的 Originheader(\"Access-Control-Allow-Origin: http://www.test.com\");//服务器允许的 methodsheader(\"Access-Control-Allow-Methods: PUT, GET, POST\");//服务器允许设置的头部字段header(\"Access-Control-Allow-Headers: custom-header-field\");//跨子域带cookie和简单请求的设置方法一样header(\"Access-Control-Allow-Credentials: true\");var_dump($_COOKIE);var_dump($_SERVER['HTTP_CUSTOM_HEADER_FIELD']); 3.注意: 像上面例子的复杂跨域请求 必须: 首先和简单请求一样, 服务器端的 Access-Control-Allow-Origin 是必须设置的, 不然首先就跨不了域; 必须: 其次, 是使用了 get, post, head 之外方法的 复杂请求, 那么就必须在服务端有对应的 Access-Control-Allow-Method, 否则: 可选: 如果你自定义了 自定义首部字段 的 复杂请求, 那么也要在服务端有对应的 Access-Control-Allow-Headers, 否则: 4.另外需要关注的是: 如果在ajax跨域的情况下, 你设置了自定义的首部字段, 那么即使你的请求类型是get, post, head, 也会变成也是复杂请求, 所以仍然会发出预请求; 分析预检请求1.上面www.test.com/index.php代码进行ajax请求的时候, HTTP请求的方法是PUT, 所以浏览器会发现, 这是一个非简单请求, 就自动发出一个”预检”请求, 要求服务器确认可以这样请求 ; 2.所以请求应该是包括 预检请求和 真正的请求 两个请求的: 3.下面是这个”预检”请求的HTTP头信息 和 响应信息: 4.可以看到, “预检”请求用的HTTP请求方法是OPTIONS方法, 表示这个请求是用来进行询问的, 头信息里面, 关键字段是Origin, 表示请求来自哪个源; 除了Origin字段，”预检”请求的头信息包括两个特殊字段:(1)Access-Control-Request-Method ：该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT ;(2)Access-Control-Request-Headers：该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段, 上例是X-Custom-Header ; 另外可以看到Access-Control-Allow-Credentials 响应首部字段, 和简单请求一样, 都是为了做跨子域cookie共享; 分析预检响应1.从之前的图中还可以看到, 在预检请求的响应中, 服务器收到”预检”请求以后, 检查了 Origin,Access-Control-Request-Method,Access-Control-Request-Headers,Access-Control-Allow-Credentials 字段以后，确认允许跨源请求，就可以做出回应 ; 2.并且预检请求部分是不会真的发送数据的:而第二次真实请求就会有真正数据! 3.上面的HTTP回应中，关键的是 Access-Control-Allow-Origin 字段，表示 http://www.test.com 可以请求数据, 该字段也可以设为星号，表示同意任意跨源请求 ; 123Access-Control-Allow-Origin: http://www.test.com或者Access-Control-Allow-Origin: * 4.如果服务器否定了”预检”请求，会返回一个正常的HTTP回应, 浏览器会认定服务器不同意预检请求，因此触发一个错误，被 XMLHttpRequest 对象的 onerror 回调函数捕获;控制台会打印出如下的报错信息 ; 其他CORS相关字段分析 Access-Control-Allow-Methods: GET, POST, PUT 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 Access-Control-Allow-Headers: X-Custom-Header 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 Access-Control-Allow-Credentials: true 该字段与简单请求时的含义相同。 Access-Control-Max-Age: 1728000 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 最后注意, 复杂请求 跨子域带cookie和 简单请求 的设置方法一样;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"05.CORS方案 simple request","slug":"2016-09-18-sameoriginpolicy-05","date":"2016-09-18T04:45:07.000Z","updated":"2018-03-01T12:13:10.000Z","comments":true,"path":"2016/09/18/2016-09-18-sameoriginpolicy-05/","link":"","permalink":"http://blog.renyimin.com/2016/09/18/2016-09-18-sameoriginpolicy-05/","excerpt":"","text":"CORS说明CORS 是一个W3C标准, 全称是 跨域资源共享(Cross-origin resource sharing), 通俗说就是我们所熟知的跨域请求; 在以前，跨域可以采用 代理、JSONP 等方式，而在Modern浏览器面前, 这些终将成为过去式, 因为有了CORS ; CORS允许浏览器向跨源服务器发出XMLHttpRequest请求, 也就是克服了AJAX只能同源使用的限制; 另外, CORS需要浏览器和服务器同时支持 (目前, 所有浏览器都支持该功能, IE浏览器不能低于IE10) ; 整个CORS通信过程都是浏览器自动完成, 不需要用户参与 对于开发者来说, CORS通信与同源的AJAX通信没有差别, 代码完全一样, 浏览器一旦发现AJAX的请求是跨源的, 就会自动添加一些附加的头信息, 有时还会多出一次附加的请求, 但用户不会有感觉; 之所以CORS通信与同源的AJAX通信的代码没有差别, 是因为: 其实实现CORS通信的关键是服务器, 只要服务器实现了CORS接口，就可以跨源通信 CORS的两类请求浏览器将CORS请求分成两类: 简单请求(simple request) 和 非简单请求(not-so-simple request) 以下情况会被归类为 非简单请求 : 以 GET, HEAD 或者 POST 以外的方法发起请求; 虽然使用 POST，但请求数据为 application/x-www-form-urlencoded, multipart/form-data 或者 text/plain 以外的数据类型, 比如说，用 POST 发送数据类型为 application/xml 或者 text/xml 的 XML 数据的请求 ; 使用自定义请求头（比如添加诸如 X-PINGOTHER） 简单请求代码案例跨域出错1.前端代码: www.test.com/index.html: 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function(k) &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;post&apos;, //这里用GET url: &apos;http://www.haha.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.跨域后端代码: www.haha.com/index.php : 12&lt;?phpecho json_encode(['name' =&gt; 'lant', 'age' =&gt; 100]); 3.由于出现跨域(而同源策略会限制Ajax跨域请求)所以会报错: 123Failed to load http://www.haha.com/index.php: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://www.test.com&apos; is therefore not allowed access. Access-Control-Allow-Origin响应头解决修改服务器后端代码: www.haha.com/index.php 如下, 发现可以正常进行 跨域ajax请求: 123&lt;?phpheader(&quot;Access-Control-Allow-Origin: http://www.test.com&quot;);echo json_encode([&apos;name&apos; =&gt; &apos;lant&apos;, &apos;age&apos; =&gt; 100]); 简单请求基本流程分析origin 请求首部字段1.对于简单请求，浏览器直接发出CORS请求, 具体来说, 就是在头信息之中, 自动增加一个 Origin 字段 ; - 浏览器发现这次跨源AJAX请求是简单请求, 就自动在头信息之中, 添加一个`Origin`字段: &lt;img src=&quot;/img/cross-domain/ajax-simple-cors.png&quot; /&gt; 2.Origin请求首部字段 用来说明本次请求来自哪个源(协议 + 域名 + 端口), 服务器根据这个值, 决定是否同意这次请求; 3.如果Origin源不在服务器的许可范围内 服务器仍然会返回一个正常的HTTP回应, 不过浏览器会发现, 这个回应的头信息并没有包含 Access-Control-Allow-Origin 字段(详见下文), 就知道出错了, 从而抛出一个错误, 被XMLHttpRequest的onerror回调函数捕获; 注意, 这种错误无法通过状态码识别, 因为HTTP回应的状态码有可能是200 ; 4.如果Origin源在服务器设置的许可范围内服务器的响应可能会出现如下几个头信息字段(当然也不一定是所有都包含, 具体还得看服务器如何进行设置): 1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 服务器响应首部字段分析上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头: Access-Control-Allow-Origin服务器要设置ajax请求可以跨域, 该字段是必须的, 它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求; Access-Control-Allow-Credentials该字段可选, 它的值是一个布尔值，表示是否允许发送Cookie, 默认情况下，Cookie不包括在CORS请求之中; 如果设置为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器;注意, 这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。当然, 如果带了cookie, 那貌似也只能是跨子域了; Access-Control-Expose-Headers该字段可选, CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma;如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定;之前提到的 Access-Control-Expose-Headers: FooBar 就可以让getResponseHeader(‘FooBar’)获取额外返回的FooBar字段的值。 目标域响应首部具体设置Access-Control-Allow-Origin1.服务器在设置的时候, 其实就是通过header函数设置上面的三个选项;2.比如之前的例子中, 如果服务器只是简单的为了实现跨域, 直接设置如下选项即可: 123&lt;?phpheader(\"Access-Control-Allow-Origin: http://www.haha.com\");echo json_encode(['name' =&gt; 'lant', 'age' =&gt; 100]); Access-Control-Allow-Credentials注意配合withCredentials属性1.CORS请求默认不会在不同域之间共享Cookie和HTTP认证信息, 如果要共享Cookie: 一方面要 服务器同意指定Access-Control-Allow-Credentials字段 : 123Access-Control-Allow-Credentials: true//php中设置如下:header(\"Access-Control-Allow-Credentials: true\"); 另一方面, 开发者必须在AJAX请求中打开 withCredentials 属性 (这一点JSONP不用设置) 1234var xhr = new XMLHttpRequest();xhr.withCredentials = true;//jquery中设置withCredentials的代码如下:$.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); 当然, 也仅限于跨子域 2.需要以上3方面都做到才可以 否则，即使服务器同意发送Cookie，浏览器也不会发送 ; 但是, 如果省略 withCredentials 设置, 有的浏览器还是会一起发送Cookie, 这时, 可以显式关闭 withCredentials ; 3.需要注意的是: 如果要发送Cookie, Access-Control-Allow-Origin 就不能设为星号*, 必须指定明确的、与请求网页一致的域名 ; 同时，Cookie依然遵循同源政策，只有顶级域名一样的情况下, 的Cookie才会共享, 其他域名的Cookie并不会上传 ; 测试代码1.前端代码 (www.test.com/index.html): 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; //设置cookie document.cookie = &quot;name=renyimin; domain=test.com&quot;; document.cookie = &quot;age=100; domain=test.com&quot;; document.cookie = &quot;gender=boy; domain=test.com&quot;; //要在跨域请求服务器时在cors请求中包含cookie, 就需要开启withCredentials属性 $.ajaxSetup(&#123;crossDomain: true, xhrFields: &#123;withCredentials: true&#125;&#125;); $(&quot;#btn&quot;).click(function(k) &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;br/&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码 (test.test.com/index.php): 12345&lt;?phpheader(\"Access-Control-Allow-Origin: http://www.test.com\");//服务器允许前端在跨子域cors请求时包含cookieheader(\"Access-Control-Allow-Credentials: true\");var_dump($_COOKIE); 3.效果: 成功实现跨域, 并且实现跨子域cookie共享! 小结 如果 www.test.com/index.html 中设置了 withCredentials 为 true , 那么在ajax请求的目标域test.test.com/index.php中必须设置: 1header(\"Access-Control-Allow-Credentials: true\"); 否则, 报错如下: 如果www.test.com/index.html中没有设置 withCredentials 属性,test.test.com/index.php也没设置 header(&quot;Access-Control-Allow-Credentials: true&quot;);那么即使www.test.com/index.html中设置了domain属性为 .test.com 的cookie键值对test.test.com/index.php 中也获取不到cookie; 如果想跨子域携带cookie, 则服务器端需要注意: Access-Control-Allow-Origin响应首部字段不能为*哦!!","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"04.JSONP方案","slug":"2016-09-17-sameoriginpolicy-04","date":"2016-09-17T11:27:31.000Z","updated":"2018-03-01T01:51:34.000Z","comments":true,"path":"2016/09/17/2016-09-17-sameoriginpolicy-04/","link":"","permalink":"http://blog.renyimin.com/2016/09/17/2016-09-17-sameoriginpolicy-04/","excerpt":"","text":"JSONPJSONP是服务器与客户端 跨源通信 的常用方法; 最大特点就是 简单适用, 老式浏览器全部支持, 服务器端改造非常小; 但问题是，JSONP只能发GET请求 ; 正常ajax请求1.服务器前端代码: www.test.com/index.html 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://www.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: www.test.com/index.php 12345678&lt;?php//参数$sex = isset($_GET['sex']) ? trim($_GET['sex']) : '';$age = isset($_GET['age']) ? trim($_GET['age']) : '';$data = [\"sex\" =&gt; $sex, \"age\" =&gt; $age];$res = json_encode($data, JSON_UNESCAPED_UNICODE); //json 数据echo $res; 3.结果: 前端ajax正常得到后端代码的响应!! 跨域Ajax请求案例1.服务器前端代码: www.test.com/index.html, 修改请求的后端连接为 test.test.com/index.php 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;json&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码不变 3.问题出现: 123Failed to load http://test.test.com/index.php?sex=%E7%94%B7&amp;age=100: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://www.test.com&apos; is therefore not allowed access. jsonp跨域方案1.服务器前端代码: www.test.com/index.html 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;jsonp&apos;, //类型 data: data, jsonp: &apos;callback&apos;, //jsonp回调参数，峰哥资料上说是必须的(经测试, 至少的jquery下是可以不带的) success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 2.服务器后端代码: test.test.com/index.php 12345678910&lt;?php//jsonp回调参数，必需$callback = isset($_GET['callback']) ? trim($_GET['callback']) : '';//参数$sex = isset($_GET['sex']) ? trim($_GET['sex']) : '';$age = isset($_GET['age']) ? trim($_GET['age']) : '';$data = [\"sex\" =&gt; $sex, \"age\" =&gt; $age];$res = json_encode($data,JSON_UNESCAPED_UNICODE); //json 数据echo '(' . $res . ')'; // 返回格式貌似需要注意 () 的使用 3.结果, 规避了同源策略限制导致的Ajax请求不能发送的问题!! 只支持GET在使用jsonp的时候, 当你把GET方法换成POST之后, 发现其实也可以进行正常的Ajax请求;仔细观察请求头, 发现你设置的POST请求方法最终还是被换成了 GET; JSONP跨子域cookie共享1.JSONP如果是像上面测试那样跨子域的话, 是可以同时考虑跨子域共享Cookie的; (但是如果是跨顶级域, 那就不能共享cookie了) 服务器前端代码: www.test.com/index.html, 增加对cookie的设置 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;https://cdn.staticfile.org/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready(function()&#123; $(&quot;#btn&quot;).click(function() &#123; document.cookie = &quot;name=renyimin; domain=test.com&quot;; var data = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;POST&apos;, //这里用GET url: &apos;http://test.test.com/index.php&apos;, dataType: &apos;jsonp&apos;, //类型 data: data, success: function (result) &#123;//返回的json数据 console.log(result); //回调输出 &#125;, timeout: 3000 &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt; 性别:&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt; &lt;br/&gt; 年龄:&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 服务器后端代码 test.test.com/index.php, 在跨子域的情况下,可以正常获取cookie 2.但是如果是跨顶级域名, 就别奢望了, 测试发现, 上述代码做如下两处改动: 设置cookie的代码如果改为: document.cookie = &quot;name=renyimin; domain=haha.com&quot;;; ajax的url改为跨顶级域名的url: ‘http://www.haha.com/index.php‘, 3.你会发现, ajax可以正常跨域, 但是cookie却不会被设置成功的! 4.小结: ajax使用jsonp跨子域的时候是可以轻松像上面设置 cookie的domain属性 来共享cookie的 ; 而下一篇介绍的ajax使用cors方案跨域的话, 即使设置了cookie的 document.domain , 也不能带上cookie, 还需要注意前端和后端的 withCredentials 头字段 ;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"03.规避同源策略限制 之 \"Ajax请求不能发送\"","slug":"2016-09-16-sameoriginpolicy-03","date":"2016-09-16T05:04:17.000Z","updated":"2018-03-01T12:16:58.000Z","comments":true,"path":"2016/09/16/2016-09-16-sameoriginpolicy-03/","link":"","permalink":"http://blog.renyimin.com/2016/09/16/2016-09-16-sameoriginpolicy-03/","excerpt":"","text":"同源政策规定, AJAX请求只能发给同源的网址, 否则就报错 ; 除了架设服务器代理(浏览器请求同源服务器，再由后者请求外部服务), 有三种方法规避这个限制 : JSONP CORS WebSocket 参考","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"02.规避同源策略限制 之 \"Cookie无法读取\"","slug":"2016-09-15-sameoriginpolicy-02","date":"2016-09-15T13:10:13.000Z","updated":"2018-03-01T12:08:45.000Z","comments":true,"path":"2016/09/15/2016-09-15-sameoriginpolicy-02/","link":"","permalink":"http://blog.renyimin.com/2016/09/15/2016-09-15-sameoriginpolicy-02/","excerpt":"","text":"回顾 之前学习同源策略基础知识的时候, 了解了同源策略的 三种行为 限制: Cookie、LocalStorage 和 IndexDB 无法读取 DOM 无法获得 AJAX 请求不能发送 注意:同源策略的限制, 并没有限制住CSRF攻击, 它并不会限制 B站点中嵌入的A站点超链接去读取A站点用户的cookie; 同源策略限制 – Cookie无法读取 准备跨域站点: www.test.com 和 test.tset.com www.test.com站点(http://www.test.com/setCookie.php)写入cookie 12&lt;?phpsetcookie(\"name\", 'renyimin'); www.test.com 站点(http://www.test.com/getCookie.php)尝试读取cookie 12&lt;?phpvar_dump($_COOKIE['name']); // 可以看到, 未跨域的情况下, 可以正常获取cookie test.test.com 站点(http://test.test.com/getCookie.php)尝试读取cookie 12&lt;?phpvar_dump($_COOKIE['name']); // 你会发现读取不到cookie, 因为已经跨域 (虽然一级域名一样, 但是由于二级域名不一致, 导致了跨域) 合理规避 Cookie无法读取的限制 虽然同源的那些限制是必要, 但是有些情况下, 我们可能需要去合理规避Cookie无法读取的限制的; 比如: 如果两个站点的顶级域名相同, 只是二级域名不同的话, 浏览器其实是允许你通过设置 document.domain 来共享 Cookie 的; 例子: 还是之前的例子, 如果www.test.com/setCookie.php代码改为: 12&lt;?phpsetcookie(&quot;name&quot;, &apos;renyimin&apos;, 0, &apos;&apos;, &apos;test.com&apos;); // 这样你会发现, www.test.com/getCookie.php 和 test.test.com/getCookie.php 都可以正常获取 www.test.com/setCookie.php 设置的cookie 小结合理规避同源策略因跨域导致的cookie无法读取的问题, 有个前提是, 跨域双方需在同顶级域下(即顶级域名一致的情况下);你在设置cookie时, 如果指定的domain参数和当前域名的顶级域名不一致, cookie 的设置就会失败;","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"01.同源策略(Same origin policy)","slug":"2016-09-15-sameoriginpolicy-01","date":"2016-09-15T11:21:54.000Z","updated":"2018-03-01T12:08:03.000Z","comments":true,"path":"2016/09/15/2016-09-15-sameoriginpolicy-01/","link":"","permalink":"http://blog.renyimin.com/2016/09/15/2016-09-15-sameoriginpolicy-01/","excerpt":"","text":"同源策略1995年，同源政策由 Netscape 公司引入浏览器。目前, 所有浏览器都实行这个政策; 最初，它的含义是指，A站点设置的Cookie，B站点不能打开，除非这两个站点属 同源，所谓同源指的是 “三个相同”: 1.协议相同http://blog.renyimin.com 和 https://blog.renyimin.com 就不是同一个源 ； 2.域名完全相同http://blog.renyimin.com/test/index.php 和 http://blog.renyimin.com/welcome/index.html 就是同一个源;但是 http://www.renyimin.com/test/index.php 和 http://blog.renyimin.com/test/index.php 就不是同一个源 ；请注意：localhost和127.0.0.1虽然都指向本机, 但也不是同一个源 ; 3.端口相同http://www.renyimin.com:8080/test/index.php 和 http://www.renyimin.com:80/test/index.php 就不是同一个源 ; 同源策略目的为了保证用户信息的安全，防止恶意的网站窃取并利用数据; 比如用户登录一家银行网站后，又去浏览其他站点, 如果没有同源策略限制, 其他站点就也能读取银行网站的 Cookie, 这样的话: 如果 Cookie 包含用户的私密信息，泄漏给第三方站点就比较危险, 当然, Cookie中包含的敏感信息通常经过加密，很难将其反向破解, 但这并不意味着绝对安全; 虽然第三方无法通过解密获取cookie中的信息, 但它可以不解密,而是直接使用Cookie去骗取银行网站的信任; 由此可见，”同源政策” 是必需的，否则, 各站点的 Cookie 可以随便共享，那互联网就毫无安全可言了 ; 同源策略的限制随着互联网的发展, “同源政策”越来越严格, 目前, 如果非同源, 共有三种行为受到限制: Cookie、LocalStorage 和 IndexDB 无法读取 DOM 无法获得 AJAX 请求不能发送 同源策略和CSRF安全问题需要注意一点: 同源策略的限制, 并没有限制住CSRF攻击 上面同源策略的几个限制中, 并没有限制 从B站点中通过超链接去跳转到A站点时, A站点去读取自身的cookie, 这样就会存在问题: 假如你当前已经登录了邮箱，或bbs，同时你又访问了另外一个站点，假设这就是一个钓鱼网站(站点中伪装了很多诱导用户去点击的超链接, 而这些超链接都是其他站点的一些敏感操作, 就这样放好诱饵等待曾经登陆过那些站点的用户去点击, 等鱼儿上钩); 假设这个网站上面可能因为某个图片吸引你，你去点击一下，而这个点击正是去往你的bbs站点进行一个发帖操作，由于当前你的浏览器状态已经是登陆了bbs站点，此时你点击这个钓鱼网站的连接是就会使用你之前登陆bbs站点在浏览器cookie罐中保存的信息, 就和正常的请求一样。于是钓鱼站点就纯天然的利用了其他站点的登陆状态，让用户在不知情的情况下，帮你发帖或干其他事情; 这也就是我们通常所说的CSRF攻击, CSRF攻击的主要目的是让用户在不知情的情况下攻击自己已登录的一个系统; 引出跨域问题由于同源策略的这些限制都是为了安全考虑, 自然是必要的; 但是有时很不方便, 可能会导致合理的用途也受到影响, 接下来将详细介绍如何在需要的时候合理地去 规避”同源政策”的限制; 参考 阮一峰","categories":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/categories/CrossDomain/"}],"tags":[{"name":"CrossDomain","slug":"CrossDomain","permalink":"http://blog.renyimin.com/tags/CrossDomain/"}]},{"title":"OOP - Iterator","slug":"OOP/2016-05-29-OOP-13-Iterator","date":"2016-05-29T13:10:02.000Z","updated":"2018-02-04T03:54:00.000Z","comments":true,"path":"2016/05/29/OOP/2016-05-29-OOP-13-Iterator/","link":"","permalink":"http://blog.renyimin.com/2016/05/29/OOP/2016-05-29-OOP-13-Iterator/","excerpt":"","text":"前言我们可以将电视机看成一个存储电视频道的集合对象, 通过遥控器可以对电视机中的电视频道集合进行操作, 如返回上一个频道、跳转到下一个频道或者跳转至指定的频道。遥控器为我们操作电视频道带来很大的方便, 用户并不需要知道这些频道到底如何存储在电视机中。 而在软件开发中, 也存在大量类似电视机一样的类, 它们可以存储多个成员对象(元素), 这些类通常称为聚合类(Aggregate Classes), 对应的对象称为聚合对象。 为了更加方便地操作这些聚合对象, 同时可以很灵活地为聚合对象增加不同的遍历方法, 我们也需要类似电视机遥控器一样的角色, 可以访问一个聚合对象中的元素但又不需要暴露它的内部结构。 本篇将要学习的迭代器模式将为聚合对象提供一个遥控器, 通过引入迭代器, 客户端无须了解聚合对象的内部结构即可实现对聚合对象中成员的遍历, 还可以根据需要很方便地增加新的遍历方式。 引用”销售管理系统中数据的遍历” 初始方案: Sunny软件公司为某商场开发了一套销售管理系统, 在对该系统进行分析和设计时, Sunny软件公司开发人员发现经常需要对系统中的商品数据、客户数据等进行遍历, 为了复用这些遍历代码, Sunny公司开发人员设计了一个抽象的数据集合类AbstractObjectList, 而将存储商品和客户等数据的类作为其子类, AbstractObjectList类结构如下图所示： 问题: 违反 单一职责, 接口隔离 Sunny软件公司开发人员通过对AbstractObjectList类结构进行分析, 发现该设计方案存在如下几个问题： 在类图中, addObject()、removeObject() 等方法用于管理数据, 而next()、isLast()、previous()、isFirst()等方法用于遍历数据。这将导致聚合类的职责过重, 它既负责存储和管理数据, 又负责遍历数据, 违反了 “单一职责原则”, 由于聚合类非常庞大, 实现代码过长, 还将给测试和维护增加难度。 如果将抽象聚合类声明为一个接口, 则在这个接口中充斥着大量方法, 不利于子类实现, 违反了 “接口隔离原则”。 如果将所有的遍历操作都交给子类来实现, 将导致子类代码庞大, 而且必须暴露AbstractObjectList的内部存储细节, 向子类公开自己的私有属性, 否则子类无法实施对数据的遍历，这将破坏AbstractObjectList类的封装性。 如何解决上述问题? 解决方案之一就是将聚合类中负责遍历数据的方法提取出来, 封装到专门的类中, 实现数据存储和数据遍历分离, 无须暴露聚合类的内部属性即可对其进行操作, 而这正是迭代器模式的意图所在。 迭代器模式 在软件开发中, 我们经常需要使用聚合对象来存储一系列数据。聚合对象拥有两个职责: 一是存储数据, 二是遍历数据。 从依赖性来看，前者是聚合对象的基本职责； 而后者既是可变化的，又是可分离的。 因此，可以将遍历数据的行为从聚合对象中分离出来，封装在一个被称之为 “迭代器” 的对象中，由迭代器来提供遍历聚合对象内部数据的行为，这将简化聚合对象的设计，更符合 “单一职责原则” 的要求。 迭代器模式(Iterator Pattern)定义：提供一种方法来访问聚合对象, 而不用暴露这个对象的内部表示, 其别名为游标(Cursor)模式。迭代器模式是一种对象行为型模式。 在迭代器模式结构中包含聚合和迭代器两个层次结构, 考虑到系统的灵活性和可扩展性, 在迭代器模式中应用了工厂方法模式, 其模式结构如下所示: 在迭代器模式结构图中包含如下几个角色: Iterator(抽象迭代器): 它定义了访问和遍历元素的接口, 声明了用于遍历数据元素的方法, 例如: 用于获取第一个元素的first()方法，用于访问下一个元素的next()方法，用于判断是否还有下一个元素的hasNext()方法，用于获取当前元素的currentItem()方法等，在具体迭代器中将实现这些方法。 ConcreteIterator(具体迭代器): 它实现了抽象迭代器接口，完成对聚合对象的遍历，同时在具体迭代器中通过游标来记录在聚合对象中所处的当前位置，在具体实现时，游标通常是一个表示位置的非负整数。 Aggregate(抽象聚合类):它用于存储和管理元素对象, 声明一个createIterator()方法用于创建一个迭代器对象, 充当抽象迭代器工厂角色。 ConcreteAggregate(具体聚合类): 它实现了在抽象聚合类中声明的createIterator()方法，该方法返回一个与该具体聚合类对应的具体迭代器ConcreteIterator实例。 在迭代器模式中, 提供了一个外部的迭代器来对聚合对象进行访问和遍历, 迭代器定义了一个访问该聚合元素的接口, 并且可以跟踪当前遍历的元素, 了解哪些元素已经遍历过而哪些没有。迭代器的引入, 将使得对一个复杂聚合对象的操作变得简单。 下面我们结合代码来对迭代器模式的结构进行进一步分析, 在迭代器模式中应用了工厂方法模式, 抽象迭代器对应于抽象产品角色, 具体迭代器对应于具体产品角色, 抽象聚合类对应于抽象工厂角色, 具体聚合类对应于具体工厂角色 在抽象迭代器中声明了用于遍历聚合对象中所存储元素的方法，典型代码如下所示： 123456interface Iterator &#123; public void first(); //将游标指向第一个元素 public void next(); //将游标指向下一个元素 public boolean hasNext(); //判断是否存在下一个元素 public Object currentItem(); //获取游标指向的当前元素 &#125; 在具体迭代器中将实现抽象迭代器声明的遍历数据的方法，如下代码所示： 123456789101112131415161718class ConcreteIterator implements Iterator &#123; private ConcreteAggregate objects; //维持一个对具体聚合对象的引用，以便于访问存储在聚合对象中的数据 private int cursor; //定义一个游标，用于记录当前访问位置 public ConcreteIterator(ConcreteAggregate objects) &#123; this.objects=objects; &#125; public void first() &#123; ...... &#125; public void next() &#123; ...... &#125; public boolean hasNext() &#123; ...... &#125; public Object currentItem() &#123; ...... &#125; &#125; 需要注意的是抽象迭代器接口的设计非常重要: 一方面需要充分满足各种遍历操作的要求, 尽量为各种遍历方法都提供声明; 另一方面又不能包含太多方法, 因为接口中方法太多将给子类的实现带来麻烦; 另外, 如果需要在具体迭代器中为聚合对象增加全新的遍历操作, 则必须修改抽象迭代器和具体迭代器的源代码，这将 违反“开闭原则”, 因此在设计时要考虑全面,避免之后修改接口。 聚合类用于存储数据并负责创建迭代器对象, 最简单的抽象聚合类代码如下所示: 123interface Aggregate &#123; Iterator createIterator(); &#125; 具体聚合类作为抽象聚合类的子类，一方面负责存储数据，另一方面实现了在抽象聚合类中声明的工厂方法createIterator()，用于返回一个与该具体聚合类对应的具体迭代器对象，代码如下所示： 1234567class ConcreteAggregate implements Aggregate &#123; ...... public Iterator createIterator() &#123; return new ConcreteIterator(this); &#125; ...... &#125; 理解迭代器模式中 具体聚合类 与 具体迭代器类 之间存在的依赖关系和关联关系。 迭代器模式中应用了工厂方法模式(每个具体的聚合对象都对应一个具体的迭代器); 销售管理系统的解决方案代码 参考 如果需要增加一个新的具体聚合类, 如客户数据集合类, 并且需要为客户数据集合类提供不同于商品数据集合类的正向遍历和逆向遍历操作, 那么只需增加一个新的聚合子类和一个新的具体迭代器类即可;原有类库代码无须修改, 符合“开闭原则”; 如果需要为ProductList类更换一个迭代器, 只需要增加一个新的具体迭代器类作为抽象迭代器类的子类, 重新实现遍历方法, 原有迭代器代码无须修改, 也符合“开闭原则”; 但是如果要在迭代器中增加新的方法, 则需要修改抽象迭代器源代码, 这将违背“开闭原则”。 PHP迭代器 PHP SPL 中已经提供了迭代器接口Iterator和容器接口IteatorAggragate 并且PHP SPL已经提供了很多具体的迭代器对象 当然, 如果SPL准备的迭代器还不够, 你可以为你的聚合类创建自定义的迭代器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;?php// 实现自己的具体迭代器class ConcreteIterator implements Iterator&#123; private $position = 0; private $array = array(); public function __construct($array) &#123; $this-&gt;array = $array; $this-&gt;position = 0; &#125; function rewind() &#123; $this-&gt;position = 0; &#125; function current() &#123; return $this-&gt;array[$this-&gt;position]; &#125; function key() &#123; return $this-&gt;position; &#125; function next() &#123; ++$this-&gt;position; &#125; function valid() &#123; return isset($this-&gt;array[$this-&gt;position]); &#125;&#125;/** * Class MyAggregate 聚合容器 */class ConcreteAggregate implements IteratorAggregate&#123; public $property; /** * 添加属性 * * @param $property */ public function addProperty($property) &#123; $this-&gt;property[] = $property; &#125; public function getIterator() &#123; return new ConcreteIterator($this-&gt;property); &#125;&#125;// Class Client 客户端测试class Client&#123; public static function test() &#123; //创建一个容器 $concreteAggregate = new ConcreteAggregate(); // 添加属性 $concreteAggregate-&gt;addProperty('属性1'); // 添加属性 $concreteAggregate-&gt;addProperty('属性2'); //给容器创建迭代器 $iterator = $concreteAggregate-&gt;getIterator(); //遍历 while($iterator-&gt;valid()) &#123; $key = $iterator-&gt;key(); $value = $iterator-&gt;current(); echo '键: '.$key.' 值: '.$value.'&lt;hr&gt;'; $iterator-&gt;next(); &#125; &#125;&#125;Client:: test(); 和 工厂方法模式非常类似,并且做到了 ‘开闭原则’, 当你具体聚合类需要更换迭代器的时候, 不用修改客户端代码, 只用维护具体的聚合类即可! (工厂方法模式, 也类似, 当你的一个日志记录器需要修改, 只用修改具体的那个工厂即可, 当然, 由于是做修改而不是扩展, 所以还是符合’开闭’原则的) 小结 迭代器模式是一种使用频率非常高的设计模式, 通过引入迭代器可以将数据的遍历功能从聚合对象中分离出来, 聚合对象只负责存储数据, 而遍历数据由迭代器来完成。 由于很多编程语言的类库都已经实现了迭代器模式, 因此在实际开发中，我们只需要直接使用Java、C#等语言已定义好的迭代器即可, 迭代器已经成为我们操作聚合对象的基本工具之一。 迭代器模式的主要优点如下： 它支持以不同的方式遍历一个聚合对象, 在同一个聚合对象上可以定义多种遍历方式。在迭代器模式中只需要用一个不同的迭代器来替换原有迭代器即可改变遍历算法, 我们也可以自己定义迭代器的子类以支持新的遍历方式。 迭代器简化了聚合类。由于引入了迭代器，在原有的聚合对象中不需要再自行提供数据遍历等方法，这样可以简化聚合类的设计。 在迭代器模式中，由于引入了抽象层，增加新的聚合类和迭代器类都很方便，无须修改原有代码，满足“开闭原则”的要求。 迭代器模式的主要缺点如下: 由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。 抽象迭代器的设计难度较大，需要充分考虑到系统将来的扩展，例如JDK内置迭代器Iterator就无法实现逆向遍历，如果需要实现逆向遍历，只能通过其子类ListIterator等来实现，而ListIterator迭代器无法用于操作Set类型的聚合对象。在自定义迭代器时，创建一个考虑全面的抽象迭代器并不是件很容易的事情。 在以下情况下可以考虑使用迭代器模式: 访问一个聚合对象的内容而无须暴露它的内部表示。将聚合对象的访问与内部数据的存储分离，使得访问聚合对象时无须了解其内部实现细节。 需要为一个聚合对象提供多种遍历方式。 为遍历不同的聚合结构提供一个统一的接口，在该接口的实现类中为不同的聚合结构提供不同的遍历方式，而客户端可以一致性地操作该接口。 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"},{"name":"对象行为型","slug":"对象行为型","permalink":"http://blog.renyimin.com/tags/对象行为型/"}]},{"title":"OOP - Proxy","slug":"OOP/2016-05-28-OOP-12-Proxy","date":"2016-05-28T12:15:36.000Z","updated":"2018-02-04T03:52:09.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-12-Proxy/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-12-Proxy/","excerpt":"","text":"前言代理模式是常用的结构型设计模式之一, 当无法直接访问某个对象或访问某个对象存在困难时可以通过一个代理对象来间接访问, 为了保证客户端使用的透明性, 所访问的真实对象与代理对象需要实现相同的接口。根据代理模式的使用目的不同, 代理模式又可以分为多种类型, 例如 保护代理、远程代理、虚拟代理、缓冲代理等, 它们应用于不同的场合, 满足用户的不同需求。 代理模式是一种对象结构型模式。在代理模式中通过引入了一个新的代理对象, 代理对象在客户端对象和目标对象之间起到中介的作用, 它去掉客户不能看到的内容和服务或者增添客户需要的额外的新服务。 代理模式 代理模式的结构比较简单, 其核心是代理类, 为了让客户端能够一致性地对待真实对象 和 代理对象, 在代理模式中引入了抽象层, 代理模式结构如下: 代理模式包含如下三个角色: Subject(抽象主题角色): 它声明了真实主题和代理主题的共同接口, 这样一来在任何使用真实主题的地方都可以使用代理主题, 客户端通常需要针对抽象主题角色进行编程。 Proxy(代理主题角色): 它包含了对真实主题的引用, 从而可以在任何时候操作真实主题对象; 在代理主题角色中提供一个与真实主题角色相同的接口, 以便在任何时候都可以替代真实主题; 代理主题角色还可以控制对真实主题的使用, 负责在需要的时候创建和删除真实主题对象, 并对真实主题对象的使用加以约束。 通常, 在代理主题角色中, 客户端在调用所引用的真实主题操作之前或之后还需要执行其他操作, 而不仅仅是单纯调用真实主题对象中的操作。 RealSubject(真实主题角色): 它定义了代理角色所代表的真实对象, 在真实主题角色中实现了真实的业务操作, 客户端可以通过代理主题角色间接调用真实主题角色中定义的操作。 代码实现: 代理模式的结构图比较简单, 但是在真实的使用和实现过程中要复杂很多, 特别是代理类的设计和实现。 抽象主题类可以是接口、抽象类或具体类, 它声明了真实主题类和代理类的公共方法, 客户端针对抽象主题类编程, 一致性地对待真实主题和代理主题, 典型的抽象主题类代码如下: 1234abstract class Subject &#123; public function Request(); &#125; 真实主题类继承了抽象主题类, 提供了业务方法的具体实现, 其典型代码如下: 1234567class RealSubject implements Subject &#123; public function Request() &#123; //业务方法具体实现代码 &#125; &#125; 代理类也是抽象主题类的子类，它维持一个对真实主题对象的引用，调用在真实主题中实现的业务方法，在调用时可以在原有业务方法的基础上附加一些新的方法来对功能进行扩充或约束，最简单的代理类实现代码如下： 123456789101112131415161718192021class Proxy implements Subject &#123; private $realSubject = new RealSubject(); //维持一个对真实主题对象的引用 public function PreRequest() &#123; // &#125; public function Request() &#123; PreRequest(); $realSubject-&gt;Request(); //调用真实主题对象的方法 PostRequest(); &#125; public function PostRequest() &#123; // &#125; &#125; 但是, 在实际开发过程中, 代理类的实现比上述代码要复杂很多, 代理模式根据其目的和实现方式不同可分为很多种类, 其中常用的几种代理模式简要说明如下: 远程代理(Remote Proxy): 为一个位于不同的地址空间的对象提供一个本地的代理对象, 这个不同的地址空间可以是在同一台主机中, 也可是在另一台主机中, 远程代理又称为大使(Ambassador)。 虚拟代理(Virtual Proxy): 如果需要创建一个资源消耗较大的对象, 先创建一个消耗相对较小的对象来表示, 真实对象只在需要时才会被真正创建。 保护代理(Protect Proxy): 控制对一个对象的访问, 可以给不同的用户提供不同级别的使用权限。 缓冲代理(Cache Proxy): 为某一个目标操作的结果提供临时的存储空间, 以便多个客户端可以共享这些结果。 智能引用代理(Smart Reference Proxy): 当一个对象被引用时, 提供一些额外的操作, 例如将对象被调用的次数记录下来等。 在这些常用的代理模式中, 有些代理类的设计非常复杂, 例如远程代理类, 它封装了底层网络通信和对远程对象的调用, 其实现较为复杂。 应用实例 实例说明: 某软件公司承接了某信息咨询公司的收费商务信息查询系统的开发任务, 该系统的基本需求如下:(1)在进行商务信息查询之前用户需要通过身份验证, 只有合法用户才能够使用该查询系统;(2)在进行商务信息查询时系统需要记录查询日志, 以便根据查询次数收取查询费用。该软件公司开发人员已完成了商务信息查询模块的开发任务, 现希望能够以一种松耦合的方式向原有系统增加身份验证和日志记录功能, 客户端代码可以无区别地对待原始的商务信息查询模块和增加新功能之后的商务信息查询模块, 而且可能在将来还要在该信息查询模块中增加一些新的功能。试使用代理模式设计并实现该收费商务信息查询系统。 实例分析及类图通过分析, 可以采用一种间接访问的方式来实现该商务信息查询系统的设计, 在客户端对象和信息查询对象之间增加一个代理对象, 让代理对象来实现身份验证和日志记录等功能, 而无须直接对原有的商务信息查询对象进行修改, 如下图:在上图中, 客户端对象通过代理对象间接访问具有商务信息查询功能的真实对象, 在代理对象中除了调用真实对象的商务信息查询功能外, 还增加了身份验证和日志记录等功能。使用代理模式设计该商务信息查询系统, 结构图如下:图中: 业务类AccessValidator用于验证用户身份; 它提供方法Validate()来实现身份验证; 业务类Logger用于记录用户查询日志; 它提供方法Log()来保存日志; Searcher充当抽象主题角色; RealSearcher充当真实主题角色; ProxySearcher充当代理主题角色; 维持了对RealSearcher对象、AccessValidator对象和Logger对象的引用。 代码: 本实例是 保护代理 和 智能引用代理 的应用实例, 在代理类ProxySearcher中实现对真实主题类的权限控制和引用计数, 如果需要在访问真实主题时增加新的访问控制机制和新功能, 只需增加一个新的代理类, 再修改配置文件, 在客户端代码中使用新增代理类即可, 源代码无须修改, 符合开闭原则。 各种代理参考代理模式效果代理模式是常用的结构型设计模式之一, 它为对象的间接访问提供了一个解决方案, 可以对对象的访问进行控制。代理模式类型较多, 其中远程代理、虚拟代理、保护代理等在软件开发中应用非常广泛。 代理模式的共同优点如下： 能够协调调用者和被调用者, 在一定程度上降低了系统的耦合度; 客户端可以针对抽象主题角色进行编程, 增加和更换代理类无须修改源代码, 符合开闭原则, 系统具有较好的灵活性和可扩展性; 此外, 不同类型的代理模式也具有独特的优点, 例如: 远程代理为位于两个不同地址空间对象的访问提供了一种实现机制, 可以将一些消耗资源较多的对象和操作移至性能更好的计算机上, 提高系统的整体运行效率; 虚拟代理通过一个消耗资源较少的对象来代表一个消耗资源较多的对象, 可以在一定程度上节省系统的运行开销; 缓冲代理为某一个操作的结果提供临时的缓存存储空间, 以便在后续使用中能够共享这些结果, 优化系统性能, 缩短执行时间; 保护代理可以控制对一个对象的访问权限, 为不同用户提供不同级别的使用权限; 代理模式的主要缺点如下: 由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢，例如保护代理。 实现代理模式需要额外的工作，而且有些代理模式的实现过程较为复杂，例如远程代理。 模式适用场景代理模式的类型较多, 不同类型的代理模式有不同的优缺点, 它们应用于不同的场合: 当客户端对象需要访问远程主机中的对象时可以使用远程代理。 当需要用一个消耗资源较少的对象来代表一个消耗资源较多的对象，从而降低系统开销、缩短运行时间时可以使用虚拟代理，例如一个对象需要很长时间才能完成加载时。 当需要为某一个被频繁访问的操作结果提供一个临时存储空间，以供多个客户端共享访问这些结果时可以使用缓冲代理。通过使用缓冲代理，系统无须在客户端每一次访问时都重新执行操作，只需直接从临时缓冲区获取操作结果即可。 当需要控制对一个对象的访问，为不同用户提供不同级别的访问权限时可以使用保护代理。 当需要为一个对象的访问（引用）提供一些额外的操作时可以使用智能引用代理。 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"},{"name":"对象结构型","slug":"对象结构型","permalink":"http://blog.renyimin.com/tags/对象结构型/"}]},{"title":"OOP - Adapter","slug":"OOP/2016-05-28-OOP-11-Adapter","date":"2016-05-28T11:02:13.000Z","updated":"2018-02-04T03:51:22.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-11-Adapter/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-11-Adapter/","excerpt":"","text":"前言 我的笔记本电脑的工作电压是20V, 而我国的家庭用电是220V, 如何让20V的笔记本电脑能够在220V的电压下工作？答案是引入一个电源适配器(AC Adapter), 俗称充电器或变压器, 有了这个电源适配器, 生活用电和笔记本电脑即可兼容! 在软件开发中, 有时也存在类似这种不兼容的情况, 我们也可以像引入一个电源适配器一样引入一个称之为适配器的角色, 来协调这些存在不兼容的结构, 这种设计方案即为适配器模式。 引用玩具厂家的例子 一开始的和谐黑枣玩具公司专门生产玩具, 生产的玩具如狗,猫,狮子,鱼等动物, 每个玩具都可以进行’张嘴’与’闭嘴’操作, 分别调用 openMouth 与 closeMouth 方法。在这个时候, 我们很容易想到可以定义一个抽象玩具类Toy, 甚至是接口Toy, 这些都不是问题, 然后其他的具体玩具类去继承/实现父类, 实现父类的方法。到现在为止, 一切看起来都很正常, 都很和谐, 代码如下: 1234567891011121314151617181920212223242526272829303132abstract class Toy&#123; public abstract function openMouth(); public abstract function closeMouth();&#125;class Dog extends Toy&#123; public function openMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Dog open Mouth\\n\"; &#125;&#125;class Cat extends Toy&#123; public function openMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Cat open Mouth\\n\"; &#125;&#125; 平衡的破坏后来为了扩大业务, 现在黑枣玩具公司与红枣遥控公司合作, 红枣遥控公司可以使用遥控设备对动物进行嘴巴控制, 不过红枣遥控公司的遥控设备是调用的动物的doMouthOpen及doMouthClose方法。黑枣玩具公司的程序员现在必须要做的是对Toy系列类进行升级改造, 使Toy能调用doMouthOpen及doMouthClose方法。(又或者让红枣公司来适应自己, 但毕竟自己做的是偏底层,想做强做大)。 考虑实现的方法时, 我们很直接地想到, 你需要的话我再在我的父类子类里给你添加这么两个方法就好啦。于是, 代码现在可能会被改造成:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162abstract class Toy&#123; public abstract function openMouth(); public abstract function closeMouth(); //为红枣遥控公司控制接口增加doMouthOpen方法 public abstract function doMouthOpen(); //为红枣遥控公司控制接口增加doMouthClose方法 public abstract function doMouthClose();&#125;class Dog extends Toy&#123; public function openMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; //增加的方法(用来适配红枣遥控公司) public function doMouthOpen() &#123; $this-&gt;doMouthOpen(); &#125; //增加的方法 public function doMouthClose() &#123; $this-&gt;closeMouth(); &#125;&#125;class Cat extends Toy&#123; public function openMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; //增加的方法 public function doMouthOpen() &#123; $this-&gt;doMouthOpen(); &#125; //增加的方法 public function doMouthClose() &#123; $this-&gt;closeMouth(); &#125;&#125; 当你一次又一次在父类子类里面重复添加着这两个方法的时候，总会想着如此重复的工作，但是, 当有数百个子类的时候, 这样做下去你就会觉得自己很傻。(其实我经常当这样的傻子) 更加烦躁当你刚改完上面的代码, 黑枣玩具公司又要与绿枣遥控公司合作, 因为绿枣遥控公司遥控设备更便宜稳定。不过绿枣遥控公司的遥控设备是调用的动物的operMouth(type)方法来实现嘴巴控制。如果type为0则闭嘴,反之张嘴。这下好了, 程序员又得对Toy及其子类进行升级, 这样下去搁谁都不淡定了, 你的代码可能如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677abstract class Toy &#123; public abstract function openMouth(); public abstract function closeMouth(); //这是给之前红枣公司增加的适配方法 public abstract function doMouthOpen(); public abstract function doMouthClose(); //这又是为绿枣遥控公司控制接口增加的适配方法 public abstract function operateMouth($type = 0); &#125; class Dog extends Toy &#123; public function openMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; public function doMouthOpen() &#123; $this-&gt;doMouthOpen(); &#125; public function doMouthClose() &#123; $this-&gt;closeMouth(); &#125; public function operateMouth($type = 0) &#123; if ($type == 0) &#123; $this-&gt;closeMouth(); &#125; else &#123; $this-&gt;operateMouth(); &#125; &#125; &#125; class Cat extends Toy &#123; public function openMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; public function doMouthOpen() &#123; $this-&gt;doMouthOpen(); &#125; public function doMouthClose() &#123; $this-&gt;closeMouth(); &#125; public function operateMouth($type = 0) &#123; if ($type == 0) &#123; $this-&gt;closeMouth(); &#125; else &#123; $this-&gt;operateMouth(); &#125; &#125; &#125; 在这个时候, 程序员就必须要动脑子想办法了, 就算自己勤快, 万一哪天紫枣,青枣,黄枣,山枣这些遥控公司全来的时候, 忽略自己不断增多的工作量不说, 这个Toy类可是越来越大, 总有一天程序员不崩溃, 系统也会崩溃。 问题在出在哪里呢？ 像上面那样编写代码, 代码违反了’开-闭’原则。 我们现在面临的是: 新的接口方法要实现, 旧的接口(Toy抽象类)也不能动, 那就是得引入一个新的类–我们本文的主角–适配器。 适配器要完成的功能很明确, 引用现有接口的方法实现新的接口的方法。 适配器模式 适配器模式(Adapter): 将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 通用类图: 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?php abstract class Toy &#123; public abstract function openMouth(); public abstract function closeMouth(); &#125; class Dog extends Toy &#123; public function openMouth() &#123; echo \"Dog open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Dog close Mouth\\n\"; &#125; &#125; class Cat extends Toy &#123; public function openMouth() &#123; echo \"Cat open Mouth\\n\"; &#125; public function closeMouth() &#123; echo \"Cat close Mouth\\n\"; &#125; &#125; //目标角色:红枣遥控公司 interface RedTarget &#123; public function doMouthOpen(); public function doMouthClose(); &#125; //目标角色:绿枣遥控公司 interface GreenTarget &#123; public function operateMouth($type = 0); &#125; //类适配器角色:红枣遥控公司 class RedAdapter implements RedTarget &#123; private $adaptee; function __construct(Toy $adaptee) &#123; $this-&gt;adaptee = $adaptee; &#125; //委派调用Adaptee的sampleMethod1方法 public function doMouthOpen() &#123; $this-&gt;adaptee-&gt;openMouth(); &#125; public function doMouthClose() &#123; $this-&gt;adaptee-&gt;closeMouth(); &#125; &#125; //类适配器角色:绿枣遥控公司 class GreenAdapter implements GreenTarget &#123; private $adaptee; function __construct(Toy $adaptee) &#123; $this-&gt;adaptee = $adaptee; &#125; //委派调用Adaptee：GreenTarget的operateMouth方法 public function operateMouth($type = 0) &#123; if ($type) &#123; $this-&gt;adaptee-&gt;openMouth(); &#125; else &#123; $this-&gt;adaptee-&gt;closeMouth(); &#125; &#125; &#125; class testDriver &#123; public function run() &#123; //实例化一只狗玩具 $adaptee_dog = new Dog(); echo \"给狗套上红枣适配器\\n\"; $adapter_red = new RedAdapter($adaptee_dog); //张嘴 $adapter_red-&gt;doMouthOpen(); //闭嘴 $adapter_red-&gt;doMouthClose(); echo \"给狗套上绿枣适配器\\n\"; $adapter_green = new GreenAdapter($adaptee_dog); //张嘴 $adapter_green-&gt;operateMouth(1); //闭嘴 $adapter_green-&gt;operateMouth(0); &#125; &#125; $test = new testDriver(); $test-&gt;run(); 最后的结果就是,Toy类及其子类在不改变自身的情况下, 通过适配器实现了不同的接口。 小结将一个类的接口转换成客户希望的另外一个接口, 使原本因不兼容而不能在一起工作的那些类,可以在一起工作。 适配器模式核心思想：把对某些相似的类的操作转化为一个统一的’接口’(这里是比喻的说话)–适配器,或者比喻为一个’界面’统一或屏蔽了那些类的细节。适配器模式还构造了一种’机制’, 使’适配’的类可以很容易的增减, 而不用修改与适配器交互的代码, 符合’减少代码间耦合’的设计原则。 参考参考参考参考 适配器参考 适配器","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"},{"name":"结构型","slug":"结构型","permalink":"http://blog.renyimin.com/tags/结构型/"}]},{"title":"Laravel-Facade","slug":"OOP/2016-05-28-OOP-10-Laravel-Facade","date":"2016-05-28T07:13:05.000Z","updated":"2018-03-02T01:52:05.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-10-Laravel-Facade/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-10-Laravel-Facade/","excerpt":"","text":"代码追踪 以获取配置项的 Illuminate\\Support\\Facades\\Config 追踪Facade? 追踪到 Illuminate\\Support\\Facades 123456789101112131415161718192021public static function __callStatic($method, $args) &#123; $instance = static::getFacadeRoot(); if (! $instance) &#123; throw new RuntimeException(&apos;A facade root has not been set.&apos;); &#125; switch (count($args)) &#123; case 0: return $instance-&gt;$method(); case 1: return $instance-&gt;$method($args[0]); case 2: return $instance-&gt;$method($args[0], $args[1]); case 3: return $instance-&gt;$method($args[0], $args[1], $args[2]); case 4: return $instance-&gt;$method($args[0], $args[1], $args[2], $args[3]); default: return call_user_func_array([$instance, $method], $args); &#125; &#125; 这也就是为什么使用Facade的类可以直接使用静态调用的方式来调用方法, 正是Facade中的 __callStatic 方法生效了!","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Facade","slug":"OOP/2016-05-28-OOP-09-Facade","date":"2016-05-28T06:30:09.000Z","updated":"2018-02-04T03:48:51.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-09-Facade/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-09-Facade/","excerpt":"","text":"前言外观模式是一种使用频率非常高的结构型设计模式, 它通过引入一个外观角色来简化客户端与子系统之间的交互, 为复杂的子系统调用提供一个统一的入口, 降低子系统与客户端的耦合度, 且客户端调用非常方便; 在软件开发中, 有时候为了完成一项较为复杂的功能, 一个客户类需要和多个业务类交互, 由于涉及到的类比较多, 导致使用时代码较为复杂，此时，特别需要一个类似服务员一样的角色, 由它来负责和多个业务类进行交互, 而客户类只需与该类交互。 外观模式 外观模式中, 一个子系统的外部与其内部的通信, 通过一个统一的外观类进行, 外观类将客户类与子系统的内部复杂性分隔开, 使得客户类只需要与外观角色打交道, 而不需要与子系统内部的很多对象打交道。(在外观模式中, 那些需要交互的业务类被称为子系统(Subsystem)) 外观模式: 为子系统中的一组接口提供一个统一的入口。外观模式定义了一个高层接口, 这个接口使得这一子系统更加容易使用。 外观模式又称为门面模式, 它是一种对象结构型模式。 外观模式是迪米特法则的一种具体实现, 通过引入一个新的外观角色可以降低原有系统的复杂度, 同时降低客户类与子系统的耦合度。 迪米特法则(Law of Demeter)又叫作最少知道原则(Least Knowledge Principle 简写LKP)就是说一个对象应当对其他对象有尽可能少的了解, 不和陌生人说话。英文简写为: LoD. 外观模式没有一个一般化的类图描述, 通常使用下图来表示外观模式 外观模式包含如下两个角色： Facade(外观角色): 在客户端可以调用它的方法, 在外观角色中可以知道相关的(一个或者多个)子系统的功能和责任; 在正常情况下, 它将所有从客户端发来的请求委派到相应的子系统去, 传递给相应的子系统对象处理。 SubSystem(子系统角色): 在软件系统中可以有一个或者多个子系统角色, 每一个子系统可以不是一个单独的类, 而是一个类的集合, 它实现子系统的功能; 每一个子系统都可以被客户端直接调用, 或者被外观角色调用, 它处理由外观类传过来的请求; 子系统并不知道外观的存在, 对于子系统而言, 外观角色仅仅是另外一个客户端而已。 外观模式的主要目的在于降低系统的复杂程度, 在面向对象软件系统中, 类与类之间的关系越多, 表示系统中类之间的耦合度太大, 这样的系统在维护和修改时都缺乏灵活性, 因为一个类的改动会导致多个类发生变化, 而外观模式的引入在很大程度上降低了类与类之间的耦合关系。 引入外观模式之后, 增加新的子系统或者移除子系统都非常方便, 客户类无须进行修改(或者极少的修改), 只需要在外观类中增加或移除对子系统的引用即可。 从这一点来说, 外观模式在一定程度上并不符合开闭原则, 增加新的子系统需要对原有系统进行一定的修改, 虽然这个修改工作量不大。 未完待续 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Observer","slug":"OOP/2016-05-28-OOP-08-Observer","date":"2016-05-28T03:13:17.000Z","updated":"2018-02-04T03:48:22.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-08-Observer/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-08-Observer/","excerpt":"","text":"前言 在日常交通中, 当红灯亮起, 来往的汽车将停止; 而绿灯亮起, 汽车可以继续前行。在这个过程中, 交通信号灯是汽车的观察目标, 而汽车是观察者; 随着交通信号灯的变化, 汽车的行为也将随之而变化, 一盏交通信号灯可以指挥多辆汽车; 在软件系统中, 有些对象之间也存在类似交通信号灯和汽车之间的关系, 一个对象的状态或行为的变化, 将导致其他对象的状态或行为也发生改变, 它们之间将产生联动。 为了更好地描述对象之间存在的这种一对多(包括一对一)的联动, 观察者模式应运而生, 它定义了对象之间一种一对多的依赖关系, 让一个对象的改变能够影响其他对象。 多人联机对战游戏的设计引出观察者模式 Sunny软件公司欲开发一款多人联机对战游戏(类似魔兽世界、星际争霸等游戏), 在该游戏中, 多个玩家可以加入同一战队组成联盟, 当战队中某一成员受到敌人攻击时将给所有其他盟友发送通知, 盟友收到通知后将作出响应。 Sunny软件公司开发人员需要提供一个设计方案来实现战队成员之间的联动。Sunny软件公司开发人员通过对系统功能需求进行分析, 发现在该系统中战队成员之间的联动过程可以简单描述为：联盟成员受到攻击–&gt;发送通知给盟友–&gt;盟友作出响应。 如果按照上述思路来设计系统, 由于联盟成员在受到攻击时需要通知他的每一个盟友, 因此每个联盟成员都需要持有其他所有盟友的信息, 这将导致系统开销较大; 因此Sunny公司开发人员决定引入一个新的角色 —— 战队控制中心 ——来负责维护和管理每个战队所有成员的信息; 当一个联盟成员受到攻击时, 将向相应的战队控制中心发送求助信息, 战队控制中心再逐一通知每个盟友盟友再作出响应。 如下图: 在图中, 受攻击的联盟成员将与战队控制中心产生联动, 战队控制中心还将与其他盟友产生联动。 如何实现对象之间的联动? 如何让一个对象的状态或行为改变时, 依赖于它的对象能够得到通知并进行相应的处理？本篇所介绍的观察者模式将为对象之间的联动提供一个优秀的解决方案, 下面就让我们正式进入观察者模式的学习。 观察者模式 观察者模式是使用频率最高的设计模式之一, 它用于建立一种对象与对象之间的依赖关系, 一个对象发生改变时将自动通知其他对象, 其他对象将相应作出反应。 观察者模式(Observer Pattern)定义： 对象之间的一种一对多依赖关系, 使得每当一个对象状态发生改变时, 其相关依赖对象皆得到通知并被自动更新。 观察者模式的别名包括发布-订阅(Publish/Subscribe)模式、模型-视图(Model/View)模式, 源-监听器(Source/Listener)模式 或 从属者(Dependents)模式。 观察者模式是一种对象行为型模式。 观察者模式描述了如何建立对象与对象之间的依赖关系, 以及如何构造满足这种需求的系统。 123观察者模式包含`观察目标` 和 `观察者` 两类对象, 一个目标可以有任意数目的与之相依赖的观察者, 一旦观察目标的状态发生改变, 所有的观察者都将得到通知。作为对这个通知的响应, **每个观察者都将监视观察目标的状态**以使其状态与目标状态同步, 这种交互也称为发布-订阅(Publish-Subscribe)。观察目标是通知的发布者, 它发出通知时并不需要知道谁是它的观察者, 可以有任意数目的观察者订阅它并接收通知。 观察者模式结构中通常包括观察目标和观察者两个继承层次结构，其结构如下图所示： 在观察者模式结构图中包含如下几个角色: Subject(目标): 目标又称为主题, 它是指被观察的对象。 在目标中定义了一个观察者集合, 一个目标可以接受任意数量的观察者来观察目标自己, 目标提供一系列方法来增加和删除观察者对象, 同时它定义了通知方法 notify()。目标类可以是接口，也可以是抽象类或具体类。 ConcreteSubject(具体目标): 具体目标是目标类的子类; 通常它包含有经常发生改变的数据, 当它的状态发生改变时, 向它的各个观察者发出通知; Observer(观察者): 观察者将对观察目标的改变做出反应, 观察者一般定义为接口, 该接口声明了更新数据的方法 update(), 因此又称为抽象观察者。 ConcreteObserver(具体观察者): 在具体观察者中维护一个指向具体目标对象的引用, 它存储了本观察者的有关状态, 这些状态需要和具体目标的状态保持一致; 它实现了在抽象观察者Observer中定义的update()方法, 通常在实现时, 可以调用具体目标类的 attach() 方法将自己添加到目标类的集合中或通过 detach() 方法将自己从目标类的集合中删除。 下面通过代码来进行分析 先看被观察的目标 12345678910111213141516171819202122232425262728293031323334353637383940414243interface Subject&#123; //注册方法, 用于向观察者集合中增加一个观察者 public function attach(Observer $observer); //注销方法, 用于在观察者集合中删除一个观察者 public function detach(Observer $observer); //通知所有注册过的观察者对象 public function notify();&#125;class ConcreteSubject&#123; // 定义一个观察者集合,用于存放所有观察者对象 private $observers = []; //注册方法, 用于向观察者集合中增加一个观察者 public function attach(Observer $observer) &#123; return array_push($this-&gt;observers, $observer); &#125; //注销方法, 用于在观察者集合中删除一个观察者 public function detach(Observer $observer) &#123; $index = array_search($observer, $this-&gt;observers); if ($index === FALSE || ! array_key_exists($index, $this-&gt;observers)) &#123; return FALSE; &#125; unset($this-&gt;observers[$index]); return TRUE; &#125; //通知所有观察者 public function notify() &#123; if (!is_array($this-&gt;observers)) return false; foreach ($this-&gt;observers as $observer) $observer-&gt;update(); return true; &#125;&#125; 观察者 1234567891011121314151617181920//抽象观察者角色interface Observer &#123; // 更新方法 public function update();&#125;class ConcreteObserver implements Observer &#123; //观察者的名称 private $name; public function __construct($name) &#123; $this-&gt;name = $name; &#125; //更新方法 public function update() &#123; echo 'Observer ', $this-&gt;name, ' has notified.&lt;br /&gt;'; &#125;&#125; 客户端操作 1234567891011121314151617//实例化一个'观察目标'$subject = new ConcreteSubject();//实例化一个观察者$observer1 = new ConcreteObserver('lant01');//'观察目标'添加第一个观察者$subject-&gt;attach($observer1);//'观察目标' 通知 已经观察了目标自己的观察者$subject-&gt;notify();echo '添加第二个观察者后, 再次通知: &lt;br/&gt;';$observer2 = new ConcreteObserver('lant02');$subject-&gt;attach($observer2);$subject-&gt;notify();echo '删除第一个观察者后, 再次通知: &lt;br/&gt;';$subject-&gt;detach($observer1);$subject-&gt;notify(); 结果: 123456Observer lant01 has notified.添加第二个观察者后, 再次通知:Observer lant01 has notified.Observer lant02 has notified.删除第一个观察者后, 再次通知:Observer lant02 has notified. 复杂情况 – 违反”开放-封闭原则” 在有些更加复杂的情况下, 具体观察者类 ConcreteObserver 的 update() 方法在执行时需要使用到 具体目标类ConcreteSubject中的状态(属性) 因此在 ConcreteObserver 与 ConcreteSubject 之间有时候还存在关联或依赖关系, 在 ConcreteObserver 中定义一个 ConcreteSubject 实例, 通过该实例获取存储在 ConcreteSubject 中的状态。 如果ConcreteObserver的update()方法不需要使用到ConcreteSubject中的状态属性，则可以对观察者模式的标准结构进行简化, 在具体观察者ConcreteObserver和具体目标ConcreteSubject之间无须维持对象引用。 如果观察者和观察目标在具体层具有关联关系, 系统的扩展性将受到一定的影响, 增加新的具体目标类有时候需要修改原有观察者的代码, 在一定程度上违反了开闭原则, 但是如果原有观察者类无须关联新增的具体目标，则系统扩展性不受影响。 多人对战游戏的解决方案注意此时不太是简单的观察者模式(不是简单 观察者 对 观察目标), 因为每个玩家既可以发布求救请求, 又可以去救援别人(此时队员对应了战队中心之后, 战队中心又对应了队员)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;?php//战队控制中心interface TeamControlCenter&#123; //盟友加入战队 public function setPlayers(Observer $observer); //盟友退出战队 public function removePlayers(Observer $observer); //通知战队其他成员 public function notify($observerName);&#125;class ConcreteTeamControlCenter implements TeamControlCenter&#123; // 用于存放本战队所有成员 private $players = []; // 战队名称 private $teamName; public function __construct($teamName) &#123; $this-&gt;teamName = $teamName; &#125; //注册方法, 用于向观察者集合中增加一个观察者 public function setPlayers(Observer $player) &#123; if (array_push($this-&gt;players, $player)) echo $player-&gt;getPlayerName() . ' 成功加入 ' . $this-&gt;teamName . '战队!' , '&lt;br/&gt;'; &#125; //注销方法, 用于在观察者集合中删除一个观察者 public function removePlayers(Observer $player) &#123; $index = array_search($player, $this-&gt;players); if ($index === FALSE || ! array_key_exists($index, $this-&gt;players)) &#123; return FALSE; &#125; unset($this-&gt;players[$index]); echo $player-&gt;getPlayerName() . ' 成功退出 ' . $this-&gt;teamName . '战队!' , '&lt;br/&gt;'; &#125; //通知所有观察者 public function notify($playerName) &#123; if (!is_array($this-&gt;players)) return false; foreach ($this-&gt;players as $player) &#123; if ($player-&gt;getPlayerName() != $playerName) $player-&gt;helpOther(); &#125; &#125;&#125;//抽象观察者角色interface Observer&#123; // 接收队友求救 public function helpOther(); // 被攻击,发出求救 public function beAttacked(TeamControlCenter $concreteTeamControlCenter); // 获取名字 public function getPlayerName();&#125;//每个玩家都是观察者class Player implements Observer&#123; //玩家(观察者)的名称 private $playerName; public function __construct($playerName) &#123; $this-&gt;playerName = $playerName; &#125; public function getPlayerName() &#123; return $this-&gt;playerName; &#125; //遭受攻击, 发求救 public function beAttacked(TeamControlCenter $concreteTeamControlCenter) &#123; echo '我是 ' . $this-&gt;playerName, ', 我被攻击了&lt;br /&gt;'; // 被攻击的话, 要通知战队中心, 让战队中心通知其他队友 $concreteTeamControlCenter-&gt;notify($this-&gt;playerName); &#125; //支援盟友 public function helpOther() &#123; echo '撑住!!!' . $this-&gt;playerName, ' 来救你了&lt;br /&gt;'; &#125;&#125;//创建战队控制中心$langya_team = new ConcreteTeamControlCenter('狼牙');//创建玩家$player1 = new Player('金刚狼');$player2 = new Player('死侍');//玩家加入一个战队$langya_team-&gt;setPlayers($player1);$langya_team-&gt;setPlayers($player2);$player1-&gt;beAttacked($langya_team); 参考参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Prototype Pattern","slug":"OOP/2016-05-28-OOP-07-Prototype","date":"2016-05-28T02:50:21.000Z","updated":"2018-02-28T13:03:53.000Z","comments":true,"path":"2016/05/28/OOP/2016-05-28-OOP-07-Prototype/","link":"","permalink":"http://blog.renyimin.com/2016/05/28/OOP/2016-05-28-OOP-07-Prototype/","excerpt":"","text":"背景1.假设一套OA自动化办公系统在使用过程中, 由于某些岗位每周工作存在重复性, 工作周报内容都大同小异, 如下图工作周报示意图:这些周报只有一些小地方存在差异，但是现行系统每周默认创建的周报都是空白报表，用户只能通过重新输入或不断复制粘贴来填写重复的周报内容，极大降低了工作效率，浪费宝贵的时间。 2.开发人员通过对问题进行仔细分析, 决定按照如下思路对工作周报模块进行重新设计和实现： 除了允许用户创建新周报外，还允许用户将创建好的周报保存为模板; 用户在再次创建周报时，可以创建全新的周报，还可以选择合适的模板复制生成一份相同的周报，然后对新生成的周报根据实际情况进行修改，产生新的周报; 只要按照如上两个步骤进行处理，工作周报的创建效率将得以大大提高。这个过程类似日常使用电脑的两个基本操作：复制和粘贴(通过对已有对象的复制和粘贴，我们可以创建大量的相同对象)。 如何在一个面向对象系统中实现对象的复制和粘贴呢？本篇介绍的原型模式正为解决此类问题而诞生。 原型模式 在使用原型模式时，我们需要首先创建一个原型对象，再通过复制这个原型对象来创建更多同类型的对象。原型模式的定义如下： 使用原型实例指定创建对象的种类, 并且通过拷贝这些原型来创建新的对象, 原型模式是一种对象创建型模式; 原型模式的工作原理很简单: 将一个原型对象传给那个要发动创建的对象, 这个要发动创建的对象通过原型对象拷贝自己来实现创建过程。由于在软件系统中我们经常会遇到需要创建多个相同或者相似对象的情况，因此原型模式在真实开发中的使用频率还是非常高的。原型模式是一种“另类”的创建型模式，创建克隆对象的工厂就是原型类自身，工厂方法由克隆方法来实现。 需要注意的是通过克隆方法所创建的对象是全新的对象, 它们在内存中拥有新的地址, 通常对克隆所产生的对象进行修改对原型对象不会造成任何影响, 每一个克隆对象都是相互独立的。通过不同的方式修改可以得到一系列相似但不完全相同的对象。 PHP中的拷贝有 深拷贝 和 浅拷贝，先来分析一下这两者的区别 浅拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 而且对其他对象的引用仍然是指向原来的对象, 即浅拷贝只负责当前对象实例, 对引用的对象不做拷贝。 深拷贝: 被拷贝对象的所有变量都含有与原对象相同的值, 除了那些引用其他对象的变量, 那些引用其他对象的变量将指向一个被拷贝的新对象，而不再是原来那些被引用的对象。(即深拷贝把要拷贝的对象所引用的对象也拷贝了一次。而这种对被引用到的对象拷贝叫做间接拷贝)。 在决定以深拷贝的方式拷贝一个对象的时候, 必须决定对间接拷贝的对象是采取 浅拷贝 还是 深拷贝。序列化深拷贝: 利用序列化来做深拷贝, 把对象写到流里的过程是序列化的过程, 这一过程称为“冷冻”或“腌咸菜”, 反序列化对象的过程叫做“解冻”或“回鲜”。 原型模式结构图 Prototype(抽象原型类): 它是声明克隆方法的接口, 是所有具体原型类的父类, 可以是抽象类也可以是接口, 甚至还可以是具体实现类; ConcretePrototype(具体原型类): 它实现在抽象原型类中声明的克隆方法, 在克隆方法中返回自己的一个克隆对象; Client(客户类): 让一个原型对象克隆原型对象自身从而创建一个新的对象, 在客户类中只需要直接实例化或通过工厂方法等方式创建一个原型对象，再通过调用该对象的克隆方法即可得到多个相同的对象。由于客户类针对抽象原型类Prototype编程，因此用户可以根据需要选择具体原型类，系统具有较好的可扩展性，增加或更换具体原型类都很方便。原型模式的核心在于如何实现克隆方法 原型模式简单演示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?phpinterface Prototype&#123; //浅拷贝 public function shallowCopy(); //深拷贝 public function deepCopy();&#125;class ConcretePrototype implements Prototype&#123; private $_name; public function __construct($name) &#123; $this-&gt;_name = $name; &#125; public function setName($name) &#123; $this-&gt;_name = $name; &#125; public function getName() &#123; return $this-&gt;_name; &#125; //浅拷贝 public function shallowCopy() &#123; return clone $this; &#125; //深拷贝 public function deepCopy() &#123; $serialize_obj = serialize($this); $clone_obj = unserialize($serialize_obj); return $clone_obj; &#125;&#125;class Demo&#123; public $string;&#125;class UsePrototype&#123; public function shallow() &#123; $demo = new Demo(); $demo-&gt;string = \"susan\"; $object_shallow_first = new ConcretePrototype($demo); $object_shallow_second = $object_shallow_first-&gt;shallowCopy(); var_dump($object_shallow_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_shallow_second-&gt;getName()); echo '&lt;br/&gt;'; $demo-&gt;string = \"sacha\"; var_dump($object_shallow_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_shallow_second-&gt;getName()); echo '&lt;br/&gt;'; &#125; public function deep() &#123; $demo = new Demo(); $demo-&gt;string = \"Siri\"; $object_deep_first = new ConcretePrototype($demo); $object_deep_second = $object_deep_first-&gt;deepCopy(); var_dump($object_deep_first-&gt;getName()); echo '&lt;br/&gt;'; var_dump($object_deep_second-&gt;getName()); echo '&lt;br/&gt;'; $demo-&gt;string = \"Demo\"; var_dump($object_deep_first-&gt;getName()); echo '&lt;br/&gt;'; // 这里就可以看出是深拷贝了, 因为这里的demo对象没有跟着上面变, 而是拷贝时的间接深度拷贝的 var_dump($object_deep_second-&gt;getName()); echo '&lt;br/&gt;'; &#125;&#125;$up = new UsePrototype;$up-&gt;shallow();echo '&lt;hr&gt;';$up-&gt;deep(); 原型管理器的引入和实现原型管理器(Prototype Manager)是将多个原型对象存储在一个集合中, 供客户端使用;它是一个专门负责克隆对象的工厂, 其中定义了一个集合用于存储原型对象, 如果需要某个原型对象的一个克隆，可以通过复制集合中对应的原型对象来获得。在原型管理器中针对抽象原型类进行编程, 以便扩展。其结构如下图： 下面通过模拟一个简单的公文管理器来介绍原型管理器的设计与实现： Sunny软件公司在日常办公中有许多公文需要创建、递交和审批，例如《可行性分析报告》、《立项建议书》、《软件需求规格说明书》、《项目进展报告》等， 为了提高工作效率，在OA系统中为各类公文均创建了模板，用户可以通过这些模板快速创建新的公文，这些公文模板需要统一进行管理，系统根据用户请求的不同生成不同的新公文。 我们使用带原型管理器的原型模式实现公文管理器的设计，其结构如下图： 原型模式总结 原型模式作为一种快速创建大量相同或相似对象的方式, 在软件开发中应用较为广泛, 很多软件提供的复制(Ctrl + C)和粘贴(Ctrl + V)操作就是原型模式的典型应用; 主要优点 当创建新的对象实例较为复杂时, 使用原型模式可以简化对象的创建过程，通过复制一个已有实例可以提高新实例的创建效率。 扩展性较好，由于在原型模式中提供了抽象原型类，在客户端可以针对抽象原型类进行编程，而将具体原型类写在配置文件中，增加或减少产品类对原有系统都没有任何影响。 原型模式提供了简化的创建结构，工厂方法模式常常需要有一个与产品类等级结构相同的工厂等级结构，而原型模式就不需要这样，原型模式中产品的复制是通过封装在原型类中的克隆方法实现的，无须专门的工厂类来创建产品。 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份并将其状态保存起来，以便在需要的时候使用（如恢复到某一历史状态），可辅助实现撤销操作。 主要缺点 需要为每一个类配备一个克隆方法，而且该克隆方法位于一个类的内部，当对已有的类进行改造时，需要修改源代码，违背了“开闭原则”。 在实现深克隆时需要编写较为复杂的代码，而且当对象之间存在多重的嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来可能会比较麻烦。 适用场景(在以下情况下可以考虑使用原型模式) 创建新对象成本较大（如初始化需要占用较长的时间，占用太多的CPU资源或网络资源），新的对象可以通过原型模式对已有对象进行复制来获得，如果是相似对象，则可以对其成员变量稍作修改。 如果系统要保存对象的状态，而对象的状态变化很小，或者对象本身占用内存较少时，可以使用原型模式配合备忘录模式来实现。 需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的几个组合状态，通过复制原型对象得到新实例可能比使用构造函数创建一个新实例更加方便。","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Abstract  Factory","slug":"OOP/2016-05-27-OOP-06-Factory","date":"2016-05-27T08:20:16.000Z","updated":"2018-02-04T03:46:03.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-06-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-06-Factory/","excerpt":"","text":"前言工厂方法模式通过引入工厂等级结构, 解决了简单工厂模式中工厂类职责太重的问题;但由于工厂方法模式中的每个工厂只生产一类产品, 可能会导致系统中存在大量的工厂类, 势必会增加系统的开销;此时, 我们可以考虑将一些相关的产品组成一个 “产品族”, 由同一个工厂来统一生产, 这就是我们本文将要学习的抽象工厂模式的基本思想。 界面皮肤库的初始设计抽象工厂未完待续 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Factory Method","slug":"OOP/2016-05-27-OOP-05-Factory","date":"2016-05-27T07:37:21.000Z","updated":"2018-02-28T10:12:55.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-05-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-05-Factory/","excerpt":"","text":"前言之前已经了解到: 简单工厂模式虽然降低了一定的耦合度, 但仍然存在一个严重问题: 当系统中需要引入新产品时, 由于静态工厂方法是通过所传入参数的不同来创建不同的产品, 所以必定还要修改工厂类的源代码, 又将违背“开闭原则”。 日志记录器的设计假设系统需要做一个日志记录器(Logger), 可以通过多种方式记录日志(文件,数据库), 用户可以通过修改配置文件灵活地更换日志记录方式。日志记录器还可能会进行一些较为复杂的初始化工作(比如连接数据库, 读取配置文件等), 这就会导致代码较长, 如果将它们都写在构造函数中, 会导致构造函数庞大, 不利于代码的修改和维护; 简单工厂模式的设计 Sunny公司开发人员最初使用简单工厂模式对日志记录器进行了设计，初始结构如下图所示: 在图中, LoggerFactory充当创建日志记录器的工厂, 提供了工厂方法 createLogger() 用于创建日志记录器, Logger是抽象日志记录器接口, 其子类为具体日志记录器。其中, 工厂类LoggerFactory代码片段如下所示: 123456789101112131415161718192021222324252627//为了突出设计重点, 代码进行了简化, 省略了具体日志记录器类的初始化代码。//日志记录器工厂 class LoggerFactory &#123; //静态工厂方法 public static function createLogger($type) &#123; if($type == 'db')) &#123; //连接数据库, 代码省略 //... //创建数据库日志记录器对象 $logger = new DatabaseLogger(); //初始化数据库日志记录器，代码省略 //... return logger; &#125; else if ($type == 'file') &#123; //创建日志文件 //... //创建文件日志记录器对象 Logger logger = new FileLogger(); //初始化文件日志记录器，代码省略 //... return logger; &#125; else &#123; return null; &#125; &#125; &#125; 正如之前学习的简单工厂模式, 存在问题: 工厂类过于庞大，包含了大量的if…else…代码，导致维护和测试难度增大; 系统扩展不灵活，如果增加新类型的日志记录器，必须修改静态工厂方法的业务逻辑，违反了“开闭原则”。 如何解决这两个问题, 这就是本文所介绍的 工厂方法模式 的动机之一。 工厂方法 在简单工厂模式中只提供一个工厂类, 该工厂类处于对产品类进行实例化的中心位置, 它需要知道每一个产品对象的创建细节, 并决定何时实例化哪一个产品类。 简单工厂模式最大的缺点是: 当有新产品要加入到系统中时, 必须修改工厂类, 需要在其中加入必要的业务逻辑, 这违背了“开闭原则”。 此外, 在简单工厂模式中, 所有的产品都由同一个工厂创建, 工厂类职责较重, 业务逻辑较为复杂, 具体产品与工厂类之间的耦合度高, 严重影响了系统的灵活性和扩展性, 而工厂方法模式则可以很好地解决这一问题! 在工厂方法模式中, 不再提供一个统一的工厂类来创建所有的产品对象, 而是提供一个抽象工厂接口来声明一个抽象的工厂方法，而由其子类 具体工厂 来实现工厂方法, 你需要针对不同的产品提供不同的工厂; 在工厂方法模式结构图中包含如下几个角色: Factory（抽象工厂）：在抽象工厂类中, 声明了工厂方法(Factory Method)，用于返回一个产品。抽象工厂是工厂方法模式的核心，所有工厂类都必须实现该接口。 ConcreteFactory（具体工厂）：它是抽象工厂类的子类，实现了抽象工厂中定义的工厂方法，并可由客户端调用，返回一个具体产品类的实例。 Product（抽象产品）：它是定义产品的接口。 ConcreteProduct（具体产品）：它实现了抽象产品接口，某种类型的具体产品由专门的具体工厂创建, 具体工厂和具体产品之间一一对应。 与简单工厂模式相比, 工厂方法模式最重要的区别是引入了抽象工厂角色，抽象工厂可以是接口，也可以是抽象类，其典型代码如下所示： 123interface Factory &#123; public Product factoryMethod(); &#125; 在抽象工厂中声明了工厂方法, 具体产品对象的创建由其子类负责, 客户端针对抽象工厂编程, 可在运行时再指定具体工厂类, 具体工厂类实现了工厂方法, 不同的具体工厂可以创建不同的具体产品，其典型代码如下所示： 12345678class ConcreteFactory implements Factory &#123; public function factoryMethod() &#123; // 在实际使用时，具体工厂类在实现工厂方法时, 除了创建具体产品对象之外， // 还可以负责产品对象的初始化工作以及一些资源和环境配置工作，例如连接数据库、创建文件等。 return new ConcreteProduct(); &#125; &#125; 在客户端代码中，只需关心工厂类即可，**不同的具体工厂可以创建不同的产品**，典型的客户端类代码片段如下所示： 1234Factory factory; factory = new ConcreteFactory(); //可通过配置文件来实现 (通过配置来决定你要具体使用哪个具体的工厂类) Product product; product = factory-&gt;factoryMethod(); 可以通过配置文件来存储具体工厂类ConcreteFactory的类名，更换新的具体工厂时无须修改源代码，系统扩展更为方便。 工厂方法模式中的工厂方法能否为静态方法？为什么？ 貌似不太可以, 因为静态方法是类的, 不是对象的! 完整实现参考 http://blog.csdn.net/lovelion/article/details/9307137 隐藏工厂方法是参考 http://blog.csdn.net/lovelion/article/details/9307561 工厂方法模式是简单工厂模式的延伸, 它继承了简单工厂模式的优点, 同时还弥补了简单工厂模式的不足。工厂方法模式是使用频率最高的设计模式之一, 是很多开源框架和API类库的核心模式。 主要优点 在工厂方法模式中，具体的工厂方法用来创建客户所需要的产品，同时还向客户隐藏了哪种具体产品类将被实例化这一细节，用户只需要关心所需产品对应的具体工厂类是哪个，无须关心创建细节，甚至无须知道具体产品类的类名; 基于工厂角色和产品角色的多态性设计是工厂方法模式的关键。它能够让工厂可以自主确定创建何种产品对象，而如何创建这个对象的细节则完全封装在具体工厂内部。工厂方法模式之所以又被称为多态工厂模式，就正是因为所有的具体工厂类都具有同一抽象父类; 使用工厂方法模式的另一个优点是在系统中加入新产品时，无须修改抽象工厂和抽象产品提供的接口，无须修改客户端，也无须修改其他的具体工厂和具体产品，而只要添加一个具体工厂和具体产品就可以了，这样，系统的可扩展性也就变得非常好，完全符合“开闭原则”。 主要缺点 在添加新产品时，需要编写新的具体产品类，而且还要提供与之对应的具体工厂类，系统中类的个数将成对增加，在一定程度上增加了系统的复杂度，有更多的类需要运行，会给系统带来一些额外的开销; 由于考虑到系统的可扩展性，需要引入抽象层，在客户端代码中均使用抽象层进行定义，增加了系统的抽象性和理解难度，且在实现时可能需要用到反射等技术，增加了系统的实现难度。 参考参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Simple Factory","slug":"OOP/2016-05-27-OOP-04-Factory","date":"2016-05-27T07:08:11.000Z","updated":"2018-02-28T09:48:40.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-04-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-04-Factory/","excerpt":"","text":"工厂模式是最常用的一类创建型设计模式, 通常所说的工厂模式是指工厂方法模式, 它也是使用频率最高的工厂模式。简单工厂模式是工厂方法模式的”小弟”, 它不属于 GoF23种设计模式, 但在软件开发中应用也较为频繁, 通常将它作为学习其他工厂模式的入门。此外，工厂方法模式还有一位”大哥” —— 抽象工厂模式。这三种工厂模式各具特色, 难度也逐个加大, 在软件开发中它们都得到了广泛的应用, 成为面向对象软件中常用的创建对象的工具。 前言 假设公司开发的CRM系统可以显示饼状图的效果, 原始设计方案如下图: 123456789101112131415161718192021222324class Client&#123; public $chartObject = null; public function __construct($type) &#123; switch ($chartType) &#123; case 'pie' : $this-&gt;chartObject = new PieChart(); break; case 'bar' : $this-&gt;chartObject = new BarChart(); break; default: //TODO break; &#125; &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125;&#125; 客户端代码通过调用 ‘Client类’ 的构造函数来创建图表对象，根据参数 ‘type’ 可以得到不同类型的图表，然后再调用show()方法来显示相应的图表。 传统设计存在问题不难看出，Client类是一个 “巨大的” 类, 在该类的设计中存在如下几个问题: Client类中包含很多 “if…else…” / “switch…case…”代码块，整个类的代码相当冗长, 阅读难度、维护难度和测试难度也越大, 而且大量条件语句的存在还将影响系统的性能，程序在执行过程中需要做大量的条件判断; Client类的职责过重, 它将各种图表对象的创建和使用集中在一个类中实现, 违反了“单一职责原则”, 不利于类的重用和维护; 当需要增加新类型的图表时，必须修改Client类的源代码，违反了’开闭原则’; 客户端只能通过new关键字来直接创建图像对象, 图像类与客户端Client类耦合度较高 (比如一旦类的名字发生变更, 你也必须修改Client代码的源代码); 客户端在创建Chart对象之前可能还需要进行大量初始化设置, 例如设置柱状图的颜色、高度等, 如果在Client类的构造函数中没有提供一个默认设置, 那就只能由客户端来完成初始设置，这些代码在每次创建图像对象时都会出现, 导致代码的重复。 面对一个如此巨大、职责如此重，且与客户端代码耦合度非常高的类，我们应该怎么办？接下来介绍的 简单工厂模式 将在 一定程度上 解决上述问题。 简单工厂模式 为了将图像对象的创建和使用分离, 使用简单工厂模式对图表库进行重构, 重构后的结构如下图所示： 在图中, Chart接口充当抽象产品类, 其子类PieChart和BarChart充当具体产品类, ChartFactory充当工厂类。 现在, 我们使用工厂类的 静态工厂方法 来创建产品对象, 如果需要更换产品, 虽然也需要更改Client源码, 但是只需修改传递给静态工厂方法中的参数即可, 例如将柱状图改为饼状图, 只需将代码 $chartObject = ChartFactory::getChart(&quot;bar&quot;); 改为：$chartObject = ChartFactory::getChart(&quot;pie&quot;); 改进 现在你会发现: 在创建具体图像对象时, 每更换一个图像对象, 都需要修改客户端代码中静态工厂方法的参数(虽然修改都很小), 这对于客户端而言, 还是违反了“开闭原则”; 有没有一种方法能够在不修改客户端代码的前提下更换具体产品对象呢？答案是肯定的，下面将介绍一种常用的实现方式: 可以将静态工厂方法的参数放到配置文件中, 在配置文件中配置即可, 这样客户端代码如下所示：由$chartObject = ChartFactory::getChart(&quot;bar&quot;);改为：$type = Config::get('chartType'); $chartObject = ChartFactory::getChart($type); 简单工厂模式总结 简单工厂模式提供了专门的工厂类用于创建对象, 将对象的创建和对象的使用分离开, 它作为一种最简单的工厂模式在软件开发中得到了较为广泛的应用; 主要优点 工厂类包含必要的判断逻辑, 可以决定在什么时候创建哪一个产品类的实例, 客户端可以免除直接创建产品对象的职责, 而仅仅“消费”产品, 简单工厂模式实现了对象创建和使用的分离; 客户端无须知道所创建的具体产品类的类名，只需要知道创建具体产品类所需要对应的参数即可, 对于一些复杂的类名, 通过简单工厂模式可以在一定程度减少使用者的记忆量; 通过引入配置文件, 可以在不修改任何客户端代码的情况下更换和增加新的具体产品类, 在一定程度上提高了系统的灵活性, 做到了 一定程度的”开放-封闭原则”, 下面会看到其实工厂类部分还是违反的。 主要缺点 由于工厂类集中了所有产品的创建逻辑, 职责过重, 一旦不能正常工作, 整个系统都要受到影响; 使用简单工厂模式势, 势必会增加系统中类的个数(引入了新的工厂类), 增加了系统的复杂度和理解难度; 系统扩展困难，一旦添加新产品就不得不修改工厂逻辑, 在产品类型较多时, 有可能造成工厂逻辑过于复杂, 不利于系统的扩展和维护。(此处还是违反“开放-封闭原则”) 简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。 适用场景, 在以下情况下可以考虑使用简单工厂模式: 工厂类负责创建的对象比较少, 由于创建的对象较少, 不会造成工厂方法中的业务逻辑太过复杂。 客户端只知道传入工厂类的参数, 对于如何创建对象并不关心。 总之, 仍未完全做到 “开放-封闭原则” 参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - 为什么要引入工厂类?","slug":"OOP/2016-05-27-OOP-03-Why-Factory","date":"2016-05-27T06:33:19.000Z","updated":"2018-02-28T09:46:37.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-03-Why-Factory/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-03-Why-Factory/","excerpt":"","text":"前言工厂模式(包括简单工厂模式、工厂方法模式 和 抽象工厂模式 )到底有什么用? 很多时候通过反射机制就可以很灵活地创建对象, 为毛还要工厂？ 对象的创建 通常有以下几种创建对象的方式： 使用new关键字直接创建对象; 通过反射机制创建对象; 通过clone()方法创建对象； 通过工厂类创建对象; 在客户端代码中直接使用new关键字是最简单的一种创建对象的方式，但是它的灵活性较差，下面通过一个简单的示例来加以说明 假设公司开发的CRM系统可以显示饼状图的效果, 原始设计方案如下图: 1234567891011121314151617181920212223242526272829303132&lt;?php class PieChart &#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125; &#125; class BarChart &#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125; &#125; //Client class Client &#123; public $chartObject = null; public function __construct() &#123; $this-&gt;chartObject = new PieChart(); &#125; public function show() &#123; $this-&gt;chartObject-&gt;display(); &#125; &#125; 在’client’类的构造函数中创建了’PieChart’类型的对象，并在’show’方法中调用了’chartObject’对象的’display()’方法, 这段代码看上去并没有什么问题; 下面我们来分析一下’Client’和’PieChart’之间的关系: ‘Client’类负责创建了一个’PieChart’类的对象, 并使用其方法’display()’来完成相应的业务处理; 也就是说’Client’即负责对象的创建, 又负责对象的使用, 创建对象和使用对象的职责耦合在一起; 同时, 这样的设计会导致一个很严重的问题: 如果在 ‘Client’ 中希望能够使用另一个种类型的图像方案, 比如使用柱状图’BarChart’类的对象，那就必须修改’Client’类的源代码, 违反了”开闭原则”。 引出工厂类最常用的一种解决方法是将 ‘chartObject’ 对象的创建职责从 ‘Client’ 类中移除, 在 ‘Client’ 类之外创建对象, 那么谁来负责创建 ‘chartObject’ 对象呢?答案是：工厂类; 通过引入工厂类, 客户类就不会再涉及对象的创建(只是对对象进行使用), 而创建对象的工厂类自然也只是负责创建对象(也不会涉及对象的使用)。引入工厂类 ChartFactory 之后的结构如下图所示: 工厂类的引入将降低维护工作量： 如果图像类的构造函数发生变更, 或者需要添加或移除不同的图像类，你只要去维护 ChartFactory 的代码, 可能就不会影响到’Client’的代码; 而且如果 Chart 抽象接口发生改变, 例如添加、移除方法或改变方法名, 只需要修改 Client, 不会给 ChartFactory 带来任何影响; 在所有的工厂模式中, 我们都强调一点: 两个类A和B之间的关系应该仅仅是 A创建B 或是 A使用B, 而不能两种关系都有。将对象的创建和使用分离, 也使得系统更加符合 ‘单一职责原则’, 有利于对功能的复用和系统的维护; 此外, 将对象的创建和使用分离还有一个好处: 防止用来实例化一个类的代码在多个类中到处都是, 可以将有关创建的代码搬移到一个工厂类中。因为有时候我们创建一个对象不只是简单调用其构造函数, 还需要设置一些参数, 可能还需要配置环境,如果将这些代码散落在每一个创建对象的客户类中, 势必会出现代码重复、创建蔓延的问题, 而这些客户类其实无须承担对象的创建工作,它们只需使用已创建好的对象就可以了。此时, 可以引入工厂类来封装对象的创建逻辑和客户代码的实例化/配置选项。 参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Singleton-02","slug":"OOP/2016-05-27-OOP-02-Singleton","date":"2016-05-27T03:41:13.000Z","updated":"2018-02-04T03:44:23.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-02-Singleton/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-02-Singleton/","excerpt":"","text":"前言之前已经对单例模式有了比较简单的认识接下来通过一些资料在系统了解一下单例模式; 单例模式动机 对于一个软件系统的某些类而言, 我们无须创建多个实例。 举个大家都熟知的例子 —— Windows任务管理器, 当你在Windows的”任务栏”的右键弹出菜单上多次点击”启动任务管理器”。通常情况下，无论我们启动任务管理多少次，Windows系统始终只能弹出一个任务管理器窗口，也就是说在一个Windows系统中，任务管理器存在唯一性。为什么要这样设计呢？我们可以从以下两个方面来分析：其一，如果能弹出多个窗口，且这些窗口的内容完全一致，全部是重复对象，这势必会浪费系统资源，任务管理器需要获取系统运行时的诸多信息，这些信息的获取需要消耗一定的系统资源，包括CPU资源及内存资源等，浪费是可耻的，而且根本没有必要显示多个内容完全相同的窗口;其二，如果弹出的多个窗口内容不一致，问题就更加严重了，这意味着在某一瞬间系统资源使用情况和进程、服务等信息存在多个状态，例如任务管理器窗口A显示“CPU使用率”为10%，窗口B显示“CPU使用率”为15%，到底哪个才是真实的呢？这纯属”调戏”用户，给用户带来误解，更不可取。由此可见，确保Windows任务管理器在系统中有且仅有一个非常重要。 回到实际开发中, 我们也经常遇到类似的情况, 为了节约系统资源, 有时需要确保系统中某个类只有唯一一个实例, 当这个唯一实例创建成功之后, 我们无法再创建一个同类型的其他对象, 所有的操作都只能基于这个唯一实例。 为了确保对象的唯一性, 我们可以通过单例模式来实现, 这就是单例模式的动机所在。 参考一个负载均衡器的例子 Sunny软件公司承接了一个服务器负载均衡(Load Balance)软件的开发工作，该软件运行在一台负载均衡服务器上，可以将并发访问和数据流量分发到服务器集群中的多台设备上进行并发处理，提高系统的整体处理能力，缩短响应时间。由于集群中的服务器需要动态删减，且客户端请求需要统一分发，因此需要确保负载均衡器的唯一性，只能有一个负载均衡器来负责服务器的管理和请求的分发，否则将会带来服务器状态的不一致以及请求分配冲突等问题。如何确保负载均衡器的唯一性是该软件成功的关键。 Sunny公司开发人员通过分析和权衡，决定使用单例模式来设计该负载均衡器, 将负载均衡器LoadBalancer设计为单例类, 其中包含一个存储服务器信息的集合serverList, 每次在serverList中随机选择一台服务器来响应客户端的请求,实现代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?phpclass LoadBalancer&#123; private static $instance = null; private $serverList = array(); private function __construct() &#123; &#125; private function __clone() &#123; &#125; public static function getInstance() &#123; if (null === self::$instance) &#123; self::$instance = new self(); &#125; return self::$instance; &#125; /** * 增加一台服务器 */ public function addServer($server) &#123; $this-&gt;serverList[] = $server; &#125; /** * 减少一台宕机的服务器 */ public function removeServer($key) &#123; unset($this-&gt;serverList[$key]); &#125; /** * 随机获取一台服务器 */ public function getServer() &#123; $random = mt_rand(0, count($this-&gt;serverList)-1); return $this-&gt;serverList[$random]; &#125;&#125;echo '&lt;pre/&gt;';$loadBalancer1 = LoadBalancer::getInstance();$loadBalancer2 = LoadBalancer::getInstance();var_dump($loadBalancer1 === $loadBalancer2);$loadBalancer1-&gt;addServer(\"Server 1\");$loadBalancer1-&gt;addServer(\"Server 2\");$loadBalancer2-&gt;addServer(\"Server 3\");$loadBalancer2-&gt;addServer(\"Server 4\");//模拟客户端请求的分发for ($i=0; $i&lt;10; $i++) &#123; $server = $loadBalancer1-&gt;getServer(); var_dump(\"分发请求至服务器： \" . $server);&#125; 结果: 1234567891011bool(true)string(36) \"分发请求至服务器： Server 1\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 4\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 3\"string(36) \"分发请求至服务器： Server 1\"string(36) \"分发请求至服务器： Server 4\" 虽然创建了四个LoadBalancer对象，但是它们实际上是同一个对象，因此，通过使用单例模式可以确保LoadBalancer对象的唯一性。 饿汉式单例与懒汉式单例的讨论涉及到多线程未完待续… 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOP - Singleton-01","slug":"OOP/2016-05-27-OOP-01-Singleton","date":"2016-05-27T03:13:34.000Z","updated":"2018-02-04T03:44:10.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-01-Singleton/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-01-Singleton/","excerpt":"","text":"常见场景举例 框架底层的 数据库模型层 就可以使用单例 ThinkPHP3.2中, 虽然 数据库模型层 使用了单例模式, 但并非传统意义上所谓 三公一私 的单例: 1234567891011121314TP3.2创建数据库模型实例的过程大概为: Think\\Model -&gt; Think\\Db -&gt; Think\\Db\\Driver\\Mysql -&gt; Think\\Db\\DriverD()/M() 方法都可以调用 Think\\Model 这个模型类虽然 Think\\Model 层并未做到单例, 即 new Model(...) 实例出的对象为非单例, 但其通过调用下层 Think\\Db 的 getInstance(), 然后简单结合一个 数据库对象池$_db(注册树模式) 来保证底层各不同数据库对象的单例性; Think\\Db 的 getInstance() 通过 数据库连接池$instance 保证了下层 Think\\Db\\Driver\\Mysql 数据库连接实例的单例性而 Think\\Db 其内部与底层沟通的方法全是static型, 用户在顶层控制器中直接new也没意义只有在顶层控制器直接 new `Think\\Db\\Driver\\Mysql` 你会获得不同的数据库连接实例, 但一般也不会直接new底层!最下层 Think\\Db\\Driver 为抽象层 所以实现单例未必需要严格按照传统的规则来, 有很多变体都可以保证实现单例; 日志类TP3中的Log类比较简单, 作为基础类, 直接各方法为静态, 很简单就做到了要想使用, 就必然是单例, 基本上你new也没什么用!…未完待续 项目的配置类…未完待续 优点 提供了对唯一实例的受控访问 由于在系统内存中只存在一个对象，因此可以节约系统资源，对于一些需要频繁创建和销毁的对象单例模式无疑可以提高系统的性能缺点 PHP语言是一种解释型的脚本语言, 这种运行机制使得每个PHP页面被解释执行后, 所有的相关资源都会被回收。 在PHP中, 所有的变量无论是全局变量还是类的静态成员, 都是页面级的, 每次页面被执行时, 都会重新建立新的对象, 所以PHP单例模式貌似只是针对单次页面级请求时出现多个应用场景并需要共享同一对象资源时是有意义的; 几个基本注意事项 通常我们都是遵循正常的”三私一公”来写单例, 但是可以看到如下代码会因为序列化,反序列化而导致单例出问题 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?phpclass Singleton&#123; private static $instance = null; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public static function getInstance() &#123; if (null === self::$instance) self::$instance = new self(); return self::$instance; &#125;&#125;echo &apos;&lt;pre/&gt;&apos;;$test_1 = Singleton::getInstance();$test_2 = Singleton::getInstance();var_dump($test_1); //实例1var_dump($test_2); //实例1var_dump($test_1 === $test_2); // trueecho &apos;unserialize, serialize:---------------------------&lt;br/&gt;&apos;;$test_1 = unserialize(serialize($test_1));var_dump($test_1); //实例2var_dump(Singleton::getInstance()); //实例1var_dump( Singleton::getInstance() === $test_1); //falseecho &apos;unserialize, serialize:---------------------------&lt;br/&gt;&apos;;$test_3 = Singleton::getInstance();var_dump($test_3); //实例1$test_3 = unserialize(serialize($test_3));var_dump($test_3); //实例3var_dump(Singleton::getInstance()); //实例1var_dump( Singleton::getInstance() === $test_3); //false 鸟哥博文其实并不能完全解决, 看下面例子: 虽然每次反序列化后的所有实例都一致, 但是一旦碰到再次反序列化, 还是会出问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?phpclass Singleton&#123; private static $instance = null; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public function __wakeup() &#123; self::$instance = $this; &#125; public static function getInstance() &#123; if (null === self::$instance) self::$instance = new self(); return self::$instance; &#125; public function __destruct() &#123; &#125;&#125;echo '&lt;pre/&gt;';$test_1 = Singleton::getInstance(); $test_2 = Singleton::getInstance(); var_dump($test_1); //实例1var_dump($test_2); //实例1var_dump($test_1 === $test_2); //trueecho 'unserialize, serialize:---------------------------&lt;br/&gt;';$test_1 = unserialize(serialize($test_1));var_dump($test_1); //实例2var_dump(Singleton::getInstance()); //实例2var_dump( Singleton::getInstance() === $test_1); //trueecho 'unserialize, serialize:---------------------------&lt;br/&gt;';$test_3 = Singleton::getInstance();var_dump($test_3); //实例2$test_3 = unserialize(serialize($test_3));var_dump($test_3); //实例3var_dump(Singleton::getInstance()); //实例3var_dump( Singleton::getInstance() === $test_3); //true 可以看到还是出现了多个不同的实例!!! 博文中有个评论比较有意思, 可以看一下: 另外, 单例模式出现继承关系时, 需要注意PHP的 self 和 static 关键字的区别 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?phpclass Singleton&#123; protected static $instance = null; protected function __construct() &#123; &#125; protected function __clone() &#123; &#125; protected function __wakeup() &#123; static::$instance = $this; &#125; public static function getInstance() &#123; if (null === static::$instance) static::$instance = new static(); return static::$instance; &#125;&#125;echo &apos;&lt;pre/&gt;&apos;;$test_1 = Singleton::getInstance();$test_2 = Singleton::getInstance();var_dump($test_1);var_dump($test_2);var_dump($test_1 === $test_2);class Log extends Singleton&#123; // 注意: 每个继承单例的子类, 必须要做清空, 否则所有的实例都是上面的实例结果 protected static $instance = null; public function write() &#123; echo &apos;success write something&apos;; &#125;&#125;class Model extends Singleton&#123; // 注意: 每个继承单例的子类, 必须要做清空, 否则所有的实例都是上面的实例结果 protected static $instance = null; public function select() &#123; echo &apos;success select something&apos;; &#125;&#125;$log_1 = Log::getInstance();$log_2 = Log::getInstance();var_dump($log_1);var_dump($log_2);var_dump($log_1 === $log_2);$model_1 = Model::getInstance();$model_2 = Model::getInstance();var_dump($model_1);var_dump($model_2);var_dump($model_1 === $model_2);var_dump($model_1 === $test_2); // false","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]},{"title":"OOD-S.O.L.I.D","slug":"OOP/2016-05-27-OOP-00-SOLID","date":"2016-05-27T02:07:12.000Z","updated":"2018-03-01T02:45:10.000Z","comments":true,"path":"2016/05/27/OOP/2016-05-27-OOP-00-SOLID/","link":"","permalink":"http://blog.renyimin.com/2016/05/27/OOP/2016-05-27-OOP-00-SOLID/","excerpt":"","text":"The Single Responsibility Principle（单一职责原则 SRP）高内聚低耦合1.单一职责原则是最简单的面向对象设计原则, 用于控制类的粒度大小。此原则的核心就是解耦和增强内聚性 2.单一职责原则定义为: 一个类或者模块应该有且只有一个被改变的原因。 一个类不能太“累”, 如果一个类承担的职责过多(耦合度就越大), 它被复用的可能性就越小。 一个职责的变化可能会影响其他的职责, 这种耦合会导致脆弱的设计, 当发生变化时, 设计会遭受到意想不到的破坏(因此要将这些职责进行分离，将不同的职责封装在不同的类中)。 而如果想要避免这种现象的发生, 就要尽可能的遵守单一职责原则。。 3.遵循单一职责的优点有: 可以降低类的复杂度, 一个类只负责一项职责, 其逻辑肯定要比负责多项职责简单的多; 提高类的可读性, 提高系统的可维护性; 变更引起的风险降低, 变更是必然的, 如果单一职责原则遵守的好, 当修改一个功能时, 可以显著降低对其他功能的影响; The Open/Closed Principle（开放封闭原则OCP）对抽象编程, 而不对具体编程1.开放-封闭原则: 一个软件实体应当对扩展开放, 对修改关闭(即软件实体应尽量在不修改原有代码的情况下进行扩展)。 当软件系统需要面对新的需求时，我们应该尽量保证系统的设计框架是稳定的。如果一个软件设计符合开闭原则，那么可以非常方便地对系统进行扩展，而且在扩展时无须修改现有代码。随着软件规模越来越大，软件寿命越来越长，软件维护成本越来越高，设计满足开闭原则的软件系统也变得越来越重要。 2.为了满足开闭原则，需要对系统进行抽象化设计，抽象化是开闭原则的关键。 3.例子, 假设公司开发的CRM系统可以显示各种类型的图表, 如 饼状图 和 柱状图 等, 为了支持多种图表显示方式, 原始设计方案如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?phpclass PieChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public $chartObject = null; public function __construct() &#123; //TODO &#125; public function display($chartType) &#123; switch ($chartType) &#123; case 'pie' : $piechart = new PieChart(); $piechart-&gt;display(); break; case 'bar' : $barchart = new BarChart(); $barchart-&gt;display(); break; default: //TODO break; &#125; &#125;&#125; 2.问题: 现在如果需要增加一个折线图LineChart, 则需要修改ChartDisplay类的display()方法的源代码, 增加新的判断逻辑, 违反了开闭原则!! 3.现对该系统进行重构, 使之符合开闭原则 引入抽象图表类AbstractChart, 并且让ChartDisplay针对抽象图表类进行编程(依赖抽象), 再在ChartDisplay的display()方法中调用具体chart对象的display()方法显示图表。 4.接下来, 只需要将LineChart也作为AbstractChart的子类, 在客户端向ChartDisplay中注入一个LineChart对象即可, 无须修改现有类库的源代码 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?phpabstract class AbstractChart&#123; protected function display() &#123; &#125;&#125;class PieChart extends AbstractChart&#123; public function display() &#123; echo 'piechart', '&lt;br/&gt;'; &#125;&#125;class BarChart extends AbstractChart&#123; public function display() &#123; echo 'barchart', '&lt;br/&gt;'; &#125;&#125;class ChartDisplay&#123; public function __construct() &#123; //TODO &#125; public function display(AbstractChart $chart) &#123; $chart-&gt;display(); &#125;&#125;$cd = new ChartDisplay();$cd-&gt;display(new PieChart());$cd-&gt;display(new BarChart()); The Liskov Substitution Principle（里氏替换原则LSP） 所有引用基类（父类）的地方必须能透明地使用其子类对象。 子类可以实现父类的抽象方法, 但是不能覆盖父类的非抽象方法, 也就是子类可以扩展父类的功能, 但是不能改变父类原有的功能; 当子类覆盖或实现父类的方法时, 方法的前置条件(即方法的形参)要比父类方法的输入参数更宽松。(PHP是弱类型语言) 当子类的方法实现父类的抽象方法时, 方法的后置条件(即方法的返回值)要比父类更严格。(PHP是弱类型语言) 貌似主要就是说, 如果依赖的类可能日后会有扩展的话, 你最好设计一个抽象父类或接口, 子类继承、实现父类;里氏代换原则是实现开闭原则的重要方式之一, 由于使用基类对象的地方都可以使用子类对象, 因此在程序中尽量使用基类类型来对对象进行定义, 而在运行时再确定其子类类型, 用子类对象来替换父类对象。 The Interface Segregation Principle（接口分离原则ISP）该原则比较好理解1.不要定义过于臃肿的接口, 接口中不要有很多不相关的逻辑方法(否则一定也违背单一职责原则); 2.过于臃肿的接口可能会强迫用户去实现接口内部用户并不需要的方法。换句话说, 使用 多个专门的接口 比使用 一个臃肿的总接口 要好很多; 如果你在类中实现了你不需要使用的接口方法, 估计也是重写为空方法, 这其实已经违背了接口分离原则。 3.也就是说，一个接口或者类应该拥有尽可能少的行为, 就是少到恰好能完成它自身的职责, 这也是保证 “软件系统模块的粒度尽可能少, 以达到高度可重用的目的”; The Dependency Inversion Principle（依赖反转原则DIP）要针对接口编程, 而不是针对实现编程1.如果说开闭原则是面向对象设计的目标的话, 那么依赖倒转原则就是面向对象设计的主要实现机制之一, 它是系统抽象化的具体实现; 2.上层不用去定义自己要依赖哪个具体的类, 而是定义自己依赖哪个 抽象; 然后让底层代码根据上层的要求, 去实现相应的 抽象; 这样就变成了底层对上层的依赖, 底层代码需要去 实现 上层代码定义的抽象; 3.在实现依赖倒转原则时, 我们需要针对抽象层编程，将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象发生依赖关系时，通过抽象来注入所依赖的对象。常用的注入方式有三种，分别是：构造注入，设值注入（Setter注入）和 接口注入。 构造注入是指通过构造函数来传入具体类的对象 设值注入是指通过Setter方法来传入具体类的对象 而接口注入是指通过在接口中声明的业务方法来传入具体类的对象这些方法在定义时使用的是抽象类型, 在运行时再传入具体类型的对象, 由子类对象来覆盖父类对象。(这里也体现了里氏替换原则LSP) 4.有必要用例子来简单说明一下由于CustomerDAO针对具体数据转换类编程, 因此在增加新的数据转换类或者更换数据转换类时都不得不修改CustomerDAO的源代码。我们可以通过引入抽象数据转换类解决该问题，在引入抽象数据转换类DataConvertor之后，CustomerDAO针对抽象类DataConvertor编程，符合依赖倒转原则。根据里氏代换原则，程序运行时，具体的数据转换类对象 将替换DataConvertor类型的对象，程序不会出现任何问题。更换具体数据转换类时无须修改源代码，只需要说明你需要哪个具体的类(可以在配置文件中配置)。如果需要增加新的具体数据转换类，只要将新增数据转换类作为DataConvertor的子类即可，原有代码无须做任何修改，满足开闭原则。重构后的结构如图2所示： 5.在上述重构过程中, 我们使用了 开闭原则、里氏代换原则 和 依赖倒转原则 , 在大多数情况下, 这三个设计原则会同时出现, 开闭原则是目标, 里氏代换原则是基础, 依赖倒转原则是手段, 它们相辅相成, 相互补充, 目标一致, 只是分析问题时所站角度不同而已。参考: http://blog.csdn.net/lovelion/article/details/7562783 小结开闭原则是目标, 里氏代换原则 和 依赖倒转原则 都是为了实现开闭原则! 参考参考","categories":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://blog.renyimin.com/tags/OOP/"}]}]}