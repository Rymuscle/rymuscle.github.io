{"meta":{"title":"Lant's Blog","subtitle":null,"description":null,"author":"Lant","url":"http://blog.renyimin.com"},"pages":[{"title":"分类","date":"2017-09-17T02:40:28.000Z","updated":"2017-09-18T09:08:09.000Z","comments":false,"path":"categories/index.html","permalink":"http://blog.renyimin.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-09-17T02:40:21.000Z","updated":"2017-09-18T09:08:03.000Z","comments":false,"path":"tags/index.html","permalink":"http://blog.renyimin.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"32. 单机部署集群 -- 普通集群","slug":"rabbitmq/2018-06-26-rabbitmq-32","date":"2018-06-26T07:28:16.000Z","updated":"2018-07-20T07:37:32.000Z","comments":true,"path":"2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","link":"","permalink":"http://blog.renyimin.com/2018/06/26/rabbitmq/2018-06-26-rabbitmq-32/","excerpt":"","text":"前言当你需要在生产环境中部署RabbitMQ时, 需要注意的是, 单实例在生产环境虽然部署起来很容易, 但是当你的rabbitmq服务器遇到内存崩溃或者断电的情况时, 这款高性能的产品就要成为你的耻辱了, 将会为你造成极大的问题!因此你需要将你的RabbitMQ变成高可用的才行; 内建集群简介 RabbitMQ最优秀的功能之一就是其内建集群, 这款消息队列中间件产品本身是基于Erlang编写, Erlang语言天生具备分布式特性(通过同步Erlang集群各节点的magic cookie来实现), 因此, RabbitMQ天然支持Clustering, 这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。 RabbitMQ内建集群用来完成两个目标: 允许生产者和消费者在RabbitMQ节点崩溃的情况下继续运行;你可以失去一个RabbitMQ节点, 同时客户端可以重新连接到集群中的任何其他节点并继续生产或者消费消息, 就像什么都没有发生一样; 通过增加更多的节点来线性扩展消息吞吐量;如果RabbitMQ正疲于应对庞大的消息通信量的话, 那么线性地增加更多的节点则会增加更多性能; 集群的类型Rabbit集群模式大概分为两种: 普通模式、镜像模式; 本篇主要介绍普通模式 普通模式 普通模式(也就是默认的集群模式), 对于该集群模式, 当你将多个节点组合成集群后, 需要注意的是: 不是每一个节点都有所有队列的完全拷贝 在非集群的单一节点中, 所有关于队列的信息(元数据、状态、内容)都完全存储在该节点上; 但是如果在普通集群模式下创建队列的话, 集群只会在当前节点而不是所有节点上创建完整的队列信息(元数据、状态、内容); 而其他非所有者的节点, 只知道队列的元数据和指向该队列存在的哪个节点的指针; 因此当集群中队列所有者的节点崩溃时, 该节点的队列和关联的绑定就都消失了, 并且附加在这些队列上的消费者就会无法获取其订阅的信息, 并且生产者也无法将匹配该队列绑定信息的消息发送到队列中; 接下来需要了解的一个问题是: 为什么在默认的集群模式下, RabbitMQ不将队列内容和状态复制到所有的节点上? 其实有两个原因 存储空间: 如果每个集群节点都拥有所有Queue的完全数据拷贝, 那么每个节点的存储空间会非常大, 集群的消息积压能力会非常弱(无法通过集群节点的扩容提高消息积压能力); 性能: 消息的发布者需要将消息复制到每一个集群节点, 对于持久化消息来说, 网络和磁盘的负载都会明显增加, 最终只能保持集群性能平稳(甚至更糟); 所以, 通过设置集群中的唯一节点来负责特定队列, 只有该负责节点才会因队列消息而遭受磁盘活动的影响所有其他节点需要将接受到的该队列的消息传递给该队列的所有者节点, 因此, 往RabbitMQ集群添加更多的节点意味着你将拥有更多的节点来传播队列, 这些新增节点为你带来了性能的提升; 但是有人可能会想: 是否可以让消费者重新连接到集群上, 这样不就可以重新创建队列了? 但需要注意的是: 因为一般如果我们的队列设置的是持久化的, 而在该队列的主节点挂掉之后, 重新连接到队列时, 一般也不会修改队列的持久化属性; 这就需要注意一个问题, 仅当你之前创建的队列为非持久化时, 你才可以重新创建该队列为持久化, 因为这是为了保证你之前的持久化队列节点在重新被恢复启动后, 其中的消息还会被恢复, 而如果你创建一个新的持久化队列, 如果覆盖之前的持久化队列, 那消息不就丢了!!所以如果之前是持久化队列, 而且还是以持久化的方式创建该队列, 集群就会报错误, 后面会进行测试! 了解内部元数据RabbitMQ内部会始终同步四种类型的内部元数据: 队列元数据: 队列名称和它的属性 (是否可持久化, 是否自动删除); 交换器元数据: 交换器名称、类型和属性 (可持久化等); 绑定元数据: 一张简单的表格展示了如何将消息路由到队列; vhost元数据: 为vhost内的队列、交换器和绑定提供命名空间和安全属性; 内存or磁盘节点 每个Rabbitmq节点, 不管是单一节点系统或者是庞大集群的一部分, 要么是内存节点(RAM node), 要么是磁盘节点(disk node): 内存节点将所有的队列、交换器、绑定、用户、权限和vhost的元数据定义都仅存储在内存中; 而磁盘节点则将元数据存储在磁盘中; 非集群单一节点: 在单一节点的非集群环境中, RabbitMQ默认会将元数据都存放在内存中; 但是, 会将标记为可持久化的队列和交换器(以及它们的绑定)存储到硬盘上, 存储到硬盘上可以确保队列和交换器在重启Rabbitmq节点后重新被创建; 集群节点类型 当你引入Rabbitmq集群后, RabbitMQ需要追踪的元数据类型包括: 集群节点位置, 以及节点与已记录的其他类型的元数据的关系; 集群对元数据的存储提供了选择:将元数据存储到磁盘上 (集群中创建节点时的默认设置) 或者 存储到RAM内存中 注意, RabbitMQ要求在集群中至少要有一个磁盘节点, 所有其他节点可以是内存节点。当节点加入或者离开集群时, 它们必须要将变更至少通知到一个磁盘节点; 如果只有一个磁盘节点, 而不凑巧的是它有刚好崩溃, 那么集群虽然可以继续路由消息, 但是不能做一下操作: 创建队列 创建交换器 创建绑定 添加用户 更改权限 添加或删除集群节点 本机配置集群 在开始配置集群前, 首先要确保现存的Rabbitmq没有运行, 因此需要关闭节点 (本机为mac, 关闭操作如下) 123renyimindeMacBook-Pro:~ renyimin$ brew services stop rabbitmqStopping `rabbitmq`... (might take a while)==&gt; Successfully stopped `rabbitmq` (label: homebrew.mxcl.rabbitmq) 可以发现一个问题, 就是停止Rabbitmq服务之后, 貌似 RabbitMQ Management 的Web UI界面还是可以正常打开运行; 所以正确的关闭节点貌似是 rabbitmqctl stop 开始配置集群前需要注意: 通常来讲, 使用 rabbitmq-server 命令启动节点之后就大功告成了, 但是如果不用额外参数的话, 该命令会使用默认的节点名称 rabbit 和监听端口 5672;所以如果你想用该命令在一台机器上同时启动5个节点的话, 那么第2，3，4，5个节点都会因为节点名称和端口号冲突而导致启动失败; 因此, 为了在本机正常启动5个节点, 可以在每次调用 rabbitmq-server前, 通过设置环境变量 RABBITMQ_NODENAME, RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口号!在此处做实验时, 将会采用 rabbit, rabbit_1,…,4 命名节点名; 端口号为5612，5613，…5615 注意, 到目前为止, 虽然尚未谈论RabbitMQ的插件, 不过你有可能已经启用了一部分插件了; 如果确实如此的话, 你需要在启动集群节点前将插件禁用!这是因为像 RabbitMQ Management 这样的插件会监听专门的端口来提供服务(例如 Management 插件的 Web UI), 目前还没讲到如何设置插件监听不同的端口, 所以当第二个节点和之后的节点启动了它们的插件后, 就会和第一个启动节点的c插件相冲突, 然后节点就都崩溃了;可以先不禁用插件, 这样在启动多个节点时, 可以根据报错一个个关闭插件也可以; (rabbitmq-plugins disable 插件名) RabbitMQ集群的搭建 启动节点 注意: 启动的时候, 直接加上 -detached 参数的话, 可能会有些报错信息比如 error : cannot_delete_plugins_expand_dir, 这就是因为需要使用root权限才可以, 你可以使用 pa aux | grep rabbitmq 查看是否三个进程都成功启动了 注意: 启动时, 貌似不能像书上那样, RABBITMQ_NODENAME 只设置节点名, 最好设置上节点host 如下: 1234567renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit_1@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ sudo RABBITMQ_NODE_PORT=5674 RABBITMQ_NODENAME=rabbit_2@localhost rabbitmq-server -detachedWarning: PID file not written; -detached was passed.renyimindeMacBook-Pro:~ renyimin$ 然后可以查看个节点状态 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost statusrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost status 现在启动了三个节点 rabbit, rabbit_1, rabbit_2, 并且每个节点都会有系统的主机名在@后; 但是每个节点仍然是独立节点, 拥有自己的元数据, 并且不知道其他节点的存在; 集群中的第一个节点rabbit,将初始元数据带入集群, 并且无需被告知加入; 而第二个和之后的节点, 将加入第一个节点rabbit, 并获取rabbit节点的元数据; 要将rabbit_1和rabbit_2节点加入rabbit, 要停止该Erlang节点上运行的rabbitmq应用程序, 并重设它们的元数据, 这样它们才可以被加入rabbit节点并且获取rabbit节点的元数据; 可以使用 rabbitmqctl 来完成这些工作 停止rabbit_1节点上的应用程序 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost stop_appStopping rabbit application on node rabbit_1@renyimindeMacBook-Pro ... 重设rabbit_1节点的元数据和状态为清空状态 12renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost resetResetting node rabbit_1@renyimindeMacBook-Pro ... 这样你就准备好了一个 停止运行的并且清空了的 rabbit 应用, 现在可以准备好将其加入到集群中的第一个节点rabbit中:注意书上的 cluster 命令好像已经不用了, 换成了 join_cluster 123renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost join_cluster rabbit@localhostClustering node rabbit_1@localhost with rabbit@localhostrenyimindeMacBook-Pro:~ renyimin$ 最后, 可以重启第二个节点的应用程序 1234renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_appStarting node rabbit_1@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ 节点rabbit_2加入集群的步骤同上, 具体操作如下: 12345678910111213renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost start_appStarting node rabbit_1@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_appStopping rabbit application on node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost resetResetting node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost join_cluster rabbit@localhostClustering node rabbit_2@localhost with rabbit@localhostrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_appStarting node rabbit_2@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ 查看集群状态, 可以在任意一个节点通过 rabbitmqctl cluster_status 进行查看 123456789101112131415161718192021222324252627282930renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_statusCluster status of node rabbit@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_statusCluster status of node rabbit_1@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost cluster_statusCluster status of node rabbit_2@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit_2@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ 注意: 上面使用比较多的 rabbitmqctl 命令的关键参数是 -n, 这会告诉rabbitmqctl命令, 你想在指定节点而非默认节点rabbit@上执行命令; 记住, Erlang节点间通过Erlang cookie的方式来允许互相通信。因为rabbitmqctl使用Erlang OPT通信机制来和Rabbit节点通信, 运行rabbitmqctl的机器和所要连接的Rabbit节点必须使用相同的Erlang cookie, 否则你会得到一个错误;当然, 上面的集群是在本机做伪集群, Erlang cookie 自然也都是一致的! 将节点从集群中删除 forget_cluster_node 1234567renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_1@localhostRemoving node rabbit_1@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_2@localhostRemoving node rabbit_2@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl forget_cluster_node rabbit_3@localhostRemoving node rabbit_3@localhost from the clusterrenyimindeMacBook-Pro:~ renyimin$ 集群节点类型设置与修改 可以在将节点加入集群时, 设定节点的类型 (参考) 比如 rabbitmqctl -n rabbit_3@localhost join_cluster --ram rabbit@localhost 之前已经通过 rabbitmqctl cluster_status 查看了集群的状态, 里面比较重要的是 nodes 部分 下面告诉你有三个节点加入了集群, 并且三个节点都是 disc 磁盘节点! 1234567[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit_2@localhost,[]&#125;]&#125;] running_nodes 部分告诉你集群中的哪些节点正在运行; 现在你可以连接到这三个running_nodes中的任何一个, 并且开始创建队列, 发布消息或者执行任何其他AMQP任务; 你也可以对节点类型进行修改, 如下将rabbit_2节点类型修改为内存节点 (注意: 修改节点类型, 需要先停止节点应用) 1234567891011121314151617181920renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost stop_appStopping rabbit application on node rabbit_2@localhost ...renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost change_cluster_node_type ramTurning rabbit_2@localhost into a ram noderenyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost start_appStarting node rabbit_2@localhost ... completed with 1 plugins.renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost cluster_statusCluster status of node rabbit_1@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost]&#125;, &#123;ram,[rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit@localhost,rabbit_1@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindeMacBook-Pro&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;]&#125;]renyimindeMacBook-Pro:~ renyimin$ 测试 运行生产者代码, 在集群中的rabbit节点中创建持久化队列 初始集群状态123456789renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl cluster_statusCluster status of node rabbit@localhost ...[&#123;nodes,[&#123;disc,[rabbit@localhost,rabbit_1@localhost,rabbit_2@localhost]&#125;]&#125;, &#123;running_nodes,[rabbit_2@localhost,rabbit_1@localhost,rabbit@localhost]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit@renyimindemacbook-pro.rrcoa.com&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit_2@localhost,[]&#125;, &#123;rabbit_1@localhost,[]&#125;, &#123;rabbit@localhost,[]&#125;]&#125;] - 运行生产者, 查看创建的队列(已经有一条msg放入队列中) 123456789101112131415161718192021renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...prefetchCountQueue 0localClusterQueue 1renyimindeMacBook-Pro:~ renyimin$ kill掉该持久化队列localClusterQueue所在的主节点rabbit 查看节点进程 12345renyimindeMacBook-Pro:~ renyimin$ ps aux | grep rabbitmqroot 2656 0.4 0.3 4150148 58156 ?? S 三01下午 5:09.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5672&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit@localhost&quot; -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672 -noshell -noinputrenyimin 28537 0.0 0.0 2423384 232 s007 R+ 3:12下午 0:00.00 grep rabbitmqroot 72516 0.0 0.5 4143168 79400 ?? S 1:03下午 0:16.71 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_2@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5674&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_2@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_2@localhost&quot; -kernel inet_dist_listen_min 25674 -kernel inet_dist_listen_max 25674 -noshell -noinputroot 71841 0.0 0.5 4138448 77104 ?? S 1:01下午 0:15.15 /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang/erts-9.3.3.1/bin/beam.smp -W w -A 64 -MBas ageffcbf -MHas ageffcbf -MBlmbcs 512 -MHlmbcs 512 -MMmcs 30 -P 1048576 -t 5000000 -stbt db -zdbbl 1280000 -K true -- -root /usr/local/Cellar/erlang@20/20.3.8.2/lib/erlang -progname erl -- -home /Users/renyimin -- -pa /usr/local/Cellar/rabbitmq/3.7.7/ebin -noshell -noinput -s rabbit boot -sname rabbit_1@localhost -boot /usr/local/opt/erlang@20/lib/erlang/bin/start_clean -conf /usr/local/Cellar/rabbitmq/3.7.5/etc/rabbitmq/rabbitmq -conf_dir /usr/local/var/lib/rabbitmq/config -conf_script_dir /usr/local/sbin -conf_schema_dir /usr/local/var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] -rabbit tcp_listeners [&#123;&quot;127.0.0.1&quot;,5673&#125;] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/usr/local/var/log/rabbitmq&quot; -rabbit lager_default_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost.log&quot; -rabbit lager_upgrade_file &quot;/usr/local/var/log/rabbitmq/rabbit_1@localhost_upgrade.log&quot; -rabbit enabled_plugins_file &quot;/usr/local/etc/rabbitmq/enabled_plugins&quot; -rabbit plugins_dir &quot;/usr/local/Cellar/rabbitmq/3.7.7/plugins&quot; -rabbit plugins_expand_dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost-plugins-expand&quot; -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/usr/local/var/lib/rabbitmq/mnesia/rabbit_1@localhost&quot; -kernel inet_dist_listen_min 25673 -kernel inet_dist_listen_max 25673 -noshell -noinput sudo kill 2656 将生产者改连 rabbit_1 节点, 重新运行生产者 报错: 挂掉的主节点中已存在该持久化队列, 如果在主节点挂掉后, 你能直接连接其他节点创建该队列的话, 此时创建的是个新队列, 要知道, 宕机的主节点中的持久化队列还在等待恢复呢, 它内部可能让然有很多msg需要恢复并被处理;所以Rabbit集群的这个问题是有原因的!! 可以重新启动该节点 sudo RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit@localhost rabbitmq-server -detached 会发现之前的持久化队列会被恢复123456789101112131415161718renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_1@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 1prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$ 此时即使生产者连接着 rabbit_1 也可以创建该同名持久化队列了 重新运行刚才连接到 rabbit_1 的生产者, 不会报错了, 而是正确往队列发布了一条消息123456renyimindeMacBook-Pro:~ renyimin$ rabbitmqctl -n rabbit_2@localhost list_queuesTimeout: 60.0 seconds ...Listing queues for vhost / ...localClusterQueue 2prefetchCountQueue 0renyimindeMacBook-Pro:~ renyimin$","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"20. 消费者预取 Consumer Prefetch","slug":"rabbitmq/2018-06-13-rabbitmq-20","date":"2018-06-13T11:23:36.000Z","updated":"2018-07-19T02:06:14.000Z","comments":true,"path":"2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","link":"","permalink":"http://blog.renyimin.com/2018/06/13/rabbitmq/2018-06-13-rabbitmq-20/","excerpt":"","text":"Consumer Prefetch 作为限制 unacked 消息数量的更自然有效的方法; AMQP 0-9-1 指定了 basic.qos 方法, 以便你在消费者进行消费时, 可以限制channel(或connection)上未确认消息的数量; 但是值得注意的是: channel 并不是理想的设定范围, 因为单个channel可能从多个队列进行消费, channel和queue需要为每个发送的消息相互协调, 以确保它们不会超出限制, 这在单台机器上会慢, 而在整个集群中使用时会非常慢; 此外, 对于许多用途, 指定适用于每个消费者的预取计数更会简单一些; 因此, RabbitMQ在 basic.qos 方法中重新定义了全局标志的含义 (在php-amqplib中basic_qos()的第三个参数a_global): 请注意, 在大多数API中, 全局标志的默认值为false; (php-amqplib的basic_qos()方法的第三个参数a_global默认也为false) 简要分析 在使用RabbitMQ时, 如果完全不配置QoS, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是采用默认方式, 将队列中的所有消息按照网络和客户端允许的速度尽快轮发到与队列绑定的consumers端; 而consumers会在本地缓存所有投递过来的messages, 这样的话, 就可能会导致 如果某个消费者的业务逻辑处理比较复杂(将会在较长时间之后才会操作完成并进行ack), 这也就导致消费慢的Consumer将会在本地堆积很多消息, 从而导致内存不足或者对其他进程造成影响 (消费者可能被撑到假死); 而其他消费能力强的Consumers, 可能已经很快地消费完成处于闲置状态, 从而造成资源浪费; 同时, 新启的消费者也无法分担已经被之前消费者缓存到其本地的消息, 所以此时即便启动更多消费者, 也无力缓解大量的 unacked 消息积压, 让你产生疑惑; 而当你设置了Qos之后, RabbitMQ虽然也是将队列中的消息尽快轮发到Consumers中, 但是因为消费者具有的 prefetch_count 消息预取值上限, 所以RabbitMQ在轮发消息的时候, 如果发现消费者的 unacked 消息达到了 prefetch_count 的值, 即使rabbitmq中有很多ready的就绪消息, 也不会给该Consumer继续投递消息了(只有消费者的 unacked 消息小于prefetch_count的值时, 才会继续通过轮发方式给该consumer投递ready消息), 如果此时有新的消费者加入, 它也将会拿到未投递出去的ready消息! 可以通过启动 prefetchCountConsumer1，prefetchCountConsumer2 两个消费者(prefetch_count 均为10), 然后使用下面测试中的生产者发送100条消息, 前期观察会发现队列中消息的最大 unacked 为20, 并且你会发现队列中处于ready状态的消息会每次2个的递减, 这就预示着, 每次这两个消费者只要 unacked 的消息书小于prefetch_count(10), Rabbitmq才会给这两个consumer各自发送一条msg; 之后如果启动了 prefetchCountConsumer3(prefetch_count为20), 此时会发现队列中消息的最大 unacked 会为40, prefetchCountConsumer3的加入会使得队列中处于ready状态的消息直接骤减20个, 最后rabbitmq中的ready消息已经为0, 每个消费者还在继续消费各自未 unacked 的消息, 最终消费完成后, 整个队列中的 unacked 消息为0; Qos的设置只有在开启手动ack后才会生效 (即, prefetch_count 在 no_ask=false 的情况下生效) 测试 一般情况下, 同一队列绑定的多个消费者都是处理同一个业务, 而且如果在同一台机器启动, 消费能力应该都差不多, 但也难免出现如: 消费者资源分配不均 或者 两个消费者在处理业务时所请求的服务端机器配置有差异(假设SLB后又2台配置不均的机器), 这种情况还是应该考虑进来的! 本测试比较简单, 主要测试在默认不设置Qos的情况下, 两个消费能力不同的消费者在处理消息时存在的问题之一: 由于这种情况下, RabbitMQ是不会考虑到Consumers端是否ack的情况, 而是只顾自己轮发消息, 这样就会导致消息被轮发完成后, 消费能力高的消费者可能很快消费完消息并处于闲置状态, 而消费能力低的消费者却在很慢地进行消费, 这样就造成了资源的浪费; 准备 创建消费者1 ‘qosCustomer1’ (简单打印消息内容) , 代码参考, 启动消费者 php artisan qosConsumer1 创建消费者2 ‘qosCustomer2’ (sleep 5秒, 模拟处理能力比较差) , 代码参考, 启动消费者 php artisan qosConsumer2 创建生产者一次向队列 ‘qosQueue’ 中推送10条消息 , 代码参考, 请求一次生产者 http://www.rabbit.com/testQos 注意需要先启动消费者, 再请求生产者; (如果先请求了生产者, 可能在启动第一个消费者之后, 其会迅速消费完10条消息, 这样就无法模拟效果了) 测试发现 qosCustomer1 : 迅速打印出结果(1,3,5,7,9), 然后就处于闲置状态了 qosCustomer2 : 还在缓慢打印(2,4,6,8,10) 可以看到, 如果不设置Qos, Rabbitmq会尽快将消息从队列中轮发投递出去, 不会对消费者的消费能力进行任何评估! 所以: 为了避免这种浪费资源的情况, 你可能就需要根据上一篇讲解的 prefetch_count 来针对不同消费者进行设置; 问题答疑测试 根据上面的描述, 有个疑问: 在默认不设置Qos的情况下, 既然生产者发布的消息会尽可能全部推送给消费者进程, 队列中会尽可能将消息全部推出, 缓存在消费者本地, 那当消费者断开时, 消息是如何恢复到队列中的? 或者不会恢复到队列中? 为了答疑, 下面进行测试 准备测试代码 创建消费者1 ‘prefetchCountConsumer1’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 创建消费者2 ‘prefetchCountConsumer2’ (sleep 5秒, 模拟耗时业务需求; prefetch=100; 简单打印消息内容), 代码参考 生产者一次向队列 ‘prefetchCountQueue’ 中推送100条消息 , 代码参考 测试: 在生产者请求一次之后(http://www.rabbit.com/prefetchCount), ready : 100, unacked: 0, total : 100, 表示队列中已经有100条消息已经就绪, 等待发出 运行第一个php artisan prefetchCountConsumer1之后, ready : 0, unacked : 100, total : 100 (也就是说, queue中已经没有 ready状态, 即准备好待发送的消息了, 消息都传递给消费者1了) 随着消费者的缓慢消费, ready : 0, unacked : 94, total : 94 () 如果模拟 挂掉第一个消费者之后, 会发现, ready : 83， unacked : 0, total : 83 (也就是说消费者意外宕掉之后, 队列中的消息会重新处于就绪状态, 等待着新的消费者来消费) 再次启动消费者2 php artisan testQosConsumerPrefetchCount2之后, ready : 0, unacked : 80, total : 80 (消息又会被全量发送给消费者2) 注意: 如果此时启动消费者1, 你会发现, 它是无法帮助消费者2进行消费的, 因为消息都在消费者2的本地, 所以队列中并没有 ready状态的就绪消息; 测试注意: 上述测试过程如果先启动两个消费者, 然后再发布消息进行测试, 你会发现, 由于两个消费者都设置了预取值, 而且相等, 所以消息仍然会快速轮发给这两个消费者; 如果将两个消费者的 prefetch_count 都设置为10, 那么你会发现, unacked 最多也就是两个消费者的prefetch_count和, 即20个 小结 消费者的 unacked 消息数量如果未达到Qos设置的 prefetch_count 量, Rabbit不会顾及消费者的消费能力, 会尽可能将queue中的消息全部推送出去给消费者; 因此, 当你发现消费者消费缓慢, 产生大量 unacked 消息时, 即便增加新的消费者, 也无法帮助之前的消费者分担消息(除非消费者1的 unacked 达到了 prefetch_count 限制), 只能分担队列中处于 ready 状态的消息; 除非你断开之前的消费者, 然后启动一个新的消费者, 消费者中积压的消息才会重新放入队列中 (因为之前的消费者挂掉之后, 其处理后的剩余消息在 queue中会恢复为 ready 状态) 但是注意: 新启动的这个消费者如果设置额prefetch_count不合理的话, 假设与之前消费者的 预取值 设置一样大, 它很快也会产生大量 unacked 消息 所以, 在新启消费者的时候, 需要设计好 prefetch_count 的大小, 然后可以启动多个消费者来共同进行消费; 扩展 rabbitmq对 basic.qos 信令的处理 首先, basic.qos 是针对 channel 进行设置的, 也就是说只有在channel建立之后才能发送basic.qos信令; RabbitMQ只支持通道级的预取计数, 而不是connection级的 或者 基于大小的预取;预取 在rabbitmq的实现中, 每个channel都对应会有一个rabbit_limiter进程, 当收到basic.qos信令后, 在rabbit_limiter进程中记录信令中prefetch_count的值, 同时记录的还有该channel未ack的消息个数; 在php-amqplib中, 可以使用 channel 的 basic_qos() 方法来进行控制, basic_qos() 有三个参数: prefetch_size : 限制预取的消息大小的参数, rabbitmq暂时没有实现 (如果prefetch_size字段不是默认值0, 则会通知客户端出错, 通知客户端RabbitMQ系统没有实现该参数的功能, 还可以参考此文)当你设置prefetch_size大于0的时候, 会出现如下报错 prefetch_count : 预取消息数量 global: 在3.3.0版本中对global这个参数的含义进行了重新定义, 即glotal=true时表示在当前channel上所有的consumer都生效(包括已有的), 否则只对设置了之后新建的consumer生效;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"08. 事务 VS Publisher Confirms(发布者确认机制)","slug":"rabbitmq/2018-06-05-rabbitmq-08","date":"2018-06-05T11:20:56.000Z","updated":"2018-07-20T06:00:07.000Z","comments":true,"path":"2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","link":"","permalink":"http://blog.renyimin.com/2018/06/05/rabbitmq/2018-06-05-rabbitmq-08/","excerpt":"","text":"问题的出现 和消息持久化相关的一个概念是 AMQP 的事务(transaction)机制; 到目前为止, 我们讨论的是将 消息, 队列 和 交换器 设置为持久化; 这一切都工作的很好, 并且RabbitMQ也负责保证消息的安全, 但是由于 发布消息的操作并不会反回任何信息给生产者, 所以你也无法得知是否消息已经到达了服务器并且服务器是否已经将消息持久化到了硬盘; 服务器可能会在把消息写入到硬盘前就宕机了, 或者消息压根就还没有发送到服务器, 服务器就宕机了, 消息会因此而丢失, 而你却不知道; 另外, 你可能是发送多条消息, 如果部分发送成功, 部分失败呢? 这你也无法得知; 事务机制 为了确保消息能够被安全发布到Broker, 如果使用标准的AMQP 0-9-1, 保证消息不会丢失的唯一方法是使用 事务机制 (将channel事务化) php-amqplib 中与事务机制有关的方法有三个, 分别是Channel里面的 txSelect(), txCommit() 以及 txRollback(); txSelect(): 用于将当前Channel设置成是transaction模式 txCommit(): 用于提交事务 txRollback(): 用于回滚事务 但是值得注意的是事务存在的问题: AMQP 0-9-1 中的事务几乎吸干了RabbitMQ的性能, 会导致事务吞吐量严重下降; 事务会使得生产者应用程序变成同步的, 而你使用消息通信就是为了避免同步; 鉴于上面的问题, 你可能不会在生产中使用事务机制, 此处只做了个简单的事务测试, 测试代码 Publisher Confirms 既然事务存在的问题让你拒绝使用它, 但是确保消息被成功投递到服务器这个问题仍需要解决; 为了避免事务机制在解决问题时导致的新问题, RabbitMQ团队拿出了更好的方案来保证消息的投递: 发送方确认模式 它模仿协议中已经存在的 消费者确认机制 要启用这个确认机制，客户端可以通过使用 channel 的 confirm.select 方法 如果设置了 confirm.select 方法的 no-wait, 代理会用 confirm.select-ok 进行响应, 不过这点你貌似也只能通过抓包来观察: 这里说的 confirm.select-ok 是代理对发布者的响应信息 (和 php-amqplib包中的 confirm_select_ok() 方法可不是一个意思, 而且php-amqplib也没对confirm_select_ok做实现) 上面也提到了, 该确认机制是模仿已经存在的 消费者确认机制, 所以, Broker也会使用类似 ack, nack 来响应Publisher: 可以通过为 set_ack_handler , set_nack_handler 设置回调, 来监测消息是否成功到达服务器, 成功则会触发 set_ack_handler, 失败则会触发 set_nack_handler 只有在负责队列的Erlang进程中发生内部错误时才会回应nack, 所以这个在测试中也一直没有使用 set_nack_handler 捕获到错误 (是对于nack的消息, 可以设置进行重发); 注意: 这两监听函数是监听服务器对 publisher 的应答的, 可不是监听 consumer 对服务器的应答的; 一旦在channel上使用 confirm.select 方法, 就说 channel 处于 确认模式, 一旦通道处于确认模式, 就不能进行事务处理; 也就是说 事务 和 Publisher Confirm 不能同时使用; 一旦通道处于确认模式, 代理和客户端都会对消息进行计数(在第一次confirm.select时从1开始计数), 然后, broker通过在相同channel上发送 basic.ack 来处理它们, 从而确认消息; delivery-tag 字段包含确认消息的序列号;最大 Delivery Tag, 递送标签是一个64位长的值，因此其最大值为9223372036854775807.由于递送标签的范围是按每个通道划分的，因此发布商或消费者在实践中不太可能运行该值 Publisher Confirms 的顺序考虑 在大多数情况下, RabbitMQ将按发布顺序向publisher确认消息(这适用于在单个频道上发布的消息); 但是, 发布者确认是异步发出的, 并且可以确认一条消息或一组消息;由于消息确认可以以不同的顺序到达, 所以, 应用程序应尽可能不取决于确认的顺序; 发布者确认存在的问题 mandatory 属性问题 测试代码publisher confirm 不需要消费者参与, 代码参考 Qos预习 针对Qos的提前预习(译文) 信道预取设置(QoS)由于消息是异步发送(推送)给客户端的, 因此在任何给定时刻通常都有不止一条消息在信道上运行; 此外, 客户的手动确认本质上也是异步的, 所以有一个 未确认的交付标签的滑动窗口, 开发人员通常会倾向于限制此窗口的大小, 以避免消费者端无限制的缓冲区问题。这是通过使用 basic.qos 方法设置 预取计数 值完成的, 该值定义了channel上允许的最大未确认递送数量, 一旦数字达到配置的计数, RabbitMQ将停止在通道上传送更多消息, 除非至少有一个未确认的消息被确认;例如, 假设在通道 “Ch” 上有未确认的交付标签5,6,7和8, 并且通道 “Ch” 的预取计数(后面会学到是prefetch_count)设置为4, 则RabbitMQ将不会在 “Ch” 上推送更多交付, 除非至少有一个未完成的交付被确认(当确认帧在 delivery_tag=8 的频道上到达时, RabbitMQ将会注意到并再发送一条消息) QoS预取设置对使用 basic.get(pull API) 获取的消息没有影响, 即使在手动确认模式下也是如此; 消费者确认模式, 预取和吞吐量(译文) 确认模式 和 QoS预取值 对消费者吞吐量有显着影响, 一般来说, 增加预取值将提高向消费者传递消息的速度, 当然, 自动确认模式可以产生最佳的传送速率 但是, 在上面两种情况下, 尚未完成交付处理的消息(unacked)数量也会增加, 从而增加消费者RAM消耗; 自动确认模式或带无限预取的手动确认模式应谨慎使用, 消费者在没有确认的情况下消耗大量消息将导致其所连接的节点上的内存消耗增长; 预取值1是最保守的, 但这将显着降低吞吐量, 特别是在消费者连接延迟较高的环境中, 对于许多应用来说, 更高的价值是合适和最佳的; 100到300范围内的Qos(prefetch_count)预取值通常提供最佳的吞吐量, 并且不会面临压垮consumer的重大风险, 而更高的值往往会遇到效率递减的规律;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]},{"title":"06. 持久化策略","slug":"rabbitmq/2018-05-28-rabbitmq-06","date":"2018-05-28T09:32:11.000Z","updated":"2018-07-20T05:53:53.000Z","comments":true,"path":"2018/05/28/rabbitmq/2018-05-28-rabbitmq-06/","link":"","permalink":"http://blog.renyimin.com/2018/05/28/rabbitmq/2018-05-28-rabbitmq-06/","excerpt":"","text":"持久化原理 RabbitMQ 默认情况下, Exchange, 队列, 消息 都是非持久的, 这意味着一旦消息服务器重启, 所有已声明的 Exchange, 队列, 以及 队列中的消息 都会丢失; RabbitMQ确保持久化的消息能在服务器重启之后恢复的方式是, 将它们写入磁盘上的一个持久化日志文件。当发布一条持久性消息到一个持久交换机上时, Rabbit会在消息提交到日志文件中之后才发送响应; 还需要注意的是, 如果之后这条消息被路由到一个非持久化队列, 则消息又会从上面的日志文件中删除, 并且无法从服务器重启中恢复; 一旦你从持久化队列中消费了一条持久性消息(并且进行了确认), RabbitMQ会在持久化日志中把这条消息标记为等待垃圾收集; 持久化方案 要做到消息持久化, 必须保证如下三点设置正确: exchange交换器: durable属性为true; queue队列: durable属性为true; 除了上述两点之外, 还需要在投递消息时候, 设置message的 delivery_mode 模式为2来标识消息为持久化消息; 另外: 一个包含持久化消息的非持久化队列, 在Rabbit Server重启之后, 该队列将会不复存在, 消息就会变成孤儿; 具体代码 持久化的问题 持久化由于会写磁盘, 所以会极大降低RabbitMQ每秒处理的消息总数, 降低吞吐量; 持久化在Rabbit内建集群环境下工作的并不好, 虽然RabbitMQ集群允许你和集群中的任何节点的任一队列进行通信, 但是如果队列所在的节点崩溃后, 如果队列是持久化的, 那么直到这个节点恢复之前, 这个队列都不会在整个集群中被创建出来; 后面在学习集群时, 会给出相应的解决方案;","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://blog.renyimin.com/tags/RabbitMQ/"}]}]}